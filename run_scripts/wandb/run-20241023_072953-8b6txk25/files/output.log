Not using distributed mode
[07:29:56.071294] job dir: /root/seg_framework/MS-Mamba/run_scripts
[07:29:56.071507] Namespace(accum_iter=1,
model='SegFormer3D',
in_channels=1,
lr=None,
blr=0.001,
min_lr=0,
dist_on_itp=False,
warmup_epochs=20,
device='cuda:0',
seed=42,
layer_decay=0.75,
clip_grad=None,
num_workers=8,
pin_mem=True,
resume='',
mask_mode='concatenate to image',
world_size=1,
embed_dim=1,
local_rank=-1,
dist_url='env://',
nb_classes=2,
data_dir='/root/MSLesSeg24/data',
datalist=None,
preprocess=False,
dim=2,
distributed=False)
[07:29:56.072367] Starting for fold 0
[07:29:56.258751] Elements in data_dir_paths: 11052
[07:29:56.294796] Elements in data_dir_paths: 1803
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[07:29:58.190058] number of params: 59620439
[07:29:58.190306] model: Vivim2D(
  (encoder): mamba_block(
    (downsample_layers): SegformerEncoder(
      (patch_embeddings): ModuleList(
        (0): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (3): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (block): ModuleList(
        (0): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): Identity()
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.003703703870996833)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.007407407741993666)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.011111111380159855)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.014814815483987331)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.018518518656492233)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.02222222276031971)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.025925926864147186)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.029629630967974663)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03333333507180214)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03703703731298447)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04074074327945709)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04444444552063942)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.048148151487112045)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.051851850003004074)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0555555559694767)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.05925925821065903)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06296296417713165)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06666667014360428)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07037036865949631)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07407407462596893)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07777778059244156)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08148147910833359)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08518518507480621)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08888889104127884)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.09259259700775146)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0962962955236435)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.10000000149011612)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (layer_norm): ModuleList(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (2): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (3): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (decoder): SegformerDecodeHead(
    (linear_c): ModuleList(
      (0): SegformerMLP(
        (proj): Linear(in_features=64, out_features=768, bias=True)
      )
      (1): SegformerMLP(
        (proj): Linear(in_features=128, out_features=768, bias=True)
      )
      (2): SegformerMLP(
        (proj): Linear(in_features=320, out_features=768, bias=True)
      )
      (3): SegformerMLP(
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
    )
    (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Conv2d(768, 150, kernel_size=(1, 1), stride=(1, 1))
  )
  (out): Conv2d(768, 1, kernel_size=(1, 1), stride=(1, 1))
)
[07:29:58.193512] base lr: 1.00e-03
[07:29:58.193579] actual lr: 1.25e-04
[07:29:58.193632] accumulate grad iterations: 1
[07:29:58.193683] effective batch size: 32
[07:29:58.195365] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.000125
    maximize: False
    weight_decay: 0.01
)
[07:29:58.197397] Start training for 50 epochs
[07:29:58.197487] Number of samples in train dataloader:  345
[07:29:58.199343] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
[07:30:08.157788] Epoch: [0]  [  0/345]  eta: 0:57:15  lr: 0.000000  loss: 1.7515 (1.7515)  time: 9.9574  data: 0.3243  max mem: 15824
[07:30:20.249435] Epoch: [0]  [ 20/345]  eta: 0:05:41  lr: 0.000000  loss: 1.7538 (1.7551)  time: 0.6045  data: 0.0001  max mem: 15824
[07:30:32.410737] Epoch: [0]  [ 40/345]  eta: 0:04:14  lr: 0.000001  loss: 1.7350 (1.7464)  time: 0.6080  data: 0.0001  max mem: 15824
[07:30:44.630305] Epoch: [0]  [ 60/345]  eta: 0:03:36  lr: 0.000001  loss: 1.7284 (1.7421)  time: 0.6109  data: 0.0001  max mem: 15824
[07:30:56.903323] Epoch: [0]  [ 80/345]  eta: 0:03:12  lr: 0.000001  loss: 1.7253 (1.7384)  time: 0.6136  data: 0.0001  max mem: 15824
[07:31:09.324590] Epoch: [0]  [100/345]  eta: 0:02:52  lr: 0.000002  loss: 1.7088 (1.7327)  time: 0.6210  data: 0.0001  max mem: 15824
[07:31:21.653828] Epoch: [0]  [120/345]  eta: 0:02:35  lr: 0.000002  loss: 1.6909 (1.7256)  time: 0.6164  data: 0.0001  max mem: 15824
[07:31:34.014006] Epoch: [0]  [140/345]  eta: 0:02:19  lr: 0.000003  loss: 1.6640 (1.7173)  time: 0.6180  data: 0.0001  max mem: 15824
[07:31:46.411198] Epoch: [0]  [160/345]  eta: 0:02:04  lr: 0.000003  loss: 1.6348 (1.7075)  time: 0.6198  data: 0.0001  max mem: 15824
[07:31:58.808366] Epoch: [0]  [180/345]  eta: 0:01:49  lr: 0.000003  loss: 1.6049 (1.6961)  time: 0.6198  data: 0.0001  max mem: 15824
[07:32:11.205834] Epoch: [0]  [200/345]  eta: 0:01:35  lr: 0.000004  loss: 1.5569 (1.6824)  time: 0.6198  data: 0.0001  max mem: 15824
[07:32:23.603228] Epoch: [0]  [220/345]  eta: 0:01:22  lr: 0.000004  loss: 1.5029 (1.6664)  time: 0.6198  data: 0.0001  max mem: 15824
[07:32:35.999562] Epoch: [0]  [240/345]  eta: 0:01:08  lr: 0.000004  loss: 1.4345 (1.6476)  time: 0.6198  data: 0.0001  max mem: 15824
[07:32:48.391541] Epoch: [0]  [260/345]  eta: 0:00:55  lr: 0.000005  loss: 1.3631 (1.6261)  time: 0.6196  data: 0.0001  max mem: 15824
[07:33:00.787176] Epoch: [0]  [280/345]  eta: 0:00:42  lr: 0.000005  loss: 1.2998 (1.6028)  time: 0.6197  data: 0.0001  max mem: 15824
[07:33:13.184757] Epoch: [0]  [300/345]  eta: 0:00:29  lr: 0.000005  loss: 1.2449 (1.5790)  time: 0.6198  data: 0.0001  max mem: 15824
[07:33:25.592237] Epoch: [0]  [320/345]  eta: 0:00:16  lr: 0.000006  loss: 1.2013 (1.5557)  time: 0.6203  data: 0.0001  max mem: 15824
[07:33:38.019186] Epoch: [0]  [340/345]  eta: 0:00:03  lr: 0.000006  loss: 1.1660 (1.5328)  time: 0.6213  data: 0.0001  max mem: 15824
[07:33:40.499101] Epoch: [0]  [344/345]  eta: 0:00:00  lr: 0.000006  loss: 1.1616 (1.5284)  time: 0.6209  data: 0.0001  max mem: 15824
[07:33:40.564761] Epoch: [0] Total time: 0:03:42 (0.6445 s / it)
[07:33:40.564953] Averaged stats: lr: 0.000006  loss: 1.1616 (1.5284)
[07:33:41.157366] Test:  [  0/345]  eta: 0:03:22  loss: 1.1552 (1.1552)  time: 0.5874  data: 0.4026  max mem: 15824
[07:33:42.953113] Test:  [ 10/345]  eta: 0:01:12  loss: 1.1546 (1.1537)  time: 0.2166  data: 0.0367  max mem: 15824
[07:33:44.750986] Test:  [ 20/345]  eta: 0:01:04  loss: 1.1547 (1.1542)  time: 0.1796  data: 0.0001  max mem: 15824
[07:33:46.550744] Test:  [ 30/345]  eta: 0:01:00  loss: 1.1550 (1.1547)  time: 0.1798  data: 0.0001  max mem: 15824
[07:33:48.359444] Test:  [ 40/345]  eta: 0:00:57  loss: 1.1554 (1.1546)  time: 0.1804  data: 0.0001  max mem: 15824
[07:33:50.167613] Test:  [ 50/345]  eta: 0:00:55  loss: 1.1544 (1.1546)  time: 0.1808  data: 0.0001  max mem: 15824
[07:33:51.980122] Test:  [ 60/345]  eta: 0:00:53  loss: 1.1544 (1.1545)  time: 0.1810  data: 0.0001  max mem: 15824
[07:33:53.799897] Test:  [ 70/345]  eta: 0:00:51  loss: 1.1529 (1.1544)  time: 0.1816  data: 0.0001  max mem: 15824
[07:33:55.621835] Test:  [ 80/345]  eta: 0:00:49  loss: 1.1525 (1.1543)  time: 0.1820  data: 0.0001  max mem: 15824
[07:33:57.444660] Test:  [ 90/345]  eta: 0:00:47  loss: 1.1532 (1.1542)  time: 0.1822  data: 0.0001  max mem: 15824
[07:33:59.269057] Test:  [100/345]  eta: 0:00:45  loss: 1.1539 (1.1543)  time: 0.1823  data: 0.0001  max mem: 15824
[07:34:01.097796] Test:  [110/345]  eta: 0:00:43  loss: 1.1539 (1.1543)  time: 0.1826  data: 0.0001  max mem: 15824
[07:34:02.928502] Test:  [120/345]  eta: 0:00:41  loss: 1.1546 (1.1544)  time: 0.1829  data: 0.0001  max mem: 15824
[07:34:04.763172] Test:  [130/345]  eta: 0:00:39  loss: 1.1541 (1.1542)  time: 0.1832  data: 0.0001  max mem: 15824
[07:34:06.602643] Test:  [140/345]  eta: 0:00:37  loss: 1.1533 (1.1542)  time: 0.1836  data: 0.0001  max mem: 15824
[07:34:08.447465] Test:  [150/345]  eta: 0:00:35  loss: 1.1532 (1.1541)  time: 0.1842  data: 0.0001  max mem: 15824
[07:34:10.296771] Test:  [160/345]  eta: 0:00:34  loss: 1.1516 (1.1541)  time: 0.1846  data: 0.0001  max mem: 15824
[07:34:12.150805] Test:  [170/345]  eta: 0:00:32  loss: 1.1528 (1.1541)  time: 0.1851  data: 0.0001  max mem: 15824
[07:34:14.006610] Test:  [180/345]  eta: 0:00:30  loss: 1.1533 (1.1541)  time: 0.1854  data: 0.0001  max mem: 15824
[07:34:15.866660] Test:  [190/345]  eta: 0:00:28  loss: 1.1530 (1.1540)  time: 0.1857  data: 0.0001  max mem: 15824
[07:34:17.731784] Test:  [200/345]  eta: 0:00:26  loss: 1.1530 (1.1540)  time: 0.1862  data: 0.0001  max mem: 15824
[07:34:19.995911] Test:  [210/345]  eta: 0:00:25  loss: 1.1531 (1.1539)  time: 0.2064  data: 0.0001  max mem: 15824
[07:34:21.879488] Test:  [220/345]  eta: 0:00:23  loss: 1.1533 (1.1539)  time: 0.2073  data: 0.0001  max mem: 15824
[07:34:23.907587] Test:  [230/345]  eta: 0:00:21  loss: 1.1535 (1.1540)  time: 0.1955  data: 0.0001  max mem: 15824
[07:34:25.781614] Test:  [240/345]  eta: 0:00:19  loss: 1.1531 (1.1538)  time: 0.1950  data: 0.0001  max mem: 15824
[07:34:27.906877] Test:  [250/345]  eta: 0:00:17  loss: 1.1531 (1.1539)  time: 0.1999  data: 0.0001  max mem: 15824
[07:34:30.048796] Test:  [260/345]  eta: 0:00:16  loss: 1.1543 (1.1539)  time: 0.2133  data: 0.0001  max mem: 15824
[07:34:32.120370] Test:  [270/345]  eta: 0:00:14  loss: 1.1532 (1.1539)  time: 0.2106  data: 0.0001  max mem: 15824
[07:34:34.051113] Test:  [280/345]  eta: 0:00:12  loss: 1.1538 (1.1539)  time: 0.2000  data: 0.0001  max mem: 15824
[07:34:36.146779] Test:  [290/345]  eta: 0:00:10  loss: 1.1538 (1.1539)  time: 0.2013  data: 0.0001  max mem: 15824
[07:34:38.310112] Test:  [300/345]  eta: 0:00:08  loss: 1.1541 (1.1540)  time: 0.2129  data: 0.0001  max mem: 15824
[07:34:40.447928] Test:  [310/345]  eta: 0:00:06  loss: 1.1549 (1.1541)  time: 0.2150  data: 0.0001  max mem: 15824
[07:34:42.593281] Test:  [320/345]  eta: 0:00:04  loss: 1.1554 (1.1541)  time: 0.2141  data: 0.0001  max mem: 15824
[07:34:44.694784] Test:  [330/345]  eta: 0:00:02  loss: 1.1544 (1.1541)  time: 0.2123  data: 0.0001  max mem: 15824
[07:34:46.643069] Test:  [340/345]  eta: 0:00:00  loss: 1.1539 (1.1541)  time: 0.2024  data: 0.0001  max mem: 15824
[07:34:47.695987] Test:  [344/345]  eta: 0:00:00  loss: 1.1541 (1.1541)  time: 0.2156  data: 0.0001  max mem: 15824
[07:34:47.774889] Test: Total time: 0:01:07 (0.1948 s / it)
[07:34:58.431433] Test:  [ 0/57]  eta: 0:00:33  loss: 1.1690 (1.1690)  time: 0.5920  data: 0.4124  max mem: 15824
[07:35:00.201515] Test:  [10/57]  eta: 0:00:10  loss: 1.1605 (1.1602)  time: 0.2147  data: 0.0376  max mem: 15824
[07:35:01.977318] Test:  [20/57]  eta: 0:00:07  loss: 1.1605 (1.1580)  time: 0.1772  data: 0.0001  max mem: 15824
[07:35:03.756514] Test:  [30/57]  eta: 0:00:05  loss: 1.1441 (1.1511)  time: 0.1777  data: 0.0001  max mem: 15824
[07:35:05.540452] Test:  [40/57]  eta: 0:00:03  loss: 1.1335 (1.1465)  time: 0.1781  data: 0.0001  max mem: 15824
[07:35:07.329019] Test:  [50/57]  eta: 0:00:01  loss: 1.1411 (1.1453)  time: 0.1786  data: 0.0001  max mem: 15824
[07:35:09.093738] Test:  [56/57]  eta: 0:00:00  loss: 1.1427 (1.1457)  time: 0.2133  data: 0.0001  max mem: 15824
[07:35:09.161615] Test: Total time: 0:00:11 (0.1986 s / it)
[07:35:10.940454] Dice score of the network on the train images: 0.000000, val images: 0.000000
[07:35:10.940671] saving best_dice_model_0 @ epoch 0
[07:35:12.043646] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[07:35:13.114168] Epoch: [1]  [  0/345]  eta: 0:06:08  lr: 0.000006  loss: 1.1476 (1.1476)  time: 1.0695  data: 0.4189  max mem: 15824
[07:35:25.373482] Epoch: [1]  [ 20/345]  eta: 0:03:26  lr: 0.000007  loss: 1.1294 (1.1303)  time: 0.6129  data: 0.0001  max mem: 15824
[07:35:37.662830] Epoch: [1]  [ 40/345]  eta: 0:03:10  lr: 0.000007  loss: 1.1066 (1.1193)  time: 0.6144  data: 0.0001  max mem: 15824
[07:35:49.969845] Epoch: [1]  [ 60/345]  eta: 0:02:57  lr: 0.000007  loss: 1.0897 (1.1097)  time: 0.6153  data: 0.0001  max mem: 15824
[07:36:02.283983] Epoch: [1]  [ 80/345]  eta: 0:02:44  lr: 0.000008  loss: 1.0741 (1.1012)  time: 0.6157  data: 0.0001  max mem: 15824
[07:36:14.618980] Epoch: [1]  [100/345]  eta: 0:02:31  lr: 0.000008  loss: 1.0618 (1.0932)  time: 0.6167  data: 0.0001  max mem: 15824
[07:36:26.981644] Epoch: [1]  [120/345]  eta: 0:02:19  lr: 0.000008  loss: 1.0472 (1.0861)  time: 0.6181  data: 0.0001  max mem: 15824
[07:36:39.339719] Epoch: [1]  [140/345]  eta: 0:02:06  lr: 0.000009  loss: 1.0338 (1.0789)  time: 0.6179  data: 0.0001  max mem: 15824
[07:36:51.689988] Epoch: [1]  [160/345]  eta: 0:01:54  lr: 0.000009  loss: 1.0163 (1.0714)  time: 0.6175  data: 0.0001  max mem: 15824
[07:37:04.040359] Epoch: [1]  [180/345]  eta: 0:01:42  lr: 0.000010  loss: 1.0105 (1.0643)  time: 0.6175  data: 0.0001  max mem: 15824
[07:37:16.387067] Epoch: [1]  [200/345]  eta: 0:01:29  lr: 0.000010  loss: 0.9967 (1.0574)  time: 0.6173  data: 0.0001  max mem: 15824
[07:37:28.744537] Epoch: [1]  [220/345]  eta: 0:01:17  lr: 0.000010  loss: 0.9832 (1.0511)  time: 0.6178  data: 0.0001  max mem: 15824
[07:37:41.090280] Epoch: [1]  [240/345]  eta: 0:01:04  lr: 0.000011  loss: 0.9799 (1.0455)  time: 0.6172  data: 0.0001  max mem: 15824
[07:37:53.407937] Epoch: [1]  [260/345]  eta: 0:00:52  lr: 0.000011  loss: 0.9699 (1.0398)  time: 0.6158  data: 0.0001  max mem: 15824
[07:38:05.735722] Epoch: [1]  [280/345]  eta: 0:00:40  lr: 0.000011  loss: 0.9648 (1.0342)  time: 0.6164  data: 0.0001  max mem: 15824
[07:38:18.066921] Epoch: [1]  [300/345]  eta: 0:00:27  lr: 0.000012  loss: 0.9598 (1.0296)  time: 0.6165  data: 0.0001  max mem: 15824
[07:38:30.398606] Epoch: [1]  [320/345]  eta: 0:00:15  lr: 0.000012  loss: 0.9409 (1.0242)  time: 0.6165  data: 0.0001  max mem: 15824
[07:38:42.729875] Epoch: [1]  [340/345]  eta: 0:00:03  lr: 0.000012  loss: 0.9405 (1.0194)  time: 0.6165  data: 0.0001  max mem: 15824
[07:38:45.197248] Epoch: [1]  [344/345]  eta: 0:00:00  lr: 0.000012  loss: 0.9424 (1.0186)  time: 0.6164  data: 0.0001  max mem: 15824
[07:38:45.278299] Epoch: [1] Total time: 0:03:33 (0.6181 s / it)
[07:38:45.278661] Averaged stats: lr: 0.000012  loss: 0.9424 (1.0186)
[07:38:45.880201] Test:  [  0/345]  eta: 0:03:25  loss: 0.9479 (0.9479)  time: 0.5959  data: 0.4134  max mem: 15824
[07:38:47.670873] Test:  [ 10/345]  eta: 0:01:12  loss: 0.9169 (0.9211)  time: 0.2169  data: 0.0377  max mem: 15824
[07:38:49.462875] Test:  [ 20/345]  eta: 0:01:04  loss: 0.9290 (0.9331)  time: 0.1790  data: 0.0001  max mem: 15824
[07:38:51.259364] Test:  [ 30/345]  eta: 0:01:00  loss: 0.9261 (0.9307)  time: 0.1794  data: 0.0001  max mem: 15824
[07:38:53.063616] Test:  [ 40/345]  eta: 0:00:57  loss: 0.9213 (0.9320)  time: 0.1800  data: 0.0001  max mem: 15824
[07:38:54.863952] Test:  [ 50/345]  eta: 0:00:55  loss: 0.9346 (0.9336)  time: 0.1802  data: 0.0001  max mem: 15824
[07:38:56.667184] Test:  [ 60/345]  eta: 0:00:53  loss: 0.9369 (0.9328)  time: 0.1801  data: 0.0001  max mem: 15824
[07:38:58.476103] Test:  [ 70/345]  eta: 0:00:51  loss: 0.9214 (0.9315)  time: 0.1805  data: 0.0001  max mem: 15824
[07:39:00.290025] Test:  [ 80/345]  eta: 0:00:49  loss: 0.9195 (0.9306)  time: 0.1811  data: 0.0001  max mem: 15824
[07:39:02.109159] Test:  [ 90/345]  eta: 0:00:47  loss: 0.9339 (0.9303)  time: 0.1816  data: 0.0001  max mem: 15824
[07:39:03.930612] Test:  [100/345]  eta: 0:00:45  loss: 0.9219 (0.9295)  time: 0.1820  data: 0.0001  max mem: 15824
[07:39:05.755233] Test:  [110/345]  eta: 0:00:43  loss: 0.9219 (0.9294)  time: 0.1822  data: 0.0001  max mem: 15824
[07:39:07.584120] Test:  [120/345]  eta: 0:00:41  loss: 0.9306 (0.9296)  time: 0.1826  data: 0.0001  max mem: 15824
[07:39:09.414180] Test:  [130/345]  eta: 0:00:39  loss: 0.9211 (0.9284)  time: 0.1829  data: 0.0001  max mem: 15824
[07:39:11.247171] Test:  [140/345]  eta: 0:00:37  loss: 0.9211 (0.9286)  time: 0.1831  data: 0.0001  max mem: 15824
[07:39:13.083005] Test:  [150/345]  eta: 0:00:35  loss: 0.9332 (0.9295)  time: 0.1834  data: 0.0001  max mem: 15824
[07:39:14.922529] Test:  [160/345]  eta: 0:00:34  loss: 0.9340 (0.9290)  time: 0.1837  data: 0.0001  max mem: 15824
[07:39:16.765067] Test:  [170/345]  eta: 0:00:32  loss: 0.9222 (0.9284)  time: 0.1840  data: 0.0001  max mem: 15824
[07:39:18.614383] Test:  [180/345]  eta: 0:00:30  loss: 0.9256 (0.9287)  time: 0.1845  data: 0.0001  max mem: 15824
[07:39:20.464236] Test:  [190/345]  eta: 0:00:28  loss: 0.9214 (0.9282)  time: 0.1849  data: 0.0001  max mem: 15824
[07:39:22.319238] Test:  [200/345]  eta: 0:00:26  loss: 0.9324 (0.9289)  time: 0.1852  data: 0.0001  max mem: 15824
[07:39:24.177428] Test:  [210/345]  eta: 0:00:24  loss: 0.9391 (0.9291)  time: 0.1856  data: 0.0001  max mem: 15824
[07:39:26.412530] Test:  [220/345]  eta: 0:00:23  loss: 0.9308 (0.9290)  time: 0.2046  data: 0.0001  max mem: 15824
[07:39:28.276927] Test:  [230/345]  eta: 0:00:21  loss: 0.9138 (0.9282)  time: 0.2049  data: 0.0001  max mem: 15824
[07:39:30.325823] Test:  [240/345]  eta: 0:00:19  loss: 0.9130 (0.9282)  time: 0.1956  data: 0.0001  max mem: 15824
[07:39:32.210702] Test:  [250/345]  eta: 0:00:17  loss: 0.9266 (0.9284)  time: 0.1966  data: 0.0001  max mem: 15824
[07:39:34.231784] Test:  [260/345]  eta: 0:00:15  loss: 0.9327 (0.9289)  time: 0.1952  data: 0.0001  max mem: 15824
[07:39:36.141712] Test:  [270/345]  eta: 0:00:14  loss: 0.9453 (0.9291)  time: 0.1965  data: 0.0001  max mem: 15824
[07:39:38.175199] Test:  [280/345]  eta: 0:00:12  loss: 0.9344 (0.9290)  time: 0.1971  data: 0.0001  max mem: 15824
[07:39:40.086656] Test:  [290/345]  eta: 0:00:10  loss: 0.9210 (0.9289)  time: 0.1972  data: 0.0001  max mem: 15824
[07:39:42.172068] Test:  [300/345]  eta: 0:00:08  loss: 0.9202 (0.9290)  time: 0.1998  data: 0.0001  max mem: 15824
[07:39:44.075056] Test:  [310/345]  eta: 0:00:06  loss: 0.9361 (0.9292)  time: 0.1994  data: 0.0001  max mem: 15824
[07:39:46.247951] Test:  [320/345]  eta: 0:00:04  loss: 0.9218 (0.9288)  time: 0.2037  data: 0.0001  max mem: 15824
[07:39:48.144821] Test:  [330/345]  eta: 0:00:02  loss: 0.9218 (0.9288)  time: 0.2034  data: 0.0001  max mem: 15824
[07:39:50.223895] Test:  [340/345]  eta: 0:00:00  loss: 0.9383 (0.9292)  time: 0.1987  data: 0.0001  max mem: 15824
[07:39:50.985221] Test:  [344/345]  eta: 0:00:00  loss: 0.9346 (0.9292)  time: 0.1989  data: 0.0001  max mem: 15824
[07:39:51.064364] Test: Total time: 0:01:05 (0.1907 s / it)
[07:40:01.738673] Test:  [ 0/57]  eta: 0:00:32  loss: 0.9988 (0.9988)  time: 0.5724  data: 0.3931  max mem: 15824
[07:40:03.509534] Test:  [10/57]  eta: 0:00:10  loss: 0.9599 (0.9735)  time: 0.2129  data: 0.0358  max mem: 15824
[07:40:05.284404] Test:  [20/57]  eta: 0:00:07  loss: 0.9668 (0.9679)  time: 0.1772  data: 0.0001  max mem: 15824
[07:40:07.064624] Test:  [30/57]  eta: 0:00:05  loss: 0.8976 (0.9131)  time: 0.1777  data: 0.0001  max mem: 15824
[07:40:08.848630] Test:  [40/57]  eta: 0:00:03  loss: 0.7678 (0.8758)  time: 0.1781  data: 0.0001  max mem: 15824
[07:40:10.632568] Test:  [50/57]  eta: 0:00:01  loss: 0.7724 (0.8674)  time: 0.1783  data: 0.0001  max mem: 15824
[07:40:11.604968] Test:  [56/57]  eta: 0:00:00  loss: 0.8668 (0.8751)  time: 0.1735  data: 0.0001  max mem: 15824
[07:40:11.683633] Test: Total time: 0:00:10 (0.1845 s / it)
[07:40:13.458667] Dice score of the network on the train images: 0.412520, val images: 0.524089
[07:40:13.458889] saving best_prec_model_0 @ epoch 1
[07:40:14.174446] saving best_rec_model_0 @ epoch 1
[07:40:14.966074] saving best_dice_model_0 @ epoch 1
[07:40:15.950184] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[07:40:16.970754] Epoch: [2]  [  0/345]  eta: 0:05:51  lr: 0.000013  loss: 0.9240 (0.9240)  time: 1.0192  data: 0.3953  max mem: 15824
[07:40:29.272409] Epoch: [2]  [ 20/345]  eta: 0:03:26  lr: 0.000013  loss: 0.9148 (0.9179)  time: 0.6150  data: 0.0001  max mem: 15824
[07:40:41.577434] Epoch: [2]  [ 40/345]  eta: 0:03:10  lr: 0.000013  loss: 0.9183 (0.9183)  time: 0.6152  data: 0.0001  max mem: 15824
[07:40:53.922020] Epoch: [2]  [ 60/345]  eta: 0:02:57  lr: 0.000014  loss: 0.9143 (0.9169)  time: 0.6172  data: 0.0001  max mem: 15824
[07:41:06.290691] Epoch: [2]  [ 80/345]  eta: 0:02:44  lr: 0.000014  loss: 0.9164 (0.9159)  time: 0.6184  data: 0.0001  max mem: 15824
[07:41:18.688886] Epoch: [2]  [100/345]  eta: 0:02:32  lr: 0.000014  loss: 0.8993 (0.9121)  time: 0.6199  data: 0.0001  max mem: 15824
[07:41:31.107520] Epoch: [2]  [120/345]  eta: 0:02:19  lr: 0.000015  loss: 0.8816 (0.9082)  time: 0.6209  data: 0.0001  max mem: 15824
[07:41:43.516661] Epoch: [2]  [140/345]  eta: 0:02:07  lr: 0.000015  loss: 0.8787 (0.9045)  time: 0.6204  data: 0.0001  max mem: 15824
[07:41:55.928711] Epoch: [2]  [160/345]  eta: 0:01:54  lr: 0.000015  loss: 0.8666 (0.9001)  time: 0.6206  data: 0.0001  max mem: 15824
[07:42:08.349093] Epoch: [2]  [180/345]  eta: 0:01:42  lr: 0.000016  loss: 0.8564 (0.8957)  time: 0.6210  data: 0.0001  max mem: 15824
[07:42:20.772106] Epoch: [2]  [200/345]  eta: 0:01:30  lr: 0.000016  loss: 0.8545 (0.8914)  time: 0.6211  data: 0.0001  max mem: 15824
[07:42:33.185291] Epoch: [2]  [220/345]  eta: 0:01:17  lr: 0.000016  loss: 0.8354 (0.8866)  time: 0.6206  data: 0.0001  max mem: 15824
[07:42:45.577832] Epoch: [2]  [240/345]  eta: 0:01:05  lr: 0.000017  loss: 0.8314 (0.8821)  time: 0.6196  data: 0.0001  max mem: 15824
[07:42:57.974296] Epoch: [2]  [260/345]  eta: 0:00:52  lr: 0.000017  loss: 0.8186 (0.8774)  time: 0.6198  data: 0.0001  max mem: 15824
[07:43:10.369050] Epoch: [2]  [280/345]  eta: 0:00:40  lr: 0.000018  loss: 0.8147 (0.8732)  time: 0.6197  data: 0.0001  max mem: 15824
[07:43:22.766425] Epoch: [2]  [300/345]  eta: 0:00:27  lr: 0.000018  loss: 0.8088 (0.8693)  time: 0.6198  data: 0.0001  max mem: 15824
[07:43:35.157521] Epoch: [2]  [320/345]  eta: 0:00:15  lr: 0.000018  loss: 0.7896 (0.8648)  time: 0.6195  data: 0.0001  max mem: 15824
[07:43:47.545871] Epoch: [2]  [340/345]  eta: 0:00:03  lr: 0.000019  loss: 0.7851 (0.8602)  time: 0.6194  data: 0.0001  max mem: 15824
[07:43:50.024007] Epoch: [2]  [344/345]  eta: 0:00:00  lr: 0.000019  loss: 0.7851 (0.8592)  time: 0.6196  data: 0.0001  max mem: 15824
[07:43:50.096784] Epoch: [2] Total time: 0:03:34 (0.6207 s / it)
[07:43:50.097284] Averaged stats: lr: 0.000019  loss: 0.7851 (0.8592)
[07:43:50.715624] Test:  [  0/345]  eta: 0:03:31  loss: 0.8551 (0.8551)  time: 0.6131  data: 0.4308  max mem: 15824
[07:43:52.503067] Test:  [ 10/345]  eta: 0:01:13  loss: 0.8491 (0.8309)  time: 0.2182  data: 0.0392  max mem: 15824
[07:43:54.299492] Test:  [ 20/345]  eta: 0:01:04  loss: 0.8220 (0.8215)  time: 0.1791  data: 0.0001  max mem: 15824
[07:43:56.101620] Test:  [ 30/345]  eta: 0:01:00  loss: 0.8036 (0.8178)  time: 0.1799  data: 0.0001  max mem: 15824
[07:43:57.905886] Test:  [ 40/345]  eta: 0:00:58  loss: 0.7852 (0.8111)  time: 0.1802  data: 0.0001  max mem: 15824
[07:43:59.705364] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7818 (0.8056)  time: 0.1801  data: 0.0001  max mem: 15824
[07:44:01.507156] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7891 (0.8059)  time: 0.1800  data: 0.0001  max mem: 15824
[07:44:03.312274] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7992 (0.8056)  time: 0.1803  data: 0.0001  max mem: 15824
[07:44:05.124213] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7992 (0.8060)  time: 0.1808  data: 0.0001  max mem: 15824
[07:44:06.939982] Test:  [ 90/345]  eta: 0:00:47  loss: 0.8162 (0.8057)  time: 0.1813  data: 0.0001  max mem: 15824
[07:44:08.760251] Test:  [100/345]  eta: 0:00:45  loss: 0.8130 (0.8066)  time: 0.1817  data: 0.0001  max mem: 15824
[07:44:10.585224] Test:  [110/345]  eta: 0:00:43  loss: 0.8050 (0.8073)  time: 0.1822  data: 0.0001  max mem: 15824
[07:44:12.409181] Test:  [120/345]  eta: 0:00:41  loss: 0.7931 (0.8042)  time: 0.1824  data: 0.0001  max mem: 15824
[07:44:14.238244] Test:  [130/345]  eta: 0:00:39  loss: 0.7837 (0.8050)  time: 0.1826  data: 0.0001  max mem: 15824
[07:44:16.071656] Test:  [140/345]  eta: 0:00:37  loss: 0.8178 (0.8059)  time: 0.1831  data: 0.0001  max mem: 15824
[07:44:17.906644] Test:  [150/345]  eta: 0:00:35  loss: 0.8138 (0.8064)  time: 0.1834  data: 0.0001  max mem: 15824
[07:44:19.745928] Test:  [160/345]  eta: 0:00:34  loss: 0.8061 (0.8069)  time: 0.1836  data: 0.0001  max mem: 15824
[07:44:21.587989] Test:  [170/345]  eta: 0:00:32  loss: 0.8159 (0.8072)  time: 0.1840  data: 0.0001  max mem: 15824
[07:44:23.433681] Test:  [180/345]  eta: 0:00:30  loss: 0.8007 (0.8059)  time: 0.1843  data: 0.0001  max mem: 15824
[07:44:25.283937] Test:  [190/345]  eta: 0:00:28  loss: 0.7880 (0.8061)  time: 0.1847  data: 0.0001  max mem: 15824
[07:44:27.135482] Test:  [200/345]  eta: 0:00:26  loss: 0.7898 (0.8057)  time: 0.1850  data: 0.0001  max mem: 15824
[07:44:28.993316] Test:  [210/345]  eta: 0:00:24  loss: 0.7898 (0.8056)  time: 0.1854  data: 0.0001  max mem: 15824
[07:44:30.852457] Test:  [220/345]  eta: 0:00:23  loss: 0.7950 (0.8046)  time: 0.1858  data: 0.0001  max mem: 15824
[07:44:32.714929] Test:  [230/345]  eta: 0:00:21  loss: 0.8008 (0.8049)  time: 0.1860  data: 0.0001  max mem: 15824
[07:44:34.580415] Test:  [240/345]  eta: 0:00:19  loss: 0.8039 (0.8044)  time: 0.1863  data: 0.0001  max mem: 15824
[07:44:36.450708] Test:  [250/345]  eta: 0:00:17  loss: 0.7943 (0.8041)  time: 0.1867  data: 0.0001  max mem: 15824
[07:44:38.324778] Test:  [260/345]  eta: 0:00:15  loss: 0.8162 (0.8053)  time: 0.1872  data: 0.0001  max mem: 15824
[07:44:40.201086] Test:  [270/345]  eta: 0:00:13  loss: 0.8104 (0.8043)  time: 0.1875  data: 0.0001  max mem: 15824
[07:44:42.082692] Test:  [280/345]  eta: 0:00:12  loss: 0.7959 (0.8045)  time: 0.1878  data: 0.0001  max mem: 15824
[07:44:43.966275] Test:  [290/345]  eta: 0:00:10  loss: 0.8115 (0.8055)  time: 0.1882  data: 0.0001  max mem: 15824
[07:44:45.854012] Test:  [300/345]  eta: 0:00:08  loss: 0.8002 (0.8050)  time: 0.1885  data: 0.0001  max mem: 15824
[07:44:47.746257] Test:  [310/345]  eta: 0:00:06  loss: 0.7842 (0.8047)  time: 0.1889  data: 0.0001  max mem: 15824
[07:44:49.642123] Test:  [320/345]  eta: 0:00:04  loss: 0.7930 (0.8045)  time: 0.1893  data: 0.0001  max mem: 15824
[07:44:51.539347] Test:  [330/345]  eta: 0:00:02  loss: 0.8078 (0.8051)  time: 0.1896  data: 0.0001  max mem: 15824
[07:44:53.435453] Test:  [340/345]  eta: 0:00:00  loss: 0.8087 (0.8051)  time: 0.1896  data: 0.0001  max mem: 15824
[07:44:54.195485] Test:  [344/345]  eta: 0:00:00  loss: 0.8087 (0.8051)  time: 0.1897  data: 0.0001  max mem: 15824
[07:44:54.273539] Test: Total time: 0:01:04 (0.1860 s / it)
[07:45:04.784937] Test:  [ 0/57]  eta: 0:00:31  loss: 0.8931 (0.8931)  time: 0.5542  data: 0.3745  max mem: 15824
[07:45:06.553915] Test:  [10/57]  eta: 0:00:09  loss: 0.8485 (0.8624)  time: 0.2111  data: 0.0341  max mem: 15824
[07:45:08.328262] Test:  [20/57]  eta: 0:00:07  loss: 0.8139 (0.8492)  time: 0.1771  data: 0.0001  max mem: 15824
[07:45:10.106339] Test:  [30/57]  eta: 0:00:05  loss: 0.7932 (0.7797)  time: 0.1776  data: 0.0001  max mem: 15824
[07:45:11.892289] Test:  [40/57]  eta: 0:00:03  loss: 0.6015 (0.7346)  time: 0.1781  data: 0.0001  max mem: 15824
[07:45:13.677511] Test:  [50/57]  eta: 0:00:01  loss: 0.6387 (0.7230)  time: 0.1785  data: 0.0001  max mem: 15824
[07:45:14.652645] Test:  [56/57]  eta: 0:00:00  loss: 0.7291 (0.7311)  time: 0.1738  data: 0.0001  max mem: 15824
[07:45:14.735278] Test: Total time: 0:00:10 (0.1843 s / it)
[07:45:16.432401] Dice score of the network on the train images: 0.539705, val images: 0.661406
[07:45:16.432618] saving best_prec_model_0 @ epoch 2
[07:45:17.775153] saving best_rec_model_0 @ epoch 2
[07:45:18.762999] saving best_dice_model_0 @ epoch 2
[07:45:20.406844] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[07:45:21.479275] Epoch: [3]  [  0/345]  eta: 0:06:09  lr: 0.000019  loss: 0.7458 (0.7458)  time: 1.0710  data: 0.4465  max mem: 15824
[07:45:33.759251] Epoch: [3]  [ 20/345]  eta: 0:03:26  lr: 0.000019  loss: 0.7636 (0.7687)  time: 0.6139  data: 0.0001  max mem: 15824
[07:45:46.094805] Epoch: [3]  [ 40/345]  eta: 0:03:11  lr: 0.000019  loss: 0.7595 (0.7648)  time: 0.6167  data: 0.0001  max mem: 15824
[07:45:58.452249] Epoch: [3]  [ 60/345]  eta: 0:02:57  lr: 0.000020  loss: 0.7368 (0.7590)  time: 0.6178  data: 0.0001  max mem: 15824
[07:46:10.833751] Epoch: [3]  [ 80/345]  eta: 0:02:44  lr: 0.000020  loss: 0.7489 (0.7572)  time: 0.6190  data: 0.0001  max mem: 15824
[07:46:23.218716] Epoch: [3]  [100/345]  eta: 0:02:32  lr: 0.000021  loss: 0.7529 (0.7558)  time: 0.6192  data: 0.0001  max mem: 15824
[07:46:35.642104] Epoch: [3]  [120/345]  eta: 0:02:19  lr: 0.000021  loss: 0.7219 (0.7504)  time: 0.6211  data: 0.0001  max mem: 15824
[07:46:48.071263] Epoch: [3]  [140/345]  eta: 0:02:07  lr: 0.000021  loss: 0.7056 (0.7448)  time: 0.6214  data: 0.0001  max mem: 15824
[07:47:00.492543] Epoch: [3]  [160/345]  eta: 0:01:54  lr: 0.000022  loss: 0.6905 (0.7387)  time: 0.6210  data: 0.0001  max mem: 15824
[07:47:12.891096] Epoch: [3]  [180/345]  eta: 0:01:42  lr: 0.000022  loss: 0.6902 (0.7335)  time: 0.6199  data: 0.0001  max mem: 15824
[07:47:25.284012] Epoch: [3]  [200/345]  eta: 0:01:30  lr: 0.000022  loss: 0.6792 (0.7285)  time: 0.6196  data: 0.0001  max mem: 15824

[07:47:37.678811] Epoch: [3]  [220/345]  eta: 0:01:17  lr: 0.000023  loss: 0.6754 (0.7244)  time: 0.6197  data: 0.0001  max mem: 15824
[07:47:50.078773] Epoch: [3]  [240/345]  eta: 0:01:05  lr: 0.000023  loss: 0.6826 (0.7209)  time: 0.6200  data: 0.0001  max mem: 15824
[07:48:02.488888] Epoch: [3]  [260/345]  eta: 0:00:52  lr: 0.000023  loss: 0.6662 (0.7164)  time: 0.6205  data: 0.0001  max mem: 15824
[07:48:14.974130] Epoch: [3]  [280/345]  eta: 0:00:40  lr: 0.000024  loss: 0.6553 (0.7129)  time: 0.6242  data: 0.0001  max mem: 15824
[07:48:27.379213] Epoch: [3]  [300/345]  eta: 0:00:27  lr: 0.000024  loss: 0.6422 (0.7084)  time: 0.6202  data: 0.0001  max mem: 15824

[07:48:39.748605] Epoch: [3]  [320/345]  eta: 0:00:15  lr: 0.000025  loss: 0.6356 (0.7038)  time: 0.6184  data: 0.0001  max mem: 15824
[07:48:52.111415] Epoch: [3]  [340/345]  eta: 0:00:03  lr: 0.000025  loss: 0.6219 (0.6991)  time: 0.6181  data: 0.0001  max mem: 15824
[07:48:54.592385] Epoch: [3]  [344/345]  eta: 0:00:00  lr: 0.000025  loss: 0.6219 (0.6982)  time: 0.6187  data: 0.0001  max mem: 15824
[07:48:54.671217] Epoch: [3] Total time: 0:03:34 (0.6211 s / it)
[07:48:54.671584] Averaged stats: lr: 0.000025  loss: 0.6219 (0.6982)
[07:48:55.268725] Test:  [  0/345]  eta: 0:03:24  loss: 0.6236 (0.6236)  time: 0.5919  data: 0.4106  max mem: 15824
[07:48:57.059369] Test:  [ 10/345]  eta: 0:01:12  loss: 0.5757 (0.5805)  time: 0.2165  data: 0.0374  max mem: 15824
[07:48:58.851833] Test:  [ 20/345]  eta: 0:01:04  loss: 0.5848 (0.5872)  time: 0.1791  data: 0.0001  max mem: 15824
[07:49:00.645441] Test:  [ 30/345]  eta: 0:01:00  loss: 0.5908 (0.5845)  time: 0.1792  data: 0.0001  max mem: 15824
[07:49:02.442879] Test:  [ 40/345]  eta: 0:00:57  loss: 0.5931 (0.5872)  time: 0.1795  data: 0.0001  max mem: 15824
[07:49:04.244877] Test:  [ 50/345]  eta: 0:00:55  loss: 0.5931 (0.5878)  time: 0.1799  data: 0.0001  max mem: 15824
[07:49:06.046205] Test:  [ 60/345]  eta: 0:00:53  loss: 0.5843 (0.5879)  time: 0.1801  data: 0.0001  max mem: 15824
[07:49:07.852160] Test:  [ 70/345]  eta: 0:00:51  loss: 0.6073 (0.5932)  time: 0.1803  data: 0.0001  max mem: 15824
[07:49:09.664497] Test:  [ 80/345]  eta: 0:00:49  loss: 0.6185 (0.5931)  time: 0.1809  data: 0.0001  max mem: 15824
[07:49:11.475997] Test:  [ 90/345]  eta: 0:00:47  loss: 0.6082 (0.5952)  time: 0.1811  data: 0.0001  max mem: 15824
[07:49:13.294957] Test:  [100/345]  eta: 0:00:45  loss: 0.5948 (0.5947)  time: 0.1815  data: 0.0001  max mem: 15824
[07:49:15.114189] Test:  [110/345]  eta: 0:00:43  loss: 0.5920 (0.5945)  time: 0.1818  data: 0.0001  max mem: 15824
[07:49:16.937195] Test:  [120/345]  eta: 0:00:41  loss: 0.5920 (0.5936)  time: 0.1821  data: 0.0001  max mem: 15824
[07:49:18.762646] Test:  [130/345]  eta: 0:00:39  loss: 0.5880 (0.5927)  time: 0.1824  data: 0.0001  max mem: 15824
[07:49:20.590272] Test:  [140/345]  eta: 0:00:37  loss: 0.5686 (0.5905)  time: 0.1826  data: 0.0001  max mem: 15824
[07:49:22.423132] Test:  [150/345]  eta: 0:00:35  loss: 0.5818 (0.5937)  time: 0.1830  data: 0.0001  max mem: 15824
[07:49:24.260077] Test:  [160/345]  eta: 0:00:33  loss: 0.6000 (0.5936)  time: 0.1834  data: 0.0001  max mem: 15824
[07:49:26.100011] Test:  [170/345]  eta: 0:00:32  loss: 0.5836 (0.5943)  time: 0.1838  data: 0.0001  max mem: 15824
[07:49:27.946215] Test:  [180/345]  eta: 0:00:30  loss: 0.5886 (0.5943)  time: 0.1842  data: 0.0001  max mem: 15824
[07:49:29.793840] Test:  [190/345]  eta: 0:00:28  loss: 0.5918 (0.5949)  time: 0.1846  data: 0.0001  max mem: 15824
[07:49:31.644039] Test:  [200/345]  eta: 0:00:26  loss: 0.5908 (0.5941)  time: 0.1848  data: 0.0001  max mem: 15824
[07:49:33.498612] Test:  [210/345]  eta: 0:00:24  loss: 0.5861 (0.5943)  time: 0.1852  data: 0.0001  max mem: 15824
[07:49:35.354819] Test:  [220/345]  eta: 0:00:22  loss: 0.5748 (0.5929)  time: 0.1855  data: 0.0001  max mem: 15824
[07:49:37.215433] Test:  [230/345]  eta: 0:00:21  loss: 0.5792 (0.5935)  time: 0.1858  data: 0.0001  max mem: 15824
[07:49:39.081561] Test:  [240/345]  eta: 0:00:19  loss: 0.5904 (0.5931)  time: 0.1863  data: 0.0001  max mem: 15824
[07:49:40.947759] Test:  [250/345]  eta: 0:00:17  loss: 0.5742 (0.5928)  time: 0.1866  data: 0.0001  max mem: 15824
[07:49:42.820277] Test:  [260/345]  eta: 0:00:15  loss: 0.5742 (0.5928)  time: 0.1869  data: 0.0001  max mem: 15824
[07:49:44.696128] Test:  [270/345]  eta: 0:00:13  loss: 0.5893 (0.5924)  time: 0.1874  data: 0.0001  max mem: 15824
[07:49:46.575261] Test:  [280/345]  eta: 0:00:12  loss: 0.6003 (0.5932)  time: 0.1877  data: 0.0001  max mem: 15824
[07:49:48.458198] Test:  [290/345]  eta: 0:00:10  loss: 0.5821 (0.5924)  time: 0.1880  data: 0.0001  max mem: 15824
[07:49:50.344058] Test:  [300/345]  eta: 0:00:08  loss: 0.5832 (0.5923)  time: 0.1884  data: 0.0001  max mem: 15824
[07:49:52.236510] Test:  [310/345]  eta: 0:00:06  loss: 0.5883 (0.5922)  time: 0.1888  data: 0.0001  max mem: 15824
[07:49:54.128537] Test:  [320/345]  eta: 0:00:04  loss: 0.5850 (0.5921)  time: 0.1892  data: 0.0001  max mem: 15824
[07:49:56.024354] Test:  [330/345]  eta: 0:00:02  loss: 0.5850 (0.5920)  time: 0.1893  data: 0.0001  max mem: 15824
[07:49:57.924376] Test:  [340/345]  eta: 0:00:00  loss: 0.5862 (0.5922)  time: 0.1897  data: 0.0001  max mem: 15824
[07:49:58.684558] Test:  [344/345]  eta: 0:00:00  loss: 0.5983 (0.5923)  time: 0.1898  data: 0.0001  max mem: 15824
[07:49:58.754914] Test: Total time: 0:01:04 (0.1857 s / it)
[07:50:09.221311] Test:  [ 0/57]  eta: 0:00:35  loss: 0.6729 (0.6729)  time: 0.6195  data: 0.4402  max mem: 15824
[07:50:10.991218] Test:  [10/57]  eta: 0:00:10  loss: 0.6238 (0.6575)  time: 0.2171  data: 0.0401  max mem: 15824
[07:50:12.763944] Test:  [20/57]  eta: 0:00:07  loss: 0.5918 (0.6147)  time: 0.1771  data: 0.0001  max mem: 15824
[07:50:14.540921] Test:  [30/57]  eta: 0:00:05  loss: 0.5183 (0.5549)  time: 0.1774  data: 0.0001  max mem: 15824
[07:50:16.328096] Test:  [40/57]  eta: 0:00:03  loss: 0.3974 (0.5188)  time: 0.1781  data: 0.0001  max mem: 15824
[07:50:18.113748] Test:  [50/57]  eta: 0:00:01  loss: 0.4137 (0.5062)  time: 0.1786  data: 0.0001  max mem: 15824
[07:50:19.086807] Test:  [56/57]  eta: 0:00:00  loss: 0.4973 (0.5147)  time: 0.1737  data: 0.0001  max mem: 15824
[07:50:19.152249] Test: Total time: 0:00:10 (0.1851 s / it)
[07:50:20.872546] Dice score of the network on the train images: 0.612501, val images: 0.720106
[07:50:20.872760] saving best_prec_model_0 @ epoch 3
[07:50:22.295504] saving best_dice_model_0 @ epoch 3
[07:50:23.735164] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[07:50:24.758143] Epoch: [4]  [  0/345]  eta: 0:05:52  lr: 0.000025  loss: 0.6189 (0.6189)  time: 1.0216  data: 0.4017  max mem: 15824
[07:50:37.018068] Epoch: [4]  [ 20/345]  eta: 0:03:25  lr: 0.000025  loss: 0.5965 (0.6048)  time: 0.6129  data: 0.0001  max mem: 15824
[07:50:49.321194] Epoch: [4]  [ 40/345]  eta: 0:03:10  lr: 0.000026  loss: 0.5849 (0.5995)  time: 0.6151  data: 0.0001  max mem: 15824
[07:51:01.638451] Epoch: [4]  [ 60/345]  eta: 0:02:57  lr: 0.000026  loss: 0.5971 (0.6005)  time: 0.6158  data: 0.0001  max mem: 15824
[07:51:14.081323] Epoch: [4]  [ 80/345]  eta: 0:02:44  lr: 0.000026  loss: 0.5859 (0.5972)  time: 0.6221  data: 0.0001  max mem: 15824
[07:51:26.493800] Epoch: [4]  [100/345]  eta: 0:02:32  lr: 0.000027  loss: 0.5823 (0.5953)  time: 0.6206  data: 0.0001  max mem: 15824
[07:51:38.908394] Epoch: [4]  [120/345]  eta: 0:02:19  lr: 0.000027  loss: 0.5662 (0.5919)  time: 0.6207  data: 0.0001  max mem: 15824
[07:51:51.342061] Epoch: [4]  [140/345]  eta: 0:02:07  lr: 0.000028  loss: 0.5877 (0.5912)  time: 0.6216  data: 0.0001  max mem: 15824
[07:52:03.751933] Epoch: [4]  [160/345]  eta: 0:01:54  lr: 0.000028  loss: 0.5624 (0.5864)  time: 0.6204  data: 0.0001  max mem: 15824
[07:52:16.145471] Epoch: [4]  [180/345]  eta: 0:01:42  lr: 0.000028  loss: 0.5460 (0.5825)  time: 0.6196  data: 0.0001  max mem: 15824
[07:52:28.544614] Epoch: [4]  [200/345]  eta: 0:01:30  lr: 0.000029  loss: 0.5251 (0.5777)  time: 0.6199  data: 0.0001  max mem: 15824
[07:52:40.958310] Epoch: [4]  [220/345]  eta: 0:01:17  lr: 0.000029  loss: 0.5334 (0.5747)  time: 0.6206  data: 0.0001  max mem: 15824
[07:52:53.362254] Epoch: [4]  [240/345]  eta: 0:01:05  lr: 0.000029  loss: 0.5501 (0.5721)  time: 0.6202  data: 0.0001  max mem: 15824
[07:53:05.776540] Epoch: [4]  [260/345]  eta: 0:00:52  lr: 0.000030  loss: 0.5194 (0.5680)  time: 0.6207  data: 0.0001  max mem: 15824
[07:53:18.179801] Epoch: [4]  [280/345]  eta: 0:00:40  lr: 0.000030  loss: 0.5267 (0.5655)  time: 0.6201  data: 0.0001  max mem: 15824
[07:53:30.593163] Epoch: [4]  [300/345]  eta: 0:00:27  lr: 0.000030  loss: 0.5206 (0.5622)  time: 0.6206  data: 0.0001  max mem: 15824
[07:53:43.082285] Epoch: [4]  [320/345]  eta: 0:00:15  lr: 0.000031  loss: 0.5083 (0.5586)  time: 0.6244  data: 0.0001  max mem: 15824
[07:53:55.473136] Epoch: [4]  [340/345]  eta: 0:00:03  lr: 0.000031  loss: 0.5165 (0.5559)  time: 0.6195  data: 0.0001  max mem: 15824
[07:53:57.953177] Epoch: [4]  [344/345]  eta: 0:00:00  lr: 0.000031  loss: 0.5165 (0.5551)  time: 0.6194  data: 0.0001  max mem: 15824
[07:53:58.028424] Epoch: [4] Total time: 0:03:34 (0.6211 s / it)
[07:53:58.028762] Averaged stats: lr: 0.000031  loss: 0.5165 (0.5551)
[07:53:58.646885] Test:  [  0/345]  eta: 0:03:31  loss: 0.5677 (0.5677)  time: 0.6128  data: 0.4322  max mem: 15824
[07:54:00.435334] Test:  [ 10/345]  eta: 0:01:13  loss: 0.4810 (0.4872)  time: 0.2182  data: 0.0394  max mem: 15824
[07:54:02.227580] Test:  [ 20/345]  eta: 0:01:04  loss: 0.4810 (0.4985)  time: 0.1790  data: 0.0001  max mem: 15824
[07:54:04.022980] Test:  [ 30/345]  eta: 0:01:00  loss: 0.4863 (0.4949)  time: 0.1793  data: 0.0001  max mem: 15824
[07:54:05.823986] Test:  [ 40/345]  eta: 0:00:57  loss: 0.5000 (0.4991)  time: 0.1798  data: 0.0001  max mem: 15824
[07:54:07.623955] Test:  [ 50/345]  eta: 0:00:55  loss: 0.5003 (0.4971)  time: 0.1800  data: 0.0001  max mem: 15824
[07:54:09.425794] Test:  [ 60/345]  eta: 0:00:53  loss: 0.4959 (0.4964)  time: 0.1800  data: 0.0001  max mem: 15824
[07:54:11.230649] Test:  [ 70/345]  eta: 0:00:51  loss: 0.5034 (0.4981)  time: 0.1803  data: 0.0001  max mem: 15824
[07:54:13.042761] Test:  [ 80/345]  eta: 0:00:49  loss: 0.5007 (0.4985)  time: 0.1808  data: 0.0001  max mem: 15824
[07:54:14.857164] Test:  [ 90/345]  eta: 0:00:47  loss: 0.5069 (0.5006)  time: 0.1813  data: 0.0001  max mem: 15824
[07:54:16.675112] Test:  [100/345]  eta: 0:00:45  loss: 0.5141 (0.5012)  time: 0.1816  data: 0.0001  max mem: 15824
[07:54:18.499136] Test:  [110/345]  eta: 0:00:43  loss: 0.4917 (0.5003)  time: 0.1820  data: 0.0001  max mem: 15824
[07:54:20.324469] Test:  [120/345]  eta: 0:00:41  loss: 0.4880 (0.5018)  time: 0.1824  data: 0.0001  max mem: 15824
[07:54:22.153944] Test:  [130/345]  eta: 0:00:39  loss: 0.4996 (0.5012)  time: 0.1827  data: 0.0001  max mem: 15824
[07:54:23.987172] Test:  [140/345]  eta: 0:00:37  loss: 0.4923 (0.5016)  time: 0.1831  data: 0.0001  max mem: 15824
[07:54:25.823769] Test:  [150/345]  eta: 0:00:35  loss: 0.5080 (0.5027)  time: 0.1834  data: 0.0001  max mem: 15824
[07:54:27.660544] Test:  [160/345]  eta: 0:00:34  loss: 0.4878 (0.5015)  time: 0.1836  data: 0.0001  max mem: 15824
[07:54:29.502097] Test:  [170/345]  eta: 0:00:32  loss: 0.4750 (0.5007)  time: 0.1839  data: 0.0001  max mem: 15824
[07:54:31.348998] Test:  [180/345]  eta: 0:00:30  loss: 0.4748 (0.4999)  time: 0.1844  data: 0.0001  max mem: 15824
[07:54:33.201122] Test:  [190/345]  eta: 0:00:28  loss: 0.5067 (0.5004)  time: 0.1849  data: 0.0001  max mem: 15824
[07:54:35.050881] Test:  [200/345]  eta: 0:00:26  loss: 0.5186 (0.5022)  time: 0.1850  data: 0.0001  max mem: 15824
[07:54:36.908236] Test:  [210/345]  eta: 0:00:24  loss: 0.5141 (0.5024)  time: 0.1853  data: 0.0001  max mem: 15824
[07:54:38.767739] Test:  [220/345]  eta: 0:00:23  loss: 0.4849 (0.5020)  time: 0.1858  data: 0.0001  max mem: 15824
[07:54:40.631377] Test:  [230/345]  eta: 0:00:21  loss: 0.4920 (0.5019)  time: 0.1861  data: 0.0001  max mem: 15824
[07:54:42.501093] Test:  [240/345]  eta: 0:00:19  loss: 0.4920 (0.5015)  time: 0.1866  data: 0.0001  max mem: 15824
[07:54:44.371681] Test:  [250/345]  eta: 0:00:17  loss: 0.4996 (0.5018)  time: 0.1870  data: 0.0001  max mem: 15824
[07:54:46.245516] Test:  [260/345]  eta: 0:00:15  loss: 0.5045 (0.5020)  time: 0.1872  data: 0.0001  max mem: 15824
[07:54:48.121639] Test:  [270/345]  eta: 0:00:13  loss: 0.5046 (0.5024)  time: 0.1874  data: 0.0001  max mem: 15824
[07:54:50.003170] Test:  [280/345]  eta: 0:00:12  loss: 0.5046 (0.5026)  time: 0.1878  data: 0.0001  max mem: 15824
[07:54:51.886128] Test:  [290/345]  eta: 0:00:10  loss: 0.5027 (0.5025)  time: 0.1882  data: 0.0001  max mem: 15824
[07:54:53.776584] Test:  [300/345]  eta: 0:00:08  loss: 0.5035 (0.5028)  time: 0.1886  data: 0.0001  max mem: 15824
[07:54:55.670931] Test:  [310/345]  eta: 0:00:06  loss: 0.5229 (0.5040)  time: 0.1892  data: 0.0001  max mem: 15824
[07:54:57.566429] Test:  [320/345]  eta: 0:00:04  loss: 0.5229 (0.5042)  time: 0.1894  data: 0.0001  max mem: 15824
[07:54:59.466297] Test:  [330/345]  eta: 0:00:02  loss: 0.5108 (0.5041)  time: 0.1897  data: 0.0001  max mem: 15824
[07:55:01.365084] Test:  [340/345]  eta: 0:00:00  loss: 0.5062 (0.5046)  time: 0.1899  data: 0.0001  max mem: 15824
[07:55:02.126923] Test:  [344/345]  eta: 0:00:00  loss: 0.5167 (0.5048)  time: 0.1900  data: 0.0001  max mem: 15824
[07:55:02.200354] Test: Total time: 0:01:04 (0.1860 s / it)
[07:55:12.745355] Test:  [ 0/57]  eta: 0:00:31  loss: 0.6151 (0.6151)  time: 0.5602  data: 0.3803  max mem: 15824
[07:55:14.516517] Test:  [10/57]  eta: 0:00:09  loss: 0.5424 (0.5622)  time: 0.2119  data: 0.0347  max mem: 15824
[07:55:16.293295] Test:  [20/57]  eta: 0:00:07  loss: 0.5424 (0.5470)  time: 0.1773  data: 0.0001  max mem: 15824
[07:55:18.073510] Test:  [30/57]  eta: 0:00:05  loss: 0.4771 (0.4982)  time: 0.1778  data: 0.0001  max mem: 15824
[07:55:19.857059] Test:  [40/57]  eta: 0:00:03  loss: 0.3685 (0.4713)  time: 0.1781  data: 0.0001  max mem: 15824
[07:55:21.642786] Test:  [50/57]  eta: 0:00:01  loss: 0.3810 (0.4609)  time: 0.1784  data: 0.0001  max mem: 15824
[07:55:22.615041] Test:  [56/57]  eta: 0:00:00  loss: 0.4361 (0.4647)  time: 0.1736  data: 0.0001  max mem: 15824
[07:55:22.681150] Test: Total time: 0:00:10 (0.1842 s / it)
[07:55:24.377651] Dice score of the network on the train images: 0.648882, val images: 0.716185
[07:55:24.377870] saving best_prec_model_0 @ epoch 4
[07:55:25.631842] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[07:55:26.647308] Epoch: [5]  [  0/345]  eta: 0:05:49  lr: 0.000031  loss: 0.5097 (0.5097)  time: 1.0142  data: 0.3898  max mem: 15824
[07:55:38.965910] Epoch: [5]  [ 20/345]  eta: 0:03:26  lr: 0.000032  loss: 0.5118 (0.5024)  time: 0.6159  data: 0.0001  max mem: 15824
[07:55:51.330522] Epoch: [5]  [ 40/345]  eta: 0:03:11  lr: 0.000032  loss: 0.4729 (0.4888)  time: 0.6182  data: 0.0001  max mem: 15824
[07:56:03.696483] Epoch: [5]  [ 60/345]  eta: 0:02:57  lr: 0.000032  loss: 0.4826 (0.4901)  time: 0.6183  data: 0.0001  max mem: 15824
[07:56:16.072176] Epoch: [5]  [ 80/345]  eta: 0:02:45  lr: 0.000033  loss: 0.4735 (0.4862)  time: 0.6187  data: 0.0001  max mem: 15824
[07:56:28.483097] Epoch: [5]  [100/345]  eta: 0:02:32  lr: 0.000033  loss: 0.4865 (0.4857)  time: 0.6205  data: 0.0001  max mem: 15824
[07:56:40.908907] Epoch: [5]  [120/345]  eta: 0:02:19  lr: 0.000033  loss: 0.4753 (0.4838)  time: 0.6212  data: 0.0001  max mem: 15824
[07:56:53.341240] Epoch: [5]  [140/345]  eta: 0:02:07  lr: 0.000034  loss: 0.4741 (0.4829)  time: 0.6216  data: 0.0001  max mem: 15824
[07:57:05.782850] Epoch: [5]  [160/345]  eta: 0:01:55  lr: 0.000034  loss: 0.4698 (0.4813)  time: 0.6220  data: 0.0001  max mem: 15824
[07:57:18.195930] Epoch: [5]  [180/345]  eta: 0:01:42  lr: 0.000035  loss: 0.4613 (0.4793)  time: 0.6206  data: 0.0001  max mem: 15824
[07:57:30.614939] Epoch: [5]  [200/345]  eta: 0:01:30  lr: 0.000035  loss: 0.4745 (0.4784)  time: 0.6209  data: 0.0001  max mem: 15824
[07:57:43.032941] Epoch: [5]  [220/345]  eta: 0:01:17  lr: 0.000035  loss: 0.4488 (0.4761)  time: 0.6209  data: 0.0001  max mem: 15824
[07:57:55.457713] Epoch: [5]  [240/345]  eta: 0:01:05  lr: 0.000036  loss: 0.4524 (0.4747)  time: 0.6212  data: 0.0001  max mem: 15824
[07:58:07.859910] Epoch: [5]  [260/345]  eta: 0:00:52  lr: 0.000036  loss: 0.4208 (0.4718)  time: 0.6201  data: 0.0001  max mem: 15824
[07:58:20.261660] Epoch: [5]  [280/345]  eta: 0:00:40  lr: 0.000036  loss: 0.4445 (0.4706)  time: 0.6200  data: 0.0001  max mem: 15824
[07:58:32.671912] Epoch: [5]  [300/345]  eta: 0:00:27  lr: 0.000037  loss: 0.4288 (0.4682)  time: 0.6205  data: 0.0001  max mem: 15824
[07:58:45.065673] Epoch: [5]  [320/345]  eta: 0:00:15  lr: 0.000037  loss: 0.4321 (0.4662)  time: 0.6196  data: 0.0001  max mem: 15824
[07:58:57.458369] Epoch: [5]  [340/345]  eta: 0:00:03  lr: 0.000037  loss: 0.4321 (0.4645)  time: 0.6196  data: 0.0001  max mem: 15824
[07:58:59.935658] Epoch: [5]  [344/345]  eta: 0:00:00  lr: 0.000037  loss: 0.4293 (0.4642)  time: 0.6195  data: 0.0001  max mem: 15824
[07:59:00.008146] Epoch: [5] Total time: 0:03:34 (0.6214 s / it)
[07:59:00.008490] Averaged stats: lr: 0.000037  loss: 0.4293 (0.4642)
[07:59:00.648565] Test:  [  0/345]  eta: 0:03:39  loss: 0.4007 (0.4007)  time: 0.6350  data: 0.4518  max mem: 15824
[07:59:02.441570] Test:  [ 10/345]  eta: 0:01:13  loss: 0.4224 (0.4138)  time: 0.2206  data: 0.0412  max mem: 15824
[07:59:04.235567] Test:  [ 20/345]  eta: 0:01:05  loss: 0.4179 (0.4165)  time: 0.1793  data: 0.0001  max mem: 15824
[07:59:06.028951] Test:  [ 30/345]  eta: 0:01:01  loss: 0.4077 (0.4136)  time: 0.1793  data: 0.0001  max mem: 15824
[07:59:07.827733] Test:  [ 40/345]  eta: 0:00:58  loss: 0.4184 (0.4163)  time: 0.1796  data: 0.0001  max mem: 15824
[07:59:09.629369] Test:  [ 50/345]  eta: 0:00:55  loss: 0.4222 (0.4197)  time: 0.1800  data: 0.0001  max mem: 15824
[07:59:11.434383] Test:  [ 60/345]  eta: 0:00:53  loss: 0.4155 (0.4185)  time: 0.1803  data: 0.0001  max mem: 15824
[07:59:13.240963] Test:  [ 70/345]  eta: 0:00:51  loss: 0.4130 (0.4187)  time: 0.1805  data: 0.0001  max mem: 15824
[07:59:15.051516] Test:  [ 80/345]  eta: 0:00:49  loss: 0.4241 (0.4214)  time: 0.1808  data: 0.0001  max mem: 15824
[07:59:16.867366] Test:  [ 90/345]  eta: 0:00:47  loss: 0.4138 (0.4201)  time: 0.1813  data: 0.0001  max mem: 15824
[07:59:18.687322] Test:  [100/345]  eta: 0:00:45  loss: 0.4138 (0.4205)  time: 0.1816  data: 0.0001  max mem: 15824
[07:59:20.509333] Test:  [110/345]  eta: 0:00:43  loss: 0.4317 (0.4214)  time: 0.1819  data: 0.0001  max mem: 15824
[07:59:22.334173] Test:  [120/345]  eta: 0:00:41  loss: 0.4310 (0.4218)  time: 0.1823  data: 0.0001  max mem: 15824
[07:59:24.162454] Test:  [130/345]  eta: 0:00:39  loss: 0.4181 (0.4210)  time: 0.1826  data: 0.0001  max mem: 15824
[07:59:25.993609] Test:  [140/345]  eta: 0:00:37  loss: 0.4090 (0.4209)  time: 0.1829  data: 0.0001  max mem: 15824
[07:59:27.827344] Test:  [150/345]  eta: 0:00:35  loss: 0.4090 (0.4204)  time: 0.1832  data: 0.0001  max mem: 15824
[07:59:29.665413] Test:  [160/345]  eta: 0:00:34  loss: 0.4336 (0.4213)  time: 0.1835  data: 0.0001  max mem: 15824
[07:59:31.509782] Test:  [170/345]  eta: 0:00:32  loss: 0.4358 (0.4215)  time: 0.1841  data: 0.0001  max mem: 15824
[07:59:33.357927] Test:  [180/345]  eta: 0:00:30  loss: 0.4263 (0.4209)  time: 0.1846  data: 0.0001  max mem: 15824
[07:59:35.207047] Test:  [190/345]  eta: 0:00:28  loss: 0.4107 (0.4208)  time: 0.1848  data: 0.0001  max mem: 15824
[07:59:37.057407] Test:  [200/345]  eta: 0:00:26  loss: 0.4064 (0.4199)  time: 0.1849  data: 0.0001  max mem: 15824
[07:59:38.912068] Test:  [210/345]  eta: 0:00:24  loss: 0.4051 (0.4189)  time: 0.1852  data: 0.0001  max mem: 15824
[07:59:40.772333] Test:  [220/345]  eta: 0:00:23  loss: 0.4122 (0.4189)  time: 0.1857  data: 0.0001  max mem: 15824
[07:59:42.634973] Test:  [230/345]  eta: 0:00:21  loss: 0.4160 (0.4191)  time: 0.1861  data: 0.0001  max mem: 15824
[07:59:44.499839] Test:  [240/345]  eta: 0:00:19  loss: 0.4227 (0.4194)  time: 0.1863  data: 0.0001  max mem: 15824
[07:59:46.368908] Test:  [250/345]  eta: 0:00:17  loss: 0.4164 (0.4195)  time: 0.1866  data: 0.0001  max mem: 15824
[07:59:48.241102] Test:  [260/345]  eta: 0:00:15  loss: 0.4242 (0.4198)  time: 0.1870  data: 0.0001  max mem: 15824
[07:59:50.116974] Test:  [270/345]  eta: 0:00:13  loss: 0.4242 (0.4196)  time: 0.1873  data: 0.0001  max mem: 15824
[07:59:51.997169] Test:  [280/345]  eta: 0:00:12  loss: 0.4117 (0.4198)  time: 0.1877  data: 0.0001  max mem: 15824
[07:59:53.879347] Test:  [290/345]  eta: 0:00:10  loss: 0.4023 (0.4190)  time: 0.1881  data: 0.0001  max mem: 15824
[07:59:55.765588] Test:  [300/345]  eta: 0:00:08  loss: 0.4064 (0.4196)  time: 0.1884  data: 0.0001  max mem: 15824
[07:59:57.656123] Test:  [310/345]  eta: 0:00:06  loss: 0.4212 (0.4195)  time: 0.1888  data: 0.0001  max mem: 15824
[07:59:59.549219] Test:  [320/345]  eta: 0:00:04  loss: 0.4108 (0.4194)  time: 0.1891  data: 0.0001  max mem: 15824
[08:00:01.448710] Test:  [330/345]  eta: 0:00:02  loss: 0.4173 (0.4194)  time: 0.1896  data: 0.0001  max mem: 15824
[08:00:03.349304] Test:  [340/345]  eta: 0:00:00  loss: 0.3971 (0.4188)  time: 0.1899  data: 0.0001  max mem: 15824
[08:00:04.110493] Test:  [344/345]  eta: 0:00:00  loss: 0.3971 (0.4191)  time: 0.1900  data: 0.0001  max mem: 15824
[08:00:04.182525] Test: Total time: 0:01:04 (0.1860 s / it)
[08:00:14.711367] Test:  [ 0/57]  eta: 0:00:33  loss: 0.5504 (0.5504)  time: 0.5809  data: 0.3998  max mem: 15824
[08:00:16.484546] Test:  [10/57]  eta: 0:00:10  loss: 0.4942 (0.5126)  time: 0.2139  data: 0.0364  max mem: 15824
[08:00:18.261877] Test:  [20/57]  eta: 0:00:07  loss: 0.4664 (0.4732)  time: 0.1774  data: 0.0001  max mem: 15824
[08:00:20.039600] Test:  [30/57]  eta: 0:00:05  loss: 0.3788 (0.4295)  time: 0.1777  data: 0.0001  max mem: 15824
[08:00:21.822612] Test:  [40/57]  eta: 0:00:03  loss: 0.3297 (0.4057)  time: 0.1780  data: 0.0001  max mem: 15824
[08:00:23.613145] Test:  [50/57]  eta: 0:00:01  loss: 0.3297 (0.3994)  time: 0.1786  data: 0.0001  max mem: 15824
[08:00:24.583905] Test:  [56/57]  eta: 0:00:00  loss: 0.3790 (0.4086)  time: 0.1737  data: 0.0000  max mem: 15824
[08:00:24.658602] Test: Total time: 0:00:10 (0.1847 s / it)
[08:00:26.368316] Dice score of the network on the train images: 0.694184, val images: 0.729422
[08:00:26.368546] saving best_prec_model_0 @ epoch 5
[08:00:27.597401] saving best_dice_model_0 @ epoch 5
[08:00:28.775553] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:00:29.814713] Epoch: [6]  [  0/345]  eta: 0:05:58  lr: 0.000038  loss: 0.4076 (0.4076)  time: 1.0381  data: 0.4128  max mem: 15824
[08:00:42.122825] Epoch: [6]  [ 20/345]  eta: 0:03:26  lr: 0.000038  loss: 0.4129 (0.4160)  time: 0.6153  data: 0.0001  max mem: 15824
[08:00:54.453249] Epoch: [6]  [ 40/345]  eta: 0:03:10  lr: 0.000038  loss: 0.4020 (0.4145)  time: 0.6165  data: 0.0001  max mem: 15824
[08:01:06.815166] Epoch: [6]  [ 60/345]  eta: 0:02:57  lr: 0.000039  loss: 0.4169 (0.4172)  time: 0.6180  data: 0.0001  max mem: 15824
[08:01:19.188798] Epoch: [6]  [ 80/345]  eta: 0:02:44  lr: 0.000039  loss: 0.4115 (0.4167)  time: 0.6186  data: 0.0001  max mem: 15824
[08:01:31.718754] Epoch: [6]  [100/345]  eta: 0:02:32  lr: 0.000039  loss: 0.4274 (0.4179)  time: 0.6264  data: 0.0001  max mem: 15824
[08:01:44.138186] Epoch: [6]  [120/345]  eta: 0:02:20  lr: 0.000040  loss: 0.4334 (0.4200)  time: 0.6209  data: 0.0001  max mem: 15824
[08:01:56.562334] Epoch: [6]  [140/345]  eta: 0:02:07  lr: 0.000040  loss: 0.4047 (0.4189)  time: 0.6212  data: 0.0001  max mem: 15824
[08:02:08.993076] Epoch: [6]  [160/345]  eta: 0:01:55  lr: 0.000040  loss: 0.4190 (0.4184)  time: 0.6215  data: 0.0001  max mem: 15824
[08:02:21.394206] Epoch: [6]  [180/345]  eta: 0:01:42  lr: 0.000041  loss: 0.4133 (0.4186)  time: 0.6200  data: 0.0001  max mem: 15824
[08:02:33.793536] Epoch: [6]  [200/345]  eta: 0:01:30  lr: 0.000041  loss: 0.3890 (0.4169)  time: 0.6199  data: 0.0001  max mem: 15824
[08:02:46.190916] Epoch: [6]  [220/345]  eta: 0:01:17  lr: 0.000041  loss: 0.3854 (0.4146)  time: 0.6198  data: 0.0001  max mem: 15824
[08:02:58.568362] Epoch: [6]  [240/345]  eta: 0:01:05  lr: 0.000042  loss: 0.3913 (0.4128)  time: 0.6188  data: 0.0001  max mem: 15824
[08:03:10.939516] Epoch: [6]  [260/345]  eta: 0:00:52  lr: 0.000042  loss: 0.3976 (0.4111)  time: 0.6185  data: 0.0001  max mem: 15824
[08:03:23.315839] Epoch: [6]  [280/345]  eta: 0:00:40  lr: 0.000043  loss: 0.3911 (0.4101)  time: 0.6188  data: 0.0001  max mem: 15824
[08:03:35.715643] Epoch: [6]  [300/345]  eta: 0:00:27  lr: 0.000043  loss: 0.3761 (0.4089)  time: 0.6199  data: 0.0001  max mem: 15824
[08:03:48.112178] Epoch: [6]  [320/345]  eta: 0:00:15  lr: 0.000043  loss: 0.3994 (0.4082)  time: 0.6198  data: 0.0001  max mem: 15824
[08:04:00.504154] Epoch: [6]  [340/345]  eta: 0:00:03  lr: 0.000044  loss: 0.4036 (0.4074)  time: 0.6196  data: 0.0001  max mem: 15824
[08:04:02.981750] Epoch: [6]  [344/345]  eta: 0:00:00  lr: 0.000044  loss: 0.4078 (0.4075)  time: 0.6194  data: 0.0001  max mem: 15824
[08:04:03.057909] Epoch: [6] Total time: 0:03:34 (0.6211 s / it)
[08:04:03.058285] Averaged stats: lr: 0.000044  loss: 0.4078 (0.4075)
[08:04:03.710196] Test:  [  0/345]  eta: 0:03:42  loss: 0.3726 (0.3726)  time: 0.6455  data: 0.4644  max mem: 15824
[08:04:05.502032] Test:  [ 10/345]  eta: 0:01:14  loss: 0.3731 (0.3740)  time: 0.2215  data: 0.0423  max mem: 15824
[08:04:07.296605] Test:  [ 20/345]  eta: 0:01:05  loss: 0.3866 (0.3886)  time: 0.1792  data: 0.0001  max mem: 15824
[08:04:09.091749] Test:  [ 30/345]  eta: 0:01:01  loss: 0.3971 (0.3935)  time: 0.1794  data: 0.0001  max mem: 15824
[08:04:10.889865] Test:  [ 40/345]  eta: 0:00:58  loss: 0.4018 (0.3989)  time: 0.1796  data: 0.0001  max mem: 15824
[08:04:12.691620] Test:  [ 50/345]  eta: 0:00:55  loss: 0.4186 (0.4037)  time: 0.1799  data: 0.0001  max mem: 15824
[08:04:14.493550] Test:  [ 60/345]  eta: 0:00:53  loss: 0.4131 (0.4015)  time: 0.1801  data: 0.0001  max mem: 15824
[08:04:16.298812] Test:  [ 70/345]  eta: 0:00:51  loss: 0.3762 (0.3978)  time: 0.1803  data: 0.0001  max mem: 15824
[08:04:18.113231] Test:  [ 80/345]  eta: 0:00:49  loss: 0.3867 (0.3965)  time: 0.1809  data: 0.0001  max mem: 15824
[08:04:19.928579] Test:  [ 90/345]  eta: 0:00:47  loss: 0.4014 (0.3983)  time: 0.1814  data: 0.0001  max mem: 15824
[08:04:21.746685] Test:  [100/345]  eta: 0:00:45  loss: 0.3912 (0.3963)  time: 0.1816  data: 0.0001  max mem: 15824
[08:04:23.569390] Test:  [110/345]  eta: 0:00:43  loss: 0.3912 (0.3972)  time: 0.1820  data: 0.0001  max mem: 15824
[08:04:25.395389] Test:  [120/345]  eta: 0:00:41  loss: 0.3928 (0.3963)  time: 0.1824  data: 0.0001  max mem: 15824
[08:04:27.228135] Test:  [130/345]  eta: 0:00:39  loss: 0.3832 (0.3957)  time: 0.1829  data: 0.0001  max mem: 15824
[08:04:29.058223] Test:  [140/345]  eta: 0:00:37  loss: 0.3795 (0.3948)  time: 0.1831  data: 0.0001  max mem: 15824
[08:04:30.893691] Test:  [150/345]  eta: 0:00:35  loss: 0.4073 (0.3961)  time: 0.1832  data: 0.0001  max mem: 15824
[08:04:32.733551] Test:  [160/345]  eta: 0:00:34  loss: 0.4134 (0.3956)  time: 0.1837  data: 0.0001  max mem: 15824
[08:04:34.575901] Test:  [170/345]  eta: 0:00:32  loss: 0.3752 (0.3953)  time: 0.1840  data: 0.0001  max mem: 15824
[08:04:36.422040] Test:  [180/345]  eta: 0:00:30  loss: 0.3991 (0.3958)  time: 0.1844  data: 0.0001  max mem: 15824
[08:04:38.273087] Test:  [190/345]  eta: 0:00:28  loss: 0.3871 (0.3952)  time: 0.1848  data: 0.0001  max mem: 15824
[08:04:40.126904] Test:  [200/345]  eta: 0:00:26  loss: 0.3680 (0.3947)  time: 0.1852  data: 0.0001  max mem: 15824
[08:04:41.981371] Test:  [210/345]  eta: 0:00:24  loss: 0.3698 (0.3943)  time: 0.1854  data: 0.0001  max mem: 15824
[08:04:43.841511] Test:  [220/345]  eta: 0:00:23  loss: 0.3727 (0.3950)  time: 0.1857  data: 0.0001  max mem: 15824
[08:04:45.702187] Test:  [230/345]  eta: 0:00:21  loss: 0.3688 (0.3940)  time: 0.1860  data: 0.0001  max mem: 15824
[08:04:47.569427] Test:  [240/345]  eta: 0:00:19  loss: 0.3798 (0.3940)  time: 0.1863  data: 0.0001  max mem: 15824
[08:04:49.443519] Test:  [250/345]  eta: 0:00:17  loss: 0.3956 (0.3937)  time: 0.1870  data: 0.0001  max mem: 15824
[08:04:51.318986] Test:  [260/345]  eta: 0:00:15  loss: 0.3949 (0.3942)  time: 0.1874  data: 0.0001  max mem: 15824
[08:04:53.198161] Test:  [270/345]  eta: 0:00:13  loss: 0.3908 (0.3946)  time: 0.1877  data: 0.0001  max mem: 15824
[08:04:55.080185] Test:  [280/345]  eta: 0:00:12  loss: 0.4094 (0.3953)  time: 0.1880  data: 0.0001  max mem: 15824
[08:04:56.966883] Test:  [290/345]  eta: 0:00:10  loss: 0.4094 (0.3958)  time: 0.1884  data: 0.0001  max mem: 15824
[08:04:58.855179] Test:  [300/345]  eta: 0:00:08  loss: 0.4074 (0.3962)  time: 0.1887  data: 0.0001  max mem: 15824
[08:05:00.750301] Test:  [310/345]  eta: 0:00:06  loss: 0.3843 (0.3964)  time: 0.1891  data: 0.0001  max mem: 15824
[08:05:02.644562] Test:  [320/345]  eta: 0:00:04  loss: 0.3977 (0.3968)  time: 0.1894  data: 0.0001  max mem: 15824
[08:05:04.542930] Test:  [330/345]  eta: 0:00:02  loss: 0.3991 (0.3963)  time: 0.1896  data: 0.0001  max mem: 15824
[08:05:06.442344] Test:  [340/345]  eta: 0:00:00  loss: 0.3957 (0.3964)  time: 0.1898  data: 0.0001  max mem: 15824
[08:05:07.204535] Test:  [344/345]  eta: 0:00:00  loss: 0.3845 (0.3962)  time: 0.1899  data: 0.0001  max mem: 15824
[08:05:07.263152] Test: Total time: 0:01:04 (0.1861 s / it)
[08:05:17.794971] Test:  [ 0/57]  eta: 0:00:31  loss: 0.5444 (0.5444)  time: 0.5546  data: 0.3706  max mem: 15824
[08:05:19.567488] Test:  [10/57]  eta: 0:00:09  loss: 0.5097 (0.5128)  time: 0.2115  data: 0.0338  max mem: 15824
[08:05:21.342752] Test:  [20/57]  eta: 0:00:07  loss: 0.4410 (0.4654)  time: 0.1773  data: 0.0001  max mem: 15824
[08:05:23.122848] Test:  [30/57]  eta: 0:00:05  loss: 0.3661 (0.4209)  time: 0.1777  data: 0.0001  max mem: 15824
[08:05:24.907769] Test:  [40/57]  eta: 0:00:03  loss: 0.3161 (0.3981)  time: 0.1782  data: 0.0001  max mem: 15824
[08:05:26.698030] Test:  [50/57]  eta: 0:00:01  loss: 0.3341 (0.3964)  time: 0.1787  data: 0.0001  max mem: 15824
[08:05:27.670411] Test:  [56/57]  eta: 0:00:00  loss: 0.3696 (0.4090)  time: 0.1738  data: 0.0000  max mem: 15824
[08:05:27.733288] Test: Total time: 0:00:10 (0.1841 s / it)
[08:05:29.466301] Dice score of the network on the train images: 0.716639, val images: 0.730812
[08:05:29.466515] saving best_prec_model_0 @ epoch 6
[08:05:30.557314] saving best_dice_model_0 @ epoch 6
[08:05:32.151344] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:05:33.182617] Epoch: [7]  [  0/345]  eta: 0:05:55  lr: 0.000044  loss: 0.4039 (0.4039)  time: 1.0300  data: 0.4036  max mem: 15824
[08:05:45.483215] Epoch: [7]  [ 20/345]  eta: 0:03:26  lr: 0.000044  loss: 0.4006 (0.3942)  time: 0.6150  data: 0.0001  max mem: 15824
[08:05:57.818237] Epoch: [7]  [ 40/345]  eta: 0:03:10  lr: 0.000044  loss: 0.3674 (0.3819)  time: 0.6167  data: 0.0001  max mem: 15824
[08:06:10.175262] Epoch: [7]  [ 60/345]  eta: 0:02:57  lr: 0.000045  loss: 0.3696 (0.3807)  time: 0.6178  data: 0.0001  max mem: 15824
[08:06:22.558140] Epoch: [7]  [ 80/345]  eta: 0:02:44  lr: 0.000045  loss: 0.3698 (0.3784)  time: 0.6191  data: 0.0001  max mem: 15824
[08:06:34.961984] Epoch: [7]  [100/345]  eta: 0:02:32  lr: 0.000046  loss: 0.3659 (0.3780)  time: 0.6201  data: 0.0001  max mem: 15824
[08:06:47.383508] Epoch: [7]  [120/345]  eta: 0:02:19  lr: 0.000046  loss: 0.3489 (0.3757)  time: 0.6210  data: 0.0001  max mem: 15824
[08:06:59.820160] Epoch: [7]  [140/345]  eta: 0:02:07  lr: 0.000046  loss: 0.3681 (0.3748)  time: 0.6218  data: 0.0001  max mem: 15824
[08:07:12.251812] Epoch: [7]  [160/345]  eta: 0:01:55  lr: 0.000047  loss: 0.3694 (0.3745)  time: 0.6215  data: 0.0001  max mem: 15824
[08:07:24.679386] Epoch: [7]  [180/345]  eta: 0:01:42  lr: 0.000047  loss: 0.3719 (0.3747)  time: 0.6213  data: 0.0001  max mem: 15824
[08:07:37.097085] Epoch: [7]  [200/345]  eta: 0:01:30  lr: 0.000047  loss: 0.3288 (0.3716)  time: 0.6208  data: 0.0001  max mem: 15824
[08:07:49.524204] Epoch: [7]  [220/345]  eta: 0:01:17  lr: 0.000048  loss: 0.3912 (0.3728)  time: 0.6213  data: 0.0001  max mem: 15824
[08:08:01.921723] Epoch: [7]  [240/345]  eta: 0:01:05  lr: 0.000048  loss: 0.3655 (0.3722)  time: 0.6198  data: 0.0001  max mem: 15824
[08:08:14.297189] Epoch: [7]  [260/345]  eta: 0:00:52  lr: 0.000048  loss: 0.3616 (0.3719)  time: 0.6187  data: 0.0001  max mem: 15824
[08:08:26.664775] Epoch: [7]  [280/345]  eta: 0:00:40  lr: 0.000049  loss: 0.3708 (0.3718)  time: 0.6183  data: 0.0001  max mem: 15824
[08:08:39.047145] Epoch: [7]  [300/345]  eta: 0:00:27  lr: 0.000049  loss: 0.3515 (0.3707)  time: 0.6191  data: 0.0001  max mem: 15824
[08:08:51.456968] Epoch: [7]  [320/345]  eta: 0:00:15  lr: 0.000050  loss: 0.3487 (0.3699)  time: 0.6204  data: 0.0001  max mem: 15824
[08:09:03.825893] Epoch: [7]  [340/345]  eta: 0:00:03  lr: 0.000050  loss: 0.3604 (0.3692)  time: 0.6184  data: 0.0001  max mem: 15824
[08:09:06.300377] Epoch: [7]  [344/345]  eta: 0:00:00  lr: 0.000050  loss: 0.3655 (0.3693)  time: 0.6185  data: 0.0001  max mem: 15824
[08:09:06.373888] Epoch: [7] Total time: 0:03:34 (0.6209 s / it)
[08:09:06.374555] Averaged stats: lr: 0.000050  loss: 0.3655 (0.3693)
[08:09:06.979193] Test:  [  0/345]  eta: 0:03:26  loss: 0.3321 (0.3321)  time: 0.5992  data: 0.4180  max mem: 15824
[08:09:08.771667] Test:  [ 10/345]  eta: 0:01:12  loss: 0.3456 (0.3422)  time: 0.2173  data: 0.0381  max mem: 15824
[08:09:10.567693] Test:  [ 20/345]  eta: 0:01:04  loss: 0.3285 (0.3389)  time: 0.1793  data: 0.0001  max mem: 15824
[08:09:12.366142] Test:  [ 30/345]  eta: 0:01:00  loss: 0.3285 (0.3409)  time: 0.1797  data: 0.0001  max mem: 15824
[08:09:14.168034] Test:  [ 40/345]  eta: 0:00:57  loss: 0.3498 (0.3424)  time: 0.1800  data: 0.0001  max mem: 15824
[08:09:15.974511] Test:  [ 50/345]  eta: 0:00:55  loss: 0.3498 (0.3447)  time: 0.1804  data: 0.0001  max mem: 15824
[08:09:17.778266] Test:  [ 60/345]  eta: 0:00:53  loss: 0.3548 (0.3453)  time: 0.1804  data: 0.0001  max mem: 15824
[08:09:19.587979] Test:  [ 70/345]  eta: 0:00:51  loss: 0.3548 (0.3476)  time: 0.1806  data: 0.0001  max mem: 15824
[08:09:21.399244] Test:  [ 80/345]  eta: 0:00:49  loss: 0.3496 (0.3470)  time: 0.1810  data: 0.0001  max mem: 15824
[08:09:23.216778] Test:  [ 90/345]  eta: 0:00:47  loss: 0.3380 (0.3452)  time: 0.1814  data: 0.0001  max mem: 15824
[08:09:25.036447] Test:  [100/345]  eta: 0:00:45  loss: 0.3204 (0.3440)  time: 0.1818  data: 0.0001  max mem: 15824
[08:09:26.858931] Test:  [110/345]  eta: 0:00:43  loss: 0.3395 (0.3442)  time: 0.1820  data: 0.0001  max mem: 15824
[08:09:28.688533] Test:  [120/345]  eta: 0:00:41  loss: 0.3470 (0.3444)  time: 0.1825  data: 0.0001  max mem: 15824
[08:09:30.520805] Test:  [130/345]  eta: 0:00:39  loss: 0.3362 (0.3424)  time: 0.1830  data: 0.0001  max mem: 15824
[08:09:32.355854] Test:  [140/345]  eta: 0:00:37  loss: 0.3236 (0.3406)  time: 0.1833  data: 0.0001  max mem: 15824
[08:09:34.192513] Test:  [150/345]  eta: 0:00:35  loss: 0.3242 (0.3399)  time: 0.1835  data: 0.0001  max mem: 15824
[08:09:36.033200] Test:  [160/345]  eta: 0:00:34  loss: 0.3361 (0.3409)  time: 0.1838  data: 0.0001  max mem: 15824
[08:09:37.877937] Test:  [170/345]  eta: 0:00:32  loss: 0.3441 (0.3412)  time: 0.1842  data: 0.0001  max mem: 15824
[08:09:39.727367] Test:  [180/345]  eta: 0:00:30  loss: 0.3336 (0.3405)  time: 0.1846  data: 0.0001  max mem: 15824
[08:09:41.578546] Test:  [190/345]  eta: 0:00:28  loss: 0.3352 (0.3414)  time: 0.1850  data: 0.0001  max mem: 15824
[08:09:43.433355] Test:  [200/345]  eta: 0:00:26  loss: 0.3586 (0.3418)  time: 0.1852  data: 0.0001  max mem: 15824
[08:09:45.290797] Test:  [210/345]  eta: 0:00:24  loss: 0.3442 (0.3417)  time: 0.1855  data: 0.0001  max mem: 15824
[08:09:47.152254] Test:  [220/345]  eta: 0:00:23  loss: 0.3499 (0.3425)  time: 0.1859  data: 0.0001  max mem: 15824
[08:09:49.017180] Test:  [230/345]  eta: 0:00:21  loss: 0.3606 (0.3428)  time: 0.1863  data: 0.0001  max mem: 15824
[08:09:50.885147] Test:  [240/345]  eta: 0:00:19  loss: 0.3505 (0.3431)  time: 0.1866  data: 0.0001  max mem: 15824
[08:09:52.759615] Test:  [250/345]  eta: 0:00:17  loss: 0.3469 (0.3430)  time: 0.1871  data: 0.0001  max mem: 15824
[08:09:54.638067] Test:  [260/345]  eta: 0:00:15  loss: 0.3374 (0.3431)  time: 0.1876  data: 0.0001  max mem: 15824
[08:09:56.514830] Test:  [270/345]  eta: 0:00:13  loss: 0.3490 (0.3432)  time: 0.1877  data: 0.0001  max mem: 15824
[08:09:58.396149] Test:  [280/345]  eta: 0:00:12  loss: 0.3511 (0.3440)  time: 0.1878  data: 0.0001  max mem: 15824
[08:10:00.284541] Test:  [290/345]  eta: 0:00:10  loss: 0.3531 (0.3440)  time: 0.1884  data: 0.0001  max mem: 15824
[08:10:02.176502] Test:  [300/345]  eta: 0:00:08  loss: 0.3411 (0.3443)  time: 0.1890  data: 0.0001  max mem: 15824
[08:10:04.072053] Test:  [310/345]  eta: 0:00:06  loss: 0.3411 (0.3445)  time: 0.1893  data: 0.0001  max mem: 15824
[08:10:05.969962] Test:  [320/345]  eta: 0:00:04  loss: 0.3469 (0.3447)  time: 0.1896  data: 0.0001  max mem: 15824
[08:10:07.866681] Test:  [330/345]  eta: 0:00:02  loss: 0.3424 (0.3445)  time: 0.1897  data: 0.0001  max mem: 15824
[08:10:09.767144] Test:  [340/345]  eta: 0:00:00  loss: 0.3327 (0.3450)  time: 0.1898  data: 0.0001  max mem: 15824
[08:10:10.528149] Test:  [344/345]  eta: 0:00:00  loss: 0.3362 (0.3452)  time: 0.1899  data: 0.0001  max mem: 15824
[08:10:10.605166] Test: Total time: 0:01:04 (0.1862 s / it)
[08:10:21.073894] Test:  [ 0/57]  eta: 0:00:32  loss: 0.4717 (0.4717)  time: 0.5667  data: 0.3867  max mem: 15824
[08:10:22.846475] Test:  [10/57]  eta: 0:00:09  loss: 0.4682 (0.4690)  time: 0.2126  data: 0.0352  max mem: 15824
[08:10:24.622460] Test:  [20/57]  eta: 0:00:07  loss: 0.4300 (0.4367)  time: 0.1773  data: 0.0001  max mem: 15824
[08:10:26.404056] Test:  [30/57]  eta: 0:00:05  loss: 0.3422 (0.3960)  time: 0.1778  data: 0.0001  max mem: 15824
[08:10:28.189500] Test:  [40/57]  eta: 0:00:03  loss: 0.3126 (0.3750)  time: 0.1783  data: 0.0001  max mem: 15824
[08:10:29.979869] Test:  [50/57]  eta: 0:00:01  loss: 0.3093 (0.3735)  time: 0.1787  data: 0.0001  max mem: 15824
[08:10:30.952370] Test:  [56/57]  eta: 0:00:00  loss: 0.3463 (0.3859)  time: 0.1738  data: 0.0001  max mem: 15824
[08:10:31.023808] Test: Total time: 0:00:10 (0.1845 s / it)
[08:10:32.719114] Dice score of the network on the train images: 0.732326, val images: 0.739234
[08:10:32.719338] saving best_prec_model_0 @ epoch 7
[08:10:34.178060] saving best_dice_model_0 @ epoch 7
[08:10:35.246376] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:10:36.322633] Epoch: [8]  [  0/345]  eta: 0:06:10  lr: 0.000050  loss: 0.3224 (0.3224)  time: 1.0751  data: 0.4541  max mem: 15824
[08:10:48.626199] Epoch: [8]  [ 20/345]  eta: 0:03:27  lr: 0.000050  loss: 0.3362 (0.3454)  time: 0.6151  data: 0.0001  max mem: 15824
[08:11:00.963786] Epoch: [8]  [ 40/345]  eta: 0:03:11  lr: 0.000051  loss: 0.3394 (0.3457)  time: 0.6168  data: 0.0001  max mem: 15824
[08:11:13.313706] Epoch: [8]  [ 60/345]  eta: 0:02:57  lr: 0.000051  loss: 0.3382 (0.3478)  time: 0.6174  data: 0.0001  max mem: 15824
[08:11:25.686527] Epoch: [8]  [ 80/345]  eta: 0:02:45  lr: 0.000051  loss: 0.3438 (0.3456)  time: 0.6186  data: 0.0001  max mem: 15824
[08:11:38.074981] Epoch: [8]  [100/345]  eta: 0:02:32  lr: 0.000052  loss: 0.3256 (0.3437)  time: 0.6194  data: 0.0001  max mem: 15824
[08:11:50.504806] Epoch: [8]  [120/345]  eta: 0:02:19  lr: 0.000052  loss: 0.3486 (0.3439)  time: 0.6214  data: 0.0001  max mem: 15824
[08:12:02.918037] Epoch: [8]  [140/345]  eta: 0:02:07  lr: 0.000053  loss: 0.3333 (0.3428)  time: 0.6206  data: 0.0001  max mem: 15824
[08:12:15.323554] Epoch: [8]  [160/345]  eta: 0:01:54  lr: 0.000053  loss: 0.3430 (0.3425)  time: 0.6202  data: 0.0001  max mem: 15824
[08:12:27.722109] Epoch: [8]  [180/345]  eta: 0:01:42  lr: 0.000053  loss: 0.3529 (0.3432)  time: 0.6199  data: 0.0001  max mem: 15824
[08:12:40.114002] Epoch: [8]  [200/345]  eta: 0:01:30  lr: 0.000054  loss: 0.3417 (0.3426)  time: 0.6195  data: 0.0001  max mem: 15824
[08:12:52.528459] Epoch: [8]  [220/345]  eta: 0:01:17  lr: 0.000054  loss: 0.3158 (0.3412)  time: 0.6207  data: 0.0001  max mem: 15824
[08:13:04.927206] Epoch: [8]  [240/345]  eta: 0:01:05  lr: 0.000054  loss: 0.3307 (0.3411)  time: 0.6199  data: 0.0001  max mem: 15824
[08:13:17.326415] Epoch: [8]  [260/345]  eta: 0:00:52  lr: 0.000055  loss: 0.3225 (0.3411)  time: 0.6199  data: 0.0001  max mem: 15824
[08:13:29.712150] Epoch: [8]  [280/345]  eta: 0:00:40  lr: 0.000055  loss: 0.3370 (0.3413)  time: 0.6192  data: 0.0001  max mem: 15824
[08:13:42.117022] Epoch: [8]  [300/345]  eta: 0:00:27  lr: 0.000055  loss: 0.3304 (0.3412)  time: 0.6202  data: 0.0001  max mem: 15824
[08:13:54.516872] Epoch: [8]  [320/345]  eta: 0:00:15  lr: 0.000056  loss: 0.3374 (0.3406)  time: 0.6199  data: 0.0001  max mem: 15824
[08:14:06.906079] Epoch: [8]  [340/345]  eta: 0:00:03  lr: 0.000056  loss: 0.3412 (0.3407)  time: 0.6194  data: 0.0001  max mem: 15824
[08:14:09.383056] Epoch: [8]  [344/345]  eta: 0:00:00  lr: 0.000056  loss: 0.3412 (0.3407)  time: 0.6192  data: 0.0001  max mem: 15824
[08:14:09.459035] Epoch: [8] Total time: 0:03:34 (0.6209 s / it)
[08:14:09.459275] Averaged stats: lr: 0.000056  loss: 0.3412 (0.3407)
[08:14:10.057408] Test:  [  0/345]  eta: 0:03:24  loss: 0.3300 (0.3300)  time: 0.5925  data: 0.4105  max mem: 15824
[08:14:11.848800] Test:  [ 10/345]  eta: 0:01:12  loss: 0.3040 (0.3107)  time: 0.2166  data: 0.0374  max mem: 15824
[08:14:13.639004] Test:  [ 20/345]  eta: 0:01:04  loss: 0.3040 (0.3097)  time: 0.1790  data: 0.0001  max mem: 15824
[08:14:15.432265] Test:  [ 30/345]  eta: 0:01:00  loss: 0.3155 (0.3108)  time: 0.1791  data: 0.0001  max mem: 15824
[08:14:17.233484] Test:  [ 40/345]  eta: 0:00:57  loss: 0.3079 (0.3122)  time: 0.1797  data: 0.0001  max mem: 15824
[08:14:19.035680] Test:  [ 50/345]  eta: 0:00:55  loss: 0.3189 (0.3155)  time: 0.1801  data: 0.0001  max mem: 15824
[08:14:20.836924] Test:  [ 60/345]  eta: 0:00:53  loss: 0.3195 (0.3156)  time: 0.1801  data: 0.0001  max mem: 15824
[08:14:22.645088] Test:  [ 70/345]  eta: 0:00:51  loss: 0.3133 (0.3158)  time: 0.1804  data: 0.0001  max mem: 15824
[08:14:24.459323] Test:  [ 80/345]  eta: 0:00:49  loss: 0.3214 (0.3165)  time: 0.1811  data: 0.0001  max mem: 15824
[08:14:26.276220] Test:  [ 90/345]  eta: 0:00:47  loss: 0.3017 (0.3149)  time: 0.1815  data: 0.0001  max mem: 15824
[08:14:28.093898] Test:  [100/345]  eta: 0:00:45  loss: 0.3037 (0.3152)  time: 0.1817  data: 0.0001  max mem: 15824
[08:14:29.916240] Test:  [110/345]  eta: 0:00:43  loss: 0.3166 (0.3158)  time: 0.1819  data: 0.0001  max mem: 15824
[08:14:31.741762] Test:  [120/345]  eta: 0:00:41  loss: 0.3166 (0.3159)  time: 0.1823  data: 0.0001  max mem: 15824
[08:14:33.571316] Test:  [130/345]  eta: 0:00:39  loss: 0.3159 (0.3162)  time: 0.1827  data: 0.0001  max mem: 15824
[08:14:35.401645] Test:  [140/345]  eta: 0:00:37  loss: 0.3122 (0.3171)  time: 0.1829  data: 0.0001  max mem: 15824
[08:14:37.237016] Test:  [150/345]  eta: 0:00:35  loss: 0.3101 (0.3168)  time: 0.1832  data: 0.0001  max mem: 15824
[08:14:39.077159] Test:  [160/345]  eta: 0:00:34  loss: 0.3074 (0.3171)  time: 0.1837  data: 0.0001  max mem: 15824
[08:14:40.919629] Test:  [170/345]  eta: 0:00:32  loss: 0.3175 (0.3173)  time: 0.1841  data: 0.0001  max mem: 15824
[08:14:42.763778] Test:  [180/345]  eta: 0:00:30  loss: 0.3219 (0.3182)  time: 0.1843  data: 0.0001  max mem: 15824
[08:14:44.613379] Test:  [190/345]  eta: 0:00:28  loss: 0.3138 (0.3177)  time: 0.1846  data: 0.0001  max mem: 15824
[08:14:46.468715] Test:  [200/345]  eta: 0:00:26  loss: 0.3061 (0.3172)  time: 0.1852  data: 0.0001  max mem: 15824
[08:14:48.326349] Test:  [210/345]  eta: 0:00:24  loss: 0.3084 (0.3179)  time: 0.1856  data: 0.0001  max mem: 15824
[08:14:50.186833] Test:  [220/345]  eta: 0:00:23  loss: 0.3162 (0.3178)  time: 0.1858  data: 0.0001  max mem: 15824
[08:14:52.050281] Test:  [230/345]  eta: 0:00:21  loss: 0.3097 (0.3173)  time: 0.1861  data: 0.0001  max mem: 15824
[08:14:53.916697] Test:  [240/345]  eta: 0:00:19  loss: 0.3108 (0.3169)  time: 0.1864  data: 0.0001  max mem: 15824
[08:14:55.786654] Test:  [250/345]  eta: 0:00:17  loss: 0.3108 (0.3167)  time: 0.1868  data: 0.0001  max mem: 15824
[08:14:57.662407] Test:  [260/345]  eta: 0:00:15  loss: 0.3094 (0.3164)  time: 0.1872  data: 0.0001  max mem: 15824
[08:14:59.540692] Test:  [270/345]  eta: 0:00:13  loss: 0.3020 (0.3162)  time: 0.1876  data: 0.0001  max mem: 15824
[08:15:01.424458] Test:  [280/345]  eta: 0:00:12  loss: 0.3198 (0.3164)  time: 0.1880  data: 0.0001  max mem: 15824
[08:15:03.307836] Test:  [290/345]  eta: 0:00:10  loss: 0.3251 (0.3165)  time: 0.1883  data: 0.0001  max mem: 15824
[08:15:05.196028] Test:  [300/345]  eta: 0:00:08  loss: 0.3296 (0.3167)  time: 0.1885  data: 0.0001  max mem: 15824
[08:15:07.088024] Test:  [310/345]  eta: 0:00:06  loss: 0.3201 (0.3166)  time: 0.1889  data: 0.0001  max mem: 15824
[08:15:08.984245] Test:  [320/345]  eta: 0:00:04  loss: 0.3178 (0.3172)  time: 0.1893  data: 0.0001  max mem: 15824
[08:15:10.883337] Test:  [330/345]  eta: 0:00:02  loss: 0.3155 (0.3170)  time: 0.1897  data: 0.0001  max mem: 15824
[08:15:12.783257] Test:  [340/345]  eta: 0:00:00  loss: 0.3139 (0.3169)  time: 0.1899  data: 0.0001  max mem: 15824
[08:15:13.545107] Test:  [344/345]  eta: 0:00:00  loss: 0.3139 (0.3168)  time: 0.1900  data: 0.0001  max mem: 15824
[08:15:13.622527] Test: Total time: 0:01:04 (0.1860 s / it)
[08:15:24.043825] Test:  [ 0/57]  eta: 0:00:31  loss: 0.4632 (0.4632)  time: 0.5529  data: 0.3735  max mem: 15824
[08:15:25.815408] Test:  [10/57]  eta: 0:00:09  loss: 0.4572 (0.4608)  time: 0.2112  data: 0.0340  max mem: 15824
[08:15:27.589319] Test:  [20/57]  eta: 0:00:07  loss: 0.4249 (0.4244)  time: 0.1772  data: 0.0001  max mem: 15824
[08:15:29.367664] Test:  [30/57]  eta: 0:00:05  loss: 0.3100 (0.3787)  time: 0.1775  data: 0.0001  max mem: 15824
[08:15:31.150830] Test:  [40/57]  eta: 0:00:03  loss: 0.2638 (0.3516)  time: 0.1780  data: 0.0001  max mem: 15824
[08:15:32.941087] Test:  [50/57]  eta: 0:00:01  loss: 0.2729 (0.3473)  time: 0.1786  data: 0.0001  max mem: 15824
[08:15:33.913728] Test:  [56/57]  eta: 0:00:00  loss: 0.3187 (0.3613)  time: 0.1737  data: 0.0000  max mem: 15824
[08:15:33.980065] Test: Total time: 0:00:10 (0.1840 s / it)
[08:15:35.704450] Dice score of the network on the train images: 0.737956, val images: 0.760464
[08:15:35.704686] saving best_prec_model_0 @ epoch 8
[08:15:36.812118] saving best_dice_model_0 @ epoch 8
[08:15:37.823322] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:15:38.831264] Epoch: [9]  [  0/345]  eta: 0:05:47  lr: 0.000056  loss: 0.3206 (0.3206)  time: 1.0067  data: 0.3807  max mem: 15824
[08:15:51.124263] Epoch: [9]  [ 20/345]  eta: 0:03:25  lr: 0.000057  loss: 0.3299 (0.3312)  time: 0.6146  data: 0.0001  max mem: 15824
[08:16:03.470470] Epoch: [9]  [ 40/345]  eta: 0:03:10  lr: 0.000057  loss: 0.3232 (0.3310)  time: 0.6173  data: 0.0001  max mem: 15824
[08:16:15.845915] Epoch: [9]  [ 60/345]  eta: 0:02:57  lr: 0.000057  loss: 0.3351 (0.3347)  time: 0.6187  data: 0.0001  max mem: 15824
[08:16:28.234515] Epoch: [9]  [ 80/345]  eta: 0:02:44  lr: 0.000058  loss: 0.3218 (0.3314)  time: 0.6194  data: 0.0001  max mem: 15824
[08:16:40.617106] Epoch: [9]  [100/345]  eta: 0:02:32  lr: 0.000058  loss: 0.3085 (0.3277)  time: 0.6191  data: 0.0001  max mem: 15824
[08:16:53.030069] Epoch: [9]  [120/345]  eta: 0:02:19  lr: 0.000058  loss: 0.3075 (0.3261)  time: 0.6206  data: 0.0001  max mem: 15824
[08:17:05.451413] Epoch: [9]  [140/345]  eta: 0:02:07  lr: 0.000059  loss: 0.3075 (0.3236)  time: 0.6210  data: 0.0001  max mem: 15824
[08:17:17.861990] Epoch: [9]  [160/345]  eta: 0:01:54  lr: 0.000059  loss: 0.3421 (0.3256)  time: 0.6205  data: 0.0001  max mem: 15824
[08:17:30.292282] Epoch: [9]  [180/345]  eta: 0:01:42  lr: 0.000060  loss: 0.3281 (0.3252)  time: 0.6215  data: 0.0001  max mem: 15824
[08:17:42.720157] Epoch: [9]  [200/345]  eta: 0:01:30  lr: 0.000060  loss: 0.2986 (0.3239)  time: 0.6213  data: 0.0001  max mem: 15824
[08:17:55.136838] Epoch: [9]  [220/345]  eta: 0:01:17  lr: 0.000060  loss: 0.3167 (0.3238)  time: 0.6208  data: 0.0001  max mem: 15824
[08:18:07.553907] Epoch: [9]  [240/345]  eta: 0:01:05  lr: 0.000061  loss: 0.3050 (0.3227)  time: 0.6208  data: 0.0001  max mem: 15824
[08:18:19.951718] Epoch: [9]  [260/345]  eta: 0:00:52  lr: 0.000061  loss: 0.3069 (0.3221)  time: 0.6198  data: 0.0001  max mem: 15824
[08:18:32.344350] Epoch: [9]  [280/345]  eta: 0:00:40  lr: 0.000061  loss: 0.3195 (0.3223)  time: 0.6196  data: 0.0001  max mem: 15824
[08:18:44.734724] Epoch: [9]  [300/345]  eta: 0:00:27  lr: 0.000062  loss: 0.2945 (0.3211)  time: 0.6195  data: 0.0001  max mem: 15824
[08:18:57.208674] Epoch: [9]  [320/345]  eta: 0:00:15  lr: 0.000062  loss: 0.3120 (0.3206)  time: 0.6236  data: 0.0001  max mem: 15824
[08:19:09.578891] Epoch: [9]  [340/345]  eta: 0:00:03  lr: 0.000062  loss: 0.3079 (0.3204)  time: 0.6185  data: 0.0001  max mem: 15824
[08:19:12.047095] Epoch: [9]  [344/345]  eta: 0:00:00  lr: 0.000062  loss: 0.3128 (0.3204)  time: 0.6180  data: 0.0001  max mem: 15824
[08:19:12.129295] Epoch: [9] Total time: 0:03:34 (0.6212 s / it)
[08:19:12.129501] Averaged stats: lr: 0.000062  loss: 0.3128 (0.3204)
[08:19:12.766749] Test:  [  0/345]  eta: 0:03:38  loss: 0.3173 (0.3173)  time: 0.6320  data: 0.4506  max mem: 15824
[08:19:14.553831] Test:  [ 10/345]  eta: 0:01:13  loss: 0.3173 (0.3130)  time: 0.2198  data: 0.0410  max mem: 15824
[08:19:16.347346] Test:  [ 20/345]  eta: 0:01:05  loss: 0.3117 (0.3148)  time: 0.1789  data: 0.0001  max mem: 15824
[08:19:18.142497] Test:  [ 30/345]  eta: 0:01:01  loss: 0.3059 (0.3118)  time: 0.1794  data: 0.0001  max mem: 15824
[08:19:19.938860] Test:  [ 40/345]  eta: 0:00:58  loss: 0.3030 (0.3082)  time: 0.1795  data: 0.0001  max mem: 15824
[08:19:21.738363] Test:  [ 50/345]  eta: 0:00:55  loss: 0.2948 (0.3063)  time: 0.1797  data: 0.0001  max mem: 15824
[08:19:23.541220] Test:  [ 60/345]  eta: 0:00:53  loss: 0.3014 (0.3092)  time: 0.1800  data: 0.0001  max mem: 15824
[08:19:25.349770] Test:  [ 70/345]  eta: 0:00:51  loss: 0.3060 (0.3096)  time: 0.1805  data: 0.0001  max mem: 15824
[08:19:27.163582] Test:  [ 80/345]  eta: 0:00:49  loss: 0.3038 (0.3111)  time: 0.1810  data: 0.0001  max mem: 15824
[08:19:28.978807] Test:  [ 90/345]  eta: 0:00:47  loss: 0.3052 (0.3099)  time: 0.1814  data: 0.0001  max mem: 15824
[08:19:30.796728] Test:  [100/345]  eta: 0:00:45  loss: 0.3052 (0.3094)  time: 0.1816  data: 0.0001  max mem: 15824
[08:19:32.619939] Test:  [110/345]  eta: 0:00:43  loss: 0.3058 (0.3090)  time: 0.1820  data: 0.0001  max mem: 15824
[08:19:34.444631] Test:  [120/345]  eta: 0:00:41  loss: 0.3043 (0.3082)  time: 0.1823  data: 0.0001  max mem: 15824
[08:19:36.275492] Test:  [130/345]  eta: 0:00:39  loss: 0.2915 (0.3072)  time: 0.1827  data: 0.0001  max mem: 15824
[08:19:38.106518] Test:  [140/345]  eta: 0:00:37  loss: 0.2922 (0.3063)  time: 0.1830  data: 0.0001  max mem: 15824
[08:19:39.942433] Test:  [150/345]  eta: 0:00:35  loss: 0.2946 (0.3057)  time: 0.1833  data: 0.0001  max mem: 15824
[08:19:41.781935] Test:  [160/345]  eta: 0:00:34  loss: 0.2990 (0.3058)  time: 0.1837  data: 0.0001  max mem: 15824
[08:19:43.624055] Test:  [170/345]  eta: 0:00:32  loss: 0.2990 (0.3053)  time: 0.1840  data: 0.0001  max mem: 15824
[08:19:45.471247] Test:  [180/345]  eta: 0:00:30  loss: 0.2839 (0.3047)  time: 0.1844  data: 0.0001  max mem: 15824
[08:19:47.321952] Test:  [190/345]  eta: 0:00:28  loss: 0.2973 (0.3041)  time: 0.1848  data: 0.0001  max mem: 15824
[08:19:49.176050] Test:  [200/345]  eta: 0:00:26  loss: 0.2996 (0.3044)  time: 0.1852  data: 0.0001  max mem: 15824
[08:19:51.034340] Test:  [210/345]  eta: 0:00:24  loss: 0.2945 (0.3036)  time: 0.1856  data: 0.0001  max mem: 15824
[08:19:52.896224] Test:  [220/345]  eta: 0:00:23  loss: 0.2881 (0.3041)  time: 0.1859  data: 0.0001  max mem: 15824
[08:19:54.760205] Test:  [230/345]  eta: 0:00:21  loss: 0.3215 (0.3049)  time: 0.1862  data: 0.0001  max mem: 15824
[08:19:56.627052] Test:  [240/345]  eta: 0:00:19  loss: 0.3196 (0.3048)  time: 0.1865  data: 0.0001  max mem: 15824
[08:19:58.497306] Test:  [250/345]  eta: 0:00:17  loss: 0.2944 (0.3047)  time: 0.1868  data: 0.0001  max mem: 15824
[08:20:00.372403] Test:  [260/345]  eta: 0:00:15  loss: 0.3010 (0.3052)  time: 0.1872  data: 0.0001  max mem: 15824
[08:20:02.250183] Test:  [270/345]  eta: 0:00:13  loss: 0.3128 (0.3056)  time: 0.1876  data: 0.0001  max mem: 15824
[08:20:04.130818] Test:  [280/345]  eta: 0:00:12  loss: 0.3215 (0.3062)  time: 0.1879  data: 0.0001  max mem: 15824
[08:20:06.015589] Test:  [290/345]  eta: 0:00:10  loss: 0.3203 (0.3063)  time: 0.1882  data: 0.0001  max mem: 15824
[08:20:07.904905] Test:  [300/345]  eta: 0:00:08  loss: 0.3066 (0.3057)  time: 0.1886  data: 0.0001  max mem: 15824
[08:20:09.797272] Test:  [310/345]  eta: 0:00:06  loss: 0.2968 (0.3055)  time: 0.1890  data: 0.0001  max mem: 15824
[08:20:11.691150] Test:  [320/345]  eta: 0:00:04  loss: 0.2968 (0.3051)  time: 0.1892  data: 0.0001  max mem: 15824
[08:20:13.587472] Test:  [330/345]  eta: 0:00:02  loss: 0.2935 (0.3051)  time: 0.1894  data: 0.0001  max mem: 15824
[08:20:15.486580] Test:  [340/345]  eta: 0:00:00  loss: 0.2857 (0.3046)  time: 0.1897  data: 0.0001  max mem: 15824
[08:20:16.247329] Test:  [344/345]  eta: 0:00:00  loss: 0.2842 (0.3044)  time: 0.1898  data: 0.0001  max mem: 15824
[08:20:16.320036] Test: Total time: 0:01:04 (0.1860 s / it)
[08:20:26.780737] Test:  [ 0/57]  eta: 0:00:32  loss: 0.4727 (0.4727)  time: 0.5755  data: 0.3955  max mem: 15824
[08:20:28.549382] Test:  [10/57]  eta: 0:00:10  loss: 0.4679 (0.4637)  time: 0.2130  data: 0.0360  max mem: 15824
[08:20:30.326532] Test:  [20/57]  eta: 0:00:07  loss: 0.4383 (0.4305)  time: 0.1772  data: 0.0001  max mem: 15824
[08:20:32.104946] Test:  [30/57]  eta: 0:00:05  loss: 0.3286 (0.3797)  time: 0.1777  data: 0.0001  max mem: 15824
[08:20:33.889300] Test:  [40/57]  eta: 0:00:03  loss: 0.2564 (0.3508)  time: 0.1781  data: 0.0001  max mem: 15824
[08:20:35.678561] Test:  [50/57]  eta: 0:00:01  loss: 0.2649 (0.3494)  time: 0.1786  data: 0.0001  max mem: 15824
[08:20:36.650379] Test:  [56/57]  eta: 0:00:00  loss: 0.3244 (0.3617)  time: 0.1738  data: 0.0000  max mem: 15824
[08:20:36.729578] Test: Total time: 0:00:10 (0.1846 s / it)
[08:20:38.442400] Dice score of the network on the train images: 0.732579, val images: 0.775608
[08:20:38.442630] saving best_rec_model_0 @ epoch 9
[08:20:40.114020] saving best_dice_model_0 @ epoch 9
[08:20:41.467948] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:20:42.493143] Epoch: [10]  [  0/345]  eta: 0:05:53  lr: 0.000063  loss: 0.3110 (0.3110)  time: 1.0238  data: 0.3973  max mem: 15824
[08:20:54.791563] Epoch: [10]  [ 20/345]  eta: 0:03:26  lr: 0.000063  loss: 0.3102 (0.3102)  time: 0.6149  data: 0.0001  max mem: 15824
[08:21:07.139815] Epoch: [10]  [ 40/345]  eta: 0:03:10  lr: 0.000063  loss: 0.2875 (0.3090)  time: 0.6174  data: 0.0001  max mem: 15824
[08:21:19.505010] Epoch: [10]  [ 60/345]  eta: 0:02:57  lr: 0.000064  loss: 0.3033 (0.3084)  time: 0.6182  data: 0.0001  max mem: 15824
[08:21:31.907467] Epoch: [10]  [ 80/345]  eta: 0:02:45  lr: 0.000064  loss: 0.3208 (0.3109)  time: 0.6201  data: 0.0001  max mem: 15824
[08:21:44.315348] Epoch: [10]  [100/345]  eta: 0:02:32  lr: 0.000064  loss: 0.2992 (0.3091)  time: 0.6204  data: 0.0001  max mem: 15824
[08:21:56.757250] Epoch: [10]  [120/345]  eta: 0:02:19  lr: 0.000065  loss: 0.2823 (0.3067)  time: 0.6221  data: 0.0001  max mem: 15824
[08:22:09.196829] Epoch: [10]  [140/345]  eta: 0:02:07  lr: 0.000065  loss: 0.2894 (0.3049)  time: 0.6219  data: 0.0001  max mem: 15824
[08:22:21.609541] Epoch: [10]  [160/345]  eta: 0:01:55  lr: 0.000065  loss: 0.2917 (0.3042)  time: 0.6206  data: 0.0001  max mem: 15824
[08:22:34.005642] Epoch: [10]  [180/345]  eta: 0:01:42  lr: 0.000066  loss: 0.2873 (0.3041)  time: 0.6198  data: 0.0001  max mem: 15824
[08:22:46.440675] Epoch: [10]  [200/345]  eta: 0:01:30  lr: 0.000066  loss: 0.3157 (0.3063)  time: 0.6217  data: 0.0001  max mem: 15824
[08:22:58.872283] Epoch: [10]  [220/345]  eta: 0:01:17  lr: 0.000066  loss: 0.2868 (0.3046)  time: 0.6215  data: 0.0001  max mem: 15824
[08:23:11.290376] Epoch: [10]  [240/345]  eta: 0:01:05  lr: 0.000067  loss: 0.2971 (0.3043)  time: 0.6209  data: 0.0001  max mem: 15824
[08:23:23.711567] Epoch: [10]  [260/345]  eta: 0:00:52  lr: 0.000067  loss: 0.2888 (0.3038)  time: 0.6210  data: 0.0001  max mem: 15824
[08:23:36.127597] Epoch: [10]  [280/345]  eta: 0:00:40  lr: 0.000068  loss: 0.2967 (0.3036)  time: 0.6208  data: 0.0001  max mem: 15824
[08:23:48.528366] Epoch: [10]  [300/345]  eta: 0:00:27  lr: 0.000068  loss: 0.2899 (0.3026)  time: 0.6200  data: 0.0001  max mem: 15824
[08:24:00.932684] Epoch: [10]  [320/345]  eta: 0:00:15  lr: 0.000068  loss: 0.2928 (0.3021)  time: 0.6202  data: 0.0001  max mem: 15824
[08:24:13.307976] Epoch: [10]  [340/345]  eta: 0:00:03  lr: 0.000069  loss: 0.2839 (0.3013)  time: 0.6187  data: 0.0001  max mem: 15824
[08:24:15.779297] Epoch: [10]  [344/345]  eta: 0:00:00  lr: 0.000069  loss: 0.2993 (0.3014)  time: 0.6183  data: 0.0001  max mem: 15824
[08:24:15.853913] Epoch: [10] Total time: 0:03:34 (0.6214 s / it)
[08:24:15.854352] Averaged stats: lr: 0.000069  loss: 0.2993 (0.3014)
[08:24:16.446457] Test:  [  0/345]  eta: 0:03:22  loss: 0.2329 (0.2329)  time: 0.5867  data: 0.4057  max mem: 15824
[08:24:18.235319] Test:  [ 10/345]  eta: 0:01:12  loss: 0.2722 (0.2871)  time: 0.2159  data: 0.0370  max mem: 15824
[08:24:20.029487] Test:  [ 20/345]  eta: 0:01:04  loss: 0.2722 (0.2838)  time: 0.1791  data: 0.0001  max mem: 15824
[08:24:21.823830] Test:  [ 30/345]  eta: 0:01:00  loss: 0.2706 (0.2805)  time: 0.1794  data: 0.0001  max mem: 15824
[08:24:23.623788] Test:  [ 40/345]  eta: 0:00:57  loss: 0.2721 (0.2800)  time: 0.1796  data: 0.0001  max mem: 15824
[08:24:25.426170] Test:  [ 50/345]  eta: 0:00:55  loss: 0.2745 (0.2781)  time: 0.1800  data: 0.0001  max mem: 15824
[08:24:27.231999] Test:  [ 60/345]  eta: 0:00:53  loss: 0.2609 (0.2775)  time: 0.1803  data: 0.0001  max mem: 15824
[08:24:29.041030] Test:  [ 70/345]  eta: 0:00:51  loss: 0.2675 (0.2764)  time: 0.1807  data: 0.0001  max mem: 15824
[08:24:30.854028] Test:  [ 80/345]  eta: 0:00:49  loss: 0.2674 (0.2757)  time: 0.1810  data: 0.0001  max mem: 15824
[08:24:32.667106] Test:  [ 90/345]  eta: 0:00:47  loss: 0.2614 (0.2770)  time: 0.1812  data: 0.0001  max mem: 15824
[08:24:34.486091] Test:  [100/345]  eta: 0:00:45  loss: 0.2718 (0.2764)  time: 0.1815  data: 0.0001  max mem: 15824
[08:24:36.309535] Test:  [110/345]  eta: 0:00:43  loss: 0.2752 (0.2772)  time: 0.1821  data: 0.0001  max mem: 15824
[08:24:38.136025] Test:  [120/345]  eta: 0:00:41  loss: 0.2865 (0.2783)  time: 0.1824  data: 0.0001  max mem: 15824
[08:24:39.964264] Test:  [130/345]  eta: 0:00:39  loss: 0.2794 (0.2767)  time: 0.1827  data: 0.0001  max mem: 15824
[08:24:41.796455] Test:  [140/345]  eta: 0:00:37  loss: 0.2794 (0.2777)  time: 0.1830  data: 0.0001  max mem: 15824
[08:24:43.633205] Test:  [150/345]  eta: 0:00:35  loss: 0.2896 (0.2778)  time: 0.1834  data: 0.0001  max mem: 15824
[08:24:45.475193] Test:  [160/345]  eta: 0:00:34  loss: 0.2727 (0.2773)  time: 0.1838  data: 0.0001  max mem: 15824
[08:24:47.318619] Test:  [170/345]  eta: 0:00:32  loss: 0.2791 (0.2783)  time: 0.1841  data: 0.0001  max mem: 15824
[08:24:49.163576] Test:  [180/345]  eta: 0:00:30  loss: 0.2832 (0.2786)  time: 0.1844  data: 0.0001  max mem: 15824
[08:24:51.015466] Test:  [190/345]  eta: 0:00:28  loss: 0.2826 (0.2791)  time: 0.1848  data: 0.0001  max mem: 15824
[08:24:52.869168] Test:  [200/345]  eta: 0:00:26  loss: 0.2826 (0.2793)  time: 0.1852  data: 0.0001  max mem: 15824
[08:24:54.727247] Test:  [210/345]  eta: 0:00:24  loss: 0.2814 (0.2796)  time: 0.1855  data: 0.0001  max mem: 15824
[08:24:56.586141] Test:  [220/345]  eta: 0:00:23  loss: 0.2811 (0.2799)  time: 0.1858  data: 0.0001  max mem: 15824
[08:24:58.450202] Test:  [230/345]  eta: 0:00:21  loss: 0.2774 (0.2801)  time: 0.1861  data: 0.0001  max mem: 15824
[08:25:00.316043] Test:  [240/345]  eta: 0:00:19  loss: 0.2778 (0.2805)  time: 0.1864  data: 0.0001  max mem: 15824
[08:25:02.189086] Test:  [250/345]  eta: 0:00:17  loss: 0.2778 (0.2797)  time: 0.1869  data: 0.0001  max mem: 15824
[08:25:04.061629] Test:  [260/345]  eta: 0:00:15  loss: 0.2807 (0.2798)  time: 0.1872  data: 0.0001  max mem: 15824
[08:25:05.938676] Test:  [270/345]  eta: 0:00:13  loss: 0.2810 (0.2796)  time: 0.1874  data: 0.0001  max mem: 15824
[08:25:07.820727] Test:  [280/345]  eta: 0:00:12  loss: 0.2738 (0.2800)  time: 0.1879  data: 0.0001  max mem: 15824
[08:25:09.705961] Test:  [290/345]  eta: 0:00:10  loss: 0.2737 (0.2796)  time: 0.1883  data: 0.0001  max mem: 15824
[08:25:11.596181] Test:  [300/345]  eta: 0:00:08  loss: 0.2695 (0.2796)  time: 0.1887  data: 0.0001  max mem: 15824
[08:25:13.488100] Test:  [310/345]  eta: 0:00:06  loss: 0.2695 (0.2794)  time: 0.1890  data: 0.0001  max mem: 15824
[08:25:15.385621] Test:  [320/345]  eta: 0:00:04  loss: 0.2765 (0.2795)  time: 0.1894  data: 0.0001  max mem: 15824
[08:25:17.284065] Test:  [330/345]  eta: 0:00:02  loss: 0.2765 (0.2797)  time: 0.1897  data: 0.0001  max mem: 15824
[08:25:19.181564] Test:  [340/345]  eta: 0:00:00  loss: 0.2747 (0.2797)  time: 0.1897  data: 0.0001  max mem: 15824
[08:25:19.942282] Test:  [344/345]  eta: 0:00:00  loss: 0.2701 (0.2795)  time: 0.1898  data: 0.0001  max mem: 15824
[08:25:20.012662] Test: Total time: 0:01:04 (0.1860 s / it)
[08:25:30.607970] Test:  [ 0/57]  eta: 0:00:33  loss: 0.4538 (0.4538)  time: 0.5823  data: 0.4004  max mem: 15824
[08:25:32.378703] Test:  [10/57]  eta: 0:00:10  loss: 0.4539 (0.4800)  time: 0.2138  data: 0.0365  max mem: 15824
[08:25:34.151910] Test:  [20/57]  eta: 0:00:07  loss: 0.4461 (0.4455)  time: 0.1771  data: 0.0001  max mem: 15824
[08:25:35.931660] Test:  [30/57]  eta: 0:00:05  loss: 0.3573 (0.3975)  time: 0.1776  data: 0.0001  max mem: 15824
[08:25:37.715567] Test:  [40/57]  eta: 0:00:03  loss: 0.3124 (0.3811)  time: 0.1781  data: 0.0001  max mem: 15824
[08:25:39.499644] Test:  [50/57]  eta: 0:00:01  loss: 0.3366 (0.3855)  time: 0.1783  data: 0.0001  max mem: 15824
[08:25:40.471058] Test:  [56/57]  eta: 0:00:00  loss: 0.3758 (0.4069)  time: 0.1734  data: 0.0000  max mem: 15824
[08:25:40.552134] Test: Total time: 0:00:10 (0.1847 s / it)
[08:25:42.276333] Dice score of the network on the train images: 0.775638, val images: 0.729579
[08:25:42.276533] saving best_prec_model_0 @ epoch 10
[08:25:43.612392] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:25:44.680628] Epoch: [11]  [  0/345]  eta: 0:06:08  lr: 0.000069  loss: 0.3324 (0.3324)  time: 1.0669  data: 0.4467  max mem: 15824
[08:25:57.009641] Epoch: [11]  [ 20/345]  eta: 0:03:27  lr: 0.000069  loss: 0.2774 (0.2780)  time: 0.6164  data: 0.0001  max mem: 15824
[08:26:09.366942] Epoch: [11]  [ 40/345]  eta: 0:03:11  lr: 0.000069  loss: 0.2924 (0.2837)  time: 0.6178  data: 0.0001  max mem: 15824
[08:26:21.845495] Epoch: [11]  [ 60/345]  eta: 0:02:58  lr: 0.000070  loss: 0.2777 (0.2808)  time: 0.6239  data: 0.0001  max mem: 15824
[08:26:34.203745] Epoch: [11]  [ 80/345]  eta: 0:02:45  lr: 0.000070  loss: 0.2813 (0.2814)  time: 0.6179  data: 0.0001  max mem: 15824
[08:26:46.608763] Epoch: [11]  [100/345]  eta: 0:02:32  lr: 0.000071  loss: 0.2799 (0.2814)  time: 0.6202  data: 0.0001  max mem: 15824
[08:26:59.029096] Epoch: [11]  [120/345]  eta: 0:02:20  lr: 0.000071  loss: 0.3061 (0.2847)  time: 0.6210  data: 0.0001  max mem: 15824
[08:27:11.446593] Epoch: [11]  [140/345]  eta: 0:02:07  lr: 0.000071  loss: 0.2785 (0.2841)  time: 0.6208  data: 0.0001  max mem: 15824
[08:27:23.878019] Epoch: [11]  [160/345]  eta: 0:01:55  lr: 0.000072  loss: 0.2798 (0.2851)  time: 0.6215  data: 0.0001  max mem: 15824
[08:27:36.306058] Epoch: [11]  [180/345]  eta: 0:01:42  lr: 0.000072  loss: 0.2847 (0.2853)  time: 0.6214  data: 0.0001  max mem: 15824
[08:27:48.742432] Epoch: [11]  [200/345]  eta: 0:01:30  lr: 0.000072  loss: 0.2980 (0.2864)  time: 0.6218  data: 0.0001  max mem: 15824
[08:28:01.174510] Epoch: [11]  [220/345]  eta: 0:01:17  lr: 0.000073  loss: 0.2903 (0.2867)  time: 0.6216  data: 0.0001  max mem: 15824
[08:28:13.594389] Epoch: [11]  [240/345]  eta: 0:01:05  lr: 0.000073  loss: 0.2698 (0.2863)  time: 0.6209  data: 0.0001  max mem: 15824
[08:28:26.015811] Epoch: [11]  [260/345]  eta: 0:00:52  lr: 0.000073  loss: 0.2873 (0.2866)  time: 0.6210  data: 0.0001  max mem: 15824
[08:28:38.439470] Epoch: [11]  [280/345]  eta: 0:00:40  lr: 0.000074  loss: 0.3030 (0.2879)  time: 0.6211  data: 0.0001  max mem: 15824
[08:28:50.876161] Epoch: [11]  [300/345]  eta: 0:00:27  lr: 0.000074  loss: 0.2929 (0.2877)  time: 0.6218  data: 0.0001  max mem: 15824
[08:29:03.296564] Epoch: [11]  [320/345]  eta: 0:00:15  lr: 0.000075  loss: 0.2850 (0.2880)  time: 0.6210  data: 0.0001  max mem: 15824
[08:29:15.718430] Epoch: [11]  [340/345]  eta: 0:00:03  lr: 0.000075  loss: 0.2788 (0.2880)  time: 0.6210  data: 0.0001  max mem: 15824
[08:29:18.203444] Epoch: [11]  [344/345]  eta: 0:00:00  lr: 0.000075  loss: 0.2823 (0.2881)  time: 0.6210  data: 0.0001  max mem: 15824
[08:29:18.276914] Epoch: [11] Total time: 0:03:34 (0.6222 s / it)
[08:29:18.277158] Averaged stats: lr: 0.000075  loss: 0.2823 (0.2881)
[08:29:18.884665] Test:  [  0/345]  eta: 0:03:27  loss: 0.2862 (0.2862)  time: 0.6018  data: 0.4205  max mem: 15824
[08:29:20.676137] Test:  [ 10/345]  eta: 0:01:12  loss: 0.2613 (0.2688)  time: 0.2175  data: 0.0383  max mem: 15824
[08:29:22.472647] Test:  [ 20/345]  eta: 0:01:04  loss: 0.2600 (0.2627)  time: 0.1793  data: 0.0001  max mem: 15824
[08:29:24.268855] Test:  [ 30/345]  eta: 0:01:00  loss: 0.2607 (0.2664)  time: 0.1796  data: 0.0001  max mem: 15824
[08:29:26.069934] Test:  [ 40/345]  eta: 0:00:57  loss: 0.2607 (0.2668)  time: 0.1798  data: 0.0001  max mem: 15824
[08:29:27.876677] Test:  [ 50/345]  eta: 0:00:55  loss: 0.2654 (0.2688)  time: 0.1803  data: 0.0001  max mem: 15824
[08:29:29.680199] Test:  [ 60/345]  eta: 0:00:53  loss: 0.2639 (0.2681)  time: 0.1805  data: 0.0001  max mem: 15824
[08:29:31.485796] Test:  [ 70/345]  eta: 0:00:51  loss: 0.2593 (0.2700)  time: 0.1804  data: 0.0001  max mem: 15824
[08:29:33.299684] Test:  [ 80/345]  eta: 0:00:49  loss: 0.2593 (0.2695)  time: 0.1809  data: 0.0001  max mem: 15824
[08:29:35.116165] Test:  [ 90/345]  eta: 0:00:47  loss: 0.2584 (0.2686)  time: 0.1815  data: 0.0001  max mem: 15824
[08:29:36.935206] Test:  [100/345]  eta: 0:00:45  loss: 0.2619 (0.2685)  time: 0.1817  data: 0.0001  max mem: 15824
[08:29:38.760124] Test:  [110/345]  eta: 0:00:43  loss: 0.2680 (0.2688)  time: 0.1821  data: 0.0001  max mem: 15824
[08:29:40.586626] Test:  [120/345]  eta: 0:00:41  loss: 0.2734 (0.2699)  time: 0.1825  data: 0.0001  max mem: 15824
[08:29:42.417000] Test:  [130/345]  eta: 0:00:39  loss: 0.2754 (0.2699)  time: 0.1828  data: 0.0001  max mem: 15824
[08:29:44.249452] Test:  [140/345]  eta: 0:00:37  loss: 0.2848 (0.2711)  time: 0.1831  data: 0.0001  max mem: 15824
[08:29:46.086089] Test:  [150/345]  eta: 0:00:35  loss: 0.2733 (0.2706)  time: 0.1834  data: 0.0001  max mem: 15824
[08:29:47.926201] Test:  [160/345]  eta: 0:00:34  loss: 0.2451 (0.2691)  time: 0.1838  data: 0.0001  max mem: 15824
[08:29:49.769579] Test:  [170/345]  eta: 0:00:32  loss: 0.2587 (0.2693)  time: 0.1841  data: 0.0001  max mem: 15824
[08:29:51.618299] Test:  [180/345]  eta: 0:00:30  loss: 0.2646 (0.2692)  time: 0.1845  data: 0.0001  max mem: 15824
[08:29:53.466576] Test:  [190/345]  eta: 0:00:28  loss: 0.2521 (0.2687)  time: 0.1848  data: 0.0001  max mem: 15824
[08:29:55.321810] Test:  [200/345]  eta: 0:00:26  loss: 0.2472 (0.2680)  time: 0.1851  data: 0.0001  max mem: 15824
[08:29:57.179864] Test:  [210/345]  eta: 0:00:24  loss: 0.2551 (0.2675)  time: 0.1856  data: 0.0001  max mem: 15824
[08:29:59.040972] Test:  [220/345]  eta: 0:00:23  loss: 0.2665 (0.2682)  time: 0.1859  data: 0.0001  max mem: 15824
[08:30:00.904551] Test:  [230/345]  eta: 0:00:21  loss: 0.2850 (0.2685)  time: 0.1862  data: 0.0001  max mem: 15824
[08:30:02.773057] Test:  [240/345]  eta: 0:00:19  loss: 0.2616 (0.2682)  time: 0.1865  data: 0.0001  max mem: 15824
[08:30:04.643889] Test:  [250/345]  eta: 0:00:17  loss: 0.2527 (0.2681)  time: 0.1869  data: 0.0001  max mem: 15824
[08:30:06.521304] Test:  [260/345]  eta: 0:00:15  loss: 0.2571 (0.2679)  time: 0.1874  data: 0.0001  max mem: 15824
[08:30:08.400114] Test:  [270/345]  eta: 0:00:13  loss: 0.2613 (0.2677)  time: 0.1877  data: 0.0001  max mem: 15824
[08:30:10.284244] Test:  [280/345]  eta: 0:00:12  loss: 0.2599 (0.2676)  time: 0.1881  data: 0.0001  max mem: 15824
[08:30:12.172791] Test:  [290/345]  eta: 0:00:10  loss: 0.2526 (0.2672)  time: 0.1886  data: 0.0001  max mem: 15824
[08:30:14.062832] Test:  [300/345]  eta: 0:00:08  loss: 0.2582 (0.2671)  time: 0.1889  data: 0.0001  max mem: 15824
[08:30:15.960092] Test:  [310/345]  eta: 0:00:06  loss: 0.2713 (0.2674)  time: 0.1893  data: 0.0001  max mem: 15824
[08:30:17.859253] Test:  [320/345]  eta: 0:00:04  loss: 0.2762 (0.2676)  time: 0.1898  data: 0.0001  max mem: 15824
[08:30:19.758691] Test:  [330/345]  eta: 0:00:02  loss: 0.2762 (0.2678)  time: 0.1899  data: 0.0001  max mem: 15824
[08:30:21.659189] Test:  [340/345]  eta: 0:00:00  loss: 0.2729 (0.2679)  time: 0.1899  data: 0.0001  max mem: 15824
[08:30:22.420241] Test:  [344/345]  eta: 0:00:00  loss: 0.2651 (0.2677)  time: 0.1900  data: 0.0001  max mem: 15824
[08:30:22.488961] Test: Total time: 0:01:04 (0.1861 s / it)
[08:30:33.008461] Test:  [ 0/57]  eta: 0:00:34  loss: 0.4794 (0.4794)  time: 0.6023  data: 0.4230  max mem: 15824
[08:30:34.778806] Test:  [10/57]  eta: 0:00:10  loss: 0.4634 (0.4628)  time: 0.2156  data: 0.0385  max mem: 15824
[08:30:36.554834] Test:  [20/57]  eta: 0:00:07  loss: 0.4077 (0.4272)  time: 0.1772  data: 0.0001  max mem: 15824
[08:30:38.335389] Test:  [30/57]  eta: 0:00:05  loss: 0.3001 (0.3766)  time: 0.1778  data: 0.0001  max mem: 15824
[08:30:40.120291] Test:  [40/57]  eta: 0:00:03  loss: 0.2714 (0.3492)  time: 0.1782  data: 0.0001  max mem: 15824
[08:30:41.906072] Test:  [50/57]  eta: 0:00:01  loss: 0.2751 (0.3484)  time: 0.1785  data: 0.0001  max mem: 15824
[08:30:42.879419] Test:  [56/57]  eta: 0:00:00  loss: 0.3254 (0.3673)  time: 0.1736  data: 0.0001  max mem: 15824
[08:30:42.949535] Test: Total time: 0:00:10 (0.1850 s / it)
[08:30:44.634600] Dice score of the network on the train images: 0.770489, val images: 0.770154
[08:30:44.634817] saving best_prec_model_0 @ epoch 11
[08:30:45.996608] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:30:47.056254] Epoch: [12]  [  0/345]  eta: 0:06:05  lr: 0.000075  loss: 0.3094 (0.3094)  time: 1.0582  data: 0.4328  max mem: 15824
[08:30:59.384302] Epoch: [12]  [ 20/345]  eta: 0:03:27  lr: 0.000075  loss: 0.2749 (0.2760)  time: 0.6163  data: 0.0001  max mem: 15824
[08:31:11.735990] Epoch: [12]  [ 40/345]  eta: 0:03:11  lr: 0.000076  loss: 0.2814 (0.2803)  time: 0.6175  data: 0.0001  max mem: 15824
[08:31:24.098267] Epoch: [12]  [ 60/345]  eta: 0:02:57  lr: 0.000076  loss: 0.2602 (0.2766)  time: 0.6181  data: 0.0001  max mem: 15824
[08:31:36.464780] Epoch: [12]  [ 80/345]  eta: 0:02:45  lr: 0.000076  loss: 0.2662 (0.2750)  time: 0.6183  data: 0.0001  max mem: 15824
[08:31:48.990153] Epoch: [12]  [100/345]  eta: 0:02:32  lr: 0.000077  loss: 0.2666 (0.2740)  time: 0.6262  data: 0.0001  max mem: 15824
[08:32:01.399255] Epoch: [12]  [120/345]  eta: 0:02:20  lr: 0.000077  loss: 0.2587 (0.2740)  time: 0.6204  data: 0.0001  max mem: 15824
[08:32:13.845093] Epoch: [12]  [140/345]  eta: 0:02:07  lr: 0.000078  loss: 0.2649 (0.2737)  time: 0.6222  data: 0.0001  max mem: 15824
[08:32:26.286870] Epoch: [12]  [160/345]  eta: 0:01:55  lr: 0.000078  loss: 0.2653 (0.2734)  time: 0.6220  data: 0.0001  max mem: 15824
[08:32:38.727416] Epoch: [12]  [180/345]  eta: 0:01:42  lr: 0.000078  loss: 0.2657 (0.2726)  time: 0.6220  data: 0.0001  max mem: 15824
[08:32:51.168459] Epoch: [12]  [200/345]  eta: 0:01:30  lr: 0.000079  loss: 0.2534 (0.2716)  time: 0.6220  data: 0.0001  max mem: 15824
[08:33:03.568176] Epoch: [12]  [220/345]  eta: 0:01:17  lr: 0.000079  loss: 0.2663 (0.2716)  time: 0.6199  data: 0.0001  max mem: 15824
[08:33:15.987990] Epoch: [12]  [240/345]  eta: 0:01:05  lr: 0.000079  loss: 0.2828 (0.2728)  time: 0.6209  data: 0.0001  max mem: 15824
[08:33:28.391924] Epoch: [12]  [260/345]  eta: 0:00:52  lr: 0.000080  loss: 0.2734 (0.2732)  time: 0.6202  data: 0.0001  max mem: 15824
[08:33:40.804105] Epoch: [12]  [280/345]  eta: 0:00:40  lr: 0.000080  loss: 0.2620 (0.2723)  time: 0.6206  data: 0.0001  max mem: 15824
[08:33:53.223138] Epoch: [12]  [300/345]  eta: 0:00:27  lr: 0.000080  loss: 0.2790 (0.2726)  time: 0.6209  data: 0.0001  max mem: 15824
[08:34:05.628944] Epoch: [12]  [320/345]  eta: 0:00:15  lr: 0.000081  loss: 0.2592 (0.2726)  time: 0.6202  data: 0.0001  max mem: 15824
[08:34:18.037339] Epoch: [12]  [340/345]  eta: 0:00:03  lr: 0.000081  loss: 0.2750 (0.2725)  time: 0.6204  data: 0.0001  max mem: 15824
[08:34:20.518135] Epoch: [12]  [344/345]  eta: 0:00:00  lr: 0.000081  loss: 0.2847 (0.2728)  time: 0.6202  data: 0.0001  max mem: 15824
[08:34:20.592722] Epoch: [12] Total time: 0:03:34 (0.6220 s / it)
[08:34:20.592984] Averaged stats: lr: 0.000081  loss: 0.2847 (0.2728)
[08:34:21.180337] Test:  [  0/345]  eta: 0:03:20  loss: 0.2465 (0.2465)  time: 0.5814  data: 0.4006  max mem: 15824
[08:34:22.972174] Test:  [ 10/345]  eta: 0:01:12  loss: 0.2710 (0.2659)  time: 0.2157  data: 0.0365  max mem: 15824
[08:34:24.767628] Test:  [ 20/345]  eta: 0:01:04  loss: 0.2584 (0.2611)  time: 0.1793  data: 0.0001  max mem: 15824
[08:34:26.562020] Test:  [ 30/345]  eta: 0:01:00  loss: 0.2506 (0.2560)  time: 0.1794  data: 0.0001  max mem: 15824
[08:34:28.360270] Test:  [ 40/345]  eta: 0:00:57  loss: 0.2460 (0.2560)  time: 0.1796  data: 0.0001  max mem: 15824
[08:34:30.161559] Test:  [ 50/345]  eta: 0:00:55  loss: 0.2552 (0.2571)  time: 0.1799  data: 0.0001  max mem: 15824
[08:34:31.971255] Test:  [ 60/345]  eta: 0:00:53  loss: 0.2552 (0.2561)  time: 0.1805  data: 0.0001  max mem: 15824
[08:34:33.781608] Test:  [ 70/345]  eta: 0:00:51  loss: 0.2547 (0.2574)  time: 0.1809  data: 0.0001  max mem: 15824
[08:34:35.590727] Test:  [ 80/345]  eta: 0:00:49  loss: 0.2544 (0.2576)  time: 0.1809  data: 0.0001  max mem: 15824
[08:34:37.407217] Test:  [ 90/345]  eta: 0:00:47  loss: 0.2536 (0.2573)  time: 0.1812  data: 0.0001  max mem: 15824
[08:34:39.229794] Test:  [100/345]  eta: 0:00:45  loss: 0.2541 (0.2575)  time: 0.1819  data: 0.0001  max mem: 15824
[08:34:41.053805] Test:  [110/345]  eta: 0:00:43  loss: 0.2649 (0.2587)  time: 0.1823  data: 0.0001  max mem: 15824
[08:34:42.881282] Test:  [120/345]  eta: 0:00:41  loss: 0.2660 (0.2601)  time: 0.1825  data: 0.0001  max mem: 15824
[08:34:44.709860] Test:  [130/345]  eta: 0:00:39  loss: 0.2485 (0.2588)  time: 0.1827  data: 0.0001  max mem: 15824
[08:34:46.543723] Test:  [140/345]  eta: 0:00:37  loss: 0.2590 (0.2604)  time: 0.1831  data: 0.0001  max mem: 15824
[08:34:48.381043] Test:  [150/345]  eta: 0:00:35  loss: 0.2682 (0.2610)  time: 0.1835  data: 0.0001  max mem: 15824
[08:34:50.225141] Test:  [160/345]  eta: 0:00:34  loss: 0.2611 (0.2604)  time: 0.1840  data: 0.0001  max mem: 15824
[08:34:52.067872] Test:  [170/345]  eta: 0:00:32  loss: 0.2584 (0.2609)  time: 0.1843  data: 0.0001  max mem: 15824
[08:34:53.916240] Test:  [180/345]  eta: 0:00:30  loss: 0.2535 (0.2601)  time: 0.1845  data: 0.0001  max mem: 15824
[08:34:55.767678] Test:  [190/345]  eta: 0:00:28  loss: 0.2466 (0.2595)  time: 0.1849  data: 0.0001  max mem: 15824
[08:34:57.623091] Test:  [200/345]  eta: 0:00:26  loss: 0.2510 (0.2597)  time: 0.1853  data: 0.0001  max mem: 15824
[08:34:59.481358] Test:  [210/345]  eta: 0:00:24  loss: 0.2548 (0.2596)  time: 0.1856  data: 0.0001  max mem: 15824
[08:35:01.344197] Test:  [220/345]  eta: 0:00:23  loss: 0.2545 (0.2597)  time: 0.1860  data: 0.0001  max mem: 15824
[08:35:03.206557] Test:  [230/345]  eta: 0:00:21  loss: 0.2586 (0.2598)  time: 0.1862  data: 0.0001  max mem: 15824
[08:35:05.074514] Test:  [240/345]  eta: 0:00:19  loss: 0.2686 (0.2605)  time: 0.1865  data: 0.0001  max mem: 15824
[08:35:06.948150] Test:  [250/345]  eta: 0:00:17  loss: 0.2724 (0.2605)  time: 0.1870  data: 0.0001  max mem: 15824
[08:35:08.821822] Test:  [260/345]  eta: 0:00:15  loss: 0.2724 (0.2608)  time: 0.1873  data: 0.0001  max mem: 15824
[08:35:10.701087] Test:  [270/345]  eta: 0:00:13  loss: 0.2730 (0.2610)  time: 0.1876  data: 0.0001  max mem: 15824
[08:35:12.585098] Test:  [280/345]  eta: 0:00:12  loss: 0.2571 (0.2608)  time: 0.1881  data: 0.0001  max mem: 15824
[08:35:14.473044] Test:  [290/345]  eta: 0:00:10  loss: 0.2556 (0.2610)  time: 0.1885  data: 0.0001  max mem: 15824
[08:35:16.364562] Test:  [300/345]  eta: 0:00:08  loss: 0.2802 (0.2612)  time: 0.1889  data: 0.0001  max mem: 15824
[08:35:18.259035] Test:  [310/345]  eta: 0:00:06  loss: 0.2669 (0.2611)  time: 0.1892  data: 0.0001  max mem: 15824
[08:35:20.153518] Test:  [320/345]  eta: 0:00:04  loss: 0.2532 (0.2608)  time: 0.1894  data: 0.0001  max mem: 15824
[08:35:22.054374] Test:  [330/345]  eta: 0:00:02  loss: 0.2630 (0.2612)  time: 0.1897  data: 0.0001  max mem: 15824
[08:35:23.952772] Test:  [340/345]  eta: 0:00:00  loss: 0.2640 (0.2613)  time: 0.1899  data: 0.0001  max mem: 15824
[08:35:24.713096] Test:  [344/345]  eta: 0:00:00  loss: 0.2754 (0.2614)  time: 0.1899  data: 0.0001  max mem: 15824
[08:35:24.784792] Test: Total time: 0:01:04 (0.1860 s / it)
[08:35:35.175970] Test:  [ 0/57]  eta: 0:00:32  loss: 0.4787 (0.4787)  time: 0.5785  data: 0.3982  max mem: 15824
[08:35:36.946242] Test:  [10/57]  eta: 0:00:10  loss: 0.4634 (0.4685)  time: 0.2135  data: 0.0363  max mem: 15824
[08:35:38.723120] Test:  [20/57]  eta: 0:00:07  loss: 0.4307 (0.4514)  time: 0.1773  data: 0.0001  max mem: 15824
[08:35:40.505649] Test:  [30/57]  eta: 0:00:05  loss: 0.3236 (0.4010)  time: 0.1779  data: 0.0001  max mem: 15824
[08:35:42.290243] Test:  [40/57]  eta: 0:00:03  loss: 0.2992 (0.3765)  time: 0.1783  data: 0.0001  max mem: 15824
[08:35:44.074245] Test:  [50/57]  eta: 0:00:01  loss: 0.3176 (0.3723)  time: 0.1784  data: 0.0001  max mem: 15824
[08:35:45.049721] Test:  [56/57]  eta: 0:00:00  loss: 0.3408 (0.3887)  time: 0.1737  data: 0.0000  max mem: 15824
[08:35:45.114688] Test: Total time: 0:00:10 (0.1845 s / it)
[08:35:46.847320] Dice score of the network on the train images: 0.789827, val images: 0.766368
[08:35:46.847539] saving best_prec_model_0 @ epoch 12
[08:35:47.954830] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:35:48.964135] Epoch: [13]  [  0/345]  eta: 0:05:47  lr: 0.000081  loss: 0.2443 (0.2443)  time: 1.0083  data: 0.3843  max mem: 15824
[08:36:01.304522] Epoch: [13]  [ 20/345]  eta: 0:03:26  lr: 0.000082  loss: 0.2692 (0.2716)  time: 0.6170  data: 0.0001  max mem: 15824
[08:36:13.646057] Epoch: [13]  [ 40/345]  eta: 0:03:11  lr: 0.000082  loss: 0.2529 (0.2625)  time: 0.6170  data: 0.0001  max mem: 15824
[08:36:25.997978] Epoch: [13]  [ 60/345]  eta: 0:02:57  lr: 0.000082  loss: 0.2623 (0.2635)  time: 0.6176  data: 0.0001  max mem: 15824
[08:36:38.367465] Epoch: [13]  [ 80/345]  eta: 0:02:44  lr: 0.000083  loss: 0.2537 (0.2648)  time: 0.6184  data: 0.0001  max mem: 15824
[08:36:50.766743] Epoch: [13]  [100/345]  eta: 0:02:32  lr: 0.000083  loss: 0.2674 (0.2663)  time: 0.6199  data: 0.0001  max mem: 15824
[08:37:03.190568] Epoch: [13]  [120/345]  eta: 0:02:19  lr: 0.000083  loss: 0.2481 (0.2643)  time: 0.6211  data: 0.0001  max mem: 15824
[08:37:15.626612] Epoch: [13]  [140/345]  eta: 0:02:07  lr: 0.000084  loss: 0.2641 (0.2646)  time: 0.6218  data: 0.0001  max mem: 15824
[08:37:28.074453] Epoch: [13]  [160/345]  eta: 0:01:55  lr: 0.000084  loss: 0.2556 (0.2639)  time: 0.6223  data: 0.0001  max mem: 15824
[08:37:40.520741] Epoch: [13]  [180/345]  eta: 0:01:42  lr: 0.000085  loss: 0.2565 (0.2637)  time: 0.6223  data: 0.0001  max mem: 15824
[08:37:52.965197] Epoch: [13]  [200/345]  eta: 0:01:30  lr: 0.000085  loss: 0.2596 (0.2641)  time: 0.6222  data: 0.0001  max mem: 15824
[08:38:05.403886] Epoch: [13]  [220/345]  eta: 0:01:17  lr: 0.000085  loss: 0.2652 (0.2643)  time: 0.6219  data: 0.0001  max mem: 15824
[08:38:17.832325] Epoch: [13]  [240/345]  eta: 0:01:05  lr: 0.000086  loss: 0.2672 (0.2650)  time: 0.6214  data: 0.0001  max mem: 15824
[08:38:30.253973] Epoch: [13]  [260/345]  eta: 0:00:52  lr: 0.000086  loss: 0.2684 (0.2650)  time: 0.6210  data: 0.0001  max mem: 15824
[08:38:42.676912] Epoch: [13]  [280/345]  eta: 0:00:40  lr: 0.000086  loss: 0.2542 (0.2650)  time: 0.6211  data: 0.0001  max mem: 15824
[08:38:55.104914] Epoch: [13]  [300/345]  eta: 0:00:27  lr: 0.000087  loss: 0.2576 (0.2650)  time: 0.6213  data: 0.0001  max mem: 15824
[08:39:07.539761] Epoch: [13]  [320/345]  eta: 0:00:15  lr: 0.000087  loss: 0.2632 (0.2651)  time: 0.6217  data: 0.0001  max mem: 15824
[08:39:19.922341] Epoch: [13]  [340/345]  eta: 0:00:03  lr: 0.000087  loss: 0.2598 (0.2648)  time: 0.6191  data: 0.0001  max mem: 15824
[08:39:22.397451] Epoch: [13]  [344/345]  eta: 0:00:00  lr: 0.000087  loss: 0.2598 (0.2646)  time: 0.6188  data: 0.0001  max mem: 15824
[08:39:22.469946] Epoch: [13] Total time: 0:03:34 (0.6218 s / it)
[08:39:22.470321] Averaged stats: lr: 0.000087  loss: 0.2598 (0.2646)
[08:39:23.103473] Test:  [  0/345]  eta: 0:03:36  loss: 0.2605 (0.2605)  time: 0.6278  data: 0.4461  max mem: 15824
[08:39:24.892638] Test:  [ 10/345]  eta: 0:01:13  loss: 0.2776 (0.2789)  time: 0.2196  data: 0.0406  max mem: 15824
[08:39:26.683977] Test:  [ 20/345]  eta: 0:01:05  loss: 0.2571 (0.2641)  time: 0.1790  data: 0.0001  max mem: 15824
[08:39:28.481496] Test:  [ 30/345]  eta: 0:01:01  loss: 0.2394 (0.2585)  time: 0.1794  data: 0.0001  max mem: 15824
[08:39:30.282796] Test:  [ 40/345]  eta: 0:00:58  loss: 0.2394 (0.2562)  time: 0.1799  data: 0.0001  max mem: 15824
[08:39:32.083410] Test:  [ 50/345]  eta: 0:00:55  loss: 0.2432 (0.2548)  time: 0.1800  data: 0.0001  max mem: 15824
[08:39:33.885761] Test:  [ 60/345]  eta: 0:00:53  loss: 0.2432 (0.2552)  time: 0.1801  data: 0.0001  max mem: 15824
[08:39:35.692626] Test:  [ 70/345]  eta: 0:00:51  loss: 0.2432 (0.2547)  time: 0.1804  data: 0.0001  max mem: 15824
[08:39:37.508374] Test:  [ 80/345]  eta: 0:00:49  loss: 0.2468 (0.2548)  time: 0.1811  data: 0.0001  max mem: 15824
[08:39:39.324727] Test:  [ 90/345]  eta: 0:00:47  loss: 0.2480 (0.2556)  time: 0.1815  data: 0.0001  max mem: 15824
[08:39:41.143636] Test:  [100/345]  eta: 0:00:45  loss: 0.2438 (0.2542)  time: 0.1817  data: 0.0001  max mem: 15824
[08:39:42.964344] Test:  [110/345]  eta: 0:00:43  loss: 0.2502 (0.2545)  time: 0.1819  data: 0.0001  max mem: 15824
[08:39:44.791245] Test:  [120/345]  eta: 0:00:41  loss: 0.2573 (0.2546)  time: 0.1823  data: 0.0001  max mem: 15824
[08:39:46.622336] Test:  [130/345]  eta: 0:00:39  loss: 0.2525 (0.2546)  time: 0.1828  data: 0.0001  max mem: 15824
[08:39:48.458064] Test:  [140/345]  eta: 0:00:37  loss: 0.2454 (0.2538)  time: 0.1833  data: 0.0001  max mem: 15824
[08:39:50.294321] Test:  [150/345]  eta: 0:00:35  loss: 0.2401 (0.2534)  time: 0.1835  data: 0.0001  max mem: 15824
[08:39:52.137379] Test:  [160/345]  eta: 0:00:34  loss: 0.2466 (0.2531)  time: 0.1839  data: 0.0001  max mem: 15824
[08:39:53.980816] Test:  [170/345]  eta: 0:00:32  loss: 0.2536 (0.2532)  time: 0.1843  data: 0.0001  max mem: 15824
[08:39:55.827019] Test:  [180/345]  eta: 0:00:30  loss: 0.2428 (0.2527)  time: 0.1844  data: 0.0001  max mem: 15824
[08:39:57.678292] Test:  [190/345]  eta: 0:00:28  loss: 0.2428 (0.2523)  time: 0.1848  data: 0.0001  max mem: 15824
[08:39:59.532681] Test:  [200/345]  eta: 0:00:26  loss: 0.2452 (0.2523)  time: 0.1852  data: 0.0001  max mem: 15824
[08:40:01.391695] Test:  [210/345]  eta: 0:00:24  loss: 0.2502 (0.2527)  time: 0.1856  data: 0.0001  max mem: 15824
[08:40:03.255771] Test:  [220/345]  eta: 0:00:23  loss: 0.2502 (0.2528)  time: 0.1861  data: 0.0001  max mem: 15824
[08:40:05.120910] Test:  [230/345]  eta: 0:00:21  loss: 0.2528 (0.2532)  time: 0.1864  data: 0.0001  max mem: 15824
[08:40:06.987963] Test:  [240/345]  eta: 0:00:19  loss: 0.2585 (0.2536)  time: 0.1866  data: 0.0001  max mem: 15824
[08:40:08.860849] Test:  [250/345]  eta: 0:00:17  loss: 0.2648 (0.2541)  time: 0.1869  data: 0.0001  max mem: 15824
[08:40:10.734946] Test:  [260/345]  eta: 0:00:15  loss: 0.2632 (0.2541)  time: 0.1873  data: 0.0001  max mem: 15824
[08:40:12.613010] Test:  [270/345]  eta: 0:00:13  loss: 0.2448 (0.2538)  time: 0.1875  data: 0.0001  max mem: 15824
[08:40:14.497252] Test:  [280/345]  eta: 0:00:12  loss: 0.2480 (0.2538)  time: 0.1881  data: 0.0001  max mem: 15824
[08:40:16.381632] Test:  [290/345]  eta: 0:00:10  loss: 0.2539 (0.2538)  time: 0.1884  data: 0.0001  max mem: 15824
[08:40:18.270683] Test:  [300/345]  eta: 0:00:08  loss: 0.2535 (0.2541)  time: 0.1886  data: 0.0001  max mem: 15824
[08:40:20.161781] Test:  [310/345]  eta: 0:00:06  loss: 0.2641 (0.2546)  time: 0.1889  data: 0.0001  max mem: 15824
[08:40:22.057297] Test:  [320/345]  eta: 0:00:04  loss: 0.2446 (0.2542)  time: 0.1893  data: 0.0001  max mem: 15824
[08:40:23.955404] Test:  [330/345]  eta: 0:00:02  loss: 0.2430 (0.2539)  time: 0.1896  data: 0.0001  max mem: 15824
[08:40:25.854664] Test:  [340/345]  eta: 0:00:00  loss: 0.2393 (0.2535)  time: 0.1898  data: 0.0001  max mem: 15824
[08:40:26.615504] Test:  [344/345]  eta: 0:00:00  loss: 0.2430 (0.2536)  time: 0.1899  data: 0.0001  max mem: 15824
[08:40:26.688759] Test: Total time: 0:01:04 (0.1861 s / it)
[08:40:37.198932] Test:  [ 0/57]  eta: 0:00:33  loss: 0.4780 (0.4780)  time: 0.5798  data: 0.3996  max mem: 15824
[08:40:38.968777] Test:  [10/57]  eta: 0:00:10  loss: 0.4358 (0.4514)  time: 0.2135  data: 0.0364  max mem: 15824
[08:40:40.747152] Test:  [20/57]  eta: 0:00:07  loss: 0.4358 (0.4346)  time: 0.1773  data: 0.0001  max mem: 15824
[08:40:42.529839] Test:  [30/57]  eta: 0:00:05  loss: 0.3278 (0.3888)  time: 0.1780  data: 0.0001  max mem: 15824
[08:40:44.315703] Test:  [40/57]  eta: 0:00:03  loss: 0.2859 (0.3690)  time: 0.1784  data: 0.0001  max mem: 15824
[08:40:46.102412] Test:  [50/57]  eta: 0:00:01  loss: 0.3222 (0.3683)  time: 0.1786  data: 0.0001  max mem: 15824
[08:40:47.074287] Test:  [56/57]  eta: 0:00:00  loss: 0.3538 (0.3808)  time: 0.1737  data: 0.0001  max mem: 15824
[08:40:47.150538] Test: Total time: 0:00:10 (0.1848 s / it)
[08:40:48.868877] Dice score of the network on the train images: 0.783795, val images: 0.755535
[08:40:48.872993] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:40:49.890813] Epoch: [14]  [  0/345]  eta: 0:05:50  lr: 0.000087  loss: 0.2554 (0.2554)  time: 1.0169  data: 0.3940  max mem: 15824
[08:41:02.237934] Epoch: [14]  [ 20/345]  eta: 0:03:26  lr: 0.000088  loss: 0.2432 (0.2484)  time: 0.6173  data: 0.0001  max mem: 15824
[08:41:14.598400] Epoch: [14]  [ 40/345]  eta: 0:03:11  lr: 0.000088  loss: 0.2495 (0.2534)  time: 0.6180  data: 0.0001  max mem: 15824
[08:41:26.992908] Epoch: [14]  [ 60/345]  eta: 0:02:58  lr: 0.000089  loss: 0.2562 (0.2550)  time: 0.6197  data: 0.0001  max mem: 15824
[08:41:39.388438] Epoch: [14]  [ 80/345]  eta: 0:02:45  lr: 0.000089  loss: 0.2437 (0.2528)  time: 0.6197  data: 0.0001  max mem: 15824
[08:41:51.796316] Epoch: [14]  [100/345]  eta: 0:02:32  lr: 0.000089  loss: 0.2570 (0.2557)  time: 0.6203  data: 0.0001  max mem: 15824
[08:42:04.229561] Epoch: [14]  [120/345]  eta: 0:02:20  lr: 0.000090  loss: 0.2606 (0.2563)  time: 0.6216  data: 0.0001  max mem: 15824
[08:42:16.676953] Epoch: [14]  [140/345]  eta: 0:02:07  lr: 0.000090  loss: 0.2507 (0.2563)  time: 0.6223  data: 0.0001  max mem: 15824
[08:42:29.105966] Epoch: [14]  [160/345]  eta: 0:01:55  lr: 0.000090  loss: 0.2568 (0.2573)  time: 0.6214  data: 0.0001  max mem: 15824
[08:42:41.532091] Epoch: [14]  [180/345]  eta: 0:01:42  lr: 0.000091  loss: 0.2372 (0.2564)  time: 0.6213  data: 0.0001  max mem: 15824
[08:42:53.961835] Epoch: [14]  [200/345]  eta: 0:01:30  lr: 0.000091  loss: 0.2500 (0.2559)  time: 0.6214  data: 0.0001  max mem: 15824
[08:43:06.394149] Epoch: [14]  [220/345]  eta: 0:01:17  lr: 0.000091  loss: 0.2562 (0.2566)  time: 0.6216  data: 0.0001  max mem: 15824
[08:43:18.820663] Epoch: [14]  [240/345]  eta: 0:01:05  lr: 0.000092  loss: 0.2569 (0.2569)  time: 0.6213  data: 0.0001  max mem: 15824
[08:43:31.221546] Epoch: [14]  [260/345]  eta: 0:00:52  lr: 0.000092  loss: 0.2493 (0.2563)  time: 0.6200  data: 0.0001  max mem: 15824
[08:43:43.642451] Epoch: [14]  [280/345]  eta: 0:00:40  lr: 0.000093  loss: 0.2414 (0.2558)  time: 0.6210  data: 0.0001  max mem: 15824
[08:43:56.044255] Epoch: [14]  [300/345]  eta: 0:00:27  lr: 0.000093  loss: 0.2605 (0.2558)  time: 0.6200  data: 0.0001  max mem: 15824
[08:44:08.451772] Epoch: [14]  [320/345]  eta: 0:00:15  lr: 0.000093  loss: 0.2491 (0.2557)  time: 0.6203  data: 0.0001  max mem: 15824
[08:44:20.861166] Epoch: [14]  [340/345]  eta: 0:00:03  lr: 0.000094  loss: 0.2538 (0.2559)  time: 0.6204  data: 0.0001  max mem: 15824
[08:44:23.338325] Epoch: [14]  [344/345]  eta: 0:00:00  lr: 0.000094  loss: 0.2496 (0.2559)  time: 0.6198  data: 0.0001  max mem: 15824
[08:44:23.410219] Epoch: [14] Total time: 0:03:34 (0.6218 s / it)
[08:44:23.410935] Averaged stats: lr: 0.000094  loss: 0.2496 (0.2559)
[08:44:24.002565] Test:  [  0/345]  eta: 0:03:22  loss: 0.2558 (0.2558)  time: 0.5863  data: 0.4048  max mem: 15824
[08:44:25.794157] Test:  [ 10/345]  eta: 0:01:12  loss: 0.2380 (0.2409)  time: 0.2161  data: 0.0369  max mem: 15824
[08:44:27.584030] Test:  [ 20/345]  eta: 0:01:04  loss: 0.2380 (0.2427)  time: 0.1790  data: 0.0001  max mem: 15824
[08:44:29.381410] Test:  [ 30/345]  eta: 0:01:00  loss: 0.2366 (0.2396)  time: 0.1793  data: 0.0001  max mem: 15824
[08:44:31.183341] Test:  [ 40/345]  eta: 0:00:57  loss: 0.2244 (0.2354)  time: 0.1799  data: 0.0001  max mem: 15824
[08:44:32.985761] Test:  [ 50/345]  eta: 0:00:55  loss: 0.2394 (0.2379)  time: 0.1802  data: 0.0001  max mem: 15824
[08:44:34.789077] Test:  [ 60/345]  eta: 0:00:53  loss: 0.2306 (0.2345)  time: 0.1802  data: 0.0001  max mem: 15824
[08:44:36.596918] Test:  [ 70/345]  eta: 0:00:51  loss: 0.2125 (0.2337)  time: 0.1805  data: 0.0001  max mem: 15824
[08:44:38.410563] Test:  [ 80/345]  eta: 0:00:49  loss: 0.2149 (0.2323)  time: 0.1810  data: 0.0001  max mem: 15824
[08:44:40.228137] Test:  [ 90/345]  eta: 0:00:47  loss: 0.2195 (0.2317)  time: 0.1815  data: 0.0001  max mem: 15824
[08:44:42.047385] Test:  [100/345]  eta: 0:00:45  loss: 0.2223 (0.2326)  time: 0.1818  data: 0.0001  max mem: 15824
[08:44:43.871189] Test:  [110/345]  eta: 0:00:43  loss: 0.2244 (0.2325)  time: 0.1821  data: 0.0001  max mem: 15824
[08:44:45.699030] Test:  [120/345]  eta: 0:00:41  loss: 0.2330 (0.2328)  time: 0.1825  data: 0.0001  max mem: 15824
[08:44:47.528560] Test:  [130/345]  eta: 0:00:39  loss: 0.2283 (0.2327)  time: 0.1828  data: 0.0001  max mem: 15824
[08:44:49.361444] Test:  [140/345]  eta: 0:00:37  loss: 0.2363 (0.2339)  time: 0.1831  data: 0.0001  max mem: 15824
[08:44:51.197624] Test:  [150/345]  eta: 0:00:35  loss: 0.2451 (0.2339)  time: 0.1834  data: 0.0001  max mem: 15824
[08:44:53.039315] Test:  [160/345]  eta: 0:00:34  loss: 0.2262 (0.2340)  time: 0.1838  data: 0.0001  max mem: 15824
[08:44:54.883353] Test:  [170/345]  eta: 0:00:32  loss: 0.2287 (0.2344)  time: 0.1842  data: 0.0001  max mem: 15824
[08:44:56.730152] Test:  [180/345]  eta: 0:00:30  loss: 0.2424 (0.2345)  time: 0.1845  data: 0.0001  max mem: 15824
[08:44:58.583404] Test:  [190/345]  eta: 0:00:28  loss: 0.2288 (0.2345)  time: 0.1849  data: 0.0001  max mem: 15824
[08:45:00.440156] Test:  [200/345]  eta: 0:00:26  loss: 0.2380 (0.2353)  time: 0.1854  data: 0.0001  max mem: 15824
[08:45:02.299249] Test:  [210/345]  eta: 0:00:24  loss: 0.2379 (0.2352)  time: 0.1857  data: 0.0001  max mem: 15824
[08:45:04.160026] Test:  [220/345]  eta: 0:00:23  loss: 0.2379 (0.2353)  time: 0.1859  data: 0.0001  max mem: 15824
[08:45:06.022614] Test:  [230/345]  eta: 0:00:21  loss: 0.2343 (0.2352)  time: 0.1861  data: 0.0001  max mem: 15824
[08:45:07.889931] Test:  [240/345]  eta: 0:00:19  loss: 0.2247 (0.2349)  time: 0.1864  data: 0.0001  max mem: 15824
[08:45:09.761732] Test:  [250/345]  eta: 0:00:17  loss: 0.2249 (0.2350)  time: 0.1869  data: 0.0001  max mem: 15824
[08:45:11.636994] Test:  [260/345]  eta: 0:00:15  loss: 0.2325 (0.2351)  time: 0.1873  data: 0.0001  max mem: 15824
[08:45:13.516697] Test:  [270/345]  eta: 0:00:13  loss: 0.2294 (0.2348)  time: 0.1877  data: 0.0001  max mem: 15824
[08:45:15.397921] Test:  [280/345]  eta: 0:00:12  loss: 0.2167 (0.2351)  time: 0.1880  data: 0.0001  max mem: 15824
[08:45:17.284219] Test:  [290/345]  eta: 0:00:10  loss: 0.2330 (0.2351)  time: 0.1883  data: 0.0001  max mem: 15824
[08:45:19.175013] Test:  [300/345]  eta: 0:00:08  loss: 0.2338 (0.2352)  time: 0.1888  data: 0.0001  max mem: 15824
[08:45:21.070730] Test:  [310/345]  eta: 0:00:06  loss: 0.2400 (0.2355)  time: 0.1893  data: 0.0001  max mem: 15824
[08:45:22.966654] Test:  [320/345]  eta: 0:00:04  loss: 0.2461 (0.2357)  time: 0.1895  data: 0.0001  max mem: 15824
[08:45:24.866247] Test:  [330/345]  eta: 0:00:02  loss: 0.2385 (0.2358)  time: 0.1897  data: 0.0001  max mem: 15824
[08:45:26.768557] Test:  [340/345]  eta: 0:00:00  loss: 0.2354 (0.2359)  time: 0.1900  data: 0.0001  max mem: 15824
[08:45:27.529416] Test:  [344/345]  eta: 0:00:00  loss: 0.2354 (0.2358)  time: 0.1901  data: 0.0001  max mem: 15824
[08:45:27.600271] Test: Total time: 0:01:04 (0.1860 s / it)
[08:45:38.077323] Test:  [ 0/57]  eta: 0:00:31  loss: 0.4266 (0.4266)  time: 0.5544  data: 0.3739  max mem: 15824
[08:45:39.851125] Test:  [10/57]  eta: 0:00:09  loss: 0.4266 (0.4430)  time: 0.2116  data: 0.0341  max mem: 15824
[08:45:41.629141] Test:  [20/57]  eta: 0:00:07  loss: 0.4078 (0.4226)  time: 0.1775  data: 0.0001  max mem: 15824
[08:45:43.411107] Test:  [30/57]  eta: 0:00:05  loss: 0.2965 (0.3768)  time: 0.1779  data: 0.0001  max mem: 15824
[08:45:45.196013] Test:  [40/57]  eta: 0:00:03  loss: 0.2847 (0.3551)  time: 0.1783  data: 0.0001  max mem: 15824
[08:45:46.980411] Test:  [50/57]  eta: 0:00:01  loss: 0.2961 (0.3577)  time: 0.1784  data: 0.0001  max mem: 15824
[08:45:47.954215] Test:  [56/57]  eta: 0:00:00  loss: 0.3523 (0.3768)  time: 0.1736  data: 0.0001  max mem: 15824
[08:45:48.023111] Test: Total time: 0:00:10 (0.1842 s / it)
[08:45:49.724142] Dice score of the network on the train images: 0.796409, val images: 0.747048
[08:45:49.728388] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:45:50.783373] Epoch: [15]  [  0/345]  eta: 0:06:03  lr: 0.000094  loss: 0.2231 (0.2231)  time: 1.0539  data: 0.4317  max mem: 15824
[08:46:03.118222] Epoch: [15]  [ 20/345]  eta: 0:03:27  lr: 0.000094  loss: 0.2352 (0.2365)  time: 0.6167  data: 0.0001  max mem: 15824
[08:46:15.461957] Epoch: [15]  [ 40/345]  eta: 0:03:11  lr: 0.000094  loss: 0.2435 (0.2448)  time: 0.6171  data: 0.0001  max mem: 15824
[08:46:27.831946] Epoch: [15]  [ 60/345]  eta: 0:02:58  lr: 0.000095  loss: 0.2446 (0.2465)  time: 0.6184  data: 0.0001  max mem: 15824
[08:46:40.232285] Epoch: [15]  [ 80/345]  eta: 0:02:45  lr: 0.000095  loss: 0.2426 (0.2463)  time: 0.6200  data: 0.0001  max mem: 15824
[08:46:52.630052] Epoch: [15]  [100/345]  eta: 0:02:32  lr: 0.000096  loss: 0.2404 (0.2455)  time: 0.6198  data: 0.0001  max mem: 15824
[08:47:05.043824] Epoch: [15]  [120/345]  eta: 0:02:20  lr: 0.000096  loss: 0.2371 (0.2446)  time: 0.6206  data: 0.0001  max mem: 15824
[08:47:17.452857] Epoch: [15]  [140/345]  eta: 0:02:07  lr: 0.000096  loss: 0.2281 (0.2437)  time: 0.6204  data: 0.0001  max mem: 15824
[08:47:29.870370] Epoch: [15]  [160/345]  eta: 0:01:55  lr: 0.000097  loss: 0.2515 (0.2448)  time: 0.6208  data: 0.0001  max mem: 15824
[08:47:42.277048] Epoch: [15]  [180/345]  eta: 0:01:42  lr: 0.000097  loss: 0.2406 (0.2445)  time: 0.6203  data: 0.0001  max mem: 15824
[08:47:54.689206] Epoch: [15]  [200/345]  eta: 0:01:30  lr: 0.000097  loss: 0.2422 (0.2453)  time: 0.6206  data: 0.0001  max mem: 15824
[08:48:07.092147] Epoch: [15]  [220/345]  eta: 0:01:17  lr: 0.000098  loss: 0.2345 (0.2446)  time: 0.6201  data: 0.0001  max mem: 15824
[08:48:19.506344] Epoch: [15]  [240/345]  eta: 0:01:05  lr: 0.000098  loss: 0.2265 (0.2431)  time: 0.6207  data: 0.0001  max mem: 15824
[08:48:31.914523] Epoch: [15]  [260/345]  eta: 0:00:52  lr: 0.000098  loss: 0.2492 (0.2436)  time: 0.6204  data: 0.0001  max mem: 15824
[08:48:44.396952] Epoch: [15]  [280/345]  eta: 0:00:40  lr: 0.000099  loss: 0.2348 (0.2433)  time: 0.6241  data: 0.0001  max mem: 15824
[08:48:56.789892] Epoch: [15]  [300/345]  eta: 0:00:27  lr: 0.000099  loss: 0.2343 (0.2429)  time: 0.6196  data: 0.0001  max mem: 15824
[08:49:09.197368] Epoch: [15]  [320/345]  eta: 0:00:15  lr: 0.000100  loss: 0.2472 (0.2435)  time: 0.6203  data: 0.0001  max mem: 15824
[08:49:21.597951] Epoch: [15]  [340/345]  eta: 0:00:03  lr: 0.000100  loss: 0.2505 (0.2438)  time: 0.6200  data: 0.0001  max mem: 15824
[08:49:24.075691] Epoch: [15]  [344/345]  eta: 0:00:00  lr: 0.000100  loss: 0.2487 (0.2435)  time: 0.6195  data: 0.0001  max mem: 15824
[08:49:24.150114] Epoch: [15] Total time: 0:03:34 (0.6215 s / it)
[08:49:24.150357] Averaged stats: lr: 0.000100  loss: 0.2487 (0.2435)
[08:49:24.755445] Test:  [  0/345]  eta: 0:03:26  loss: 0.2291 (0.2291)  time: 0.5994  data: 0.4158  max mem: 15824
[08:49:26.546071] Test:  [ 10/345]  eta: 0:01:12  loss: 0.2120 (0.2202)  time: 0.2172  data: 0.0379  max mem: 15824
[08:49:28.342240] Test:  [ 20/345]  eta: 0:01:04  loss: 0.2179 (0.2325)  time: 0.1793  data: 0.0001  max mem: 15824
[08:49:30.140793] Test:  [ 30/345]  eta: 0:01:00  loss: 0.2464 (0.2369)  time: 0.1797  data: 0.0001  max mem: 15824
[08:49:31.944002] Test:  [ 40/345]  eta: 0:00:57  loss: 0.2355 (0.2365)  time: 0.1800  data: 0.0001  max mem: 15824
[08:49:33.747066] Test:  [ 50/345]  eta: 0:00:55  loss: 0.2302 (0.2370)  time: 0.1803  data: 0.0001  max mem: 15824
[08:49:35.548971] Test:  [ 60/345]  eta: 0:00:53  loss: 0.2306 (0.2378)  time: 0.1802  data: 0.0001  max mem: 15824
[08:49:37.354952] Test:  [ 70/345]  eta: 0:00:51  loss: 0.2338 (0.2362)  time: 0.1803  data: 0.0001  max mem: 15824
[08:49:39.168949] Test:  [ 80/345]  eta: 0:00:49  loss: 0.2244 (0.2346)  time: 0.1809  data: 0.0001  max mem: 15824
[08:49:40.985557] Test:  [ 90/345]  eta: 0:00:47  loss: 0.2176 (0.2335)  time: 0.1815  data: 0.0001  max mem: 15824
[08:49:42.804877] Test:  [100/345]  eta: 0:00:45  loss: 0.2293 (0.2345)  time: 0.1817  data: 0.0001  max mem: 15824
[08:49:44.626821] Test:  [110/345]  eta: 0:00:43  loss: 0.2306 (0.2335)  time: 0.1820  data: 0.0001  max mem: 15824
[08:49:46.452631] Test:  [120/345]  eta: 0:00:41  loss: 0.2297 (0.2336)  time: 0.1823  data: 0.0001  max mem: 15824
[08:49:48.284704] Test:  [130/345]  eta: 0:00:39  loss: 0.2297 (0.2330)  time: 0.1828  data: 0.0001  max mem: 15824
[08:49:50.118195] Test:  [140/345]  eta: 0:00:37  loss: 0.2290 (0.2333)  time: 0.1832  data: 0.0001  max mem: 15824
[08:49:51.955587] Test:  [150/345]  eta: 0:00:35  loss: 0.2297 (0.2326)  time: 0.1835  data: 0.0001  max mem: 15824
[08:49:53.797689] Test:  [160/345]  eta: 0:00:34  loss: 0.2124 (0.2324)  time: 0.1839  data: 0.0001  max mem: 15824
[08:49:55.643742] Test:  [170/345]  eta: 0:00:32  loss: 0.2173 (0.2324)  time: 0.1843  data: 0.0001  max mem: 15824
[08:49:57.494142] Test:  [180/345]  eta: 0:00:30  loss: 0.2415 (0.2330)  time: 0.1847  data: 0.0001  max mem: 15824
[08:49:59.346928] Test:  [190/345]  eta: 0:00:28  loss: 0.2303 (0.2329)  time: 0.1851  data: 0.0001  max mem: 15824
[08:50:01.203578] Test:  [200/345]  eta: 0:00:26  loss: 0.2323 (0.2329)  time: 0.1854  data: 0.0001  max mem: 15824
[08:50:03.063360] Test:  [210/345]  eta: 0:00:24  loss: 0.2242 (0.2323)  time: 0.1858  data: 0.0001  max mem: 15824
[08:50:04.929948] Test:  [220/345]  eta: 0:00:23  loss: 0.2242 (0.2330)  time: 0.1863  data: 0.0001  max mem: 15824
[08:50:06.796479] Test:  [230/345]  eta: 0:00:21  loss: 0.2396 (0.2328)  time: 0.1866  data: 0.0001  max mem: 15824
[08:50:08.667044] Test:  [240/345]  eta: 0:00:19  loss: 0.2265 (0.2324)  time: 0.1868  data: 0.0001  max mem: 15824
[08:50:10.538917] Test:  [250/345]  eta: 0:00:17  loss: 0.2246 (0.2322)  time: 0.1871  data: 0.0001  max mem: 15824
[08:50:12.412328] Test:  [260/345]  eta: 0:00:15  loss: 0.2217 (0.2317)  time: 0.1872  data: 0.0001  max mem: 15824
[08:50:14.291164] Test:  [270/345]  eta: 0:00:13  loss: 0.2251 (0.2323)  time: 0.1876  data: 0.0001  max mem: 15824
[08:50:16.173917] Test:  [280/345]  eta: 0:00:12  loss: 0.2312 (0.2324)  time: 0.1880  data: 0.0001  max mem: 15824
[08:50:18.061292] Test:  [290/345]  eta: 0:00:10  loss: 0.2137 (0.2316)  time: 0.1884  data: 0.0001  max mem: 15824
[08:50:19.951832] Test:  [300/345]  eta: 0:00:08  loss: 0.2105 (0.2312)  time: 0.1888  data: 0.0001  max mem: 15824
[08:50:21.847213] Test:  [310/345]  eta: 0:00:06  loss: 0.2281 (0.2315)  time: 0.1892  data: 0.0001  max mem: 15824
[08:50:23.746086] Test:  [320/345]  eta: 0:00:04  loss: 0.2480 (0.2317)  time: 0.1896  data: 0.0001  max mem: 15824
[08:50:25.646403] Test:  [330/345]  eta: 0:00:02  loss: 0.2489 (0.2319)  time: 0.1899  data: 0.0001  max mem: 15824
[08:50:27.552534] Test:  [340/345]  eta: 0:00:00  loss: 0.2341 (0.2320)  time: 0.1903  data: 0.0001  max mem: 15824
[08:50:28.313524] Test:  [344/345]  eta: 0:00:00  loss: 0.2348 (0.2319)  time: 0.1903  data: 0.0001  max mem: 15824
[08:50:28.388564] Test: Total time: 0:01:04 (0.1862 s / it)
[08:50:38.916817] Test:  [ 0/57]  eta: 0:00:35  loss: 0.4319 (0.4319)  time: 0.6190  data: 0.4393  max mem: 15824
[08:50:40.688699] Test:  [10/57]  eta: 0:00:10  loss: 0.4319 (0.4552)  time: 0.2173  data: 0.0400  max mem: 15824
[08:50:42.466732] Test:  [20/57]  eta: 0:00:07  loss: 0.4403 (0.4380)  time: 0.1774  data: 0.0001  max mem: 15824
[08:50:44.248174] Test:  [30/57]  eta: 0:00:05  loss: 0.3180 (0.3844)  time: 0.1779  data: 0.0001  max mem: 15824
[08:50:46.032946] Test:  [40/57]  eta: 0:00:03  loss: 0.2852 (0.3577)  time: 0.1782  data: 0.0001  max mem: 15824
[08:50:47.818763] Test:  [50/57]  eta: 0:00:01  loss: 0.2907 (0.3576)  time: 0.1785  data: 0.0001  max mem: 15824
[08:50:48.792253] Test:  [56/57]  eta: 0:00:00  loss: 0.3291 (0.3715)  time: 0.1736  data: 0.0001  max mem: 15824
[08:50:48.865643] Test: Total time: 0:00:10 (0.1854 s / it)
[08:50:50.580738] Dice score of the network on the train images: 0.784850, val images: 0.779086
[08:50:50.580966] saving best_dice_model_0 @ epoch 15
[08:50:51.623752] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:50:52.628786] Epoch: [16]  [  0/345]  eta: 0:05:46  lr: 0.000100  loss: 0.2549 (0.2549)  time: 1.0038  data: 0.3791  max mem: 15824
[08:51:04.955896] Epoch: [16]  [ 20/345]  eta: 0:03:26  lr: 0.000100  loss: 0.2188 (0.2347)  time: 0.6163  data: 0.0001  max mem: 15824
[08:51:17.297213] Epoch: [16]  [ 40/345]  eta: 0:03:10  lr: 0.000101  loss: 0.2341 (0.2328)  time: 0.6170  data: 0.0001  max mem: 15824
[08:51:29.677856] Epoch: [16]  [ 60/345]  eta: 0:02:57  lr: 0.000101  loss: 0.2407 (0.2356)  time: 0.6190  data: 0.0001  max mem: 15824
[08:51:42.054431] Epoch: [16]  [ 80/345]  eta: 0:02:44  lr: 0.000101  loss: 0.2433 (0.2375)  time: 0.6188  data: 0.0001  max mem: 15824
[08:51:54.452803] Epoch: [16]  [100/345]  eta: 0:02:32  lr: 0.000102  loss: 0.2383 (0.2376)  time: 0.6199  data: 0.0001  max mem: 15824
[08:52:06.872450] Epoch: [16]  [120/345]  eta: 0:02:19  lr: 0.000102  loss: 0.2333 (0.2380)  time: 0.6209  data: 0.0001  max mem: 15824
[08:52:19.287531] Epoch: [16]  [140/345]  eta: 0:02:07  lr: 0.000103  loss: 0.2309 (0.2373)  time: 0.6207  data: 0.0001  max mem: 15824
[08:52:31.821369] Epoch: [16]  [160/345]  eta: 0:01:55  lr: 0.000103  loss: 0.2329 (0.2374)  time: 0.6266  data: 0.0001  max mem: 15824
[08:52:44.233682] Epoch: [16]  [180/345]  eta: 0:01:42  lr: 0.000103  loss: 0.2302 (0.2376)  time: 0.6206  data: 0.0001  max mem: 15824
[08:52:56.656337] Epoch: [16]  [200/345]  eta: 0:01:30  lr: 0.000104  loss: 0.2345 (0.2382)  time: 0.6211  data: 0.0001  max mem: 15824
[08:53:09.067507] Epoch: [16]  [220/345]  eta: 0:01:17  lr: 0.000104  loss: 0.2334 (0.2373)  time: 0.6205  data: 0.0001  max mem: 15824
[08:53:21.475261] Epoch: [16]  [240/345]  eta: 0:01:05  lr: 0.000104  loss: 0.2363 (0.2370)  time: 0.6203  data: 0.0001  max mem: 15824
[08:53:33.854817] Epoch: [16]  [260/345]  eta: 0:00:52  lr: 0.000105  loss: 0.2346 (0.2367)  time: 0.6189  data: 0.0001  max mem: 15824
[08:53:46.218303] Epoch: [16]  [280/345]  eta: 0:00:40  lr: 0.000105  loss: 0.2295 (0.2363)  time: 0.6181  data: 0.0001  max mem: 15824
[08:53:58.607234] Epoch: [16]  [300/345]  eta: 0:00:27  lr: 0.000105  loss: 0.2396 (0.2367)  time: 0.6194  data: 0.0001  max mem: 15824
[08:54:10.975422] Epoch: [16]  [320/345]  eta: 0:00:15  lr: 0.000106  loss: 0.2327 (0.2366)  time: 0.6184  data: 0.0001  max mem: 15824
[08:54:23.367838] Epoch: [16]  [340/345]  eta: 0:00:03  lr: 0.000106  loss: 0.2275 (0.2363)  time: 0.6196  data: 0.0001  max mem: 15824
[08:54:25.842675] Epoch: [16]  [344/345]  eta: 0:00:00  lr: 0.000106  loss: 0.2260 (0.2364)  time: 0.6196  data: 0.0001  max mem: 15824
[08:54:25.915339] Epoch: [16] Total time: 0:03:34 (0.6211 s / it)
[08:54:25.915627] Averaged stats: lr: 0.000106  loss: 0.2260 (0.2364)
[08:54:26.577334] Test:  [  0/345]  eta: 0:03:46  loss: 0.2897 (0.2897)  time: 0.6568  data: 0.4754  max mem: 15824
[08:54:28.366026] Test:  [ 10/345]  eta: 0:01:14  loss: 0.2416 (0.2465)  time: 0.2222  data: 0.0433  max mem: 15824
[08:54:30.164392] Test:  [ 20/345]  eta: 0:01:05  loss: 0.2365 (0.2441)  time: 0.1792  data: 0.0001  max mem: 15824
[08:54:31.962661] Test:  [ 30/345]  eta: 0:01:01  loss: 0.2413 (0.2473)  time: 0.1797  data: 0.0001  max mem: 15824
[08:54:33.765485] Test:  [ 40/345]  eta: 0:00:58  loss: 0.2413 (0.2463)  time: 0.1800  data: 0.0001  max mem: 15824
[08:54:35.568912] Test:  [ 50/345]  eta: 0:00:55  loss: 0.2401 (0.2444)  time: 0.1802  data: 0.0001  max mem: 15824
[08:54:37.373522] Test:  [ 60/345]  eta: 0:00:53  loss: 0.2319 (0.2438)  time: 0.1803  data: 0.0001  max mem: 15824
[08:54:39.182855] Test:  [ 70/345]  eta: 0:00:51  loss: 0.2319 (0.2441)  time: 0.1806  data: 0.0001  max mem: 15824
[08:54:40.995495] Test:  [ 80/345]  eta: 0:00:49  loss: 0.2459 (0.2455)  time: 0.1810  data: 0.0001  max mem: 15824
[08:54:42.811746] Test:  [ 90/345]  eta: 0:00:47  loss: 0.2449 (0.2460)  time: 0.1814  data: 0.0001  max mem: 15824
[08:54:44.630413] Test:  [100/345]  eta: 0:00:45  loss: 0.2430 (0.2462)  time: 0.1817  data: 0.0001  max mem: 15824
[08:54:46.453017] Test:  [110/345]  eta: 0:00:43  loss: 0.2371 (0.2459)  time: 0.1820  data: 0.0001  max mem: 15824
[08:54:48.278841] Test:  [120/345]  eta: 0:00:41  loss: 0.2422 (0.2458)  time: 0.1824  data: 0.0001  max mem: 15824
[08:54:50.108976] Test:  [130/345]  eta: 0:00:39  loss: 0.2439 (0.2457)  time: 0.1827  data: 0.0001  max mem: 15824
[08:54:51.942166] Test:  [140/345]  eta: 0:00:37  loss: 0.2439 (0.2459)  time: 0.1831  data: 0.0001  max mem: 15824
[08:54:53.778582] Test:  [150/345]  eta: 0:00:35  loss: 0.2447 (0.2459)  time: 0.1834  data: 0.0001  max mem: 15824
[08:54:55.619683] Test:  [160/345]  eta: 0:00:34  loss: 0.2440 (0.2456)  time: 0.1838  data: 0.0001  max mem: 15824
[08:54:57.463009] Test:  [170/345]  eta: 0:00:32  loss: 0.2428 (0.2462)  time: 0.1842  data: 0.0001  max mem: 15824
[08:54:59.308384] Test:  [180/345]  eta: 0:00:30  loss: 0.2428 (0.2455)  time: 0.1844  data: 0.0001  max mem: 15824
[08:55:01.158561] Test:  [190/345]  eta: 0:00:28  loss: 0.2307 (0.2447)  time: 0.1847  data: 0.0001  max mem: 15824
[08:55:03.011662] Test:  [200/345]  eta: 0:00:26  loss: 0.2246 (0.2441)  time: 0.1851  data: 0.0001  max mem: 15824
[08:55:04.869822] Test:  [210/345]  eta: 0:00:24  loss: 0.2339 (0.2445)  time: 0.1855  data: 0.0001  max mem: 15824
[08:55:06.730939] Test:  [220/345]  eta: 0:00:23  loss: 0.2514 (0.2450)  time: 0.1859  data: 0.0001  max mem: 15824
[08:55:08.597026] Test:  [230/345]  eta: 0:00:21  loss: 0.2562 (0.2453)  time: 0.1863  data: 0.0001  max mem: 15824
[08:55:10.465601] Test:  [240/345]  eta: 0:00:19  loss: 0.2478 (0.2455)  time: 0.1867  data: 0.0001  max mem: 15824
[08:55:12.335030] Test:  [250/345]  eta: 0:00:17  loss: 0.2438 (0.2453)  time: 0.1868  data: 0.0001  max mem: 15824
[08:55:14.208994] Test:  [260/345]  eta: 0:00:15  loss: 0.2464 (0.2456)  time: 0.1871  data: 0.0001  max mem: 15824
[08:55:16.085887] Test:  [270/345]  eta: 0:00:13  loss: 0.2506 (0.2456)  time: 0.1875  data: 0.0001  max mem: 15824
[08:55:17.966358] Test:  [280/345]  eta: 0:00:12  loss: 0.2502 (0.2455)  time: 0.1878  data: 0.0001  max mem: 15824
[08:55:19.851500] Test:  [290/345]  eta: 0:00:10  loss: 0.2407 (0.2456)  time: 0.1882  data: 0.0001  max mem: 15824
[08:55:21.744520] Test:  [300/345]  eta: 0:00:08  loss: 0.2438 (0.2458)  time: 0.1888  data: 0.0001  max mem: 15824
[08:55:23.638625] Test:  [310/345]  eta: 0:00:06  loss: 0.2477 (0.2458)  time: 0.1893  data: 0.0001  max mem: 15824
[08:55:25.533751] Test:  [320/345]  eta: 0:00:04  loss: 0.2472 (0.2460)  time: 0.1894  data: 0.0001  max mem: 15824
[08:55:27.434372] Test:  [330/345]  eta: 0:00:02  loss: 0.2478 (0.2464)  time: 0.1897  data: 0.0001  max mem: 15824
[08:55:29.334179] Test:  [340/345]  eta: 0:00:00  loss: 0.2432 (0.2463)  time: 0.1900  data: 0.0001  max mem: 15824
[08:55:30.096242] Test:  [344/345]  eta: 0:00:00  loss: 0.2301 (0.2461)  time: 0.1901  data: 0.0001  max mem: 15824
[08:55:30.174479] Test: Total time: 0:01:04 (0.1862 s / it)
[08:55:40.715147] Test:  [ 0/57]  eta: 0:00:32  loss: 0.5470 (0.5470)  time: 0.5723  data: 0.3926  max mem: 15824
[08:55:42.485612] Test:  [10/57]  eta: 0:00:10  loss: 0.4867 (0.5123)  time: 0.2129  data: 0.0358  max mem: 15824
[08:55:44.265645] Test:  [20/57]  eta: 0:00:07  loss: 0.4899 (0.4988)  time: 0.1774  data: 0.0001  max mem: 15824
[08:55:46.046984] Test:  [30/57]  eta: 0:00:05  loss: 0.3553 (0.4436)  time: 0.1780  data: 0.0001  max mem: 15824
[08:55:47.834205] Test:  [40/57]  eta: 0:00:03  loss: 0.3429 (0.4243)  time: 0.1784  data: 0.0001  max mem: 15824
[08:55:49.621765] Test:  [50/57]  eta: 0:00:01  loss: 0.3980 (0.4313)  time: 0.1787  data: 0.0001  max mem: 15824
[08:55:50.596648] Test:  [56/57]  eta: 0:00:00  loss: 0.4352 (0.4521)  time: 0.1738  data: 0.0000  max mem: 15824
[08:55:50.669739] Test: Total time: 0:00:10 (0.1847 s / it)
[08:55:52.382141] Dice score of the network on the train images: 0.822057, val images: 0.723731
[08:55:52.382364] saving best_prec_model_0 @ epoch 16
[08:55:53.574002] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:55:54.639214] Epoch: [17]  [  0/345]  eta: 0:06:07  lr: 0.000106  loss: 0.2093 (0.2093)  time: 1.0640  data: 0.4404  max mem: 15824
[08:56:06.959427] Epoch: [17]  [ 20/345]  eta: 0:03:27  lr: 0.000107  loss: 0.2173 (0.2262)  time: 0.6160  data: 0.0001  max mem: 15824
[08:56:19.322628] Epoch: [17]  [ 40/345]  eta: 0:03:11  lr: 0.000107  loss: 0.2348 (0.2294)  time: 0.6181  data: 0.0001  max mem: 15824
[08:56:31.721293] Epoch: [17]  [ 60/345]  eta: 0:02:58  lr: 0.000107  loss: 0.2246 (0.2296)  time: 0.6199  data: 0.0001  max mem: 15824
[08:56:44.114700] Epoch: [17]  [ 80/345]  eta: 0:02:45  lr: 0.000108  loss: 0.2495 (0.2350)  time: 0.6196  data: 0.0001  max mem: 15824
[08:56:56.501859] Epoch: [17]  [100/345]  eta: 0:02:32  lr: 0.000108  loss: 0.2236 (0.2337)  time: 0.6193  data: 0.0001  max mem: 15824
[08:57:08.867810] Epoch: [17]  [120/345]  eta: 0:02:19  lr: 0.000108  loss: 0.2388 (0.2344)  time: 0.6182  data: 0.0001  max mem: 15824
[08:57:21.256748] Epoch: [17]  [140/345]  eta: 0:02:07  lr: 0.000109  loss: 0.2350 (0.2350)  time: 0.6194  data: 0.0001  max mem: 15824
[08:57:33.681430] Epoch: [17]  [160/345]  eta: 0:01:55  lr: 0.000109  loss: 0.2249 (0.2343)  time: 0.6212  data: 0.0001  max mem: 15824
[08:57:46.109360] Epoch: [17]  [180/345]  eta: 0:01:42  lr: 0.000110  loss: 0.2133 (0.2324)  time: 0.6214  data: 0.0001  max mem: 15824
[08:57:58.521476] Epoch: [17]  [200/345]  eta: 0:01:30  lr: 0.000110  loss: 0.2114 (0.2323)  time: 0.6206  data: 0.0001  max mem: 15824
[08:58:10.934979] Epoch: [17]  [220/345]  eta: 0:01:17  lr: 0.000110  loss: 0.2287 (0.2320)  time: 0.6206  data: 0.0001  max mem: 15824
[08:58:23.339379] Epoch: [17]  [240/345]  eta: 0:01:05  lr: 0.000111  loss: 0.2412 (0.2324)  time: 0.6202  data: 0.0001  max mem: 15824
[08:58:35.738236] Epoch: [17]  [260/345]  eta: 0:00:52  lr: 0.000111  loss: 0.2378 (0.2327)  time: 0.6199  data: 0.0001  max mem: 15824
[08:58:48.132763] Epoch: [17]  [280/345]  eta: 0:00:40  lr: 0.000111  loss: 0.2193 (0.2316)  time: 0.6197  data: 0.0001  max mem: 15824
[08:59:00.545444] Epoch: [17]  [300/345]  eta: 0:00:27  lr: 0.000112  loss: 0.2244 (0.2312)  time: 0.6206  data: 0.0001  max mem: 15824
[08:59:12.950850] Epoch: [17]  [320/345]  eta: 0:00:15  lr: 0.000112  loss: 0.2255 (0.2311)  time: 0.6202  data: 0.0001  max mem: 15824
[08:59:25.336859] Epoch: [17]  [340/345]  eta: 0:00:03  lr: 0.000112  loss: 0.2388 (0.2316)  time: 0.6193  data: 0.0001  max mem: 15824
[08:59:27.811919] Epoch: [17]  [344/345]  eta: 0:00:00  lr: 0.000112  loss: 0.2414 (0.2318)  time: 0.6190  data: 0.0001  max mem: 15824
[08:59:27.885134] Epoch: [17] Total time: 0:03:34 (0.6212 s / it)
[08:59:27.885502] Averaged stats: lr: 0.000112  loss: 0.2414 (0.2318)
[08:59:28.486243] Test:  [  0/345]  eta: 0:03:24  loss: 0.1928 (0.1928)  time: 0.5937  data: 0.4126  max mem: 15824
[08:59:30.280531] Test:  [ 10/345]  eta: 0:01:12  loss: 0.2103 (0.2138)  time: 0.2170  data: 0.0376  max mem: 15824
[08:59:32.072094] Test:  [ 20/345]  eta: 0:01:04  loss: 0.2239 (0.2233)  time: 0.1792  data: 0.0001  max mem: 15824
[08:59:33.868044] Test:  [ 30/345]  eta: 0:01:00  loss: 0.2210 (0.2208)  time: 0.1793  data: 0.0001  max mem: 15824
[08:59:35.669759] Test:  [ 40/345]  eta: 0:00:57  loss: 0.2242 (0.2279)  time: 0.1798  data: 0.0001  max mem: 15824
[08:59:37.470573] Test:  [ 50/345]  eta: 0:00:55  loss: 0.2324 (0.2267)  time: 0.1801  data: 0.0001  max mem: 15824
[08:59:39.273521] Test:  [ 60/345]  eta: 0:00:53  loss: 0.2343 (0.2275)  time: 0.1801  data: 0.0001  max mem: 15824
[08:59:41.082269] Test:  [ 70/345]  eta: 0:00:51  loss: 0.2356 (0.2286)  time: 0.1805  data: 0.0001  max mem: 15824
[08:59:42.896371] Test:  [ 80/345]  eta: 0:00:49  loss: 0.2224 (0.2273)  time: 0.1811  data: 0.0001  max mem: 15824
[08:59:44.712975] Test:  [ 90/345]  eta: 0:00:47  loss: 0.2277 (0.2271)  time: 0.1815  data: 0.0001  max mem: 15824
[08:59:46.534626] Test:  [100/345]  eta: 0:00:45  loss: 0.2266 (0.2272)  time: 0.1819  data: 0.0001  max mem: 15824
[08:59:48.357254] Test:  [110/345]  eta: 0:00:43  loss: 0.2170 (0.2267)  time: 0.1822  data: 0.0001  max mem: 15824
[08:59:50.183940] Test:  [120/345]  eta: 0:00:41  loss: 0.2089 (0.2249)  time: 0.1824  data: 0.0001  max mem: 15824
[08:59:52.014133] Test:  [130/345]  eta: 0:00:39  loss: 0.2083 (0.2243)  time: 0.1828  data: 0.0001  max mem: 15824
[08:59:53.850141] Test:  [140/345]  eta: 0:00:37  loss: 0.2245 (0.2245)  time: 0.1832  data: 0.0001  max mem: 15824
[08:59:55.685792] Test:  [150/345]  eta: 0:00:35  loss: 0.2269 (0.2253)  time: 0.1835  data: 0.0001  max mem: 15824
[08:59:57.525888] Test:  [160/345]  eta: 0:00:34  loss: 0.2253 (0.2255)  time: 0.1837  data: 0.0001  max mem: 15824
[08:59:59.369940] Test:  [170/345]  eta: 0:00:32  loss: 0.2322 (0.2259)  time: 0.1841  data: 0.0001  max mem: 15824
[09:00:01.215275] Test:  [180/345]  eta: 0:00:30  loss: 0.2235 (0.2256)  time: 0.1844  data: 0.0001  max mem: 15824
[09:00:03.066724] Test:  [190/345]  eta: 0:00:28  loss: 0.2196 (0.2255)  time: 0.1848  data: 0.0001  max mem: 15824
[09:00:04.920379] Test:  [200/345]  eta: 0:00:26  loss: 0.2318 (0.2258)  time: 0.1852  data: 0.0001  max mem: 15824
[09:00:06.777741] Test:  [210/345]  eta: 0:00:24  loss: 0.2259 (0.2254)  time: 0.1855  data: 0.0001  max mem: 15824
[09:00:08.639354] Test:  [220/345]  eta: 0:00:23  loss: 0.2191 (0.2252)  time: 0.1859  data: 0.0001  max mem: 15824
[09:00:10.505817] Test:  [230/345]  eta: 0:00:21  loss: 0.2198 (0.2250)  time: 0.1863  data: 0.0001  max mem: 15824
[09:00:12.375010] Test:  [240/345]  eta: 0:00:19  loss: 0.2264 (0.2255)  time: 0.1867  data: 0.0001  max mem: 15824
[09:00:14.247133] Test:  [250/345]  eta: 0:00:17  loss: 0.2264 (0.2254)  time: 0.1870  data: 0.0001  max mem: 15824
[09:00:16.121152] Test:  [260/345]  eta: 0:00:15  loss: 0.2238 (0.2255)  time: 0.1872  data: 0.0001  max mem: 15824
[09:00:17.998630] Test:  [270/345]  eta: 0:00:13  loss: 0.2238 (0.2253)  time: 0.1875  data: 0.0001  max mem: 15824
[09:00:19.879981] Test:  [280/345]  eta: 0:00:12  loss: 0.2213 (0.2252)  time: 0.1879  data: 0.0001  max mem: 15824
[09:00:21.763659] Test:  [290/345]  eta: 0:00:10  loss: 0.2202 (0.2250)  time: 0.1882  data: 0.0001  max mem: 15824
[09:00:23.653918] Test:  [300/345]  eta: 0:00:08  loss: 0.2230 (0.2257)  time: 0.1886  data: 0.0001  max mem: 15824
[09:00:25.548511] Test:  [310/345]  eta: 0:00:06  loss: 0.2148 (0.2250)  time: 0.1892  data: 0.0001  max mem: 15824
[09:00:27.443858] Test:  [320/345]  eta: 0:00:04  loss: 0.2126 (0.2248)  time: 0.1894  data: 0.0001  max mem: 15824
[09:00:29.342562] Test:  [330/345]  eta: 0:00:02  loss: 0.2147 (0.2247)  time: 0.1896  data: 0.0001  max mem: 15824
[09:00:31.241003] Test:  [340/345]  eta: 0:00:00  loss: 0.2266 (0.2246)  time: 0.1898  data: 0.0001  max mem: 15824
[09:00:32.002273] Test:  [344/345]  eta: 0:00:00  loss: 0.2257 (0.2245)  time: 0.1899  data: 0.0001  max mem: 15824
[09:00:32.069837] Test: Total time: 0:01:04 (0.1860 s / it)
[09:00:42.590576] Test:  [ 0/57]  eta: 0:00:32  loss: 0.4237 (0.4237)  time: 0.5649  data: 0.3842  max mem: 15824
[09:00:44.363448] Test:  [10/57]  eta: 0:00:09  loss: 0.4237 (0.4427)  time: 0.2125  data: 0.0350  max mem: 15824
[09:00:46.140828] Test:  [20/57]  eta: 0:00:07  loss: 0.4199 (0.4227)  time: 0.1774  data: 0.0001  max mem: 15824
[09:00:47.922395] Test:  [30/57]  eta: 0:00:05  loss: 0.3076 (0.3735)  time: 0.1779  data: 0.0001  max mem: 15824
[09:00:49.709352] Test:  [40/57]  eta: 0:00:03  loss: 0.2743 (0.3505)  time: 0.1784  data: 0.0001  max mem: 15824
[09:00:51.498059] Test:  [50/57]  eta: 0:00:01  loss: 0.2980 (0.3516)  time: 0.1787  data: 0.0001  max mem: 15824
[09:00:52.469855] Test:  [56/57]  eta: 0:00:00  loss: 0.3409 (0.3687)  time: 0.1737  data: 0.0001  max mem: 15824
[09:00:52.536612] Test: Total time: 0:00:10 (0.1844 s / it)
[09:00:54.269900] Dice score of the network on the train images: 0.795216, val images: 0.765127
[09:00:54.274165] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:00:55.318546] Epoch: [18]  [  0/345]  eta: 0:06:00  lr: 0.000113  loss: 0.2134 (0.2134)  time: 1.0436  data: 0.4197  max mem: 15824
[09:01:07.662415] Epoch: [18]  [ 20/345]  eta: 0:03:27  lr: 0.000113  loss: 0.2244 (0.2275)  time: 0.6171  data: 0.0001  max mem: 15824
[09:01:20.030130] Epoch: [18]  [ 40/345]  eta: 0:03:11  lr: 0.000113  loss: 0.2139 (0.2255)  time: 0.6183  data: 0.0001  max mem: 15824
[09:01:32.403222] Epoch: [18]  [ 60/345]  eta: 0:02:58  lr: 0.000114  loss: 0.2245 (0.2279)  time: 0.6186  data: 0.0001  max mem: 15824
[09:01:44.786998] Epoch: [18]  [ 80/345]  eta: 0:02:45  lr: 0.000114  loss: 0.2309 (0.2286)  time: 0.6191  data: 0.0001  max mem: 15824
[09:01:57.175089] Epoch: [18]  [100/345]  eta: 0:02:32  lr: 0.000114  loss: 0.2167 (0.2279)  time: 0.6194  data: 0.0001  max mem: 15824
[09:02:09.574177] Epoch: [18]  [120/345]  eta: 0:02:20  lr: 0.000115  loss: 0.2248 (0.2281)  time: 0.6199  data: 0.0001  max mem: 15824
[09:02:21.972017] Epoch: [18]  [140/345]  eta: 0:02:07  lr: 0.000115  loss: 0.2365 (0.2290)  time: 0.6198  data: 0.0001  max mem: 15824
[09:02:34.383585] Epoch: [18]  [160/345]  eta: 0:01:55  lr: 0.000115  loss: 0.2260 (0.2285)  time: 0.6205  data: 0.0001  max mem: 15824
[09:02:46.804495] Epoch: [18]  [180/345]  eta: 0:01:42  lr: 0.000116  loss: 0.2183 (0.2278)  time: 0.6210  data: 0.0001  max mem: 15824
[09:02:59.220676] Epoch: [18]  [200/345]  eta: 0:01:30  lr: 0.000116  loss: 0.2217 (0.2275)  time: 0.6208  data: 0.0001  max mem: 15824
[09:03:11.612688] Epoch: [18]  [220/345]  eta: 0:01:17  lr: 0.000116  loss: 0.2157 (0.2270)  time: 0.6195  data: 0.0001  max mem: 15824
[09:03:24.001178] Epoch: [18]  [240/345]  eta: 0:01:05  lr: 0.000117  loss: 0.2223 (0.2270)  time: 0.6194  data: 0.0001  max mem: 15824
[09:03:36.383836] Epoch: [18]  [260/345]  eta: 0:00:52  lr: 0.000117  loss: 0.2244 (0.2279)  time: 0.6191  data: 0.0001  max mem: 15824
[09:03:48.774695] Epoch: [18]  [280/345]  eta: 0:00:40  lr: 0.000118  loss: 0.2220 (0.2274)  time: 0.6195  data: 0.0001  max mem: 15824
[09:04:01.173026] Epoch: [18]  [300/345]  eta: 0:00:27  lr: 0.000118  loss: 0.2253 (0.2274)  time: 0.6199  data: 0.0001  max mem: 15824
[09:04:13.564040] Epoch: [18]  [320/345]  eta: 0:00:15  lr: 0.000118  loss: 0.2290 (0.2283)  time: 0.6195  data: 0.0001  max mem: 15824
[09:04:25.933887] Epoch: [18]  [340/345]  eta: 0:00:03  lr: 0.000119  loss: 0.2255 (0.2283)  time: 0.6184  data: 0.0001  max mem: 15824
[09:04:28.405504] Epoch: [18]  [344/345]  eta: 0:00:00  lr: 0.000119  loss: 0.2172 (0.2281)  time: 0.6180  data: 0.0001  max mem: 15824
[09:04:28.477920] Epoch: [18] Total time: 0:03:34 (0.6209 s / it)
[09:04:28.478048] Averaged stats: lr: 0.000119  loss: 0.2172 (0.2281)
[09:04:29.066568] Test:  [  0/345]  eta: 0:03:21  loss: 0.1950 (0.1950)  time: 0.5845  data: 0.4027  max mem: 15824
[09:04:30.855681] Test:  [ 10/345]  eta: 0:01:12  loss: 0.2241 (0.2216)  time: 0.2157  data: 0.0367  max mem: 15824
[09:04:32.650017] Test:  [ 20/345]  eta: 0:01:04  loss: 0.2235 (0.2178)  time: 0.1791  data: 0.0001  max mem: 15824
[09:04:34.450636] Test:  [ 30/345]  eta: 0:01:00  loss: 0.2195 (0.2150)  time: 0.1797  data: 0.0001  max mem: 15824
[09:04:36.252968] Test:  [ 40/345]  eta: 0:00:57  loss: 0.2195 (0.2182)  time: 0.1801  data: 0.0001  max mem: 15824
[09:04:38.053680] Test:  [ 50/345]  eta: 0:00:55  loss: 0.2159 (0.2173)  time: 0.1801  data: 0.0001  max mem: 15824
[09:04:39.856399] Test:  [ 60/345]  eta: 0:00:53  loss: 0.2080 (0.2153)  time: 0.1801  data: 0.0001  max mem: 15824
[09:04:41.665830] Test:  [ 70/345]  eta: 0:00:51  loss: 0.2051 (0.2158)  time: 0.1805  data: 0.0001  max mem: 15824
[09:04:43.477864] Test:  [ 80/345]  eta: 0:00:49  loss: 0.2066 (0.2153)  time: 0.1810  data: 0.0001  max mem: 15824
[09:04:45.293252] Test:  [ 90/345]  eta: 0:00:47  loss: 0.2066 (0.2146)  time: 0.1813  data: 0.0001  max mem: 15824
[09:04:47.111527] Test:  [100/345]  eta: 0:00:45  loss: 0.1968 (0.2144)  time: 0.1816  data: 0.0001  max mem: 15824
[09:04:48.932604] Test:  [110/345]  eta: 0:00:43  loss: 0.1968 (0.2138)  time: 0.1819  data: 0.0001  max mem: 15824
[09:04:50.758746] Test:  [120/345]  eta: 0:00:41  loss: 0.2071 (0.2143)  time: 0.1823  data: 0.0001  max mem: 15824
[09:04:52.586276] Test:  [130/345]  eta: 0:00:39  loss: 0.2116 (0.2158)  time: 0.1826  data: 0.0001  max mem: 15824
[09:04:54.416990] Test:  [140/345]  eta: 0:00:37  loss: 0.2136 (0.2164)  time: 0.1829  data: 0.0001  max mem: 15824
[09:04:56.252071] Test:  [150/345]  eta: 0:00:35  loss: 0.2123 (0.2166)  time: 0.1832  data: 0.0001  max mem: 15824
[09:04:58.093235] Test:  [160/345]  eta: 0:00:34  loss: 0.2173 (0.2164)  time: 0.1838  data: 0.0001  max mem: 15824
[09:04:59.935530] Test:  [170/345]  eta: 0:00:32  loss: 0.2146 (0.2159)  time: 0.1841  data: 0.0001  max mem: 15824
[09:05:01.780461] Test:  [180/345]  eta: 0:00:30  loss: 0.2056 (0.2150)  time: 0.1843  data: 0.0001  max mem: 15824
[09:05:03.630413] Test:  [190/345]  eta: 0:00:28  loss: 0.2055 (0.2150)  time: 0.1847  data: 0.0001  max mem: 15824
[09:05:05.484548] Test:  [200/345]  eta: 0:00:26  loss: 0.2148 (0.2153)  time: 0.1851  data: 0.0001  max mem: 15824
[09:05:07.340193] Test:  [210/345]  eta: 0:00:24  loss: 0.2255 (0.2159)  time: 0.1854  data: 0.0001  max mem: 15824
[09:05:09.199786] Test:  [220/345]  eta: 0:00:23  loss: 0.2097 (0.2157)  time: 0.1857  data: 0.0001  max mem: 15824
[09:05:11.064052] Test:  [230/345]  eta: 0:00:21  loss: 0.2045 (0.2156)  time: 0.1861  data: 0.0001  max mem: 15824
[09:05:12.933794] Test:  [240/345]  eta: 0:00:19  loss: 0.2028 (0.2157)  time: 0.1866  data: 0.0001  max mem: 15824
[09:05:14.804794] Test:  [250/345]  eta: 0:00:17  loss: 0.2084 (0.2157)  time: 0.1870  data: 0.0001  max mem: 15824
[09:05:16.679526] Test:  [260/345]  eta: 0:00:15  loss: 0.2108 (0.2154)  time: 0.1872  data: 0.0001  max mem: 15824
[09:05:18.556482] Test:  [270/345]  eta: 0:00:13  loss: 0.2060 (0.2154)  time: 0.1875  data: 0.0001  max mem: 15824
[09:05:20.437407] Test:  [280/345]  eta: 0:00:12  loss: 0.2139 (0.2155)  time: 0.1878  data: 0.0001  max mem: 15824
[09:05:22.321716] Test:  [290/345]  eta: 0:00:10  loss: 0.2109 (0.2151)  time: 0.1882  data: 0.0001  max mem: 15824
[09:05:24.211647] Test:  [300/345]  eta: 0:00:08  loss: 0.2086 (0.2150)  time: 0.1886  data: 0.0001  max mem: 15824
[09:05:26.105962] Test:  [310/345]  eta: 0:00:06  loss: 0.2054 (0.2148)  time: 0.1891  data: 0.0001  max mem: 15824
[09:05:28.004422] Test:  [320/345]  eta: 0:00:04  loss: 0.1997 (0.2146)  time: 0.1896  data: 0.0001  max mem: 15824
[09:05:29.905896] Test:  [330/345]  eta: 0:00:02  loss: 0.2154 (0.2148)  time: 0.1899  data: 0.0001  max mem: 15824
[09:05:31.805136] Test:  [340/345]  eta: 0:00:00  loss: 0.2248 (0.2150)  time: 0.1900  data: 0.0001  max mem: 15824
[09:05:32.566513] Test:  [344/345]  eta: 0:00:00  loss: 0.2243 (0.2150)  time: 0.1900  data: 0.0001  max mem: 15824
[09:05:32.627406] Test: Total time: 0:01:04 (0.1859 s / it)
[09:05:43.291541] Test:  [ 0/57]  eta: 0:00:34  loss: 0.4686 (0.4686)  time: 0.6060  data: 0.4262  max mem: 15824
[09:05:45.066196] Test:  [10/57]  eta: 0:00:10  loss: 0.4548 (0.4649)  time: 0.2163  data: 0.0388  max mem: 15824
[09:05:46.844055] Test:  [20/57]  eta: 0:00:07  loss: 0.4357 (0.4380)  time: 0.1776  data: 0.0001  max mem: 15824
[09:05:48.626986] Test:  [30/57]  eta: 0:00:05  loss: 0.3107 (0.3891)  time: 0.1780  data: 0.0001  max mem: 15824
[09:05:50.414004] Test:  [40/57]  eta: 0:00:03  loss: 0.2908 (0.3683)  time: 0.1784  data: 0.0001  max mem: 15824
[09:05:52.202252] Test:  [50/57]  eta: 0:00:01  loss: 0.3220 (0.3717)  time: 0.1787  data: 0.0001  max mem: 15824
[09:05:53.174922] Test:  [56/57]  eta: 0:00:00  loss: 0.3696 (0.3826)  time: 0.1737  data: 0.0000  max mem: 15824
[09:05:53.240980] Test: Total time: 0:00:10 (0.1852 s / it)
[09:05:54.971804] Dice score of the network on the train images: 0.810894, val images: 0.764184
[09:05:54.976372] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:05:56.028287] Epoch: [19]  [  0/345]  eta: 0:06:02  lr: 0.000119  loss: 0.2291 (0.2291)  time: 1.0508  data: 0.4248  max mem: 15824
[09:06:08.328952] Epoch: [19]  [ 20/345]  eta: 0:03:26  lr: 0.000119  loss: 0.2421 (0.2370)  time: 0.6150  data: 0.0001  max mem: 15824
[09:06:20.702608] Epoch: [19]  [ 40/345]  eta: 0:03:11  lr: 0.000119  loss: 0.2167 (0.2286)  time: 0.6186  data: 0.0001  max mem: 15824
[09:06:33.102772] Epoch: [19]  [ 60/345]  eta: 0:02:58  lr: 0.000120  loss: 0.2180 (0.2248)  time: 0.6200  data: 0.0001  max mem: 15824
[09:06:45.496715] Epoch: [19]  [ 80/345]  eta: 0:02:45  lr: 0.000120  loss: 0.2052 (0.2198)  time: 0.6196  data: 0.0001  max mem: 15824

[09:06:57.904441] Epoch: [19]  [100/345]  eta: 0:02:32  lr: 0.000121  loss: 0.2118 (0.2197)  time: 0.6203  data: 0.0001  max mem: 15824
[09:07:10.335233] Epoch: [19]  [120/345]  eta: 0:02:20  lr: 0.000121  loss: 0.2144 (0.2206)  time: 0.6215  data: 0.0001  max mem: 15824
[09:07:22.778116] Epoch: [19]  [140/345]  eta: 0:02:07  lr: 0.000121  loss: 0.2224 (0.2216)  time: 0.6221  data: 0.0001  max mem: 15824
[09:07:35.192481] Epoch: [19]  [160/345]  eta: 0:01:55  lr: 0.000122  loss: 0.2160 (0.2212)  time: 0.6207  data: 0.0001  max mem: 15824
[09:07:47.612571] Epoch: [19]  [180/345]  eta: 0:01:42  lr: 0.000122  loss: 0.2291 (0.2224)  time: 0.6210  data: 0.0001  max mem: 15824
[09:08:00.042326] Epoch: [19]  [200/345]  eta: 0:01:30  lr: 0.000122  loss: 0.2323 (0.2227)  time: 0.6214  data: 0.0001  max mem: 15824
[09:08:12.467230] Epoch: [19]  [220/345]  eta: 0:01:17  lr: 0.000123  loss: 0.2182 (0.2228)  time: 0.6212  data: 0.0001  max mem: 15824
[09:08:24.877374] Epoch: [19]  [240/345]  eta: 0:01:05  lr: 0.000123  loss: 0.2222 (0.2228)  time: 0.6205  data: 0.0001  max mem: 15824
[09:08:37.282752] Epoch: [19]  [260/345]  eta: 0:00:52  lr: 0.000123  loss: 0.2140 (0.2225)  time: 0.6202  data: 0.0001  max mem: 15824
[09:08:49.679426] Epoch: [19]  [280/345]  eta: 0:00:40  lr: 0.000124  loss: 0.2177 (0.2228)  time: 0.6198  data: 0.0001  max mem: 15824
[09:09:02.080524] Epoch: [19]  [300/345]  eta: 0:00:27  lr: 0.000124  loss: 0.2016 (0.2218)  time: 0.6200  data: 0.0001  max mem: 15824
[09:09:14.472627] Epoch: [19]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.2072 (0.2217)  time: 0.6196  data: 0.0001  max mem: 15824
[09:09:26.858743] Epoch: [19]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.2327 (0.2218)  time: 0.6193  data: 0.0001  max mem: 15824
[09:09:29.331249] Epoch: [19]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.2288 (0.2221)  time: 0.6187  data: 0.0001  max mem: 15824
[09:09:29.405674] Epoch: [19] Total time: 0:03:34 (0.6215 s / it)
[09:09:29.405970] Averaged stats: lr: 0.000125  loss: 0.2288 (0.2221)
[09:09:30.011643] Test:  [  0/345]  eta: 0:03:27  loss: 0.2306 (0.2306)  time: 0.6003  data: 0.4179  max mem: 15824
[09:09:31.799915] Test:  [ 10/345]  eta: 0:01:12  loss: 0.2074 (0.2234)  time: 0.2170  data: 0.0381  max mem: 15824
[09:09:33.592957] Test:  [ 20/345]  eta: 0:01:04  loss: 0.2074 (0.2247)  time: 0.1790  data: 0.0001  max mem: 15824
[09:09:35.391112] Test:  [ 30/345]  eta: 0:01:00  loss: 0.2073 (0.2189)  time: 0.1795  data: 0.0001  max mem: 15824
[09:09:37.191759] Test:  [ 40/345]  eta: 0:00:57  loss: 0.2072 (0.2181)  time: 0.1799  data: 0.0001  max mem: 15824
[09:09:38.996666] Test:  [ 50/345]  eta: 0:00:55  loss: 0.2170 (0.2174)  time: 0.1802  data: 0.0001  max mem: 15824
[09:09:40.802320] Test:  [ 60/345]  eta: 0:00:53  loss: 0.2085 (0.2159)  time: 0.1805  data: 0.0001  max mem: 15824
[09:09:42.610582] Test:  [ 70/345]  eta: 0:00:51  loss: 0.2015 (0.2147)  time: 0.1806  data: 0.0001  max mem: 15824
[09:09:44.423565] Test:  [ 80/345]  eta: 0:00:49  loss: 0.2108 (0.2150)  time: 0.1810  data: 0.0001  max mem: 15824
[09:09:46.240456] Test:  [ 90/345]  eta: 0:00:47  loss: 0.2235 (0.2158)  time: 0.1814  data: 0.0001  max mem: 15824
[09:09:48.058929] Test:  [100/345]  eta: 0:00:45  loss: 0.2148 (0.2155)  time: 0.1817  data: 0.0001  max mem: 15824
[09:09:49.883281] Test:  [110/345]  eta: 0:00:43  loss: 0.2102 (0.2163)  time: 0.1821  data: 0.0001  max mem: 15824
[09:09:51.709192] Test:  [120/345]  eta: 0:00:41  loss: 0.2108 (0.2160)  time: 0.1824  data: 0.0001  max mem: 15824
[09:09:53.537301] Test:  [130/345]  eta: 0:00:39  loss: 0.2108 (0.2156)  time: 0.1826  data: 0.0001  max mem: 15824
[09:09:55.370285] Test:  [140/345]  eta: 0:00:37  loss: 0.2142 (0.2159)  time: 0.1830  data: 0.0001  max mem: 15824
[09:09:57.205957] Test:  [150/345]  eta: 0:00:35  loss: 0.2284 (0.2168)  time: 0.1834  data: 0.0001  max mem: 15824
[09:09:59.047316] Test:  [160/345]  eta: 0:00:34  loss: 0.2131 (0.2163)  time: 0.1838  data: 0.0001  max mem: 15824
[09:10:00.891751] Test:  [170/345]  eta: 0:00:32  loss: 0.2026 (0.2153)  time: 0.1842  data: 0.0001  max mem: 15824
[09:10:02.737648] Test:  [180/345]  eta: 0:00:30  loss: 0.2091 (0.2160)  time: 0.1845  data: 0.0001  max mem: 15824
[09:10:04.587690] Test:  [190/345]  eta: 0:00:28  loss: 0.2211 (0.2159)  time: 0.1847  data: 0.0001  max mem: 15824
[09:10:06.442180] Test:  [200/345]  eta: 0:00:26  loss: 0.2072 (0.2152)  time: 0.1852  data: 0.0001  max mem: 15824
[09:10:08.299129] Test:  [210/345]  eta: 0:00:24  loss: 0.2079 (0.2153)  time: 0.1855  data: 0.0001  max mem: 15824
[09:10:10.159519] Test:  [220/345]  eta: 0:00:23  loss: 0.2201 (0.2160)  time: 0.1858  data: 0.0001  max mem: 15824
[09:10:12.023283] Test:  [230/345]  eta: 0:00:21  loss: 0.2178 (0.2164)  time: 0.1861  data: 0.0001  max mem: 15824
[09:10:13.890966] Test:  [240/345]  eta: 0:00:19  loss: 0.2210 (0.2168)  time: 0.1865  data: 0.0001  max mem: 15824
[09:10:15.761597] Test:  [250/345]  eta: 0:00:17  loss: 0.2210 (0.2169)  time: 0.1869  data: 0.0001  max mem: 15824
[09:10:17.635892] Test:  [260/345]  eta: 0:00:15  loss: 0.2046 (0.2169)  time: 0.1872  data: 0.0001  max mem: 15824
[09:10:19.514120] Test:  [270/345]  eta: 0:00:13  loss: 0.2046 (0.2166)  time: 0.1876  data: 0.0001  max mem: 15824
[09:10:21.396375] Test:  [280/345]  eta: 0:00:12  loss: 0.2082 (0.2167)  time: 0.1880  data: 0.0001  max mem: 15824
[09:10:23.281601] Test:  [290/345]  eta: 0:00:10  loss: 0.2149 (0.2165)  time: 0.1883  data: 0.0001  max mem: 15824
[09:10:25.172282] Test:  [300/345]  eta: 0:00:08  loss: 0.2068 (0.2160)  time: 0.1887  data: 0.0001  max mem: 15824
[09:10:27.065656] Test:  [310/345]  eta: 0:00:06  loss: 0.2068 (0.2158)  time: 0.1891  data: 0.0001  max mem: 15824
[09:10:28.962150] Test:  [320/345]  eta: 0:00:04  loss: 0.2129 (0.2161)  time: 0.1894  data: 0.0001  max mem: 15824
[09:10:30.860761] Test:  [330/345]  eta: 0:00:02  loss: 0.2159 (0.2161)  time: 0.1897  data: 0.0001  max mem: 15824
[09:10:32.759180] Test:  [340/345]  eta: 0:00:00  loss: 0.2108 (0.2159)  time: 0.1898  data: 0.0001  max mem: 15824
[09:10:33.519892] Test:  [344/345]  eta: 0:00:00  loss: 0.2078 (0.2158)  time: 0.1898  data: 0.0001  max mem: 15824
[09:10:33.592679] Test: Total time: 0:01:04 (0.1860 s / it)
[09:10:44.162483] Test:  [ 0/57]  eta: 0:00:32  loss: 0.4763 (0.4763)  time: 0.5616  data: 0.3805  max mem: 15824
[09:10:45.937513] Test:  [10/57]  eta: 0:00:09  loss: 0.4333 (0.4421)  time: 0.2123  data: 0.0347  max mem: 15824
[09:10:47.715768] Test:  [20/57]  eta: 0:00:07  loss: 0.4315 (0.4256)  time: 0.1776  data: 0.0001  max mem: 15824
[09:10:49.497936] Test:  [30/57]  eta: 0:00:05  loss: 0.2948 (0.3799)  time: 0.1780  data: 0.0001  max mem: 15824
[09:10:51.284505] Test:  [40/57]  eta: 0:00:03  loss: 0.2914 (0.3624)  time: 0.1784  data: 0.0001  max mem: 15824
[09:10:53.070370] Test:  [50/57]  eta: 0:00:01  loss: 0.3198 (0.3709)  time: 0.1786  data: 0.0001  max mem: 15824
[09:10:54.044458] Test:  [56/57]  eta: 0:00:00  loss: 0.3810 (0.3875)  time: 0.1737  data: 0.0000  max mem: 15824
[09:10:54.122523] Test: Total time: 0:00:10 (0.1846 s / it)
[09:10:55.820904] Dice score of the network on the train images: 0.817553, val images: 0.744933
[09:10:55.825680] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:10:56.824652] Epoch: [20]  [  0/345]  eta: 0:05:44  lr: 0.000125  loss: 0.2183 (0.2183)  time: 0.9980  data: 0.3736  max mem: 15824
[09:11:09.157077] Epoch: [20]  [ 20/345]  eta: 0:03:26  lr: 0.000125  loss: 0.2107 (0.2161)  time: 0.6166  data: 0.0001  max mem: 15824
[09:11:21.520448] Epoch: [20]  [ 40/345]  eta: 0:03:11  lr: 0.000125  loss: 0.2136 (0.2168)  time: 0.6181  data: 0.0001  max mem: 15824
[09:11:33.888783] Epoch: [20]  [ 60/345]  eta: 0:02:57  lr: 0.000125  loss: 0.2194 (0.2181)  time: 0.6184  data: 0.0001  max mem: 15824
[09:11:46.284899] Epoch: [20]  [ 80/345]  eta: 0:02:45  lr: 0.000125  loss: 0.2133 (0.2168)  time: 0.6198  data: 0.0001  max mem: 15824
[09:11:58.682220] Epoch: [20]  [100/345]  eta: 0:02:32  lr: 0.000125  loss: 0.2197 (0.2167)  time: 0.6198  data: 0.0001  max mem: 15824
[09:12:11.103531] Epoch: [20]  [120/345]  eta: 0:02:19  lr: 0.000125  loss: 0.2117 (0.2165)  time: 0.6210  data: 0.0001  max mem: 15824
[09:12:23.524258] Epoch: [20]  [140/345]  eta: 0:02:07  lr: 0.000125  loss: 0.2220 (0.2172)  time: 0.6210  data: 0.0001  max mem: 15824
[09:12:35.938023] Epoch: [20]  [160/345]  eta: 0:01:55  lr: 0.000125  loss: 0.2052 (0.2165)  time: 0.6206  data: 0.0001  max mem: 15824
[09:12:48.366976] Epoch: [20]  [180/345]  eta: 0:01:42  lr: 0.000125  loss: 0.2032 (0.2162)  time: 0.6214  data: 0.0001  max mem: 15824
[09:13:00.767683] Epoch: [20]  [200/345]  eta: 0:01:30  lr: 0.000125  loss: 0.2046 (0.2151)  time: 0.6200  data: 0.0001  max mem: 15824
[09:13:13.193600] Epoch: [20]  [220/345]  eta: 0:01:17  lr: 0.000125  loss: 0.2007 (0.2143)  time: 0.6212  data: 0.0001  max mem: 15824
[09:13:25.596116] Epoch: [20]  [240/345]  eta: 0:01:05  lr: 0.000125  loss: 0.2222 (0.2149)  time: 0.6201  data: 0.0001  max mem: 15824
[09:13:37.987162] Epoch: [20]  [260/345]  eta: 0:00:52  lr: 0.000125  loss: 0.2128 (0.2150)  time: 0.6195  data: 0.0001  max mem: 15824

[09:13:50.388697] Epoch: [20]  [280/345]  eta: 0:00:40  lr: 0.000125  loss: 0.2077 (0.2152)  time: 0.6200  data: 0.0001  max mem: 15824
[09:14:02.777958] Epoch: [20]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.2147 (0.2156)  time: 0.6194  data: 0.0001  max mem: 15824
[09:14:15.174893] Epoch: [20]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.2167 (0.2158)  time: 0.6198  data: 0.0001  max mem: 15824
[09:14:27.564987] Epoch: [20]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.2272 (0.2165)  time: 0.6195  data: 0.0001  max mem: 15824
[09:14:30.043097] Epoch: [20]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.2289 (0.2167)  time: 0.6194  data: 0.0001  max mem: 15824
[09:14:30.118111] Epoch: [20] Total time: 0:03:34 (0.6211 s / it)
[09:14:30.118456] Averaged stats: lr: 0.000125  loss: 0.2289 (0.2167)
[09:14:30.771849] Test:  [  0/345]  eta: 0:03:43  loss: 0.1948 (0.1948)  time: 0.6482  data: 0.4667  max mem: 15824
[09:14:32.559417] Test:  [ 10/345]  eta: 0:01:14  loss: 0.2041 (0.2037)  time: 0.2214  data: 0.0425  max mem: 15824
[09:14:34.356562] Test:  [ 20/345]  eta: 0:01:05  loss: 0.2041 (0.2075)  time: 0.1792  data: 0.0001  max mem: 15824
[09:14:36.155670] Test:  [ 30/345]  eta: 0:01:01  loss: 0.2119 (0.2064)  time: 0.1797  data: 0.0001  max mem: 15824
[09:14:37.959894] Test:  [ 40/345]  eta: 0:00:58  loss: 0.2091 (0.2052)  time: 0.1801  data: 0.0001  max mem: 15824
[09:14:39.760363] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1974 (0.2042)  time: 0.1802  data: 0.0001  max mem: 15824
[09:14:41.568164] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1972 (0.2023)  time: 0.1803  data: 0.0001  max mem: 15824
[09:14:43.379108] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1972 (0.2040)  time: 0.1809  data: 0.0001  max mem: 15824
[09:14:45.191144] Test:  [ 80/345]  eta: 0:00:49  loss: 0.2061 (0.2050)  time: 0.1811  data: 0.0001  max mem: 15824
[09:14:47.009445] Test:  [ 90/345]  eta: 0:00:47  loss: 0.2061 (0.2045)  time: 0.1815  data: 0.0001  max mem: 15824
[09:14:48.827998] Test:  [100/345]  eta: 0:00:45  loss: 0.2074 (0.2051)  time: 0.1818  data: 0.0001  max mem: 15824
[09:14:50.650973] Test:  [110/345]  eta: 0:00:43  loss: 0.2052 (0.2052)  time: 0.1820  data: 0.0001  max mem: 15824
[09:14:52.477039] Test:  [120/345]  eta: 0:00:41  loss: 0.2068 (0.2057)  time: 0.1824  data: 0.0001  max mem: 15824
[09:14:54.304722] Test:  [130/345]  eta: 0:00:39  loss: 0.2068 (0.2052)  time: 0.1826  data: 0.0001  max mem: 15824
[09:14:56.138318] Test:  [140/345]  eta: 0:00:37  loss: 0.1978 (0.2042)  time: 0.1830  data: 0.0001  max mem: 15824
[09:14:57.978711] Test:  [150/345]  eta: 0:00:35  loss: 0.2095 (0.2051)  time: 0.1836  data: 0.0001  max mem: 15824
[09:14:59.821184] Test:  [160/345]  eta: 0:00:34  loss: 0.2171 (0.2055)  time: 0.1841  data: 0.0001  max mem: 15824
[09:15:01.665659] Test:  [170/345]  eta: 0:00:32  loss: 0.1997 (0.2050)  time: 0.1843  data: 0.0001  max mem: 15824
[09:15:03.514694] Test:  [180/345]  eta: 0:00:30  loss: 0.1861 (0.2044)  time: 0.1846  data: 0.0001  max mem: 15824
[09:15:05.364365] Test:  [190/345]  eta: 0:00:28  loss: 0.1963 (0.2043)  time: 0.1849  data: 0.0001  max mem: 15824
[09:15:07.218786] Test:  [200/345]  eta: 0:00:26  loss: 0.2028 (0.2043)  time: 0.1851  data: 0.0001  max mem: 15824
[09:15:09.077705] Test:  [210/345]  eta: 0:00:24  loss: 0.2148 (0.2049)  time: 0.1856  data: 0.0001  max mem: 15824
[09:15:10.940549] Test:  [220/345]  eta: 0:00:23  loss: 0.2148 (0.2048)  time: 0.1860  data: 0.0001  max mem: 15824
[09:15:12.804404] Test:  [230/345]  eta: 0:00:21  loss: 0.2041 (0.2049)  time: 0.1863  data: 0.0001  max mem: 15824
[09:15:14.675935] Test:  [240/345]  eta: 0:00:19  loss: 0.2021 (0.2051)  time: 0.1867  data: 0.0001  max mem: 15824
[09:15:16.549427] Test:  [250/345]  eta: 0:00:17  loss: 0.2004 (0.2048)  time: 0.1872  data: 0.0001  max mem: 15824
[09:15:18.425359] Test:  [260/345]  eta: 0:00:15  loss: 0.2032 (0.2047)  time: 0.1874  data: 0.0001  max mem: 15824
[09:15:20.304482] Test:  [270/345]  eta: 0:00:13  loss: 0.2030 (0.2044)  time: 0.1877  data: 0.0001  max mem: 15824
[09:15:22.189228] Test:  [280/345]  eta: 0:00:12  loss: 0.2023 (0.2041)  time: 0.1881  data: 0.0001  max mem: 15824
[09:15:24.073467] Test:  [290/345]  eta: 0:00:10  loss: 0.2002 (0.2041)  time: 0.1884  data: 0.0001  max mem: 15824
[09:15:25.964484] Test:  [300/345]  eta: 0:00:08  loss: 0.2039 (0.2039)  time: 0.1887  data: 0.0001  max mem: 15824
[09:15:27.858168] Test:  [310/345]  eta: 0:00:06  loss: 0.2032 (0.2040)  time: 0.1892  data: 0.0001  max mem: 15824
[09:15:29.753007] Test:  [320/345]  eta: 0:00:04  loss: 0.1990 (0.2042)  time: 0.1894  data: 0.0001  max mem: 15824
[09:15:31.651611] Test:  [330/345]  eta: 0:00:02  loss: 0.1997 (0.2047)  time: 0.1896  data: 0.0001  max mem: 15824
[09:15:33.552474] Test:  [340/345]  eta: 0:00:00  loss: 0.1940 (0.2047)  time: 0.1899  data: 0.0001  max mem: 15824
[09:15:34.313071] Test:  [344/345]  eta: 0:00:00  loss: 0.2004 (0.2048)  time: 0.1900  data: 0.0001  max mem: 15824
[09:15:34.396193] Test: Total time: 0:01:04 (0.1863 s / it)
[09:15:44.902215] Test:  [ 0/57]  eta: 0:00:34  loss: 0.4798 (0.4798)  time: 0.5979  data: 0.4174  max mem: 15824
[09:15:46.674685] Test:  [10/57]  eta: 0:00:10  loss: 0.4515 (0.4627)  time: 0.2154  data: 0.0380  max mem: 15824
[09:15:48.454476] Test:  [20/57]  eta: 0:00:07  loss: 0.4281 (0.4432)  time: 0.1775  data: 0.0001  max mem: 15824
[09:15:50.233670] Test:  [30/57]  eta: 0:00:05  loss: 0.3386 (0.3902)  time: 0.1779  data: 0.0001  max mem: 15824
[09:15:52.018457] Test:  [40/57]  eta: 0:00:03  loss: 0.2842 (0.3681)  time: 0.1781  data: 0.0001  max mem: 15824
[09:15:53.804814] Test:  [50/57]  eta: 0:00:01  loss: 0.3236 (0.3705)  time: 0.1785  data: 0.0001  max mem: 15824
[09:15:54.774769] Test:  [56/57]  eta: 0:00:00  loss: 0.3632 (0.3805)  time: 0.1734  data: 0.0000  max mem: 15824
[09:15:54.845863] Test: Total time: 0:00:10 (0.1849 s / it)
[09:15:56.567916] Dice score of the network on the train images: 0.819566, val images: 0.775684
[09:15:56.572205] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:15:57.572951] Epoch: [21]  [  0/345]  eta: 0:05:44  lr: 0.000125  loss: 0.1753 (0.1753)  time: 0.9996  data: 0.3738  max mem: 15824
[09:16:09.897493] Epoch: [21]  [ 20/345]  eta: 0:03:26  lr: 0.000125  loss: 0.2117 (0.2110)  time: 0.6162  data: 0.0001  max mem: 15824
[09:16:22.259927] Epoch: [21]  [ 40/345]  eta: 0:03:11  lr: 0.000125  loss: 0.1985 (0.2080)  time: 0.6181  data: 0.0001  max mem: 15824
[09:16:34.653676] Epoch: [21]  [ 60/345]  eta: 0:02:57  lr: 0.000125  loss: 0.2013 (0.2070)  time: 0.6196  data: 0.0001  max mem: 15824
[09:16:47.046521] Epoch: [21]  [ 80/345]  eta: 0:02:45  lr: 0.000124  loss: 0.1912 (0.2049)  time: 0.6196  data: 0.0001  max mem: 15824
[09:16:59.441545] Epoch: [21]  [100/345]  eta: 0:02:32  lr: 0.000124  loss: 0.2076 (0.2057)  time: 0.6197  data: 0.0001  max mem: 15824
[09:17:11.876626] Epoch: [21]  [120/345]  eta: 0:02:20  lr: 0.000124  loss: 0.2137 (0.2077)  time: 0.6217  data: 0.0001  max mem: 15824
[09:17:24.306003] Epoch: [21]  [140/345]  eta: 0:02:07  lr: 0.000124  loss: 0.2170 (0.2094)  time: 0.6214  data: 0.0001  max mem: 15824
[09:17:36.738382] Epoch: [21]  [160/345]  eta: 0:01:55  lr: 0.000124  loss: 0.2122 (0.2093)  time: 0.6216  data: 0.0001  max mem: 15824
[09:17:49.151534] Epoch: [21]  [180/345]  eta: 0:01:42  lr: 0.000124  loss: 0.1947 (0.2085)  time: 0.6206  data: 0.0001  max mem: 15824
[09:18:01.572517] Epoch: [21]  [200/345]  eta: 0:01:30  lr: 0.000124  loss: 0.2060 (0.2085)  time: 0.6210  data: 0.0001  max mem: 15824
[09:18:13.977615] Epoch: [21]  [220/345]  eta: 0:01:17  lr: 0.000124  loss: 0.2044 (0.2085)  time: 0.6202  data: 0.0001  max mem: 15824
[09:18:26.375348] Epoch: [21]  [240/345]  eta: 0:01:05  lr: 0.000124  loss: 0.1900 (0.2077)  time: 0.6198  data: 0.0001  max mem: 15824
[09:18:38.768556] Epoch: [21]  [260/345]  eta: 0:00:52  lr: 0.000124  loss: 0.1983 (0.2072)  time: 0.6196  data: 0.0001  max mem: 15824
[09:18:51.169780] Epoch: [21]  [280/345]  eta: 0:00:40  lr: 0.000124  loss: 0.1998 (0.2066)  time: 0.6200  data: 0.0001  max mem: 15824
[09:19:03.565582] Epoch: [21]  [300/345]  eta: 0:00:27  lr: 0.000124  loss: 0.2034 (0.2065)  time: 0.6197  data: 0.0001  max mem: 15824
[09:19:15.955227] Epoch: [21]  [320/345]  eta: 0:00:15  lr: 0.000124  loss: 0.2127 (0.2066)  time: 0.6194  data: 0.0001  max mem: 15824
[09:19:28.352256] Epoch: [21]  [340/345]  eta: 0:00:03  lr: 0.000124  loss: 0.2114 (0.2072)  time: 0.6198  data: 0.0001  max mem: 15824
[09:19:30.832794] Epoch: [21]  [344/345]  eta: 0:00:00  lr: 0.000124  loss: 0.2135 (0.2072)  time: 0.6197  data: 0.0001  max mem: 15824
[09:19:30.909465] Epoch: [21] Total time: 0:03:34 (0.6213 s / it)
[09:19:30.909804] Averaged stats: lr: 0.000124  loss: 0.2135 (0.2072)
[09:19:31.546453] Test:  [  0/345]  eta: 0:03:37  loss: 0.1752 (0.1752)  time: 0.6313  data: 0.4486  max mem: 15824
[09:19:33.335743] Test:  [ 10/345]  eta: 0:01:13  loss: 0.1850 (0.1910)  time: 0.2200  data: 0.0409  max mem: 15824
[09:19:35.130545] Test:  [ 20/345]  eta: 0:01:05  loss: 0.1827 (0.1877)  time: 0.1791  data: 0.0001  max mem: 15824
[09:19:36.925422] Test:  [ 30/345]  eta: 0:01:01  loss: 0.1827 (0.1906)  time: 0.1794  data: 0.0001  max mem: 15824
[09:19:38.723219] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1830 (0.1894)  time: 0.1796  data: 0.0001  max mem: 15824
[09:19:40.524341] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1910 (0.1910)  time: 0.1799  data: 0.0001  max mem: 15824
[09:19:42.326172] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1965 (0.1921)  time: 0.1801  data: 0.0001  max mem: 15824
[09:19:44.133683] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1956 (0.1921)  time: 0.1804  data: 0.0001  max mem: 15824
[09:19:45.947478] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1793 (0.1906)  time: 0.1810  data: 0.0001  max mem: 15824
[09:19:47.765831] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1819 (0.1905)  time: 0.1815  data: 0.0001  max mem: 15824
[09:19:49.584586] Test:  [100/345]  eta: 0:00:45  loss: 0.1860 (0.1901)  time: 0.1818  data: 0.0001  max mem: 15824
[09:19:51.408034] Test:  [110/345]  eta: 0:00:43  loss: 0.1896 (0.1907)  time: 0.1820  data: 0.0001  max mem: 15824
[09:19:53.232622] Test:  [120/345]  eta: 0:00:41  loss: 0.1939 (0.1905)  time: 0.1823  data: 0.0001  max mem: 15824
[09:19:55.063160] Test:  [130/345]  eta: 0:00:39  loss: 0.1869 (0.1905)  time: 0.1827  data: 0.0001  max mem: 15824
[09:19:56.897359] Test:  [140/345]  eta: 0:00:37  loss: 0.2043 (0.1914)  time: 0.1832  data: 0.0001  max mem: 15824
[09:19:58.734984] Test:  [150/345]  eta: 0:00:35  loss: 0.2048 (0.1922)  time: 0.1835  data: 0.0001  max mem: 15824
[09:20:00.577048] Test:  [160/345]  eta: 0:00:34  loss: 0.1922 (0.1925)  time: 0.1839  data: 0.0001  max mem: 15824
[09:20:02.420599] Test:  [170/345]  eta: 0:00:32  loss: 0.2001 (0.1930)  time: 0.1842  data: 0.0001  max mem: 15824
[09:20:04.268815] Test:  [180/345]  eta: 0:00:30  loss: 0.2001 (0.1931)  time: 0.1845  data: 0.0001  max mem: 15824
[09:20:06.119669] Test:  [190/345]  eta: 0:00:28  loss: 0.1971 (0.1938)  time: 0.1849  data: 0.0001  max mem: 15824
[09:20:07.975341] Test:  [200/345]  eta: 0:00:26  loss: 0.1971 (0.1942)  time: 0.1853  data: 0.0001  max mem: 15824
[09:20:09.834064] Test:  [210/345]  eta: 0:00:24  loss: 0.1916 (0.1942)  time: 0.1857  data: 0.0001  max mem: 15824
[09:20:11.694147] Test:  [220/345]  eta: 0:00:23  loss: 0.1867 (0.1941)  time: 0.1859  data: 0.0001  max mem: 15824
[09:20:13.558944] Test:  [230/345]  eta: 0:00:21  loss: 0.1909 (0.1949)  time: 0.1862  data: 0.0001  max mem: 15824
[09:20:15.426919] Test:  [240/345]  eta: 0:00:19  loss: 0.2035 (0.1956)  time: 0.1866  data: 0.0001  max mem: 15824
[09:20:17.299433] Test:  [250/345]  eta: 0:00:17  loss: 0.2023 (0.1955)  time: 0.1870  data: 0.0001  max mem: 15824
[09:20:19.172937] Test:  [260/345]  eta: 0:00:15  loss: 0.1902 (0.1952)  time: 0.1872  data: 0.0001  max mem: 15824
[09:20:21.048448] Test:  [270/345]  eta: 0:00:13  loss: 0.1941 (0.1952)  time: 0.1874  data: 0.0001  max mem: 15824
[09:20:22.931737] Test:  [280/345]  eta: 0:00:12  loss: 0.1941 (0.1953)  time: 0.1879  data: 0.0001  max mem: 15824
[09:20:24.816517] Test:  [290/345]  eta: 0:00:10  loss: 0.1949 (0.1958)  time: 0.1883  data: 0.0001  max mem: 15824
[09:20:26.704445] Test:  [300/345]  eta: 0:00:08  loss: 0.1972 (0.1959)  time: 0.1886  data: 0.0001  max mem: 15824
[09:20:28.598749] Test:  [310/345]  eta: 0:00:06  loss: 0.1921 (0.1957)  time: 0.1890  data: 0.0001  max mem: 15824
[09:20:30.493695] Test:  [320/345]  eta: 0:00:04  loss: 0.1906 (0.1958)  time: 0.1894  data: 0.0001  max mem: 15824
[09:20:32.393856] Test:  [330/345]  eta: 0:00:02  loss: 0.1917 (0.1958)  time: 0.1897  data: 0.0001  max mem: 15824
[09:20:34.294460] Test:  [340/345]  eta: 0:00:00  loss: 0.1970 (0.1961)  time: 0.1900  data: 0.0001  max mem: 15824
[09:20:35.055544] Test:  [344/345]  eta: 0:00:00  loss: 0.1970 (0.1960)  time: 0.1901  data: 0.0001  max mem: 15824
[09:20:35.130200] Test: Total time: 0:01:04 (0.1861 s / it)
[09:20:45.806841] Test:  [ 0/57]  eta: 0:00:33  loss: 0.4706 (0.4706)  time: 0.5844  data: 0.4040  max mem: 15824
[09:20:47.581988] Test:  [10/57]  eta: 0:00:10  loss: 0.4670 (0.4725)  time: 0.2144  data: 0.0368  max mem: 15824
[09:20:49.358770] Test:  [20/57]  eta: 0:00:07  loss: 0.4220 (0.4452)  time: 0.1775  data: 0.0001  max mem: 15824
[09:20:51.141407] Test:  [30/57]  eta: 0:00:05  loss: 0.3074 (0.3897)  time: 0.1779  data: 0.0001  max mem: 15824
[09:20:52.928591] Test:  [40/57]  eta: 0:00:03  loss: 0.2772 (0.3643)  time: 0.1784  data: 0.0001  max mem: 15824
[09:20:54.714974] Test:  [50/57]  eta: 0:00:01  loss: 0.3054 (0.3706)  time: 0.1786  data: 0.0001  max mem: 15824
[09:20:55.689566] Test:  [56/57]  eta: 0:00:00  loss: 0.3674 (0.3855)  time: 0.1737  data: 0.0000  max mem: 15824
[09:20:55.762696] Test: Total time: 0:00:10 (0.1849 s / it)
[09:20:57.459811] Dice score of the network on the train images: 0.817326, val images: 0.772485
[09:20:57.464259] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:20:58.498716] Epoch: [22]  [  0/345]  eta: 0:05:56  lr: 0.000124  loss: 0.1971 (0.1971)  time: 1.0324  data: 0.4052  max mem: 15824
[09:21:10.832167] Epoch: [22]  [ 20/345]  eta: 0:03:26  lr: 0.000124  loss: 0.2037 (0.2053)  time: 0.6166  data: 0.0001  max mem: 15824
[09:21:23.213043] Epoch: [22]  [ 40/345]  eta: 0:03:11  lr: 0.000123  loss: 0.2092 (0.2071)  time: 0.6190  data: 0.0001  max mem: 15824
[09:21:35.607931] Epoch: [22]  [ 60/345]  eta: 0:02:58  lr: 0.000123  loss: 0.1917 (0.2033)  time: 0.6197  data: 0.0001  max mem: 15824
[09:21:48.010099] Epoch: [22]  [ 80/345]  eta: 0:02:45  lr: 0.000123  loss: 0.1911 (0.2019)  time: 0.6201  data: 0.0001  max mem: 15824
[09:22:00.409905] Epoch: [22]  [100/345]  eta: 0:02:32  lr: 0.000123  loss: 0.1882 (0.2015)  time: 0.6199  data: 0.0001  max mem: 15824
[09:22:12.824163] Epoch: [22]  [120/345]  eta: 0:02:20  lr: 0.000123  loss: 0.1908 (0.2027)  time: 0.6207  data: 0.0001  max mem: 15824
[09:22:25.242515] Epoch: [22]  [140/345]  eta: 0:02:07  lr: 0.000123  loss: 0.2055 (0.2036)  time: 0.6209  data: 0.0001  max mem: 15824
[09:22:37.657250] Epoch: [22]  [160/345]  eta: 0:01:55  lr: 0.000123  loss: 0.2027 (0.2033)  time: 0.6207  data: 0.0001  max mem: 15824
[09:22:50.068185] Epoch: [22]  [180/345]  eta: 0:01:42  lr: 0.000123  loss: 0.1913 (0.2026)  time: 0.6205  data: 0.0001  max mem: 15824
[09:23:02.467152] Epoch: [22]  [200/345]  eta: 0:01:30  lr: 0.000123  loss: 0.1956 (0.2025)  time: 0.6199  data: 0.0001  max mem: 15824
[09:23:14.866361] Epoch: [22]  [220/345]  eta: 0:01:17  lr: 0.000123  loss: 0.1921 (0.2026)  time: 0.6199  data: 0.0001  max mem: 15824
[09:23:27.278673] Epoch: [22]  [240/345]  eta: 0:01:05  lr: 0.000123  loss: 0.1917 (0.2023)  time: 0.6206  data: 0.0001  max mem: 15824
[09:23:39.671042] Epoch: [22]  [260/345]  eta: 0:00:52  lr: 0.000122  loss: 0.2030 (0.2025)  time: 0.6196  data: 0.0001  max mem: 15824
[09:23:52.069952] Epoch: [22]  [280/345]  eta: 0:00:40  lr: 0.000122  loss: 0.2014 (0.2027)  time: 0.6199  data: 0.0001  max mem: 15824
[09:24:04.463830] Epoch: [22]  [300/345]  eta: 0:00:27  lr: 0.000122  loss: 0.1924 (0.2026)  time: 0.6196  data: 0.0001  max mem: 15824
[09:24:16.860493] Epoch: [22]  [320/345]  eta: 0:00:15  lr: 0.000122  loss: 0.1893 (0.2020)  time: 0.6198  data: 0.0001  max mem: 15824
[09:24:29.246770] Epoch: [22]  [340/345]  eta: 0:00:03  lr: 0.000122  loss: 0.2009 (0.2018)  time: 0.6193  data: 0.0001  max mem: 15824
[09:24:31.715295] Epoch: [22]  [344/345]  eta: 0:00:00  lr: 0.000122  loss: 0.1920 (0.2016)  time: 0.6186  data: 0.0001  max mem: 15824
[09:24:31.782355] Epoch: [22] Total time: 0:03:34 (0.6212 s / it)
[09:24:31.782616] Averaged stats: lr: 0.000122  loss: 0.1920 (0.2016)
[09:24:32.375720] Test:  [  0/345]  eta: 0:03:22  loss: 0.1433 (0.1433)  time: 0.5876  data: 0.4047  max mem: 15824
[09:24:34.164633] Test:  [ 10/345]  eta: 0:01:12  loss: 0.2008 (0.1919)  time: 0.2159  data: 0.0369  max mem: 15824
[09:24:35.962656] Test:  [ 20/345]  eta: 0:01:04  loss: 0.2008 (0.1936)  time: 0.1793  data: 0.0001  max mem: 15824
[09:24:37.761652] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1917 (0.1930)  time: 0.1798  data: 0.0001  max mem: 15824
[09:24:39.563951] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1878 (0.1942)  time: 0.1800  data: 0.0001  max mem: 15824
[09:24:41.364637] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1843 (0.1925)  time: 0.1801  data: 0.0001  max mem: 15824
[09:24:43.173093] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1853 (0.1914)  time: 0.1804  data: 0.0001  max mem: 15824
[09:24:44.985195] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1857 (0.1898)  time: 0.1810  data: 0.0001  max mem: 15824
[09:24:46.796125] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1852 (0.1892)  time: 0.1811  data: 0.0001  max mem: 15824
[09:24:48.611142] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1896 (0.1891)  time: 0.1812  data: 0.0001  max mem: 15824
[09:24:50.431809] Test:  [100/345]  eta: 0:00:45  loss: 0.1791 (0.1880)  time: 0.1817  data: 0.0001  max mem: 15824
[09:24:52.254371] Test:  [110/345]  eta: 0:00:43  loss: 0.1780 (0.1876)  time: 0.1821  data: 0.0001  max mem: 15824
[09:24:54.082765] Test:  [120/345]  eta: 0:00:41  loss: 0.1764 (0.1867)  time: 0.1825  data: 0.0001  max mem: 15824
[09:24:55.911547] Test:  [130/345]  eta: 0:00:39  loss: 0.1769 (0.1868)  time: 0.1828  data: 0.0001  max mem: 15824
[09:24:57.746202] Test:  [140/345]  eta: 0:00:37  loss: 0.1907 (0.1875)  time: 0.1831  data: 0.0001  max mem: 15824
[09:24:59.583406] Test:  [150/345]  eta: 0:00:35  loss: 0.1907 (0.1878)  time: 0.1835  data: 0.0001  max mem: 15824
[09:25:01.425595] Test:  [160/345]  eta: 0:00:34  loss: 0.1852 (0.1883)  time: 0.1839  data: 0.0001  max mem: 15824
[09:25:03.269469] Test:  [170/345]  eta: 0:00:32  loss: 0.1812 (0.1887)  time: 0.1842  data: 0.0001  max mem: 15824
[09:25:05.116814] Test:  [180/345]  eta: 0:00:30  loss: 0.1823 (0.1888)  time: 0.1845  data: 0.0001  max mem: 15824
[09:25:06.968174] Test:  [190/345]  eta: 0:00:28  loss: 0.1823 (0.1886)  time: 0.1849  data: 0.0001  max mem: 15824
[09:25:08.822732] Test:  [200/345]  eta: 0:00:26  loss: 0.1823 (0.1883)  time: 0.1852  data: 0.0001  max mem: 15824
[09:25:10.680917] Test:  [210/345]  eta: 0:00:24  loss: 0.1875 (0.1887)  time: 0.1856  data: 0.0001  max mem: 15824
[09:25:12.542995] Test:  [220/345]  eta: 0:00:23  loss: 0.1858 (0.1885)  time: 0.1860  data: 0.0001  max mem: 15824
[09:25:14.407706] Test:  [230/345]  eta: 0:00:21  loss: 0.1832 (0.1888)  time: 0.1863  data: 0.0001  max mem: 15824
[09:25:16.277120] Test:  [240/345]  eta: 0:00:19  loss: 0.1890 (0.1889)  time: 0.1866  data: 0.0001  max mem: 15824
[09:25:18.148484] Test:  [250/345]  eta: 0:00:17  loss: 0.1801 (0.1884)  time: 0.1870  data: 0.0001  max mem: 15824
[09:25:20.022312] Test:  [260/345]  eta: 0:00:15  loss: 0.1741 (0.1884)  time: 0.1872  data: 0.0001  max mem: 15824
[09:25:21.899425] Test:  [270/345]  eta: 0:00:13  loss: 0.1735 (0.1880)  time: 0.1875  data: 0.0001  max mem: 15824
[09:25:23.781521] Test:  [280/345]  eta: 0:00:12  loss: 0.1754 (0.1880)  time: 0.1879  data: 0.0001  max mem: 15824
[09:25:25.665448] Test:  [290/345]  eta: 0:00:10  loss: 0.1980 (0.1889)  time: 0.1882  data: 0.0001  max mem: 15824
[09:25:27.555023] Test:  [300/345]  eta: 0:00:08  loss: 0.1866 (0.1884)  time: 0.1886  data: 0.0001  max mem: 15824
[09:25:29.447546] Test:  [310/345]  eta: 0:00:06  loss: 0.1800 (0.1885)  time: 0.1890  data: 0.0001  max mem: 15824
[09:25:31.345270] Test:  [320/345]  eta: 0:00:04  loss: 0.1902 (0.1888)  time: 0.1895  data: 0.0001  max mem: 15824
[09:25:33.244910] Test:  [330/345]  eta: 0:00:02  loss: 0.1822 (0.1885)  time: 0.1898  data: 0.0001  max mem: 15824
[09:25:35.145281] Test:  [340/345]  eta: 0:00:00  loss: 0.1883 (0.1887)  time: 0.1899  data: 0.0001  max mem: 15824
[09:25:35.906994] Test:  [344/345]  eta: 0:00:00  loss: 0.1831 (0.1886)  time: 0.1901  data: 0.0001  max mem: 15824
[09:25:35.965975] Test: Total time: 0:01:04 (0.1860 s / it)
[09:25:46.510437] Test:  [ 0/57]  eta: 0:00:33  loss: 0.4686 (0.4686)  time: 0.5791  data: 0.3985  max mem: 15824
[09:25:48.285287] Test:  [10/57]  eta: 0:00:10  loss: 0.4522 (0.4482)  time: 0.2139  data: 0.0363  max mem: 15824
[09:25:50.062081] Test:  [20/57]  eta: 0:00:07  loss: 0.4195 (0.4351)  time: 0.1775  data: 0.0001  max mem: 15824
[09:25:51.845444] Test:  [30/57]  eta: 0:00:05  loss: 0.3363 (0.3876)  time: 0.1779  data: 0.0001  max mem: 15824
[09:25:53.632780] Test:  [40/57]  eta: 0:00:03  loss: 0.2917 (0.3690)  time: 0.1785  data: 0.0001  max mem: 15824
[09:25:55.418524] Test:  [50/57]  eta: 0:00:01  loss: 0.3306 (0.3751)  time: 0.1786  data: 0.0001  max mem: 15824
[09:25:56.392471] Test:  [56/57]  eta: 0:00:00  loss: 0.3802 (0.3900)  time: 0.1737  data: 0.0000  max mem: 15824
[09:25:56.463864] Test: Total time: 0:00:10 (0.1848 s / it)
[09:25:58.182483] Dice score of the network on the train images: 0.832387, val images: 0.750247
[09:25:58.187231] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:25:59.216055] Epoch: [23]  [  0/345]  eta: 0:05:54  lr: 0.000122  loss: 0.2268 (0.2268)  time: 1.0277  data: 0.4039  max mem: 15824
[09:26:11.542575] Epoch: [23]  [ 20/345]  eta: 0:03:26  lr: 0.000122  loss: 0.2001 (0.2068)  time: 0.6163  data: 0.0001  max mem: 15824
[09:26:23.911746] Epoch: [23]  [ 40/345]  eta: 0:03:11  lr: 0.000122  loss: 0.1971 (0.2052)  time: 0.6184  data: 0.0001  max mem: 15824
[09:26:36.294303] Epoch: [23]  [ 60/345]  eta: 0:02:58  lr: 0.000122  loss: 0.2011 (0.2055)  time: 0.6191  data: 0.0001  max mem: 15824
[09:26:48.691431] Epoch: [23]  [ 80/345]  eta: 0:02:45  lr: 0.000121  loss: 0.1952 (0.2034)  time: 0.6198  data: 0.0001  max mem: 15824
[09:27:01.103345] Epoch: [23]  [100/345]  eta: 0:02:32  lr: 0.000121  loss: 0.2076 (0.2029)  time: 0.6205  data: 0.0001  max mem: 15824
[09:27:13.524472] Epoch: [23]  [120/345]  eta: 0:02:20  lr: 0.000121  loss: 0.1949 (0.2022)  time: 0.6210  data: 0.0001  max mem: 15824
[09:27:25.959371] Epoch: [23]  [140/345]  eta: 0:02:07  lr: 0.000121  loss: 0.1889 (0.2007)  time: 0.6217  data: 0.0001  max mem: 15824
[09:27:38.382607] Epoch: [23]  [160/345]  eta: 0:01:55  lr: 0.000121  loss: 0.1991 (0.2002)  time: 0.6211  data: 0.0001  max mem: 15824
[09:27:50.795197] Epoch: [23]  [180/345]  eta: 0:01:42  lr: 0.000121  loss: 0.1957 (0.2000)  time: 0.6206  data: 0.0001  max mem: 15824
[09:28:03.193572] Epoch: [23]  [200/345]  eta: 0:01:30  lr: 0.000121  loss: 0.1884 (0.1995)  time: 0.6199  data: 0.0001  max mem: 15824
[09:28:15.730163] Epoch: [23]  [220/345]  eta: 0:01:17  lr: 0.000121  loss: 0.1928 (0.1989)  time: 0.6268  data: 0.0001  max mem: 15824
[09:28:28.143918] Epoch: [23]  [240/345]  eta: 0:01:05  lr: 0.000120  loss: 0.1832 (0.1981)  time: 0.6206  data: 0.0001  max mem: 15824
[09:28:40.535357] Epoch: [23]  [260/345]  eta: 0:00:52  lr: 0.000120  loss: 0.2015 (0.1988)  time: 0.6195  data: 0.0001  max mem: 15824
[09:28:52.914879] Epoch: [23]  [280/345]  eta: 0:00:40  lr: 0.000120  loss: 0.2081 (0.1992)  time: 0.6189  data: 0.0001  max mem: 15824
[09:29:05.310622] Epoch: [23]  [300/345]  eta: 0:00:27  lr: 0.000120  loss: 0.1920 (0.1988)  time: 0.6197  data: 0.0001  max mem: 15824
[09:29:17.707537] Epoch: [23]  [320/345]  eta: 0:00:15  lr: 0.000120  loss: 0.1883 (0.1986)  time: 0.6197  data: 0.0001  max mem: 15824
[09:29:30.087253] Epoch: [23]  [340/345]  eta: 0:00:03  lr: 0.000120  loss: 0.1938 (0.1984)  time: 0.6189  data: 0.0001  max mem: 15824
[09:29:32.561911] Epoch: [23]  [344/345]  eta: 0:00:00  lr: 0.000120  loss: 0.1952 (0.1984)  time: 0.6189  data: 0.0001  max mem: 15824
[09:29:32.639823] Epoch: [23] Total time: 0:03:34 (0.6216 s / it)
[09:29:32.640064] Averaged stats: lr: 0.000120  loss: 0.1952 (0.1984)
[09:29:33.270835] Test:  [  0/345]  eta: 0:03:35  loss: 0.1651 (0.1651)  time: 0.6238  data: 0.4412  max mem: 15824
[09:29:35.060236] Test:  [ 10/345]  eta: 0:01:13  loss: 0.1937 (0.1904)  time: 0.2193  data: 0.0402  max mem: 15824
[09:29:36.853896] Test:  [ 20/345]  eta: 0:01:05  loss: 0.1869 (0.1872)  time: 0.1791  data: 0.0001  max mem: 15824
[09:29:38.649198] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1824 (0.1885)  time: 0.1794  data: 0.0001  max mem: 15824
[09:29:40.448185] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1881 (0.1908)  time: 0.1797  data: 0.0001  max mem: 15824
[09:29:42.248567] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1881 (0.1903)  time: 0.1799  data: 0.0001  max mem: 15824
[09:29:44.051530] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1869 (0.1894)  time: 0.1801  data: 0.0001  max mem: 15824
[09:29:45.861454] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1833 (0.1895)  time: 0.1806  data: 0.0001  max mem: 15824
[09:29:47.672793] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1845 (0.1896)  time: 0.1810  data: 0.0001  max mem: 15824
[09:29:49.490521] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1845 (0.1890)  time: 0.1814  data: 0.0001  max mem: 15824
[09:29:51.308305] Test:  [100/345]  eta: 0:00:45  loss: 0.1816 (0.1885)  time: 0.1817  data: 0.0001  max mem: 15824
[09:29:53.132284] Test:  [110/345]  eta: 0:00:43  loss: 0.1740 (0.1883)  time: 0.1820  data: 0.0001  max mem: 15824
[09:29:54.959304] Test:  [120/345]  eta: 0:00:41  loss: 0.1912 (0.1894)  time: 0.1825  data: 0.0001  max mem: 15824
[09:29:56.788957] Test:  [130/345]  eta: 0:00:39  loss: 0.1918 (0.1896)  time: 0.1828  data: 0.0001  max mem: 15824
[09:29:58.621095] Test:  [140/345]  eta: 0:00:37  loss: 0.1902 (0.1898)  time: 0.1830  data: 0.0001  max mem: 15824
[09:30:00.459692] Test:  [150/345]  eta: 0:00:35  loss: 0.1860 (0.1898)  time: 0.1835  data: 0.0001  max mem: 15824
[09:30:02.301077] Test:  [160/345]  eta: 0:00:34  loss: 0.1826 (0.1896)  time: 0.1839  data: 0.0001  max mem: 15824
[09:30:04.146839] Test:  [170/345]  eta: 0:00:32  loss: 0.1908 (0.1901)  time: 0.1843  data: 0.0001  max mem: 15824
[09:30:05.997715] Test:  [180/345]  eta: 0:00:30  loss: 0.1918 (0.1900)  time: 0.1848  data: 0.0001  max mem: 15824
[09:30:07.850543] Test:  [190/345]  eta: 0:00:28  loss: 0.1903 (0.1906)  time: 0.1851  data: 0.0001  max mem: 15824
[09:30:09.707445] Test:  [200/345]  eta: 0:00:26  loss: 0.1906 (0.1908)  time: 0.1854  data: 0.0001  max mem: 15824
[09:30:11.564905] Test:  [210/345]  eta: 0:00:24  loss: 0.1964 (0.1914)  time: 0.1857  data: 0.0001  max mem: 15824
[09:30:13.427606] Test:  [220/345]  eta: 0:00:23  loss: 0.1964 (0.1918)  time: 0.1859  data: 0.0001  max mem: 15824
[09:30:15.290794] Test:  [230/345]  eta: 0:00:21  loss: 0.1794 (0.1915)  time: 0.1862  data: 0.0001  max mem: 15824
[09:30:17.160174] Test:  [240/345]  eta: 0:00:19  loss: 0.1794 (0.1913)  time: 0.1866  data: 0.0001  max mem: 15824
[09:30:19.030784] Test:  [250/345]  eta: 0:00:17  loss: 0.1794 (0.1912)  time: 0.1869  data: 0.0001  max mem: 15824
[09:30:20.908248] Test:  [260/345]  eta: 0:00:15  loss: 0.1789 (0.1910)  time: 0.1873  data: 0.0001  max mem: 15824
[09:30:22.786673] Test:  [270/345]  eta: 0:00:13  loss: 0.1934 (0.1911)  time: 0.1877  data: 0.0001  max mem: 15824
[09:30:24.670657] Test:  [280/345]  eta: 0:00:12  loss: 0.2024 (0.1915)  time: 0.1881  data: 0.0001  max mem: 15824
[09:30:26.557950] Test:  [290/345]  eta: 0:00:10  loss: 0.1992 (0.1918)  time: 0.1885  data: 0.0001  max mem: 15824
[09:30:28.446523] Test:  [300/345]  eta: 0:00:08  loss: 0.1846 (0.1915)  time: 0.1887  data: 0.0001  max mem: 15824
[09:30:30.338978] Test:  [310/345]  eta: 0:00:06  loss: 0.1693 (0.1911)  time: 0.1890  data: 0.0001  max mem: 15824
[09:30:32.237254] Test:  [320/345]  eta: 0:00:04  loss: 0.1706 (0.1906)  time: 0.1895  data: 0.0001  max mem: 15824
[09:30:34.138820] Test:  [330/345]  eta: 0:00:02  loss: 0.1825 (0.1907)  time: 0.1899  data: 0.0001  max mem: 15824
[09:30:36.040482] Test:  [340/345]  eta: 0:00:00  loss: 0.1975 (0.1908)  time: 0.1901  data: 0.0001  max mem: 15824
[09:30:36.800684] Test:  [344/345]  eta: 0:00:00  loss: 0.1975 (0.1909)  time: 0.1901  data: 0.0001  max mem: 15824
[09:30:36.866427] Test: Total time: 0:01:04 (0.1861 s / it)
[09:30:47.350128] Test:  [ 0/57]  eta: 0:00:34  loss: 0.4462 (0.4462)  time: 0.6103  data: 0.4299  max mem: 15824
[09:30:49.121863] Test:  [10/57]  eta: 0:00:10  loss: 0.4295 (0.4514)  time: 0.2165  data: 0.0392  max mem: 15824
[09:30:50.899620] Test:  [20/57]  eta: 0:00:07  loss: 0.4295 (0.4344)  time: 0.1774  data: 0.0001  max mem: 15824
[09:30:52.681468] Test:  [30/57]  eta: 0:00:05  loss: 0.3481 (0.3910)  time: 0.1779  data: 0.0001  max mem: 15824
[09:30:54.468726] Test:  [40/57]  eta: 0:00:03  loss: 0.3059 (0.3762)  time: 0.1784  data: 0.0001  max mem: 15824
[09:30:56.254907] Test:  [50/57]  eta: 0:00:01  loss: 0.3576 (0.3827)  time: 0.1786  data: 0.0001  max mem: 15824
[09:30:57.226841] Test:  [56/57]  eta: 0:00:00  loss: 0.3994 (0.3963)  time: 0.1736  data: 0.0000  max mem: 15824
[09:30:57.293832] Test: Total time: 0:00:10 (0.1852 s / it)
[09:30:59.006562] Dice score of the network on the train images: 0.836652, val images: 0.748511
[09:30:59.010537] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:31:00.011026] Epoch: [24]  [  0/345]  eta: 0:05:44  lr: 0.000120  loss: 0.1655 (0.1655)  time: 0.9994  data: 0.3758  max mem: 15824
[09:31:12.328963] Epoch: [24]  [ 20/345]  eta: 0:03:26  lr: 0.000119  loss: 0.1815 (0.1946)  time: 0.6158  data: 0.0001  max mem: 15824
[09:31:24.666925] Epoch: [24]  [ 40/345]  eta: 0:03:10  lr: 0.000119  loss: 0.1950 (0.1978)  time: 0.6168  data: 0.0001  max mem: 15824
[09:31:37.056010] Epoch: [24]  [ 60/345]  eta: 0:02:57  lr: 0.000119  loss: 0.1915 (0.1966)  time: 0.6194  data: 0.0001  max mem: 15824
[09:31:49.452923] Epoch: [24]  [ 80/345]  eta: 0:02:45  lr: 0.000119  loss: 0.1956 (0.1964)  time: 0.6198  data: 0.0001  max mem: 15824
[09:32:01.976312] Epoch: [24]  [100/345]  eta: 0:02:32  lr: 0.000119  loss: 0.1907 (0.1964)  time: 0.6261  data: 0.0001  max mem: 15824
[09:32:14.392480] Epoch: [24]  [120/345]  eta: 0:02:20  lr: 0.000119  loss: 0.1875 (0.1955)  time: 0.6208  data: 0.0001  max mem: 15824
[09:32:26.808196] Epoch: [24]  [140/345]  eta: 0:02:07  lr: 0.000118  loss: 0.1827 (0.1942)  time: 0.6207  data: 0.0001  max mem: 15824
[09:32:39.235027] Epoch: [24]  [160/345]  eta: 0:01:55  lr: 0.000118  loss: 0.1932 (0.1943)  time: 0.6213  data: 0.0001  max mem: 15824
[09:32:51.654042] Epoch: [24]  [180/345]  eta: 0:01:42  lr: 0.000118  loss: 0.2029 (0.1962)  time: 0.6209  data: 0.0001  max mem: 15824
[09:33:04.054190] Epoch: [24]  [200/345]  eta: 0:01:30  lr: 0.000118  loss: 0.2043 (0.1971)  time: 0.6200  data: 0.0001  max mem: 15824
[09:33:16.426544] Epoch: [24]  [220/345]  eta: 0:01:17  lr: 0.000118  loss: 0.1973 (0.1968)  time: 0.6186  data: 0.0001  max mem: 15824
[09:33:28.839703] Epoch: [24]  [240/345]  eta: 0:01:05  lr: 0.000118  loss: 0.1907 (0.1970)  time: 0.6206  data: 0.0001  max mem: 15824
[09:33:41.225365] Epoch: [24]  [260/345]  eta: 0:00:52  lr: 0.000117  loss: 0.1892 (0.1969)  time: 0.6192  data: 0.0001  max mem: 15824
[09:33:53.602623] Epoch: [24]  [280/345]  eta: 0:00:40  lr: 0.000117  loss: 0.1874 (0.1965)  time: 0.6188  data: 0.0001  max mem: 15824
[09:34:05.992556] Epoch: [24]  [300/345]  eta: 0:00:27  lr: 0.000117  loss: 0.1891 (0.1962)  time: 0.6195  data: 0.0001  max mem: 15824
[09:34:18.364964] Epoch: [24]  [320/345]  eta: 0:00:15  lr: 0.000117  loss: 0.1804 (0.1957)  time: 0.6186  data: 0.0001  max mem: 15824
[09:34:30.760619] Epoch: [24]  [340/345]  eta: 0:00:03  lr: 0.000117  loss: 0.1908 (0.1959)  time: 0.6197  data: 0.0001  max mem: 15824
[09:34:33.238696] Epoch: [24]  [344/345]  eta: 0:00:00  lr: 0.000117  loss: 0.2059 (0.1959)  time: 0.6199  data: 0.0001  max mem: 15824
[09:34:33.313860] Epoch: [24] Total time: 0:03:34 (0.6212 s / it)
[09:34:33.314256] Averaged stats: lr: 0.000117  loss: 0.2059 (0.1959)
[09:34:33.932900] Test:  [  0/345]  eta: 0:03:31  loss: 0.1767 (0.1767)  time: 0.6133  data: 0.4300  max mem: 15824
[09:34:35.723820] Test:  [ 10/345]  eta: 0:01:13  loss: 0.1730 (0.1785)  time: 0.2185  data: 0.0392  max mem: 15824
[09:34:37.517494] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1866 (0.1869)  time: 0.1792  data: 0.0001  max mem: 15824
[09:34:39.311926] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1964 (0.1904)  time: 0.1793  data: 0.0001  max mem: 15824
[09:34:41.109298] Test:  [ 40/345]  eta: 0:00:57  loss: 0.2013 (0.1949)  time: 0.1795  data: 0.0001  max mem: 15824
[09:34:42.911477] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1929 (0.1935)  time: 0.1799  data: 0.0001  max mem: 15824
[09:34:44.714315] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1883 (0.1924)  time: 0.1802  data: 0.0001  max mem: 15824
[09:34:46.522390] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1837 (0.1901)  time: 0.1805  data: 0.0001  max mem: 15824
[09:34:48.333789] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1837 (0.1900)  time: 0.1809  data: 0.0001  max mem: 15824
[09:34:50.150520] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1914 (0.1906)  time: 0.1813  data: 0.0001  max mem: 15824
[09:34:51.968923] Test:  [100/345]  eta: 0:00:45  loss: 0.1825 (0.1903)  time: 0.1817  data: 0.0001  max mem: 15824
[09:34:53.792922] Test:  [110/345]  eta: 0:00:43  loss: 0.1805 (0.1894)  time: 0.1821  data: 0.0001  max mem: 15824
[09:34:55.619996] Test:  [120/345]  eta: 0:00:41  loss: 0.1903 (0.1900)  time: 0.1825  data: 0.0001  max mem: 15824
[09:34:57.451331] Test:  [130/345]  eta: 0:00:39  loss: 0.1903 (0.1893)  time: 0.1829  data: 0.0001  max mem: 15824
[09:34:59.282825] Test:  [140/345]  eta: 0:00:37  loss: 0.1761 (0.1889)  time: 0.1831  data: 0.0001  max mem: 15824
[09:35:01.118143] Test:  [150/345]  eta: 0:00:35  loss: 0.1836 (0.1893)  time: 0.1833  data: 0.0001  max mem: 15824
[09:35:02.957496] Test:  [160/345]  eta: 0:00:34  loss: 0.1994 (0.1897)  time: 0.1837  data: 0.0001  max mem: 15824
[09:35:04.803257] Test:  [170/345]  eta: 0:00:32  loss: 0.1935 (0.1899)  time: 0.1842  data: 0.0001  max mem: 15824
[09:35:06.651404] Test:  [180/345]  eta: 0:00:30  loss: 0.1841 (0.1895)  time: 0.1846  data: 0.0001  max mem: 15824
[09:35:08.503115] Test:  [190/345]  eta: 0:00:28  loss: 0.1790 (0.1899)  time: 0.1849  data: 0.0001  max mem: 15824
[09:35:10.356612] Test:  [200/345]  eta: 0:00:26  loss: 0.1959 (0.1906)  time: 0.1852  data: 0.0001  max mem: 15824
[09:35:12.213359] Test:  [210/345]  eta: 0:00:24  loss: 0.1959 (0.1909)  time: 0.1854  data: 0.0001  max mem: 15824
[09:35:14.073803] Test:  [220/345]  eta: 0:00:23  loss: 0.1913 (0.1912)  time: 0.1858  data: 0.0001  max mem: 15824
[09:35:15.938602] Test:  [230/345]  eta: 0:00:21  loss: 0.1842 (0.1910)  time: 0.1862  data: 0.0001  max mem: 15824
[09:35:17.806283] Test:  [240/345]  eta: 0:00:19  loss: 0.1880 (0.1913)  time: 0.1866  data: 0.0001  max mem: 15824
[09:35:19.675351] Test:  [250/345]  eta: 0:00:17  loss: 0.1762 (0.1906)  time: 0.1868  data: 0.0001  max mem: 15824
[09:35:21.549812] Test:  [260/345]  eta: 0:00:15  loss: 0.1764 (0.1908)  time: 0.1871  data: 0.0001  max mem: 15824
[09:35:23.427771] Test:  [270/345]  eta: 0:00:13  loss: 0.1834 (0.1906)  time: 0.1876  data: 0.0001  max mem: 15824
[09:35:25.310696] Test:  [280/345]  eta: 0:00:12  loss: 0.1834 (0.1905)  time: 0.1880  data: 0.0001  max mem: 15824
[09:35:27.194136] Test:  [290/345]  eta: 0:00:10  loss: 0.1796 (0.1903)  time: 0.1883  data: 0.0001  max mem: 15824
[09:35:29.082991] Test:  [300/345]  eta: 0:00:08  loss: 0.1783 (0.1902)  time: 0.1886  data: 0.0001  max mem: 15824
[09:35:30.974410] Test:  [310/345]  eta: 0:00:06  loss: 0.1864 (0.1903)  time: 0.1890  data: 0.0001  max mem: 15824
[09:35:32.870392] Test:  [320/345]  eta: 0:00:04  loss: 0.1972 (0.1906)  time: 0.1893  data: 0.0001  max mem: 15824
[09:35:34.770608] Test:  [330/345]  eta: 0:00:02  loss: 0.1815 (0.1901)  time: 0.1897  data: 0.0001  max mem: 15824
[09:35:36.671454] Test:  [340/345]  eta: 0:00:00  loss: 0.1802 (0.1901)  time: 0.1900  data: 0.0001  max mem: 15824
[09:35:37.431698] Test:  [344/345]  eta: 0:00:00  loss: 0.1802 (0.1899)  time: 0.1900  data: 0.0001  max mem: 15824
[09:35:37.509073] Test: Total time: 0:01:04 (0.1861 s / it)
[09:35:48.012052] Test:  [ 0/57]  eta: 0:00:31  loss: 0.4462 (0.4462)  time: 0.5560  data: 0.3760  max mem: 15824
[09:35:49.784268] Test:  [10/57]  eta: 0:00:09  loss: 0.4093 (0.4456)  time: 0.2116  data: 0.0343  max mem: 15824
[09:35:51.564462] Test:  [20/57]  eta: 0:00:07  loss: 0.4093 (0.4284)  time: 0.1775  data: 0.0001  max mem: 15824
[09:35:53.347058] Test:  [30/57]  eta: 0:00:05  loss: 0.3105 (0.3752)  time: 0.1781  data: 0.0001  max mem: 15824
[09:35:55.134473] Test:  [40/57]  eta: 0:00:03  loss: 0.2697 (0.3507)  time: 0.1784  data: 0.0001  max mem: 15824
[09:35:56.920857] Test:  [50/57]  eta: 0:00:01  loss: 0.2988 (0.3531)  time: 0.1786  data: 0.0001  max mem: 15824
[09:35:57.894844] Test:  [56/57]  eta: 0:00:00  loss: 0.3414 (0.3636)  time: 0.1737  data: 0.0000  max mem: 15824
[09:35:57.972486] Test: Total time: 0:00:10 (0.1845 s / it)
[09:35:59.682843] Dice score of the network on the train images: 0.809504, val images: 0.778791
[09:35:59.686964] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:36:00.692556] Epoch: [25]  [  0/345]  eta: 0:05:46  lr: 0.000117  loss: 0.1766 (0.1766)  time: 1.0045  data: 0.3791  max mem: 15824
[09:36:13.024658] Epoch: [25]  [ 20/345]  eta: 0:03:26  lr: 0.000116  loss: 0.1877 (0.1871)  time: 0.6166  data: 0.0001  max mem: 15824
[09:36:25.387025] Epoch: [25]  [ 40/345]  eta: 0:03:11  lr: 0.000116  loss: 0.1805 (0.1844)  time: 0.6181  data: 0.0001  max mem: 15824
[09:36:37.764676] Epoch: [25]  [ 60/345]  eta: 0:02:57  lr: 0.000116  loss: 0.1808 (0.1847)  time: 0.6188  data: 0.0001  max mem: 15824
[09:36:50.159708] Epoch: [25]  [ 80/345]  eta: 0:02:45  lr: 0.000116  loss: 0.1866 (0.1857)  time: 0.6197  data: 0.0001  max mem: 15824
[09:37:02.579354] Epoch: [25]  [100/345]  eta: 0:02:32  lr: 0.000116  loss: 0.1766 (0.1856)  time: 0.6209  data: 0.0001  max mem: 15824
[09:37:15.006281] Epoch: [25]  [120/345]  eta: 0:02:20  lr: 0.000115  loss: 0.1794 (0.1850)  time: 0.6213  data: 0.0001  max mem: 15824
[09:37:27.430727] Epoch: [25]  [140/345]  eta: 0:02:07  lr: 0.000115  loss: 0.1910 (0.1856)  time: 0.6212  data: 0.0001  max mem: 15824
[09:37:39.872019] Epoch: [25]  [160/345]  eta: 0:01:55  lr: 0.000115  loss: 0.1848 (0.1858)  time: 0.6220  data: 0.0001  max mem: 15824
[09:37:52.297489] Epoch: [25]  [180/345]  eta: 0:01:42  lr: 0.000115  loss: 0.1932 (0.1872)  time: 0.6212  data: 0.0001  max mem: 15824
[09:38:04.726864] Epoch: [25]  [200/345]  eta: 0:01:30  lr: 0.000115  loss: 0.2022 (0.1888)  time: 0.6214  data: 0.0001  max mem: 15824
[09:38:17.150331] Epoch: [25]  [220/345]  eta: 0:01:17  lr: 0.000114  loss: 0.1886 (0.1898)  time: 0.6211  data: 0.0001  max mem: 15824
[09:38:29.564606] Epoch: [25]  [240/345]  eta: 0:01:05  lr: 0.000114  loss: 0.1916 (0.1902)  time: 0.6207  data: 0.0001  max mem: 15824
[09:38:41.973957] Epoch: [25]  [260/345]  eta: 0:00:52  lr: 0.000114  loss: 0.1943 (0.1905)  time: 0.6204  data: 0.0001  max mem: 15824
[09:38:54.384540] Epoch: [25]  [280/345]  eta: 0:00:40  lr: 0.000114  loss: 0.1935 (0.1905)  time: 0.6205  data: 0.0001  max mem: 15824
[09:39:06.796321] Epoch: [25]  [300/345]  eta: 0:00:27  lr: 0.000114  loss: 0.1774 (0.1902)  time: 0.6205  data: 0.0001  max mem: 15824
[09:39:19.189601] Epoch: [25]  [320/345]  eta: 0:00:15  lr: 0.000113  loss: 0.1944 (0.1905)  time: 0.6196  data: 0.0001  max mem: 15824
[09:39:31.580207] Epoch: [25]  [340/345]  eta: 0:00:03  lr: 0.000113  loss: 0.1903 (0.1909)  time: 0.6195  data: 0.0001  max mem: 15824
[09:39:34.058593] Epoch: [25]  [344/345]  eta: 0:00:00  lr: 0.000113  loss: 0.1921 (0.1909)  time: 0.6194  data: 0.0001  max mem: 15824
[09:39:34.133012] Epoch: [25] Total time: 0:03:34 (0.6216 s / it)
[09:39:34.133154] Averaged stats: lr: 0.000113  loss: 0.1921 (0.1909)
[09:39:34.794661] Test:  [  0/345]  eta: 0:03:46  loss: 0.1641 (0.1641)  time: 0.6569  data: 0.4752  max mem: 15824
[09:39:36.586297] Test:  [ 10/345]  eta: 0:01:14  loss: 0.1781 (0.1768)  time: 0.2225  data: 0.0433  max mem: 15824
[09:39:38.380810] Test:  [ 20/345]  eta: 0:01:05  loss: 0.1781 (0.1820)  time: 0.1792  data: 0.0001  max mem: 15824
[09:39:40.175003] Test:  [ 30/345]  eta: 0:01:01  loss: 0.1717 (0.1797)  time: 0.1794  data: 0.0001  max mem: 15824
[09:39:41.971288] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1703 (0.1776)  time: 0.1795  data: 0.0001  max mem: 15824
[09:39:43.771755] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1736 (0.1792)  time: 0.1798  data: 0.0001  max mem: 15824
[09:39:45.574351] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1823 (0.1797)  time: 0.1801  data: 0.0001  max mem: 15824
[09:39:47.381539] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1803 (0.1806)  time: 0.1804  data: 0.0001  max mem: 15824
[09:39:49.193507] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1789 (0.1807)  time: 0.1809  data: 0.0001  max mem: 15824
[09:39:51.010600] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1709 (0.1804)  time: 0.1814  data: 0.0001  max mem: 15824
[09:39:52.829082] Test:  [100/345]  eta: 0:00:45  loss: 0.1829 (0.1816)  time: 0.1817  data: 0.0001  max mem: 15824
[09:39:54.651677] Test:  [110/345]  eta: 0:00:43  loss: 0.1874 (0.1822)  time: 0.1820  data: 0.0001  max mem: 15824
[09:39:56.477245] Test:  [120/345]  eta: 0:00:41  loss: 0.1906 (0.1834)  time: 0.1823  data: 0.0001  max mem: 15824
[09:39:58.305455] Test:  [130/345]  eta: 0:00:39  loss: 0.1868 (0.1831)  time: 0.1826  data: 0.0001  max mem: 15824
[09:40:00.138066] Test:  [140/345]  eta: 0:00:37  loss: 0.1731 (0.1823)  time: 0.1830  data: 0.0001  max mem: 15824
[09:40:01.974233] Test:  [150/345]  eta: 0:00:35  loss: 0.1719 (0.1819)  time: 0.1834  data: 0.0001  max mem: 15824
[09:40:03.815867] Test:  [160/345]  eta: 0:00:34  loss: 0.1721 (0.1818)  time: 0.1838  data: 0.0001  max mem: 15824
[09:40:05.660933] Test:  [170/345]  eta: 0:00:32  loss: 0.1790 (0.1822)  time: 0.1843  data: 0.0001  max mem: 15824
[09:40:07.509524] Test:  [180/345]  eta: 0:00:30  loss: 0.1903 (0.1828)  time: 0.1846  data: 0.0001  max mem: 15824
[09:40:09.357999] Test:  [190/345]  eta: 0:00:28  loss: 0.1883 (0.1830)  time: 0.1848  data: 0.0001  max mem: 15824
[09:40:11.211140] Test:  [200/345]  eta: 0:00:26  loss: 0.1842 (0.1837)  time: 0.1850  data: 0.0001  max mem: 15824
[09:40:13.069949] Test:  [210/345]  eta: 0:00:24  loss: 0.1925 (0.1842)  time: 0.1855  data: 0.0001  max mem: 15824
[09:40:14.932402] Test:  [220/345]  eta: 0:00:23  loss: 0.1744 (0.1838)  time: 0.1860  data: 0.0001  max mem: 15824
[09:40:16.798495] Test:  [230/345]  eta: 0:00:21  loss: 0.1819 (0.1851)  time: 0.1864  data: 0.0001  max mem: 15824
[09:40:18.667356] Test:  [240/345]  eta: 0:00:19  loss: 0.1819 (0.1847)  time: 0.1867  data: 0.0001  max mem: 15824
[09:40:20.539169] Test:  [250/345]  eta: 0:00:17  loss: 0.1806 (0.1847)  time: 0.1870  data: 0.0001  max mem: 15824
[09:40:22.413288] Test:  [260/345]  eta: 0:00:15  loss: 0.1834 (0.1846)  time: 0.1872  data: 0.0001  max mem: 15824
[09:40:24.291469] Test:  [270/345]  eta: 0:00:13  loss: 0.1751 (0.1846)  time: 0.1876  data: 0.0001  max mem: 15824
[09:40:26.172291] Test:  [280/345]  eta: 0:00:12  loss: 0.1811 (0.1847)  time: 0.1879  data: 0.0001  max mem: 15824
[09:40:28.058271] Test:  [290/345]  eta: 0:00:10  loss: 0.1802 (0.1846)  time: 0.1883  data: 0.0001  max mem: 15824
[09:40:29.948522] Test:  [300/345]  eta: 0:00:08  loss: 0.1842 (0.1849)  time: 0.1888  data: 0.0001  max mem: 15824
[09:40:31.840862] Test:  [310/345]  eta: 0:00:06  loss: 0.1888 (0.1849)  time: 0.1891  data: 0.0001  max mem: 15824
[09:40:33.739083] Test:  [320/345]  eta: 0:00:04  loss: 0.1888 (0.1854)  time: 0.1895  data: 0.0001  max mem: 15824
[09:40:35.640414] Test:  [330/345]  eta: 0:00:02  loss: 0.1935 (0.1855)  time: 0.1899  data: 0.0001  max mem: 15824
[09:40:37.542400] Test:  [340/345]  eta: 0:00:00  loss: 0.1736 (0.1855)  time: 0.1901  data: 0.0001  max mem: 15824
[09:40:38.302790] Test:  [344/345]  eta: 0:00:00  loss: 0.1711 (0.1855)  time: 0.1901  data: 0.0001  max mem: 15824
[09:40:38.374358] Test: Total time: 0:01:04 (0.1862 s / it)
[09:40:48.982610] Test:  [ 0/57]  eta: 0:00:35  loss: 0.5161 (0.5161)  time: 0.6211  data: 0.4408  max mem: 15824
[09:40:50.753633] Test:  [10/57]  eta: 0:00:10  loss: 0.4192 (0.4529)  time: 0.2174  data: 0.0401  max mem: 15824
[09:40:52.530440] Test:  [20/57]  eta: 0:00:07  loss: 0.4113 (0.4323)  time: 0.1773  data: 0.0001  max mem: 15824
[09:40:54.313861] Test:  [30/57]  eta: 0:00:05  loss: 0.2932 (0.3774)  time: 0.1779  data: 0.0001  max mem: 15824
[09:40:56.100492] Test:  [40/57]  eta: 0:00:03  loss: 0.2618 (0.3517)  time: 0.1784  data: 0.0001  max mem: 15824
[09:40:57.888129] Test:  [50/57]  eta: 0:00:01  loss: 0.3014 (0.3526)  time: 0.1787  data: 0.0001  max mem: 15824
[09:40:58.861428] Test:  [56/57]  eta: 0:00:00  loss: 0.3403 (0.3675)  time: 0.1738  data: 0.0001  max mem: 15824
[09:40:58.930692] Test: Total time: 0:00:10 (0.1854 s / it)
[09:41:00.637035] Dice score of the network on the train images: 0.818031, val images: 0.783109
[09:41:00.637264] saving best_dice_model_0 @ epoch 25
[09:41:01.747169] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:41:02.787782] Epoch: [26]  [  0/345]  eta: 0:05:58  lr: 0.000113  loss: 0.1788 (0.1788)  time: 1.0394  data: 0.4143  max mem: 15824
[09:41:15.123953] Epoch: [26]  [ 20/345]  eta: 0:03:26  lr: 0.000113  loss: 0.1954 (0.1965)  time: 0.6168  data: 0.0001  max mem: 15824
[09:41:27.483959] Epoch: [26]  [ 40/345]  eta: 0:03:11  lr: 0.000113  loss: 0.1860 (0.1906)  time: 0.6179  data: 0.0001  max mem: 15824
[09:41:39.872464] Epoch: [26]  [ 60/345]  eta: 0:02:58  lr: 0.000112  loss: 0.1905 (0.1898)  time: 0.6194  data: 0.0001  max mem: 15824
[09:41:52.258040] Epoch: [26]  [ 80/345]  eta: 0:02:45  lr: 0.000112  loss: 0.1958 (0.1917)  time: 0.6192  data: 0.0001  max mem: 15824
[09:42:04.673477] Epoch: [26]  [100/345]  eta: 0:02:32  lr: 0.000112  loss: 0.1705 (0.1884)  time: 0.6207  data: 0.0001  max mem: 15824
[09:42:17.089384] Epoch: [26]  [120/345]  eta: 0:02:20  lr: 0.000112  loss: 0.1739 (0.1884)  time: 0.6207  data: 0.0001  max mem: 15824
[09:42:29.519260] Epoch: [26]  [140/345]  eta: 0:02:07  lr: 0.000111  loss: 0.1864 (0.1880)  time: 0.6214  data: 0.0001  max mem: 15824
[09:42:41.947814] Epoch: [26]  [160/345]  eta: 0:01:55  lr: 0.000111  loss: 0.1799 (0.1870)  time: 0.6214  data: 0.0001  max mem: 15824
[09:42:54.379847] Epoch: [26]  [180/345]  eta: 0:01:42  lr: 0.000111  loss: 0.1762 (0.1864)  time: 0.6216  data: 0.0001  max mem: 15824
[09:43:06.809961] Epoch: [26]  [200/345]  eta: 0:01:30  lr: 0.000111  loss: 0.1886 (0.1861)  time: 0.6215  data: 0.0001  max mem: 15824
[09:43:19.217947] Epoch: [26]  [220/345]  eta: 0:01:17  lr: 0.000110  loss: 0.1754 (0.1856)  time: 0.6204  data: 0.0001  max mem: 15824
[09:43:31.629616] Epoch: [26]  [240/345]  eta: 0:01:05  lr: 0.000110  loss: 0.1777 (0.1855)  time: 0.6205  data: 0.0001  max mem: 15824
[09:43:44.045258] Epoch: [26]  [260/345]  eta: 0:00:52  lr: 0.000110  loss: 0.1842 (0.1854)  time: 0.6207  data: 0.0001  max mem: 15824
[09:43:56.439847] Epoch: [26]  [280/345]  eta: 0:00:40  lr: 0.000110  loss: 0.1777 (0.1853)  time: 0.6197  data: 0.0001  max mem: 15824
[09:44:08.841272] Epoch: [26]  [300/345]  eta: 0:00:27  lr: 0.000110  loss: 0.1788 (0.1853)  time: 0.6200  data: 0.0001  max mem: 15824
[09:44:21.247499] Epoch: [26]  [320/345]  eta: 0:00:15  lr: 0.000109  loss: 0.1913 (0.1858)  time: 0.6203  data: 0.0001  max mem: 15824
[09:44:33.643918] Epoch: [26]  [340/345]  eta: 0:00:03  lr: 0.000109  loss: 0.1805 (0.1861)  time: 0.6198  data: 0.0001  max mem: 15824
[09:44:36.123316] Epoch: [26]  [344/345]  eta: 0:00:00  lr: 0.000109  loss: 0.1872 (0.1864)  time: 0.6197  data: 0.0001  max mem: 15824
[09:44:36.199110] Epoch: [26] Total time: 0:03:34 (0.6216 s / it)
[09:44:36.199768] Averaged stats: lr: 0.000109  loss: 0.1872 (0.1864)
[09:44:36.810855] Test:  [  0/345]  eta: 0:03:28  loss: 0.1519 (0.1519)  time: 0.6054  data: 0.4230  max mem: 15824
[09:44:38.601659] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1790 (0.1797)  time: 0.2178  data: 0.0385  max mem: 15824
[09:44:40.391914] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1790 (0.1769)  time: 0.1790  data: 0.0001  max mem: 15824
[09:44:42.189487] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1754 (0.1778)  time: 0.1793  data: 0.0001  max mem: 15824
[09:44:43.991188] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1878 (0.1787)  time: 0.1799  data: 0.0001  max mem: 15824
[09:44:45.792306] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1857 (0.1801)  time: 0.1801  data: 0.0001  max mem: 15824
[09:44:47.594673] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1888 (0.1830)  time: 0.1801  data: 0.0001  max mem: 15824
[09:44:49.399733] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1912 (0.1854)  time: 0.1803  data: 0.0001  max mem: 15824
[09:44:51.211391] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1786 (0.1835)  time: 0.1808  data: 0.0001  max mem: 15824
[09:44:53.025260] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1716 (0.1838)  time: 0.1812  data: 0.0001  max mem: 15824
[09:44:54.844821] Test:  [100/345]  eta: 0:00:45  loss: 0.1724 (0.1833)  time: 0.1816  data: 0.0001  max mem: 15824
[09:44:56.668282] Test:  [110/345]  eta: 0:00:43  loss: 0.1702 (0.1825)  time: 0.1821  data: 0.0001  max mem: 15824
[09:44:58.493651] Test:  [120/345]  eta: 0:00:41  loss: 0.1702 (0.1825)  time: 0.1824  data: 0.0001  max mem: 15824
[09:45:00.321033] Test:  [130/345]  eta: 0:00:39  loss: 0.1741 (0.1827)  time: 0.1826  data: 0.0001  max mem: 15824
[09:45:02.153019] Test:  [140/345]  eta: 0:00:37  loss: 0.1822 (0.1829)  time: 0.1829  data: 0.0001  max mem: 15824
[09:45:03.988158] Test:  [150/345]  eta: 0:00:35  loss: 0.1734 (0.1821)  time: 0.1833  data: 0.0001  max mem: 15824
[09:45:05.827545] Test:  [160/345]  eta: 0:00:34  loss: 0.1641 (0.1811)  time: 0.1837  data: 0.0001  max mem: 15824
[09:45:07.670338] Test:  [170/345]  eta: 0:00:32  loss: 0.1645 (0.1810)  time: 0.1840  data: 0.0001  max mem: 15824
[09:45:09.516351] Test:  [180/345]  eta: 0:00:30  loss: 0.1780 (0.1807)  time: 0.1844  data: 0.0001  max mem: 15824
[09:45:11.367543] Test:  [190/345]  eta: 0:00:28  loss: 0.1836 (0.1810)  time: 0.1848  data: 0.0001  max mem: 15824
[09:45:13.225255] Test:  [200/345]  eta: 0:00:26  loss: 0.1805 (0.1811)  time: 0.1854  data: 0.0001  max mem: 15824
[09:45:15.082922] Test:  [210/345]  eta: 0:00:24  loss: 0.1754 (0.1810)  time: 0.1857  data: 0.0001  max mem: 15824
[09:45:16.943558] Test:  [220/345]  eta: 0:00:23  loss: 0.1747 (0.1808)  time: 0.1859  data: 0.0001  max mem: 15824
[09:45:18.809715] Test:  [230/345]  eta: 0:00:21  loss: 0.1747 (0.1804)  time: 0.1863  data: 0.0001  max mem: 15824
[09:45:20.678741] Test:  [240/345]  eta: 0:00:19  loss: 0.1730 (0.1805)  time: 0.1867  data: 0.0001  max mem: 15824
[09:45:22.550675] Test:  [250/345]  eta: 0:00:17  loss: 0.1698 (0.1802)  time: 0.1870  data: 0.0001  max mem: 15824
[09:45:24.424793] Test:  [260/345]  eta: 0:00:15  loss: 0.1701 (0.1802)  time: 0.1872  data: 0.0001  max mem: 15824
[09:45:26.301584] Test:  [270/345]  eta: 0:00:13  loss: 0.1808 (0.1803)  time: 0.1875  data: 0.0001  max mem: 15824
[09:45:28.182439] Test:  [280/345]  eta: 0:00:12  loss: 0.1733 (0.1800)  time: 0.1878  data: 0.0001  max mem: 15824
[09:45:30.067398] Test:  [290/345]  eta: 0:00:10  loss: 0.1825 (0.1804)  time: 0.1882  data: 0.0001  max mem: 15824
[09:45:31.955858] Test:  [300/345]  eta: 0:00:08  loss: 0.1801 (0.1801)  time: 0.1886  data: 0.0001  max mem: 15824
[09:45:33.849190] Test:  [310/345]  eta: 0:00:06  loss: 0.1710 (0.1800)  time: 0.1890  data: 0.0001  max mem: 15824
[09:45:35.744238] Test:  [320/345]  eta: 0:00:04  loss: 0.1799 (0.1800)  time: 0.1894  data: 0.0001  max mem: 15824
[09:45:37.645049] Test:  [330/345]  eta: 0:00:02  loss: 0.1737 (0.1799)  time: 0.1897  data: 0.0001  max mem: 15824
[09:45:39.544251] Test:  [340/345]  eta: 0:00:00  loss: 0.1737 (0.1800)  time: 0.1899  data: 0.0001  max mem: 15824
[09:45:40.305099] Test:  [344/345]  eta: 0:00:00  loss: 0.1746 (0.1800)  time: 0.1899  data: 0.0001  max mem: 15824
[09:45:40.375835] Test: Total time: 0:01:04 (0.1860 s / it)
[09:45:50.931911] Test:  [ 0/57]  eta: 0:00:33  loss: 0.4721 (0.4721)  time: 0.5926  data: 0.4114  max mem: 15824
[09:45:52.703176] Test:  [10/57]  eta: 0:00:10  loss: 0.4046 (0.4454)  time: 0.2148  data: 0.0375  max mem: 15824
[09:45:54.483468] Test:  [20/57]  eta: 0:00:07  loss: 0.4008 (0.4232)  time: 0.1775  data: 0.0001  max mem: 15824
[09:45:56.267241] Test:  [30/57]  eta: 0:00:05  loss: 0.3001 (0.3740)  time: 0.1781  data: 0.0001  max mem: 15824
[09:45:58.051404] Test:  [40/57]  eta: 0:00:03  loss: 0.2929 (0.3522)  time: 0.1783  data: 0.0001  max mem: 15824
[09:45:59.839604] Test:  [50/57]  eta: 0:00:01  loss: 0.3047 (0.3546)  time: 0.1786  data: 0.0001  max mem: 15824
[09:46:00.811852] Test:  [56/57]  eta: 0:00:00  loss: 0.3461 (0.3675)  time: 0.1736  data: 0.0001  max mem: 15824
[09:46:00.889855] Test: Total time: 0:00:10 (0.1851 s / it)
[09:46:02.589537] Dice score of the network on the train images: 0.828987, val images: 0.773262
[09:46:02.593819] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:46:03.616544] Epoch: [27]  [  0/345]  eta: 0:05:52  lr: 0.000109  loss: 0.1913 (0.1913)  time: 1.0218  data: 0.3928  max mem: 15824
[09:46:15.915102] Epoch: [27]  [ 20/345]  eta: 0:03:26  lr: 0.000109  loss: 0.1816 (0.1840)  time: 0.6149  data: 0.0001  max mem: 15824
[09:46:28.255893] Epoch: [27]  [ 40/345]  eta: 0:03:10  lr: 0.000108  loss: 0.1758 (0.1798)  time: 0.6170  data: 0.0001  max mem: 15824
[09:46:40.646025] Epoch: [27]  [ 60/345]  eta: 0:02:57  lr: 0.000108  loss: 0.1779 (0.1826)  time: 0.6195  data: 0.0001  max mem: 15824
[09:46:53.020355] Epoch: [27]  [ 80/345]  eta: 0:02:44  lr: 0.000108  loss: 0.1738 (0.1815)  time: 0.6187  data: 0.0001  max mem: 15824
[09:47:05.440253] Epoch: [27]  [100/345]  eta: 0:02:32  lr: 0.000108  loss: 0.1736 (0.1802)  time: 0.6209  data: 0.0001  max mem: 15824
[09:47:17.873451] Epoch: [27]  [120/345]  eta: 0:02:19  lr: 0.000107  loss: 0.1780 (0.1818)  time: 0.6216  data: 0.0001  max mem: 15824
[09:47:30.279064] Epoch: [27]  [140/345]  eta: 0:02:07  lr: 0.000107  loss: 0.1759 (0.1813)  time: 0.6202  data: 0.0001  max mem: 15824
[09:47:42.710681] Epoch: [27]  [160/345]  eta: 0:01:55  lr: 0.000107  loss: 0.1786 (0.1813)  time: 0.6215  data: 0.0001  max mem: 15824
[09:47:55.118786] Epoch: [27]  [180/345]  eta: 0:01:42  lr: 0.000107  loss: 0.1860 (0.1819)  time: 0.6204  data: 0.0001  max mem: 15824
[09:48:07.515302] Epoch: [27]  [200/345]  eta: 0:01:30  lr: 0.000106  loss: 0.1788 (0.1819)  time: 0.6198  data: 0.0001  max mem: 15824
[09:48:19.932538] Epoch: [27]  [220/345]  eta: 0:01:17  lr: 0.000106  loss: 0.1748 (0.1813)  time: 0.6208  data: 0.0001  max mem: 15824
[09:48:32.326743] Epoch: [27]  [240/345]  eta: 0:01:05  lr: 0.000106  loss: 0.1714 (0.1807)  time: 0.6197  data: 0.0001  max mem: 15824
[09:48:44.729970] Epoch: [27]  [260/345]  eta: 0:00:52  lr: 0.000106  loss: 0.1714 (0.1803)  time: 0.6201  data: 0.0001  max mem: 15824
[09:48:57.142365] Epoch: [27]  [280/345]  eta: 0:00:40  lr: 0.000105  loss: 0.1704 (0.1802)  time: 0.6206  data: 0.0001  max mem: 15824
[09:49:09.545958] Epoch: [27]  [300/345]  eta: 0:00:27  lr: 0.000105  loss: 0.1668 (0.1797)  time: 0.6201  data: 0.0001  max mem: 15824
[09:49:21.954023] Epoch: [27]  [320/345]  eta: 0:00:15  lr: 0.000105  loss: 0.1766 (0.1800)  time: 0.6204  data: 0.0001  max mem: 15824
[09:49:34.346043] Epoch: [27]  [340/345]  eta: 0:00:03  lr: 0.000104  loss: 0.1728 (0.1800)  time: 0.6196  data: 0.0001  max mem: 15824
[09:49:36.824842] Epoch: [27]  [344/345]  eta: 0:00:00  lr: 0.000104  loss: 0.1840 (0.1802)  time: 0.6198  data: 0.0001  max mem: 15824
[09:49:36.901581] Epoch: [27] Total time: 0:03:34 (0.6212 s / it)
[09:49:36.901959] Averaged stats: lr: 0.000104  loss: 0.1840 (0.1802)
[09:49:37.494283] Test:  [  0/345]  eta: 0:03:21  loss: 0.1574 (0.1574)  time: 0.5852  data: 0.4042  max mem: 15824
[09:49:39.284457] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1788 (0.1796)  time: 0.2159  data: 0.0368  max mem: 15824
[09:49:41.076905] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1754 (0.1782)  time: 0.1791  data: 0.0001  max mem: 15824
[09:49:42.873132] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1729 (0.1784)  time: 0.1794  data: 0.0001  max mem: 15824
[09:49:44.674241] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1769 (0.1783)  time: 0.1798  data: 0.0001  max mem: 15824
[09:49:46.474481] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1784 (0.1767)  time: 0.1800  data: 0.0001  max mem: 15824
[09:49:48.276337] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1635 (0.1744)  time: 0.1800  data: 0.0001  max mem: 15824
[09:49:50.082213] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1635 (0.1742)  time: 0.1803  data: 0.0001  max mem: 15824
[09:49:51.895169] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1619 (0.1734)  time: 0.1809  data: 0.0001  max mem: 15824
[09:49:53.712099] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1635 (0.1744)  time: 0.1814  data: 0.0001  max mem: 15824
[09:49:55.533615] Test:  [100/345]  eta: 0:00:45  loss: 0.1673 (0.1743)  time: 0.1819  data: 0.0001  max mem: 15824
[09:49:57.355705] Test:  [110/345]  eta: 0:00:43  loss: 0.1679 (0.1744)  time: 0.1821  data: 0.0001  max mem: 15824
[09:49:59.181337] Test:  [120/345]  eta: 0:00:41  loss: 0.1679 (0.1744)  time: 0.1823  data: 0.0001  max mem: 15824
[09:50:01.009892] Test:  [130/345]  eta: 0:00:39  loss: 0.1631 (0.1731)  time: 0.1826  data: 0.0001  max mem: 15824
[09:50:02.843464] Test:  [140/345]  eta: 0:00:37  loss: 0.1657 (0.1733)  time: 0.1830  data: 0.0001  max mem: 15824
[09:50:04.681456] Test:  [150/345]  eta: 0:00:35  loss: 0.1712 (0.1729)  time: 0.1835  data: 0.0001  max mem: 15824
[09:50:06.523782] Test:  [160/345]  eta: 0:00:34  loss: 0.1725 (0.1730)  time: 0.1840  data: 0.0001  max mem: 15824
[09:50:08.367781] Test:  [170/345]  eta: 0:00:32  loss: 0.1746 (0.1727)  time: 0.1843  data: 0.0001  max mem: 15824
[09:50:10.214294] Test:  [180/345]  eta: 0:00:30  loss: 0.1761 (0.1734)  time: 0.1845  data: 0.0001  max mem: 15824
[09:50:12.063685] Test:  [190/345]  eta: 0:00:28  loss: 0.1806 (0.1742)  time: 0.1847  data: 0.0001  max mem: 15824
[09:50:13.919005] Test:  [200/345]  eta: 0:00:26  loss: 0.1784 (0.1741)  time: 0.1852  data: 0.0001  max mem: 15824
[09:50:15.777437] Test:  [210/345]  eta: 0:00:24  loss: 0.1801 (0.1747)  time: 0.1856  data: 0.0001  max mem: 15824
[09:50:17.638681] Test:  [220/345]  eta: 0:00:23  loss: 0.1801 (0.1747)  time: 0.1859  data: 0.0001  max mem: 15824
[09:50:19.503523] Test:  [230/345]  eta: 0:00:21  loss: 0.1783 (0.1748)  time: 0.1862  data: 0.0001  max mem: 15824
[09:50:21.370266] Test:  [240/345]  eta: 0:00:19  loss: 0.1757 (0.1748)  time: 0.1865  data: 0.0001  max mem: 15824
[09:50:23.240897] Test:  [250/345]  eta: 0:00:17  loss: 0.1719 (0.1748)  time: 0.1868  data: 0.0001  max mem: 15824
[09:50:25.114161] Test:  [260/345]  eta: 0:00:15  loss: 0.1719 (0.1747)  time: 0.1871  data: 0.0001  max mem: 15824
[09:50:26.991553] Test:  [270/345]  eta: 0:00:13  loss: 0.1717 (0.1747)  time: 0.1875  data: 0.0001  max mem: 15824
[09:50:28.874914] Test:  [280/345]  eta: 0:00:12  loss: 0.1617 (0.1746)  time: 0.1880  data: 0.0001  max mem: 15824
[09:50:30.759129] Test:  [290/345]  eta: 0:00:10  loss: 0.1621 (0.1745)  time: 0.1883  data: 0.0001  max mem: 15824
[09:50:32.647542] Test:  [300/345]  eta: 0:00:08  loss: 0.1677 (0.1747)  time: 0.1886  data: 0.0001  max mem: 15824
[09:50:34.539897] Test:  [310/345]  eta: 0:00:06  loss: 0.1679 (0.1745)  time: 0.1890  data: 0.0001  max mem: 15824
[09:50:36.436331] Test:  [320/345]  eta: 0:00:04  loss: 0.1679 (0.1746)  time: 0.1894  data: 0.0001  max mem: 15824
[09:50:38.336130] Test:  [330/345]  eta: 0:00:02  loss: 0.1763 (0.1746)  time: 0.1898  data: 0.0001  max mem: 15824
[09:50:40.237033] Test:  [340/345]  eta: 0:00:00  loss: 0.1732 (0.1746)  time: 0.1900  data: 0.0001  max mem: 15824
[09:50:40.997818] Test:  [344/345]  eta: 0:00:00  loss: 0.1724 (0.1746)  time: 0.1900  data: 0.0001  max mem: 15824
[09:50:41.070288] Test: Total time: 0:01:04 (0.1860 s / it)
[09:50:51.637954] Test:  [ 0/57]  eta: 0:00:33  loss: 0.4915 (0.4915)  time: 0.5906  data: 0.4092  max mem: 15824
[09:50:53.414726] Test:  [10/57]  eta: 0:00:10  loss: 0.4491 (0.4671)  time: 0.2151  data: 0.0373  max mem: 15824
[09:50:55.192117] Test:  [20/57]  eta: 0:00:07  loss: 0.4277 (0.4429)  time: 0.1776  data: 0.0001  max mem: 15824
[09:50:56.977020] Test:  [30/57]  eta: 0:00:05  loss: 0.3004 (0.3886)  time: 0.1780  data: 0.0001  max mem: 15824
[09:50:58.766032] Test:  [40/57]  eta: 0:00:03  loss: 0.2882 (0.3658)  time: 0.1786  data: 0.0001  max mem: 15824
[09:51:00.552642] Test:  [50/57]  eta: 0:00:01  loss: 0.3188 (0.3728)  time: 0.1787  data: 0.0001  max mem: 15824
[09:51:01.525178] Test:  [56/57]  eta: 0:00:00  loss: 0.3539 (0.3904)  time: 0.1738  data: 0.0001  max mem: 15824
[09:51:01.594445] Test: Total time: 0:00:10 (0.1850 s / it)
[09:51:03.310813] Dice score of the network on the train images: 0.832678, val images: 0.758989
[09:51:03.314787] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:51:04.327601] Epoch: [28]  [  0/345]  eta: 0:05:49  lr: 0.000104  loss: 0.1898 (0.1898)  time: 1.0119  data: 0.3880  max mem: 15824
[09:51:16.664844] Epoch: [28]  [ 20/345]  eta: 0:03:26  lr: 0.000104  loss: 0.1650 (0.1733)  time: 0.6168  data: 0.0001  max mem: 15824
[09:51:29.035400] Epoch: [28]  [ 40/345]  eta: 0:03:11  lr: 0.000104  loss: 0.1800 (0.1763)  time: 0.6185  data: 0.0001  max mem: 15824
[09:51:41.391926] Epoch: [28]  [ 60/345]  eta: 0:02:57  lr: 0.000103  loss: 0.1731 (0.1759)  time: 0.6178  data: 0.0001  max mem: 15824
[09:51:53.759441] Epoch: [28]  [ 80/345]  eta: 0:02:45  lr: 0.000103  loss: 0.1728 (0.1757)  time: 0.6183  data: 0.0001  max mem: 15824
[09:52:06.171552] Epoch: [28]  [100/345]  eta: 0:02:32  lr: 0.000103  loss: 0.1779 (0.1775)  time: 0.6206  data: 0.0001  max mem: 15824
[09:52:18.604243] Epoch: [28]  [120/345]  eta: 0:02:19  lr: 0.000103  loss: 0.1818 (0.1782)  time: 0.6216  data: 0.0001  max mem: 15824
[09:52:31.178963] Epoch: [28]  [140/345]  eta: 0:02:07  lr: 0.000102  loss: 0.1663 (0.1776)  time: 0.6287  data: 0.0001  max mem: 15824
[09:52:43.569445] Epoch: [28]  [160/345]  eta: 0:01:55  lr: 0.000102  loss: 0.1791 (0.1778)  time: 0.6195  data: 0.0001  max mem: 15824
[09:52:56.001088] Epoch: [28]  [180/345]  eta: 0:01:42  lr: 0.000102  loss: 0.1674 (0.1766)  time: 0.6215  data: 0.0001  max mem: 15824
[09:53:08.398842] Epoch: [28]  [200/345]  eta: 0:01:30  lr: 0.000101  loss: 0.1690 (0.1766)  time: 0.6198  data: 0.0001  max mem: 15824
[09:53:20.790892] Epoch: [28]  [220/345]  eta: 0:01:17  lr: 0.000101  loss: 0.1687 (0.1763)  time: 0.6196  data: 0.0001  max mem: 15824
[09:53:33.189365] Epoch: [28]  [240/345]  eta: 0:01:05  lr: 0.000101  loss: 0.1694 (0.1760)  time: 0.6199  data: 0.0001  max mem: 15824
[09:53:45.586236] Epoch: [28]  [260/345]  eta: 0:00:52  lr: 0.000101  loss: 0.1758 (0.1762)  time: 0.6198  data: 0.0001  max mem: 15824
[09:53:57.985381] Epoch: [28]  [280/345]  eta: 0:00:40  lr: 0.000100  loss: 0.1667 (0.1759)  time: 0.6199  data: 0.0001  max mem: 15824
[09:54:10.380097] Epoch: [28]  [300/345]  eta: 0:00:27  lr: 0.000100  loss: 0.1748 (0.1760)  time: 0.6197  data: 0.0001  max mem: 15824
[09:54:22.775845] Epoch: [28]  [320/345]  eta: 0:00:15  lr: 0.000100  loss: 0.1711 (0.1759)  time: 0.6197  data: 0.0001  max mem: 15824
[09:54:35.167861] Epoch: [28]  [340/345]  eta: 0:00:03  lr: 0.000099  loss: 0.1784 (0.1766)  time: 0.6196  data: 0.0001  max mem: 15824
[09:54:37.648613] Epoch: [28]  [344/345]  eta: 0:00:00  lr: 0.000099  loss: 0.1753 (0.1768)  time: 0.6195  data: 0.0001  max mem: 15824
[09:54:37.713748] Epoch: [28] Total time: 0:03:34 (0.6214 s / it)
[09:54:37.713986] Averaged stats: lr: 0.000099  loss: 0.1753 (0.1768)
[09:54:38.326864] Test:  [  0/345]  eta: 0:03:29  loss: 0.1656 (0.1656)  time: 0.6076  data: 0.4261  max mem: 15824
[09:54:40.115807] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1664 (0.1684)  time: 0.2178  data: 0.0388  max mem: 15824
[09:54:41.911778] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1523 (0.1581)  time: 0.1792  data: 0.0001  max mem: 15824
[09:54:43.710436] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1520 (0.1621)  time: 0.1797  data: 0.0001  max mem: 15824
[09:54:45.510726] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1597 (0.1608)  time: 0.1799  data: 0.0001  max mem: 15824
[09:54:47.315554] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1560 (0.1599)  time: 0.1802  data: 0.0001  max mem: 15824
[09:54:49.118856] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1615 (0.1622)  time: 0.1803  data: 0.0001  max mem: 15824
[09:54:50.925657] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1816 (0.1655)  time: 0.1804  data: 0.0001  max mem: 15824
[09:54:52.737811] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1829 (0.1669)  time: 0.1809  data: 0.0001  max mem: 15824
[09:54:54.556435] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1748 (0.1685)  time: 0.1815  data: 0.0001  max mem: 15824
[09:54:56.379025] Test:  [100/345]  eta: 0:00:45  loss: 0.1742 (0.1691)  time: 0.1820  data: 0.0001  max mem: 15824
[09:54:58.203457] Test:  [110/345]  eta: 0:00:43  loss: 0.1617 (0.1679)  time: 0.1823  data: 0.0001  max mem: 15824
[09:55:00.028663] Test:  [120/345]  eta: 0:00:41  loss: 0.1594 (0.1676)  time: 0.1824  data: 0.0001  max mem: 15824
[09:55:01.856756] Test:  [130/345]  eta: 0:00:39  loss: 0.1651 (0.1681)  time: 0.1826  data: 0.0001  max mem: 15824
[09:55:03.689429] Test:  [140/345]  eta: 0:00:37  loss: 0.1551 (0.1673)  time: 0.1830  data: 0.0001  max mem: 15824
[09:55:05.527414] Test:  [150/345]  eta: 0:00:35  loss: 0.1613 (0.1675)  time: 0.1835  data: 0.0001  max mem: 15824
[09:55:07.368951] Test:  [160/345]  eta: 0:00:34  loss: 0.1643 (0.1671)  time: 0.1839  data: 0.0001  max mem: 15824
[09:55:09.214233] Test:  [170/345]  eta: 0:00:32  loss: 0.1656 (0.1673)  time: 0.1843  data: 0.0001  max mem: 15824
[09:55:11.063301] Test:  [180/345]  eta: 0:00:30  loss: 0.1634 (0.1672)  time: 0.1847  data: 0.0001  max mem: 15824
[09:55:12.913621] Test:  [190/345]  eta: 0:00:28  loss: 0.1634 (0.1673)  time: 0.1849  data: 0.0001  max mem: 15824
[09:55:14.769629] Test:  [200/345]  eta: 0:00:26  loss: 0.1658 (0.1675)  time: 0.1853  data: 0.0001  max mem: 15824
[09:55:16.627942] Test:  [210/345]  eta: 0:00:24  loss: 0.1556 (0.1668)  time: 0.1857  data: 0.0001  max mem: 15824
[09:55:18.491258] Test:  [220/345]  eta: 0:00:23  loss: 0.1584 (0.1667)  time: 0.1860  data: 0.0001  max mem: 15824
[09:55:20.355448] Test:  [230/345]  eta: 0:00:21  loss: 0.1675 (0.1670)  time: 0.1863  data: 0.0001  max mem: 15824
[09:55:22.225380] Test:  [240/345]  eta: 0:00:19  loss: 0.1688 (0.1676)  time: 0.1866  data: 0.0001  max mem: 15824
[09:55:24.097632] Test:  [250/345]  eta: 0:00:17  loss: 0.1845 (0.1681)  time: 0.1870  data: 0.0001  max mem: 15824
[09:55:25.973142] Test:  [260/345]  eta: 0:00:15  loss: 0.1745 (0.1683)  time: 0.1873  data: 0.0001  max mem: 15824
[09:55:27.853555] Test:  [270/345]  eta: 0:00:13  loss: 0.1768 (0.1687)  time: 0.1877  data: 0.0001  max mem: 15824
[09:55:29.735287] Test:  [280/345]  eta: 0:00:12  loss: 0.1764 (0.1689)  time: 0.1880  data: 0.0001  max mem: 15824
[09:55:31.618284] Test:  [290/345]  eta: 0:00:10  loss: 0.1661 (0.1690)  time: 0.1882  data: 0.0001  max mem: 15824
[09:55:33.506519] Test:  [300/345]  eta: 0:00:08  loss: 0.1673 (0.1692)  time: 0.1885  data: 0.0001  max mem: 15824
[09:55:35.399553] Test:  [310/345]  eta: 0:00:06  loss: 0.1579 (0.1688)  time: 0.1890  data: 0.0001  max mem: 15824
[09:55:37.297778] Test:  [320/345]  eta: 0:00:04  loss: 0.1554 (0.1687)  time: 0.1895  data: 0.0001  max mem: 15824
[09:55:39.200322] Test:  [330/345]  eta: 0:00:02  loss: 0.1698 (0.1686)  time: 0.1900  data: 0.0001  max mem: 15824
[09:55:41.099096] Test:  [340/345]  eta: 0:00:00  loss: 0.1591 (0.1686)  time: 0.1900  data: 0.0001  max mem: 15824
[09:55:41.859915] Test:  [344/345]  eta: 0:00:00  loss: 0.1480 (0.1683)  time: 0.1900  data: 0.0001  max mem: 15824
[09:55:41.936551] Test: Total time: 0:01:04 (0.1861 s / it)
[09:55:52.571296] Test:  [ 0/57]  eta: 0:00:35  loss: 0.4704 (0.4704)  time: 0.6237  data: 0.4439  max mem: 15824
[09:55:54.345844] Test:  [10/57]  eta: 0:00:10  loss: 0.4261 (0.4456)  time: 0.2179  data: 0.0404  max mem: 15824
[09:55:56.124498] Test:  [20/57]  eta: 0:00:07  loss: 0.4146 (0.4239)  time: 0.1776  data: 0.0001  max mem: 15824
[09:55:57.907522] Test:  [30/57]  eta: 0:00:05  loss: 0.2899 (0.3697)  time: 0.1780  data: 0.0001  max mem: 15824
[09:55:59.691725] Test:  [40/57]  eta: 0:00:03  loss: 0.2713 (0.3445)  time: 0.1783  data: 0.0001  max mem: 15824
[09:56:01.479965] Test:  [50/57]  eta: 0:00:01  loss: 0.2893 (0.3508)  time: 0.1786  data: 0.0001  max mem: 15824
[09:56:02.451243] Test:  [56/57]  eta: 0:00:00  loss: 0.3350 (0.3652)  time: 0.1736  data: 0.0000  max mem: 15824
[09:56:02.531314] Test: Total time: 0:00:10 (0.1857 s / it)
[09:56:04.243948] Dice score of the network on the train images: 0.830409, val images: 0.781888
[09:56:04.247579] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:56:05.254371] Epoch: [29]  [  0/345]  eta: 0:05:47  lr: 0.000099  loss: 0.1625 (0.1625)  time: 1.0059  data: 0.3745  max mem: 15824
[09:56:17.585217] Epoch: [29]  [ 20/345]  eta: 0:03:26  lr: 0.000099  loss: 0.1684 (0.1699)  time: 0.6165  data: 0.0001  max mem: 15824
[09:56:29.944499] Epoch: [29]  [ 40/345]  eta: 0:03:11  lr: 0.000099  loss: 0.1665 (0.1690)  time: 0.6179  data: 0.0001  max mem: 15824
[09:56:42.303485] Epoch: [29]  [ 60/345]  eta: 0:02:57  lr: 0.000098  loss: 0.1905 (0.1740)  time: 0.6179  data: 0.0001  max mem: 15824
[09:56:54.704025] Epoch: [29]  [ 80/345]  eta: 0:02:45  lr: 0.000098  loss: 0.1658 (0.1736)  time: 0.6200  data: 0.0001  max mem: 15824
[09:57:07.120336] Epoch: [29]  [100/345]  eta: 0:02:32  lr: 0.000098  loss: 0.1883 (0.1757)  time: 0.6208  data: 0.0001  max mem: 15824
[09:57:19.538064] Epoch: [29]  [120/345]  eta: 0:02:19  lr: 0.000097  loss: 0.1768 (0.1763)  time: 0.6208  data: 0.0001  max mem: 15824
[09:57:31.962883] Epoch: [29]  [140/345]  eta: 0:02:07  lr: 0.000097  loss: 0.1639 (0.1746)  time: 0.6212  data: 0.0001  max mem: 15824
[09:57:44.393727] Epoch: [29]  [160/345]  eta: 0:01:55  lr: 0.000097  loss: 0.1783 (0.1755)  time: 0.6215  data: 0.0001  max mem: 15824
[09:57:56.804152] Epoch: [29]  [180/345]  eta: 0:01:42  lr: 0.000096  loss: 0.1697 (0.1752)  time: 0.6205  data: 0.0001  max mem: 15824
[09:58:09.222322] Epoch: [29]  [200/345]  eta: 0:01:30  lr: 0.000096  loss: 0.1569 (0.1737)  time: 0.6209  data: 0.0001  max mem: 15824
[09:58:21.631182] Epoch: [29]  [220/345]  eta: 0:01:17  lr: 0.000096  loss: 0.1580 (0.1729)  time: 0.6204  data: 0.0001  max mem: 15824
[09:58:34.030658] Epoch: [29]  [240/345]  eta: 0:01:05  lr: 0.000095  loss: 0.1632 (0.1722)  time: 0.6199  data: 0.0001  max mem: 15824
[09:58:46.427466] Epoch: [29]  [260/345]  eta: 0:00:52  lr: 0.000095  loss: 0.1824 (0.1731)  time: 0.6198  data: 0.0001  max mem: 15824
[09:58:58.819549] Epoch: [29]  [280/345]  eta: 0:00:40  lr: 0.000095  loss: 0.1547 (0.1726)  time: 0.6196  data: 0.0001  max mem: 15824
[09:59:11.230697] Epoch: [29]  [300/345]  eta: 0:00:27  lr: 0.000094  loss: 0.1669 (0.1723)  time: 0.6205  data: 0.0001  max mem: 15824
[09:59:23.612052] Epoch: [29]  [320/345]  eta: 0:00:15  lr: 0.000094  loss: 0.1645 (0.1720)  time: 0.6190  data: 0.0001  max mem: 15824
[09:59:36.009058] Epoch: [29]  [340/345]  eta: 0:00:03  lr: 0.000094  loss: 0.1751 (0.1718)  time: 0.6198  data: 0.0001  max mem: 15824
[09:59:38.492533] Epoch: [29]  [344/345]  eta: 0:00:00  lr: 0.000094  loss: 0.1751 (0.1718)  time: 0.6199  data: 0.0001  max mem: 15824
[09:59:38.572258] Epoch: [29] Total time: 0:03:34 (0.6212 s / it)
[09:59:38.572602] Averaged stats: lr: 0.000094  loss: 0.1751 (0.1718)
[09:59:39.207426] Test:  [  0/345]  eta: 0:03:37  loss: 0.1493 (0.1493)  time: 0.6295  data: 0.4477  max mem: 15824
[09:59:40.996708] Test:  [ 10/345]  eta: 0:01:13  loss: 0.1493 (0.1510)  time: 0.2198  data: 0.0408  max mem: 15824
[09:59:42.792392] Test:  [ 20/345]  eta: 0:01:05  loss: 0.1508 (0.1553)  time: 0.1792  data: 0.0001  max mem: 15824
[09:59:44.592470] Test:  [ 30/345]  eta: 0:01:01  loss: 0.1569 (0.1566)  time: 0.1797  data: 0.0001  max mem: 15824
[09:59:46.393895] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1649 (0.1601)  time: 0.1800  data: 0.0001  max mem: 15824
[09:59:48.199219] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1620 (0.1603)  time: 0.1803  data: 0.0001  max mem: 15824
[09:59:50.001840] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1604 (0.1601)  time: 0.1803  data: 0.0001  max mem: 15824
[09:59:51.812052] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1579 (0.1606)  time: 0.1806  data: 0.0001  max mem: 15824
[09:59:53.624256] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1654 (0.1618)  time: 0.1811  data: 0.0001  max mem: 15824
[09:59:55.440904] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1653 (0.1623)  time: 0.1814  data: 0.0001  max mem: 15824
[09:59:57.260969] Test:  [100/345]  eta: 0:00:45  loss: 0.1574 (0.1625)  time: 0.1818  data: 0.0001  max mem: 15824
[09:59:59.084661] Test:  [110/345]  eta: 0:00:43  loss: 0.1507 (0.1615)  time: 0.1821  data: 0.0001  max mem: 15824
[10:00:00.911192] Test:  [120/345]  eta: 0:00:41  loss: 0.1576 (0.1617)  time: 0.1824  data: 0.0001  max mem: 15824
[10:00:02.739462] Test:  [130/345]  eta: 0:00:39  loss: 0.1523 (0.1608)  time: 0.1827  data: 0.0001  max mem: 15824
[10:00:04.573018] Test:  [140/345]  eta: 0:00:37  loss: 0.1487 (0.1614)  time: 0.1830  data: 0.0001  max mem: 15824
[10:00:06.408542] Test:  [150/345]  eta: 0:00:35  loss: 0.1684 (0.1620)  time: 0.1834  data: 0.0001  max mem: 15824
[10:00:08.249659] Test:  [160/345]  eta: 0:00:34  loss: 0.1654 (0.1617)  time: 0.1838  data: 0.0001  max mem: 15824
[10:00:10.095029] Test:  [170/345]  eta: 0:00:32  loss: 0.1528 (0.1616)  time: 0.1843  data: 0.0001  max mem: 15824
[10:00:11.940313] Test:  [180/345]  eta: 0:00:30  loss: 0.1519 (0.1619)  time: 0.1845  data: 0.0001  max mem: 15824
[10:00:13.790482] Test:  [190/345]  eta: 0:00:28  loss: 0.1578 (0.1617)  time: 0.1847  data: 0.0001  max mem: 15824
[10:00:15.645024] Test:  [200/345]  eta: 0:00:26  loss: 0.1596 (0.1618)  time: 0.1852  data: 0.0001  max mem: 15824
[10:00:17.501855] Test:  [210/345]  eta: 0:00:24  loss: 0.1656 (0.1623)  time: 0.1855  data: 0.0001  max mem: 15824
[10:00:19.364390] Test:  [220/345]  eta: 0:00:23  loss: 0.1657 (0.1624)  time: 0.1859  data: 0.0001  max mem: 15824
[10:00:21.231480] Test:  [230/345]  eta: 0:00:21  loss: 0.1625 (0.1623)  time: 0.1864  data: 0.0001  max mem: 15824
[10:00:23.101121] Test:  [240/345]  eta: 0:00:19  loss: 0.1635 (0.1628)  time: 0.1868  data: 0.0001  max mem: 15824
[10:00:24.975878] Test:  [250/345]  eta: 0:00:17  loss: 0.1806 (0.1631)  time: 0.1872  data: 0.0001  max mem: 15824
[10:00:26.849677] Test:  [260/345]  eta: 0:00:15  loss: 0.1628 (0.1628)  time: 0.1874  data: 0.0001  max mem: 15824
[10:00:28.728466] Test:  [270/345]  eta: 0:00:13  loss: 0.1649 (0.1632)  time: 0.1876  data: 0.0001  max mem: 15824
[10:00:30.609768] Test:  [280/345]  eta: 0:00:12  loss: 0.1733 (0.1638)  time: 0.1879  data: 0.0001  max mem: 15824
[10:00:32.495805] Test:  [290/345]  eta: 0:00:10  loss: 0.1696 (0.1639)  time: 0.1883  data: 0.0001  max mem: 15824
[10:00:34.386791] Test:  [300/345]  eta: 0:00:08  loss: 0.1571 (0.1636)  time: 0.1888  data: 0.0001  max mem: 15824
[10:00:36.281372] Test:  [310/345]  eta: 0:00:06  loss: 0.1539 (0.1633)  time: 0.1892  data: 0.0001  max mem: 15824
[10:00:38.179195] Test:  [320/345]  eta: 0:00:04  loss: 0.1519 (0.1631)  time: 0.1896  data: 0.0001  max mem: 15824
[10:00:40.079939] Test:  [330/345]  eta: 0:00:02  loss: 0.1613 (0.1634)  time: 0.1899  data: 0.0001  max mem: 15824
[10:00:41.980266] Test:  [340/345]  eta: 0:00:00  loss: 0.1656 (0.1636)  time: 0.1900  data: 0.0001  max mem: 15824
[10:00:42.741404] Test:  [344/345]  eta: 0:00:00  loss: 0.1613 (0.1636)  time: 0.1901  data: 0.0001  max mem: 15824
[10:00:42.812490] Test: Total time: 0:01:04 (0.1862 s / it)
[10:00:53.368798] Test:  [ 0/57]  eta: 0:00:33  loss: 0.4966 (0.4966)  time: 0.5823  data: 0.4026  max mem: 15824
[10:00:55.142422] Test:  [10/57]  eta: 0:00:10  loss: 0.4287 (0.4671)  time: 0.2141  data: 0.0367  max mem: 15824
[10:00:56.916584] Test:  [20/57]  eta: 0:00:07  loss: 0.4287 (0.4490)  time: 0.1773  data: 0.0001  max mem: 15824
[10:00:58.697487] Test:  [30/57]  eta: 0:00:05  loss: 0.3063 (0.3913)  time: 0.1777  data: 0.0001  max mem: 15824
[10:01:00.485360] Test:  [40/57]  eta: 0:00:03  loss: 0.2971 (0.3657)  time: 0.1784  data: 0.0001  max mem: 15824
[10:01:02.271054] Test:  [50/57]  eta: 0:00:01  loss: 0.3100 (0.3696)  time: 0.1786  data: 0.0001  max mem: 15824
[10:01:03.246360] Test:  [56/57]  eta: 0:00:00  loss: 0.3656 (0.3839)  time: 0.1738  data: 0.0000  max mem: 15824
[10:01:03.314089] Test: Total time: 0:00:10 (0.1847 s / it)
[10:01:05.022023] Dice score of the network on the train images: 0.841320, val images: 0.771172
[10:01:05.026075] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:01:06.029566] Epoch: [30]  [  0/345]  eta: 0:05:45  lr: 0.000094  loss: 0.1760 (0.1760)  time: 1.0024  data: 0.3750  max mem: 15824
[10:01:18.334115] Epoch: [30]  [ 20/345]  eta: 0:03:25  lr: 0.000093  loss: 0.1732 (0.1749)  time: 0.6152  data: 0.0001  max mem: 15824
[10:01:30.691981] Epoch: [30]  [ 40/345]  eta: 0:03:10  lr: 0.000093  loss: 0.1716 (0.1746)  time: 0.6178  data: 0.0001  max mem: 15824
[10:01:43.086319] Epoch: [30]  [ 60/345]  eta: 0:02:57  lr: 0.000093  loss: 0.1691 (0.1737)  time: 0.6197  data: 0.0001  max mem: 15824
[10:01:55.485335] Epoch: [30]  [ 80/345]  eta: 0:02:45  lr: 0.000092  loss: 0.1686 (0.1737)  time: 0.6199  data: 0.0001  max mem: 15824
[10:02:07.900930] Epoch: [30]  [100/345]  eta: 0:02:32  lr: 0.000092  loss: 0.1615 (0.1721)  time: 0.6207  data: 0.0001  max mem: 15824
[10:02:20.324718] Epoch: [30]  [120/345]  eta: 0:02:20  lr: 0.000092  loss: 0.1696 (0.1721)  time: 0.6211  data: 0.0001  max mem: 15824
[10:02:32.758083] Epoch: [30]  [140/345]  eta: 0:02:07  lr: 0.000091  loss: 0.1587 (0.1717)  time: 0.6216  data: 0.0001  max mem: 15824
[10:02:45.175103] Epoch: [30]  [160/345]  eta: 0:01:55  lr: 0.000091  loss: 0.1695 (0.1715)  time: 0.6208  data: 0.0001  max mem: 15824
[10:02:57.600525] Epoch: [30]  [180/345]  eta: 0:01:42  lr: 0.000091  loss: 0.1688 (0.1717)  time: 0.6212  data: 0.0001  max mem: 15824
[10:03:10.009463] Epoch: [30]  [200/345]  eta: 0:01:30  lr: 0.000090  loss: 0.1574 (0.1711)  time: 0.6204  data: 0.0001  max mem: 15824
[10:03:22.434181] Epoch: [30]  [220/345]  eta: 0:01:17  lr: 0.000090  loss: 0.1610 (0.1703)  time: 0.6212  data: 0.0001  max mem: 15824
[10:03:34.833830] Epoch: [30]  [240/345]  eta: 0:01:05  lr: 0.000090  loss: 0.1722 (0.1700)  time: 0.6199  data: 0.0001  max mem: 15824
[10:03:47.243973] Epoch: [30]  [260/345]  eta: 0:00:52  lr: 0.000089  loss: 0.1705 (0.1701)  time: 0.6205  data: 0.0001  max mem: 15824
[10:03:59.650021] Epoch: [30]  [280/345]  eta: 0:00:40  lr: 0.000089  loss: 0.1537 (0.1692)  time: 0.6203  data: 0.0001  max mem: 15824
[10:04:12.041479] Epoch: [30]  [300/345]  eta: 0:00:27  lr: 0.000089  loss: 0.1716 (0.1695)  time: 0.6195  data: 0.0001  max mem: 15824
[10:04:24.436341] Epoch: [30]  [320/345]  eta: 0:00:15  lr: 0.000088  loss: 0.1688 (0.1696)  time: 0.6197  data: 0.0001  max mem: 15824
[10:04:36.829419] Epoch: [30]  [340/345]  eta: 0:00:03  lr: 0.000088  loss: 0.1724 (0.1697)  time: 0.6196  data: 0.0001  max mem: 15824
[10:04:39.306962] Epoch: [30]  [344/345]  eta: 0:00:00  lr: 0.000088  loss: 0.1701 (0.1696)  time: 0.6194  data: 0.0001  max mem: 15824
[10:04:39.378339] Epoch: [30] Total time: 0:03:34 (0.6213 s / it)
[10:04:39.378647] Averaged stats: lr: 0.000088  loss: 0.1701 (0.1696)
[10:04:40.012451] Test:  [  0/345]  eta: 0:03:36  loss: 0.1395 (0.1395)  time: 0.6276  data: 0.4469  max mem: 15824
[10:04:41.800783] Test:  [ 10/345]  eta: 0:01:13  loss: 0.1395 (0.1444)  time: 0.2196  data: 0.0407  max mem: 15824
[10:04:43.594226] Test:  [ 20/345]  eta: 0:01:05  loss: 0.1535 (0.1562)  time: 0.1790  data: 0.0001  max mem: 15824
[10:04:45.392256] Test:  [ 30/345]  eta: 0:01:01  loss: 0.1645 (0.1590)  time: 0.1795  data: 0.0001  max mem: 15824
[10:04:47.191054] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1619 (0.1597)  time: 0.1798  data: 0.0001  max mem: 15824
[10:04:48.994780] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1419 (0.1576)  time: 0.1801  data: 0.0001  max mem: 15824
[10:04:50.801533] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1455 (0.1587)  time: 0.1804  data: 0.0001  max mem: 15824
[10:04:52.608598] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1653 (0.1589)  time: 0.1806  data: 0.0001  max mem: 15824
[10:04:54.421616] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1635 (0.1590)  time: 0.1809  data: 0.0001  max mem: 15824
[10:04:56.240519] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1640 (0.1594)  time: 0.1815  data: 0.0001  max mem: 15824
[10:04:58.060155] Test:  [100/345]  eta: 0:00:45  loss: 0.1433 (0.1583)  time: 0.1819  data: 0.0001  max mem: 15824
[10:04:59.883484] Test:  [110/345]  eta: 0:00:43  loss: 0.1464 (0.1580)  time: 0.1821  data: 0.0001  max mem: 15824
[10:05:01.709739] Test:  [120/345]  eta: 0:00:41  loss: 0.1590 (0.1589)  time: 0.1824  data: 0.0001  max mem: 15824
[10:05:03.539892] Test:  [130/345]  eta: 0:00:39  loss: 0.1569 (0.1582)  time: 0.1828  data: 0.0001  max mem: 15824
[10:05:05.374464] Test:  [140/345]  eta: 0:00:37  loss: 0.1538 (0.1586)  time: 0.1832  data: 0.0001  max mem: 15824
[10:05:07.211158] Test:  [150/345]  eta: 0:00:35  loss: 0.1520 (0.1582)  time: 0.1835  data: 0.0001  max mem: 15824
[10:05:09.050760] Test:  [160/345]  eta: 0:00:34  loss: 0.1490 (0.1582)  time: 0.1838  data: 0.0001  max mem: 15824
[10:05:10.895622] Test:  [170/345]  eta: 0:00:32  loss: 0.1562 (0.1582)  time: 0.1842  data: 0.0001  max mem: 15824
[10:05:12.742893] Test:  [180/345]  eta: 0:00:30  loss: 0.1579 (0.1585)  time: 0.1845  data: 0.0001  max mem: 15824
[10:05:14.593916] Test:  [190/345]  eta: 0:00:28  loss: 0.1486 (0.1578)  time: 0.1848  data: 0.0001  max mem: 15824
[10:05:16.447942] Test:  [200/345]  eta: 0:00:26  loss: 0.1417 (0.1576)  time: 0.1852  data: 0.0001  max mem: 15824
[10:05:18.307789] Test:  [210/345]  eta: 0:00:24  loss: 0.1494 (0.1575)  time: 0.1856  data: 0.0001  max mem: 15824
[10:05:20.168845] Test:  [220/345]  eta: 0:00:23  loss: 0.1469 (0.1571)  time: 0.1860  data: 0.0001  max mem: 15824
[10:05:22.034688] Test:  [230/345]  eta: 0:00:21  loss: 0.1469 (0.1572)  time: 0.1863  data: 0.0001  max mem: 15824
[10:05:23.903363] Test:  [240/345]  eta: 0:00:19  loss: 0.1503 (0.1570)  time: 0.1867  data: 0.0001  max mem: 15824
[10:05:25.777744] Test:  [250/345]  eta: 0:00:17  loss: 0.1490 (0.1570)  time: 0.1871  data: 0.0001  max mem: 15824
[10:05:27.654511] Test:  [260/345]  eta: 0:00:15  loss: 0.1507 (0.1572)  time: 0.1874  data: 0.0001  max mem: 15824
[10:05:29.533306] Test:  [270/345]  eta: 0:00:13  loss: 0.1520 (0.1568)  time: 0.1877  data: 0.0001  max mem: 15824
[10:05:31.416144] Test:  [280/345]  eta: 0:00:12  loss: 0.1419 (0.1569)  time: 0.1880  data: 0.0001  max mem: 15824
[10:05:33.303197] Test:  [290/345]  eta: 0:00:10  loss: 0.1443 (0.1568)  time: 0.1884  data: 0.0001  max mem: 15824
[10:05:35.192520] Test:  [300/345]  eta: 0:00:08  loss: 0.1478 (0.1568)  time: 0.1888  data: 0.0001  max mem: 15824
[10:05:37.085503] Test:  [310/345]  eta: 0:00:06  loss: 0.1526 (0.1570)  time: 0.1891  data: 0.0001  max mem: 15824
[10:05:38.983293] Test:  [320/345]  eta: 0:00:04  loss: 0.1516 (0.1568)  time: 0.1895  data: 0.0001  max mem: 15824
[10:05:40.883461] Test:  [330/345]  eta: 0:00:02  loss: 0.1545 (0.1571)  time: 0.1898  data: 0.0001  max mem: 15824
[10:05:42.782277] Test:  [340/345]  eta: 0:00:00  loss: 0.1565 (0.1570)  time: 0.1899  data: 0.0001  max mem: 15824
[10:05:43.543405] Test:  [344/345]  eta: 0:00:00  loss: 0.1577 (0.1573)  time: 0.1900  data: 0.0001  max mem: 15824
[10:05:43.607121] Test: Total time: 0:01:04 (0.1862 s / it)
[10:05:54.236769] Test:  [ 0/57]  eta: 0:00:35  loss: 0.4851 (0.4851)  time: 0.6186  data: 0.4387  max mem: 15824
[10:05:56.008829] Test:  [10/57]  eta: 0:00:10  loss: 0.4263 (0.4566)  time: 0.2173  data: 0.0400  max mem: 15824
[10:05:57.787576] Test:  [20/57]  eta: 0:00:07  loss: 0.4203 (0.4377)  time: 0.1775  data: 0.0001  max mem: 15824
[10:05:59.567880] Test:  [30/57]  eta: 0:00:05  loss: 0.3099 (0.3839)  time: 0.1779  data: 0.0001  max mem: 15824
[10:06:01.352171] Test:  [40/57]  eta: 0:00:03  loss: 0.2787 (0.3611)  time: 0.1782  data: 0.0001  max mem: 15824
[10:06:03.138476] Test:  [50/57]  eta: 0:00:01  loss: 0.3173 (0.3630)  time: 0.1785  data: 0.0001  max mem: 15824
[10:06:04.111494] Test:  [56/57]  eta: 0:00:00  loss: 0.3631 (0.3759)  time: 0.1737  data: 0.0000  max mem: 15824
[10:06:04.186236] Test: Total time: 0:00:10 (0.1854 s / it)
[10:06:05.922377] Dice score of the network on the train images: 0.847098, val images: 0.774393
[10:06:05.926653] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:06:06.963010] Epoch: [31]  [  0/345]  eta: 0:05:57  lr: 0.000088  loss: 0.1506 (0.1506)  time: 1.0352  data: 0.4093  max mem: 15824
[10:06:19.302391] Epoch: [31]  [ 20/345]  eta: 0:03:26  lr: 0.000088  loss: 0.1537 (0.1604)  time: 0.6169  data: 0.0001  max mem: 15824
[10:06:31.687308] Epoch: [31]  [ 40/345]  eta: 0:03:11  lr: 0.000087  loss: 0.1565 (0.1583)  time: 0.6192  data: 0.0001  max mem: 15824
[10:06:44.073245] Epoch: [31]  [ 60/345]  eta: 0:02:58  lr: 0.000087  loss: 0.1540 (0.1579)  time: 0.6192  data: 0.0001  max mem: 15824
[10:06:56.477246] Epoch: [31]  [ 80/345]  eta: 0:02:45  lr: 0.000087  loss: 0.1528 (0.1598)  time: 0.6202  data: 0.0001  max mem: 15824
[10:07:08.905472] Epoch: [31]  [100/345]  eta: 0:02:32  lr: 0.000086  loss: 0.1606 (0.1600)  time: 0.6214  data: 0.0001  max mem: 15824
[10:07:21.362655] Epoch: [31]  [120/345]  eta: 0:02:20  lr: 0.000086  loss: 0.1591 (0.1598)  time: 0.6228  data: 0.0001  max mem: 15824
[10:07:33.806136] Epoch: [31]  [140/345]  eta: 0:02:07  lr: 0.000085  loss: 0.1539 (0.1590)  time: 0.6221  data: 0.0001  max mem: 15824

[10:07:46.251012] Epoch: [31]  [160/345]  eta: 0:01:55  lr: 0.000085  loss: 0.1617 (0.1600)  time: 0.6222  data: 0.0001  max mem: 15824
[10:07:58.646847] Epoch: [31]  [180/345]  eta: 0:01:42  lr: 0.000085  loss: 0.1560 (0.1595)  time: 0.6197  data: 0.0001  max mem: 15824
[10:08:11.044297] Epoch: [31]  [200/345]  eta: 0:01:30  lr: 0.000084  loss: 0.1622 (0.1599)  time: 0.6198  data: 0.0001  max mem: 15824
[10:08:23.459980] Epoch: [31]  [220/345]  eta: 0:01:17  lr: 0.000084  loss: 0.1592 (0.1601)  time: 0.6207  data: 0.0001  max mem: 15824
[10:08:35.854878] Epoch: [31]  [240/345]  eta: 0:01:05  lr: 0.000084  loss: 0.1623 (0.1610)  time: 0.6197  data: 0.0001  max mem: 15824
[10:08:48.245840] Epoch: [31]  [260/345]  eta: 0:00:52  lr: 0.000083  loss: 0.1576 (0.1609)  time: 0.6195  data: 0.0001  max mem: 15824
[10:09:00.620447] Epoch: [31]  [280/345]  eta: 0:00:40  lr: 0.000083  loss: 0.1592 (0.1614)  time: 0.6187  data: 0.0001  max mem: 15824
[10:09:13.007283] Epoch: [31]  [300/345]  eta: 0:00:27  lr: 0.000083  loss: 0.1593 (0.1616)  time: 0.6193  data: 0.0001  max mem: 15824
[10:09:25.399277] Epoch: [31]  [320/345]  eta: 0:00:15  lr: 0.000082  loss: 0.1708 (0.1622)  time: 0.6196  data: 0.0001  max mem: 15824
[10:09:37.776714] Epoch: [31]  [340/345]  eta: 0:00:03  lr: 0.000082  loss: 0.1544 (0.1620)  time: 0.6188  data: 0.0001  max mem: 15824
[10:09:40.254981] Epoch: [31]  [344/345]  eta: 0:00:00  lr: 0.000082  loss: 0.1532 (0.1620)  time: 0.6189  data: 0.0001  max mem: 15824
[10:09:40.325238] Epoch: [31] Total time: 0:03:34 (0.6214 s / it)
[10:09:40.325459] Averaged stats: lr: 0.000082  loss: 0.1532 (0.1620)
[10:09:40.926681] Test:  [  0/345]  eta: 0:03:25  loss: 0.1592 (0.1592)  time: 0.5968  data: 0.4150  max mem: 15824
[10:09:42.719964] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1460 (0.1457)  time: 0.2172  data: 0.0378  max mem: 15824
[10:09:44.519825] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1470 (0.1498)  time: 0.1796  data: 0.0001  max mem: 15824
[10:09:46.320427] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1543 (0.1536)  time: 0.1800  data: 0.0001  max mem: 15824
[10:09:48.121314] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1621 (0.1571)  time: 0.1800  data: 0.0001  max mem: 15824
[10:09:49.922951] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1661 (0.1590)  time: 0.1801  data: 0.0001  max mem: 15824
[10:09:51.729366] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1489 (0.1555)  time: 0.1803  data: 0.0001  max mem: 15824
[10:09:53.542524] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1489 (0.1565)  time: 0.1809  data: 0.0001  max mem: 15824
[10:09:55.354840] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1544 (0.1556)  time: 0.1812  data: 0.0001  max mem: 15824
[10:09:57.173808] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1551 (0.1552)  time: 0.1815  data: 0.0001  max mem: 15824
[10:09:58.991958] Test:  [100/345]  eta: 0:00:45  loss: 0.1551 (0.1553)  time: 0.1818  data: 0.0001  max mem: 15824
[10:10:00.813898] Test:  [110/345]  eta: 0:00:43  loss: 0.1505 (0.1553)  time: 0.1819  data: 0.0001  max mem: 15824
[10:10:02.640845] Test:  [120/345]  eta: 0:00:41  loss: 0.1521 (0.1554)  time: 0.1824  data: 0.0001  max mem: 15824
[10:10:04.469803] Test:  [130/345]  eta: 0:00:39  loss: 0.1529 (0.1549)  time: 0.1827  data: 0.0001  max mem: 15824
[10:10:06.304938] Test:  [140/345]  eta: 0:00:37  loss: 0.1507 (0.1544)  time: 0.1831  data: 0.0001  max mem: 15824
[10:10:08.143237] Test:  [150/345]  eta: 0:00:35  loss: 0.1507 (0.1545)  time: 0.1836  data: 0.0001  max mem: 15824
[10:10:09.987026] Test:  [160/345]  eta: 0:00:34  loss: 0.1549 (0.1550)  time: 0.1840  data: 0.0001  max mem: 15824
[10:10:11.834426] Test:  [170/345]  eta: 0:00:32  loss: 0.1510 (0.1547)  time: 0.1845  data: 0.0001  max mem: 15824
[10:10:13.682204] Test:  [180/345]  eta: 0:00:30  loss: 0.1569 (0.1553)  time: 0.1847  data: 0.0001  max mem: 15824
[10:10:15.533366] Test:  [190/345]  eta: 0:00:28  loss: 0.1520 (0.1547)  time: 0.1849  data: 0.0001  max mem: 15824
[10:10:17.390100] Test:  [200/345]  eta: 0:00:26  loss: 0.1462 (0.1548)  time: 0.1853  data: 0.0001  max mem: 15824
[10:10:19.248159] Test:  [210/345]  eta: 0:00:24  loss: 0.1564 (0.1548)  time: 0.1857  data: 0.0001  max mem: 15824
[10:10:21.111287] Test:  [220/345]  eta: 0:00:23  loss: 0.1606 (0.1552)  time: 0.1860  data: 0.0001  max mem: 15824
[10:10:22.980290] Test:  [230/345]  eta: 0:00:21  loss: 0.1572 (0.1546)  time: 0.1865  data: 0.0001  max mem: 15824
[10:10:24.848823] Test:  [240/345]  eta: 0:00:19  loss: 0.1382 (0.1545)  time: 0.1868  data: 0.0001  max mem: 15824
[10:10:26.721357] Test:  [250/345]  eta: 0:00:17  loss: 0.1434 (0.1542)  time: 0.1870  data: 0.0001  max mem: 15824
[10:10:28.599003] Test:  [260/345]  eta: 0:00:15  loss: 0.1471 (0.1539)  time: 0.1874  data: 0.0001  max mem: 15824
[10:10:30.478950] Test:  [270/345]  eta: 0:00:13  loss: 0.1471 (0.1537)  time: 0.1878  data: 0.0001  max mem: 15824
[10:10:32.363823] Test:  [280/345]  eta: 0:00:12  loss: 0.1526 (0.1539)  time: 0.1882  data: 0.0001  max mem: 15824
[10:10:34.250615] Test:  [290/345]  eta: 0:00:10  loss: 0.1549 (0.1541)  time: 0.1885  data: 0.0001  max mem: 15824
[10:10:36.140347] Test:  [300/345]  eta: 0:00:08  loss: 0.1614 (0.1544)  time: 0.1888  data: 0.0001  max mem: 15824
[10:10:38.033541] Test:  [310/345]  eta: 0:00:06  loss: 0.1566 (0.1542)  time: 0.1891  data: 0.0001  max mem: 15824
[10:10:39.931647] Test:  [320/345]  eta: 0:00:04  loss: 0.1542 (0.1545)  time: 0.1895  data: 0.0001  max mem: 15824
[10:10:41.832417] Test:  [330/345]  eta: 0:00:02  loss: 0.1600 (0.1549)  time: 0.1899  data: 0.0001  max mem: 15824
[10:10:43.734254] Test:  [340/345]  eta: 0:00:00  loss: 0.1518 (0.1546)  time: 0.1901  data: 0.0001  max mem: 15824
[10:10:44.497405] Test:  [344/345]  eta: 0:00:00  loss: 0.1548 (0.1548)  time: 0.1902  data: 0.0001  max mem: 15824
[10:10:44.565991] Test: Total time: 0:01:04 (0.1862 s / it)
[10:10:55.184752] Test:  [ 0/57]  eta: 0:00:33  loss: 0.5064 (0.5064)  time: 0.5816  data: 0.4008  max mem: 15824
[10:10:56.958760] Test:  [10/57]  eta: 0:00:10  loss: 0.4485 (0.4684)  time: 0.2141  data: 0.0365  max mem: 15824
[10:10:58.737269] Test:  [20/57]  eta: 0:00:07  loss: 0.4356 (0.4478)  time: 0.1776  data: 0.0001  max mem: 15824
[10:11:00.519776] Test:  [30/57]  eta: 0:00:05  loss: 0.3321 (0.3969)  time: 0.1780  data: 0.0001  max mem: 15824
[10:11:02.309508] Test:  [40/57]  eta: 0:00:03  loss: 0.2950 (0.3793)  time: 0.1785  data: 0.0001  max mem: 15824
[10:11:04.095572] Test:  [50/57]  eta: 0:00:01  loss: 0.3483 (0.3831)  time: 0.1787  data: 0.0001  max mem: 15824
[10:11:05.069581] Test:  [56/57]  eta: 0:00:00  loss: 0.3853 (0.3949)  time: 0.1737  data: 0.0001  max mem: 15824
[10:11:05.143964] Test: Total time: 0:00:10 (0.1849 s / it)
[10:11:06.884671] Dice score of the network on the train images: 0.855859, val images: 0.753771
[10:11:06.889040] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:11:07.917570] Epoch: [32]  [  0/345]  eta: 0:05:54  lr: 0.000082  loss: 0.1485 (0.1485)  time: 1.0275  data: 0.4041  max mem: 15824
[10:11:20.211513] Epoch: [32]  [ 20/345]  eta: 0:03:26  lr: 0.000081  loss: 0.1592 (0.1621)  time: 0.6146  data: 0.0001  max mem: 15824
[10:11:32.551909] Epoch: [32]  [ 40/345]  eta: 0:03:10  lr: 0.000081  loss: 0.1623 (0.1613)  time: 0.6170  data: 0.0001  max mem: 15824
[10:11:45.037952] Epoch: [32]  [ 60/345]  eta: 0:02:58  lr: 0.000081  loss: 0.1538 (0.1591)  time: 0.6243  data: 0.0001  max mem: 15824
[10:11:57.436840] Epoch: [32]  [ 80/345]  eta: 0:02:45  lr: 0.000080  loss: 0.1551 (0.1601)  time: 0.6199  data: 0.0001  max mem: 15824
[10:12:09.865788] Epoch: [32]  [100/345]  eta: 0:02:32  lr: 0.000080  loss: 0.1582 (0.1605)  time: 0.6214  data: 0.0001  max mem: 15824
[10:12:22.301482] Epoch: [32]  [120/345]  eta: 0:02:20  lr: 0.000080  loss: 0.1566 (0.1605)  time: 0.6217  data: 0.0001  max mem: 15824
[10:12:34.748681] Epoch: [32]  [140/345]  eta: 0:02:07  lr: 0.000079  loss: 0.1540 (0.1597)  time: 0.6223  data: 0.0001  max mem: 15824
[10:12:47.182319] Epoch: [32]  [160/345]  eta: 0:01:55  lr: 0.000079  loss: 0.1567 (0.1600)  time: 0.6216  data: 0.0001  max mem: 15824
[10:12:59.610824] Epoch: [32]  [180/345]  eta: 0:01:42  lr: 0.000079  loss: 0.1712 (0.1605)  time: 0.6214  data: 0.0001  max mem: 15824
[10:13:12.029414] Epoch: [32]  [200/345]  eta: 0:01:30  lr: 0.000078  loss: 0.1629 (0.1610)  time: 0.6209  data: 0.0001  max mem: 15824
[10:13:24.448719] Epoch: [32]  [220/345]  eta: 0:01:17  lr: 0.000078  loss: 0.1552 (0.1606)  time: 0.6209  data: 0.0001  max mem: 15824
[10:13:36.863468] Epoch: [32]  [240/345]  eta: 0:01:05  lr: 0.000077  loss: 0.1532 (0.1599)  time: 0.6207  data: 0.0001  max mem: 15824
[10:13:49.259027] Epoch: [32]  [260/345]  eta: 0:00:52  lr: 0.000077  loss: 0.1573 (0.1599)  time: 0.6197  data: 0.0001  max mem: 15824
[10:14:01.652414] Epoch: [32]  [280/345]  eta: 0:00:40  lr: 0.000077  loss: 0.1581 (0.1597)  time: 0.6196  data: 0.0001  max mem: 15824
[10:14:14.049496] Epoch: [32]  [300/345]  eta: 0:00:27  lr: 0.000076  loss: 0.1507 (0.1597)  time: 0.6198  data: 0.0001  max mem: 15824
[10:14:26.443929] Epoch: [32]  [320/345]  eta: 0:00:15  lr: 0.000076  loss: 0.1561 (0.1599)  time: 0.6197  data: 0.0001  max mem: 15824
[10:14:38.836899] Epoch: [32]  [340/345]  eta: 0:00:03  lr: 0.000076  loss: 0.1634 (0.1601)  time: 0.6196  data: 0.0001  max mem: 15824
[10:14:41.318278] Epoch: [32]  [344/345]  eta: 0:00:00  lr: 0.000076  loss: 0.1621 (0.1601)  time: 0.6196  data: 0.0001  max mem: 15824
[10:14:41.383502] Epoch: [32] Total time: 0:03:34 (0.6217 s / it)
[10:14:41.383745] Averaged stats: lr: 0.000076  loss: 0.1621 (0.1601)
[10:14:41.988909] Test:  [  0/345]  eta: 0:03:27  loss: 0.1561 (0.1561)  time: 0.6005  data: 0.4184  max mem: 15824
[10:14:43.780760] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1506 (0.1515)  time: 0.2174  data: 0.0381  max mem: 15824
[10:14:45.573662] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1411 (0.1479)  time: 0.1792  data: 0.0001  max mem: 15824
[10:14:47.369474] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1408 (0.1457)  time: 0.1794  data: 0.0001  max mem: 15824
[10:14:49.168587] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1468 (0.1480)  time: 0.1797  data: 0.0001  max mem: 15824
[10:14:50.969608] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1490 (0.1484)  time: 0.1799  data: 0.0001  max mem: 15824
[10:14:52.774951] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1479 (0.1482)  time: 0.1803  data: 0.0001  max mem: 15824
[10:14:54.586501] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1495 (0.1501)  time: 0.1808  data: 0.0001  max mem: 15824
[10:14:56.398299] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1620 (0.1513)  time: 0.1811  data: 0.0001  max mem: 15824
[10:14:58.214085] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1565 (0.1520)  time: 0.1813  data: 0.0001  max mem: 15824
[10:15:00.032957] Test:  [100/345]  eta: 0:00:45  loss: 0.1538 (0.1526)  time: 0.1817  data: 0.0001  max mem: 15824
[10:15:01.857973] Test:  [110/345]  eta: 0:00:43  loss: 0.1538 (0.1537)  time: 0.1821  data: 0.0001  max mem: 15824
[10:15:03.685127] Test:  [120/345]  eta: 0:00:41  loss: 0.1495 (0.1537)  time: 0.1825  data: 0.0001  max mem: 15824
[10:15:05.513201] Test:  [130/345]  eta: 0:00:39  loss: 0.1526 (0.1541)  time: 0.1827  data: 0.0001  max mem: 15824
[10:15:07.347269] Test:  [140/345]  eta: 0:00:37  loss: 0.1553 (0.1541)  time: 0.1830  data: 0.0001  max mem: 15824
[10:15:09.184905] Test:  [150/345]  eta: 0:00:35  loss: 0.1565 (0.1545)  time: 0.1835  data: 0.0001  max mem: 15824
[10:15:11.026510] Test:  [160/345]  eta: 0:00:34  loss: 0.1587 (0.1548)  time: 0.1839  data: 0.0001  max mem: 15824
[10:15:12.870787] Test:  [170/345]  eta: 0:00:32  loss: 0.1488 (0.1546)  time: 0.1842  data: 0.0001  max mem: 15824
[10:15:14.720553] Test:  [180/345]  eta: 0:00:30  loss: 0.1462 (0.1547)  time: 0.1846  data: 0.0001  max mem: 15824
[10:15:16.573998] Test:  [190/345]  eta: 0:00:28  loss: 0.1475 (0.1545)  time: 0.1851  data: 0.0001  max mem: 15824
[10:15:18.432403] Test:  [200/345]  eta: 0:00:26  loss: 0.1482 (0.1543)  time: 0.1855  data: 0.0001  max mem: 15824
[10:15:20.292077] Test:  [210/345]  eta: 0:00:24  loss: 0.1477 (0.1541)  time: 0.1858  data: 0.0001  max mem: 15824
[10:15:22.155983] Test:  [220/345]  eta: 0:00:23  loss: 0.1549 (0.1542)  time: 0.1861  data: 0.0001  max mem: 15824
[10:15:24.022150] Test:  [230/345]  eta: 0:00:21  loss: 0.1549 (0.1542)  time: 0.1864  data: 0.0001  max mem: 15824
[10:15:25.890427] Test:  [240/345]  eta: 0:00:19  loss: 0.1512 (0.1543)  time: 0.1866  data: 0.0001  max mem: 15824
[10:15:27.763818] Test:  [250/345]  eta: 0:00:17  loss: 0.1492 (0.1541)  time: 0.1870  data: 0.0001  max mem: 15824
[10:15:29.641239] Test:  [260/345]  eta: 0:00:15  loss: 0.1492 (0.1541)  time: 0.1875  data: 0.0001  max mem: 15824
[10:15:31.521683] Test:  [270/345]  eta: 0:00:13  loss: 0.1493 (0.1539)  time: 0.1878  data: 0.0001  max mem: 15824
[10:15:33.405069] Test:  [280/345]  eta: 0:00:12  loss: 0.1493 (0.1541)  time: 0.1881  data: 0.0001  max mem: 15824
[10:15:35.294249] Test:  [290/345]  eta: 0:00:10  loss: 0.1492 (0.1541)  time: 0.1886  data: 0.0001  max mem: 15824
[10:15:37.186331] Test:  [300/345]  eta: 0:00:08  loss: 0.1492 (0.1540)  time: 0.1890  data: 0.0001  max mem: 15824
[10:15:39.081375] Test:  [310/345]  eta: 0:00:06  loss: 0.1397 (0.1536)  time: 0.1893  data: 0.0001  max mem: 15824
[10:15:40.977727] Test:  [320/345]  eta: 0:00:04  loss: 0.1416 (0.1537)  time: 0.1895  data: 0.0001  max mem: 15824
[10:15:42.878105] Test:  [330/345]  eta: 0:00:02  loss: 0.1511 (0.1538)  time: 0.1898  data: 0.0001  max mem: 15824
[10:15:44.778188] Test:  [340/345]  eta: 0:00:00  loss: 0.1500 (0.1537)  time: 0.1900  data: 0.0001  max mem: 15824
[10:15:45.539946] Test:  [344/345]  eta: 0:00:00  loss: 0.1510 (0.1538)  time: 0.1901  data: 0.0001  max mem: 15824
[10:15:45.608399] Test: Total time: 0:01:04 (0.1861 s / it)
[10:15:56.214410] Test:  [ 0/57]  eta: 0:00:34  loss: 0.5232 (0.5232)  time: 0.6140  data: 0.4336  max mem: 15824
[10:15:57.986171] Test:  [10/57]  eta: 0:00:10  loss: 0.4454 (0.4703)  time: 0.2168  data: 0.0395  max mem: 15824
[10:15:59.763061] Test:  [20/57]  eta: 0:00:07  loss: 0.4449 (0.4499)  time: 0.1774  data: 0.0001  max mem: 15824
[10:16:01.545138] Test:  [30/57]  eta: 0:00:05  loss: 0.3061 (0.3913)  time: 0.1779  data: 0.0001  max mem: 15824
[10:16:03.333576] Test:  [40/57]  eta: 0:00:03  loss: 0.2898 (0.3659)  time: 0.1785  data: 0.0001  max mem: 15824
[10:16:05.119789] Test:  [50/57]  eta: 0:00:01  loss: 0.3184 (0.3718)  time: 0.1787  data: 0.0001  max mem: 15824
[10:16:06.093815] Test:  [56/57]  eta: 0:00:00  loss: 0.3446 (0.3897)  time: 0.1738  data: 0.0000  max mem: 15824
[10:16:06.173130] Test: Total time: 0:00:10 (0.1855 s / it)
[10:16:07.911706] Dice score of the network on the train images: 0.849952, val images: 0.769127
[10:16:07.915689] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:16:08.946803] Epoch: [33]  [  0/345]  eta: 0:05:55  lr: 0.000075  loss: 0.2016 (0.2016)  time: 1.0303  data: 0.4041  max mem: 15824
[10:16:21.250844] Epoch: [33]  [ 20/345]  eta: 0:03:26  lr: 0.000075  loss: 0.1499 (0.1571)  time: 0.6152  data: 0.0001  max mem: 15824
[10:16:33.605899] Epoch: [33]  [ 40/345]  eta: 0:03:11  lr: 0.000075  loss: 0.1536 (0.1594)  time: 0.6177  data: 0.0001  max mem: 15824
[10:16:45.981678] Epoch: [33]  [ 60/345]  eta: 0:02:57  lr: 0.000074  loss: 0.1506 (0.1592)  time: 0.6187  data: 0.0001  max mem: 15824
[10:16:58.374354] Epoch: [33]  [ 80/345]  eta: 0:02:45  lr: 0.000074  loss: 0.1531 (0.1589)  time: 0.6196  data: 0.0001  max mem: 15824
[10:17:10.773221] Epoch: [33]  [100/345]  eta: 0:02:32  lr: 0.000074  loss: 0.1638 (0.1611)  time: 0.6199  data: 0.0001  max mem: 15824
[10:17:23.186522] Epoch: [33]  [120/345]  eta: 0:02:19  lr: 0.000073  loss: 0.1533 (0.1605)  time: 0.6206  data: 0.0001  max mem: 15824
[10:17:35.607146] Epoch: [33]  [140/345]  eta: 0:02:07  lr: 0.000073  loss: 0.1502 (0.1598)  time: 0.6210  data: 0.0001  max mem: 15824
[10:17:48.025263] Epoch: [33]  [160/345]  eta: 0:01:55  lr: 0.000073  loss: 0.1567 (0.1598)  time: 0.6209  data: 0.0001  max mem: 15824
[10:18:00.475738] Epoch: [33]  [180/345]  eta: 0:01:42  lr: 0.000072  loss: 0.1473 (0.1593)  time: 0.6225  data: 0.0001  max mem: 15824
[10:18:12.891197] Epoch: [33]  [200/345]  eta: 0:01:30  lr: 0.000072  loss: 0.1511 (0.1584)  time: 0.6207  data: 0.0001  max mem: 15824
[10:18:25.286290] Epoch: [33]  [220/345]  eta: 0:01:17  lr: 0.000071  loss: 0.1583 (0.1589)  time: 0.6197  data: 0.0001  max mem: 15824
[10:18:37.680731] Epoch: [33]  [240/345]  eta: 0:01:05  lr: 0.000071  loss: 0.1575 (0.1588)  time: 0.6197  data: 0.0001  max mem: 15824
[10:18:50.075852] Epoch: [33]  [260/345]  eta: 0:00:52  lr: 0.000071  loss: 0.1468 (0.1580)  time: 0.6197  data: 0.0001  max mem: 15824
[10:19:02.473347] Epoch: [33]  [280/345]  eta: 0:00:40  lr: 0.000070  loss: 0.1555 (0.1579)  time: 0.6198  data: 0.0001  max mem: 15824
[10:19:14.867987] Epoch: [33]  [300/345]  eta: 0:00:27  lr: 0.000070  loss: 0.1505 (0.1575)  time: 0.6197  data: 0.0001  max mem: 15824
[10:19:27.265164] Epoch: [33]  [320/345]  eta: 0:00:15  lr: 0.000070  loss: 0.1468 (0.1576)  time: 0.6198  data: 0.0001  max mem: 15824
[10:19:39.643524] Epoch: [33]  [340/345]  eta: 0:00:03  lr: 0.000069  loss: 0.1421 (0.1570)  time: 0.6189  data: 0.0001  max mem: 15824
[10:19:42.116430] Epoch: [33]  [344/345]  eta: 0:00:00  lr: 0.000069  loss: 0.1480 (0.1569)  time: 0.6186  data: 0.0001  max mem: 15824
[10:19:42.189240] Epoch: [33] Total time: 0:03:34 (0.6211 s / it)
[10:19:42.189865] Averaged stats: lr: 0.000069  loss: 0.1480 (0.1569)
[10:19:42.781927] Test:  [  0/345]  eta: 0:03:21  loss: 0.1367 (0.1367)  time: 0.5850  data: 0.4004  max mem: 15824
[10:19:44.571263] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1395 (0.1464)  time: 0.2158  data: 0.0365  max mem: 15824
[10:19:46.367751] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1474 (0.1479)  time: 0.1792  data: 0.0001  max mem: 15824
[10:19:48.168164] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1428 (0.1469)  time: 0.1798  data: 0.0001  max mem: 15824
[10:19:49.970156] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1355 (0.1490)  time: 0.1801  data: 0.0001  max mem: 15824
[10:19:51.770783] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1385 (0.1484)  time: 0.1801  data: 0.0001  max mem: 15824
[10:19:53.577083] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1385 (0.1477)  time: 0.1803  data: 0.0001  max mem: 15824
[10:19:55.388412] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1428 (0.1483)  time: 0.1808  data: 0.0001  max mem: 15824
[10:19:57.201220] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1435 (0.1479)  time: 0.1811  data: 0.0001  max mem: 15824
[10:19:59.020670] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1474 (0.1483)  time: 0.1815  data: 0.0001  max mem: 15824
[10:20:00.841241] Test:  [100/345]  eta: 0:00:45  loss: 0.1470 (0.1481)  time: 0.1819  data: 0.0001  max mem: 15824
[10:20:02.665127] Test:  [110/345]  eta: 0:00:43  loss: 0.1429 (0.1477)  time: 0.1822  data: 0.0001  max mem: 15824
[10:20:04.492992] Test:  [120/345]  eta: 0:00:41  loss: 0.1423 (0.1472)  time: 0.1825  data: 0.0001  max mem: 15824
[10:20:06.324381] Test:  [130/345]  eta: 0:00:39  loss: 0.1396 (0.1471)  time: 0.1829  data: 0.0001  max mem: 15824
[10:20:08.161107] Test:  [140/345]  eta: 0:00:37  loss: 0.1445 (0.1476)  time: 0.1833  data: 0.0001  max mem: 15824
[10:20:09.999772] Test:  [150/345]  eta: 0:00:35  loss: 0.1465 (0.1473)  time: 0.1837  data: 0.0001  max mem: 15824
[10:20:11.841418] Test:  [160/345]  eta: 0:00:34  loss: 0.1398 (0.1468)  time: 0.1840  data: 0.0001  max mem: 15824
[10:20:13.687278] Test:  [170/345]  eta: 0:00:32  loss: 0.1399 (0.1472)  time: 0.1843  data: 0.0001  max mem: 15824
[10:20:15.536051] Test:  [180/345]  eta: 0:00:30  loss: 0.1519 (0.1478)  time: 0.1847  data: 0.0001  max mem: 15824
[10:20:17.389978] Test:  [190/345]  eta: 0:00:28  loss: 0.1504 (0.1479)  time: 0.1851  data: 0.0001  max mem: 15824
[10:20:19.245579] Test:  [200/345]  eta: 0:00:26  loss: 0.1486 (0.1479)  time: 0.1854  data: 0.0001  max mem: 15824
[10:20:21.104876] Test:  [210/345]  eta: 0:00:24  loss: 0.1560 (0.1480)  time: 0.1857  data: 0.0001  max mem: 15824
[10:20:22.967003] Test:  [220/345]  eta: 0:00:23  loss: 0.1482 (0.1478)  time: 0.1860  data: 0.0001  max mem: 15824
[10:20:24.832932] Test:  [230/345]  eta: 0:00:21  loss: 0.1460 (0.1475)  time: 0.1863  data: 0.0001  max mem: 15824
[10:20:26.701933] Test:  [240/345]  eta: 0:00:19  loss: 0.1327 (0.1473)  time: 0.1867  data: 0.0001  max mem: 15824
[10:20:28.575721] Test:  [250/345]  eta: 0:00:17  loss: 0.1476 (0.1476)  time: 0.1871  data: 0.0001  max mem: 15824
[10:20:30.452749] Test:  [260/345]  eta: 0:00:15  loss: 0.1432 (0.1470)  time: 0.1875  data: 0.0001  max mem: 15824
[10:20:32.332724] Test:  [270/345]  eta: 0:00:13  loss: 0.1349 (0.1469)  time: 0.1878  data: 0.0001  max mem: 15824
[10:20:34.218925] Test:  [280/345]  eta: 0:00:12  loss: 0.1367 (0.1469)  time: 0.1882  data: 0.0001  max mem: 15824
[10:20:36.107427] Test:  [290/345]  eta: 0:00:10  loss: 0.1481 (0.1470)  time: 0.1887  data: 0.0001  max mem: 15824
[10:20:37.998124] Test:  [300/345]  eta: 0:00:08  loss: 0.1483 (0.1469)  time: 0.1889  data: 0.0001  max mem: 15824
[10:20:39.894930] Test:  [310/345]  eta: 0:00:06  loss: 0.1383 (0.1467)  time: 0.1893  data: 0.0001  max mem: 15824
[10:20:41.794444] Test:  [320/345]  eta: 0:00:04  loss: 0.1384 (0.1467)  time: 0.1898  data: 0.0001  max mem: 15824
[10:20:43.695754] Test:  [330/345]  eta: 0:00:02  loss: 0.1484 (0.1466)  time: 0.1900  data: 0.0001  max mem: 15824
[10:20:45.600951] Test:  [340/345]  eta: 0:00:00  loss: 0.1423 (0.1469)  time: 0.1903  data: 0.0001  max mem: 15824
[10:20:46.362148] Test:  [344/345]  eta: 0:00:00  loss: 0.1423 (0.1468)  time: 0.1903  data: 0.0001  max mem: 15824
[10:20:46.431689] Test: Total time: 0:01:04 (0.1862 s / it)
[10:20:56.939788] Test:  [ 0/57]  eta: 0:00:33  loss: 0.5065 (0.5065)  time: 0.5814  data: 0.4001  max mem: 15824
[10:20:58.714556] Test:  [10/57]  eta: 0:00:10  loss: 0.4402 (0.4666)  time: 0.2141  data: 0.0365  max mem: 15824
[10:21:00.494916] Test:  [20/57]  eta: 0:00:07  loss: 0.4288 (0.4459)  time: 0.1777  data: 0.0001  max mem: 15824
[10:21:02.277893] Test:  [30/57]  eta: 0:00:05  loss: 0.3301 (0.3913)  time: 0.1781  data: 0.0001  max mem: 15824
[10:21:04.063222] Test:  [40/57]  eta: 0:00:03  loss: 0.2683 (0.3691)  time: 0.1784  data: 0.0001  max mem: 15824
[10:21:05.853681] Test:  [50/57]  eta: 0:00:01  loss: 0.3323 (0.3757)  time: 0.1787  data: 0.0001  max mem: 15824
[10:21:06.826983] Test:  [56/57]  eta: 0:00:00  loss: 0.3616 (0.3912)  time: 0.1738  data: 0.0000  max mem: 15824
[10:21:06.892601] Test: Total time: 0:00:10 (0.1848 s / it)
[10:21:08.611001] Dice score of the network on the train images: 0.856385, val images: 0.763307
[10:21:08.615253] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:21:09.637139] Epoch: [34]  [  0/345]  eta: 0:05:52  lr: 0.000069  loss: 0.2055 (0.2055)  time: 1.0209  data: 0.3963  max mem: 15824
[10:21:21.985707] Epoch: [34]  [ 20/345]  eta: 0:03:26  lr: 0.000069  loss: 0.1530 (0.1600)  time: 0.6174  data: 0.0001  max mem: 15824
[10:21:34.377894] Epoch: [34]  [ 40/345]  eta: 0:03:11  lr: 0.000068  loss: 0.1614 (0.1619)  time: 0.6196  data: 0.0001  max mem: 15824
[10:21:46.788713] Epoch: [34]  [ 60/345]  eta: 0:02:58  lr: 0.000068  loss: 0.1433 (0.1584)  time: 0.6205  data: 0.0001  max mem: 15824
[10:21:59.200876] Epoch: [34]  [ 80/345]  eta: 0:02:45  lr: 0.000068  loss: 0.1482 (0.1566)  time: 0.6206  data: 0.0001  max mem: 15824
[10:22:11.624722] Epoch: [34]  [100/345]  eta: 0:02:32  lr: 0.000067  loss: 0.1501 (0.1557)  time: 0.6211  data: 0.0001  max mem: 15824
[10:22:24.073069] Epoch: [34]  [120/345]  eta: 0:02:20  lr: 0.000067  loss: 0.1522 (0.1553)  time: 0.6224  data: 0.0001  max mem: 15824
[10:22:36.531484] Epoch: [34]  [140/345]  eta: 0:02:07  lr: 0.000066  loss: 0.1456 (0.1549)  time: 0.6229  data: 0.0001  max mem: 15824
[10:22:48.982477] Epoch: [34]  [160/345]  eta: 0:01:55  lr: 0.000066  loss: 0.1455 (0.1549)  time: 0.6225  data: 0.0001  max mem: 15824
[10:23:01.445154] Epoch: [34]  [180/345]  eta: 0:01:42  lr: 0.000066  loss: 0.1574 (0.1549)  time: 0.6231  data: 0.0001  max mem: 15824
[10:23:13.908035] Epoch: [34]  [200/345]  eta: 0:01:30  lr: 0.000065  loss: 0.1426 (0.1541)  time: 0.6231  data: 0.0001  max mem: 15824
[10:23:26.349323] Epoch: [34]  [220/345]  eta: 0:01:17  lr: 0.000065  loss: 0.1566 (0.1546)  time: 0.6220  data: 0.0001  max mem: 15824
[10:23:38.786245] Epoch: [34]  [240/345]  eta: 0:01:05  lr: 0.000064  loss: 0.1512 (0.1544)  time: 0.6218  data: 0.0001  max mem: 15824
[10:23:51.206141] Epoch: [34]  [260/345]  eta: 0:00:52  lr: 0.000064  loss: 0.1439 (0.1537)  time: 0.6209  data: 0.0001  max mem: 15824
[10:24:03.635561] Epoch: [34]  [280/345]  eta: 0:00:40  lr: 0.000064  loss: 0.1452 (0.1536)  time: 0.6214  data: 0.0001  max mem: 15824
[10:24:16.060399] Epoch: [34]  [300/345]  eta: 0:00:28  lr: 0.000063  loss: 0.1371 (0.1529)  time: 0.6212  data: 0.0001  max mem: 15824
[10:24:28.491524] Epoch: [34]  [320/345]  eta: 0:00:15  lr: 0.000063  loss: 0.1472 (0.1526)  time: 0.6215  data: 0.0001  max mem: 15824
[10:24:40.908092] Epoch: [34]  [340/345]  eta: 0:00:03  lr: 0.000063  loss: 0.1493 (0.1522)  time: 0.6208  data: 0.0001  max mem: 15824
[10:24:43.387944] Epoch: [34]  [344/345]  eta: 0:00:00  lr: 0.000063  loss: 0.1440 (0.1521)  time: 0.6205  data: 0.0001  max mem: 15824
[10:24:43.461765] Epoch: [34] Total time: 0:03:34 (0.6227 s / it)
[10:24:43.462173] Averaged stats: lr: 0.000063  loss: 0.1440 (0.1521)
[10:24:44.072927] Test:  [  0/345]  eta: 0:03:29  loss: 0.1803 (0.1803)  time: 0.6059  data: 0.4233  max mem: 15824
[10:24:45.869248] Test:  [ 10/345]  eta: 0:01:13  loss: 0.1317 (0.1406)  time: 0.2183  data: 0.0386  max mem: 15824
[10:24:47.668782] Test:  [ 20/345]  eta: 0:01:05  loss: 0.1315 (0.1392)  time: 0.1797  data: 0.0001  max mem: 15824
[10:24:49.471772] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1339 (0.1402)  time: 0.1801  data: 0.0001  max mem: 15824
[10:24:51.273406] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1421 (0.1428)  time: 0.1802  data: 0.0001  max mem: 15824
[10:24:53.075535] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1405 (0.1418)  time: 0.1801  data: 0.0001  max mem: 15824
[10:24:54.883116] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1270 (0.1405)  time: 0.1804  data: 0.0001  max mem: 15824
[10:24:56.695635] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1313 (0.1403)  time: 0.1809  data: 0.0001  max mem: 15824
[10:24:58.508652] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1339 (0.1395)  time: 0.1812  data: 0.0001  max mem: 15824
[10:25:00.325496] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1355 (0.1409)  time: 0.1814  data: 0.0001  max mem: 15824
[10:25:02.147648] Test:  [100/345]  eta: 0:00:45  loss: 0.1464 (0.1420)  time: 0.1819  data: 0.0001  max mem: 15824
[10:25:03.972751] Test:  [110/345]  eta: 0:00:43  loss: 0.1464 (0.1421)  time: 0.1823  data: 0.0001  max mem: 15824
[10:25:05.798263] Test:  [120/345]  eta: 0:00:41  loss: 0.1415 (0.1431)  time: 0.1825  data: 0.0001  max mem: 15824
[10:25:07.631194] Test:  [130/345]  eta: 0:00:39  loss: 0.1369 (0.1427)  time: 0.1829  data: 0.0001  max mem: 15824
[10:25:09.467872] Test:  [140/345]  eta: 0:00:37  loss: 0.1261 (0.1419)  time: 0.1834  data: 0.0001  max mem: 15824
[10:25:11.306291] Test:  [150/345]  eta: 0:00:35  loss: 0.1337 (0.1424)  time: 0.1837  data: 0.0001  max mem: 15824
[10:25:13.147655] Test:  [160/345]  eta: 0:00:34  loss: 0.1409 (0.1424)  time: 0.1839  data: 0.0001  max mem: 15824
[10:25:14.995801] Test:  [170/345]  eta: 0:00:32  loss: 0.1325 (0.1422)  time: 0.1844  data: 0.0001  max mem: 15824
[10:25:16.845547] Test:  [180/345]  eta: 0:00:30  loss: 0.1340 (0.1425)  time: 0.1848  data: 0.0001  max mem: 15824
[10:25:18.698528] Test:  [190/345]  eta: 0:00:28  loss: 0.1367 (0.1423)  time: 0.1851  data: 0.0001  max mem: 15824
[10:25:20.555331] Test:  [200/345]  eta: 0:00:26  loss: 0.1349 (0.1424)  time: 0.1854  data: 0.0001  max mem: 15824
[10:25:22.419247] Test:  [210/345]  eta: 0:00:24  loss: 0.1404 (0.1426)  time: 0.1860  data: 0.0001  max mem: 15824
[10:25:24.283858] Test:  [220/345]  eta: 0:00:23  loss: 0.1404 (0.1427)  time: 0.1864  data: 0.0001  max mem: 15824
[10:25:26.152439] Test:  [230/345]  eta: 0:00:21  loss: 0.1419 (0.1426)  time: 0.1866  data: 0.0001  max mem: 15824
[10:25:28.022782] Test:  [240/345]  eta: 0:00:19  loss: 0.1446 (0.1430)  time: 0.1869  data: 0.0001  max mem: 15824
[10:25:29.897306] Test:  [250/345]  eta: 0:00:17  loss: 0.1449 (0.1429)  time: 0.1872  data: 0.0001  max mem: 15824
[10:25:31.773770] Test:  [260/345]  eta: 0:00:15  loss: 0.1414 (0.1434)  time: 0.1875  data: 0.0001  max mem: 15824
[10:25:33.653482] Test:  [270/345]  eta: 0:00:13  loss: 0.1488 (0.1436)  time: 0.1877  data: 0.0001  max mem: 15824
[10:25:35.538334] Test:  [280/345]  eta: 0:00:12  loss: 0.1356 (0.1432)  time: 0.1882  data: 0.0001  max mem: 15824
[10:25:37.425481] Test:  [290/345]  eta: 0:00:10  loss: 0.1350 (0.1431)  time: 0.1885  data: 0.0001  max mem: 15824
[10:25:39.318162] Test:  [300/345]  eta: 0:00:08  loss: 0.1372 (0.1428)  time: 0.1889  data: 0.0001  max mem: 15824
[10:25:41.214267] Test:  [310/345]  eta: 0:00:06  loss: 0.1363 (0.1427)  time: 0.1894  data: 0.0001  max mem: 15824
[10:25:43.112705] Test:  [320/345]  eta: 0:00:04  loss: 0.1363 (0.1426)  time: 0.1897  data: 0.0001  max mem: 15824
[10:25:45.014503] Test:  [330/345]  eta: 0:00:02  loss: 0.1361 (0.1428)  time: 0.1900  data: 0.0001  max mem: 15824
[10:25:46.918095] Test:  [340/345]  eta: 0:00:00  loss: 0.1454 (0.1431)  time: 0.1902  data: 0.0001  max mem: 15824
[10:25:47.680638] Test:  [344/345]  eta: 0:00:00  loss: 0.1440 (0.1431)  time: 0.1903  data: 0.0001  max mem: 15824
[10:25:47.746621] Test: Total time: 0:01:04 (0.1863 s / it)
[10:25:58.299448] Test:  [ 0/57]  eta: 0:00:33  loss: 0.4928 (0.4928)  time: 0.5817  data: 0.4002  max mem: 15824
[10:26:00.073857] Test:  [10/57]  eta: 0:00:10  loss: 0.4401 (0.4626)  time: 0.2141  data: 0.0365  max mem: 15824
[10:26:01.853450] Test:  [20/57]  eta: 0:00:07  loss: 0.4392 (0.4500)  time: 0.1776  data: 0.0001  max mem: 15824
[10:26:03.637146] Test:  [30/57]  eta: 0:00:05  loss: 0.2990 (0.3937)  time: 0.1781  data: 0.0001  max mem: 15824
[10:26:05.425057] Test:  [40/57]  eta: 0:00:03  loss: 0.2830 (0.3687)  time: 0.1785  data: 0.0001  max mem: 15824
[10:26:07.215954] Test:  [50/57]  eta: 0:00:01  loss: 0.3015 (0.3724)  time: 0.1789  data: 0.0001  max mem: 15824
[10:26:08.188340] Test:  [56/57]  eta: 0:00:00  loss: 0.3607 (0.3869)  time: 0.1738  data: 0.0000  max mem: 15824
[10:26:08.253921] Test: Total time: 0:00:10 (0.1849 s / it)
[10:26:09.960921] Dice score of the network on the train images: 0.857845, val images: 0.769146
[10:26:09.964923] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:26:10.994452] Epoch: [35]  [  0/345]  eta: 0:05:54  lr: 0.000063  loss: 0.1519 (0.1519)  time: 1.0286  data: 0.4045  max mem: 15824
[10:26:23.314911] Epoch: [35]  [ 20/345]  eta: 0:03:26  lr: 0.000062  loss: 0.1384 (0.1433)  time: 0.6160  data: 0.0001  max mem: 15824
[10:26:35.683234] Epoch: [35]  [ 40/345]  eta: 0:03:11  lr: 0.000062  loss: 0.1394 (0.1450)  time: 0.6184  data: 0.0001  max mem: 15824
[10:26:48.079889] Epoch: [35]  [ 60/345]  eta: 0:02:58  lr: 0.000061  loss: 0.1493 (0.1464)  time: 0.6198  data: 0.0001  max mem: 15824
[10:27:00.485699] Epoch: [35]  [ 80/345]  eta: 0:02:45  lr: 0.000061  loss: 0.1526 (0.1486)  time: 0.6202  data: 0.0001  max mem: 15824
[10:27:12.912918] Epoch: [35]  [100/345]  eta: 0:02:32  lr: 0.000061  loss: 0.1403 (0.1474)  time: 0.6213  data: 0.0001  max mem: 15824
[10:27:25.346497] Epoch: [35]  [120/345]  eta: 0:02:20  lr: 0.000060  loss: 0.1357 (0.1465)  time: 0.6216  data: 0.0001  max mem: 15824
[10:27:37.800190] Epoch: [35]  [140/345]  eta: 0:02:07  lr: 0.000060  loss: 0.1426 (0.1462)  time: 0.6226  data: 0.0001  max mem: 15824
[10:27:50.267312] Epoch: [35]  [160/345]  eta: 0:01:55  lr: 0.000059  loss: 0.1502 (0.1474)  time: 0.6233  data: 0.0001  max mem: 15824
[10:28:02.732343] Epoch: [35]  [180/345]  eta: 0:01:42  lr: 0.000059  loss: 0.1459 (0.1472)  time: 0.6232  data: 0.0001  max mem: 15824
[10:28:15.193389] Epoch: [35]  [200/345]  eta: 0:01:30  lr: 0.000059  loss: 0.1573 (0.1485)  time: 0.6230  data: 0.0001  max mem: 15824
[10:28:27.650863] Epoch: [35]  [220/345]  eta: 0:01:17  lr: 0.000058  loss: 0.1601 (0.1494)  time: 0.6228  data: 0.0001  max mem: 15824
[10:28:40.098154] Epoch: [35]  [240/345]  eta: 0:01:05  lr: 0.000058  loss: 0.1473 (0.1493)  time: 0.6223  data: 0.0001  max mem: 15824
[10:28:52.545861] Epoch: [35]  [260/345]  eta: 0:00:52  lr: 0.000058  loss: 0.1507 (0.1496)  time: 0.6223  data: 0.0001  max mem: 15824
[10:29:04.985402] Epoch: [35]  [280/345]  eta: 0:00:40  lr: 0.000057  loss: 0.1406 (0.1497)  time: 0.6219  data: 0.0001  max mem: 15824
[10:29:17.424955] Epoch: [35]  [300/345]  eta: 0:00:28  lr: 0.000057  loss: 0.1478 (0.1498)  time: 0.6219  data: 0.0001  max mem: 15824

[10:29:29.835056] Epoch: [35]  [320/345]  eta: 0:00:15  lr: 0.000056  loss: 0.1435 (0.1497)  time: 0.6205  data: 0.0001  max mem: 15824
[10:29:42.250863] Epoch: [35]  [340/345]  eta: 0:00:03  lr: 0.000056  loss: 0.1538 (0.1498)  time: 0.6207  data: 0.0001  max mem: 15824
[10:29:44.729683] Epoch: [35]  [344/345]  eta: 0:00:00  lr: 0.000056  loss: 0.1538 (0.1497)  time: 0.6204  data: 0.0001  max mem: 15824
[10:29:44.811458] Epoch: [35] Total time: 0:03:34 (0.6227 s / it)
[10:29:44.811816] Averaged stats: lr: 0.000056  loss: 0.1538 (0.1497)
[10:29:45.432972] Test:  [  0/345]  eta: 0:03:32  loss: 0.1460 (0.1460)  time: 0.6156  data: 0.4334  max mem: 15824
[10:29:47.230763] Test:  [ 10/345]  eta: 0:01:13  loss: 0.1429 (0.1388)  time: 0.2193  data: 0.0395  max mem: 15824
[10:29:49.030968] Test:  [ 20/345]  eta: 0:01:05  loss: 0.1425 (0.1440)  time: 0.1798  data: 0.0001  max mem: 15824
[10:29:50.834795] Test:  [ 30/345]  eta: 0:01:01  loss: 0.1373 (0.1414)  time: 0.1801  data: 0.0001  max mem: 15824
[10:29:52.636721] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1350 (0.1405)  time: 0.1802  data: 0.0001  max mem: 15824
[10:29:54.442351] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1350 (0.1404)  time: 0.1803  data: 0.0001  max mem: 15824
[10:29:56.253261] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1396 (0.1408)  time: 0.1808  data: 0.0001  max mem: 15824
[10:29:58.065087] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1396 (0.1406)  time: 0.1811  data: 0.0001  max mem: 15824
[10:29:59.881177] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1307 (0.1391)  time: 0.1813  data: 0.0001  max mem: 15824
[10:30:01.702281] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1343 (0.1400)  time: 0.1818  data: 0.0001  max mem: 15824
[10:30:03.526476] Test:  [100/345]  eta: 0:00:45  loss: 0.1449 (0.1398)  time: 0.1822  data: 0.0001  max mem: 15824
[10:30:05.354469] Test:  [110/345]  eta: 0:00:43  loss: 0.1339 (0.1395)  time: 0.1825  data: 0.0001  max mem: 15824
[10:30:07.183584] Test:  [120/345]  eta: 0:00:41  loss: 0.1359 (0.1396)  time: 0.1828  data: 0.0001  max mem: 15824
[10:30:09.016606] Test:  [130/345]  eta: 0:00:39  loss: 0.1385 (0.1394)  time: 0.1830  data: 0.0001  max mem: 15824
[10:30:10.854024] Test:  [140/345]  eta: 0:00:37  loss: 0.1429 (0.1401)  time: 0.1835  data: 0.0001  max mem: 15824
[10:30:12.696941] Test:  [150/345]  eta: 0:00:35  loss: 0.1429 (0.1399)  time: 0.1840  data: 0.0001  max mem: 15824
[10:30:14.541765] Test:  [160/345]  eta: 0:00:34  loss: 0.1326 (0.1396)  time: 0.1843  data: 0.0001  max mem: 15824
[10:30:16.391605] Test:  [170/345]  eta: 0:00:32  loss: 0.1432 (0.1401)  time: 0.1847  data: 0.0001  max mem: 15824
[10:30:18.243143] Test:  [180/345]  eta: 0:00:30  loss: 0.1446 (0.1402)  time: 0.1850  data: 0.0001  max mem: 15824
[10:30:20.099404] Test:  [190/345]  eta: 0:00:28  loss: 0.1366 (0.1400)  time: 0.1853  data: 0.0001  max mem: 15824
[10:30:21.959803] Test:  [200/345]  eta: 0:00:26  loss: 0.1403 (0.1404)  time: 0.1858  data: 0.0001  max mem: 15824
[10:30:23.824502] Test:  [210/345]  eta: 0:00:24  loss: 0.1475 (0.1407)  time: 0.1862  data: 0.0001  max mem: 15824
[10:30:25.689857] Test:  [220/345]  eta: 0:00:23  loss: 0.1475 (0.1408)  time: 0.1864  data: 0.0001  max mem: 15824
[10:30:27.560238] Test:  [230/345]  eta: 0:00:21  loss: 0.1339 (0.1403)  time: 0.1867  data: 0.0001  max mem: 15824
[10:30:29.434852] Test:  [240/345]  eta: 0:00:19  loss: 0.1306 (0.1401)  time: 0.1872  data: 0.0001  max mem: 15824
[10:30:31.312413] Test:  [250/345]  eta: 0:00:17  loss: 0.1298 (0.1396)  time: 0.1875  data: 0.0001  max mem: 15824
[10:30:33.190549] Test:  [260/345]  eta: 0:00:15  loss: 0.1310 (0.1397)  time: 0.1877  data: 0.0001  max mem: 15824
[10:30:35.074836] Test:  [270/345]  eta: 0:00:13  loss: 0.1407 (0.1398)  time: 0.1881  data: 0.0001  max mem: 15824
[10:30:36.962482] Test:  [280/345]  eta: 0:00:12  loss: 0.1396 (0.1397)  time: 0.1885  data: 0.0001  max mem: 15824
[10:30:38.852723] Test:  [290/345]  eta: 0:00:10  loss: 0.1422 (0.1399)  time: 0.1888  data: 0.0001  max mem: 15824
[10:30:40.747945] Test:  [300/345]  eta: 0:00:08  loss: 0.1526 (0.1405)  time: 0.1892  data: 0.0001  max mem: 15824
[10:30:42.647154] Test:  [310/345]  eta: 0:00:06  loss: 0.1487 (0.1407)  time: 0.1897  data: 0.0001  max mem: 15824
[10:30:44.549601] Test:  [320/345]  eta: 0:00:04  loss: 0.1367 (0.1405)  time: 0.1900  data: 0.0001  max mem: 15824
[10:30:46.454852] Test:  [330/345]  eta: 0:00:02  loss: 0.1291 (0.1403)  time: 0.1903  data: 0.0001  max mem: 15824
[10:30:48.359361] Test:  [340/345]  eta: 0:00:00  loss: 0.1336 (0.1403)  time: 0.1904  data: 0.0001  max mem: 15824
[10:30:49.121191] Test:  [344/345]  eta: 0:00:00  loss: 0.1318 (0.1403)  time: 0.1904  data: 0.0001  max mem: 15824
[10:30:49.190008] Test: Total time: 0:01:04 (0.1866 s / it)
[10:30:59.714380] Test:  [ 0/57]  eta: 0:00:33  loss: 0.5135 (0.5135)  time: 0.5824  data: 0.4021  max mem: 15824
[10:31:01.489883] Test:  [10/57]  eta: 0:00:10  loss: 0.4544 (0.4715)  time: 0.2143  data: 0.0366  max mem: 15824
[10:31:03.270390] Test:  [20/57]  eta: 0:00:07  loss: 0.4475 (0.4540)  time: 0.1777  data: 0.0001  max mem: 15824
[10:31:05.055584] Test:  [30/57]  eta: 0:00:05  loss: 0.3073 (0.3966)  time: 0.1782  data: 0.0001  max mem: 15824
[10:31:06.845912] Test:  [40/57]  eta: 0:00:03  loss: 0.2953 (0.3717)  time: 0.1787  data: 0.0001  max mem: 15824
[10:31:08.638217] Test:  [50/57]  eta: 0:00:01  loss: 0.3214 (0.3763)  time: 0.1791  data: 0.0001  max mem: 15824
[10:31:09.611499] Test:  [56/57]  eta: 0:00:00  loss: 0.3663 (0.3927)  time: 0.1741  data: 0.0001  max mem: 15824
[10:31:09.676827] Test: Total time: 0:00:10 (0.1850 s / it)
[10:31:11.420574] Dice score of the network on the train images: 0.859545, val images: 0.770945
[10:31:11.424890] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:31:12.434549] Epoch: [36]  [  0/345]  eta: 0:05:47  lr: 0.000056  loss: 0.1660 (0.1660)  time: 1.0087  data: 0.3818  max mem: 15824
[10:31:24.769014] Epoch: [36]  [ 20/345]  eta: 0:03:26  lr: 0.000056  loss: 0.1331 (0.1419)  time: 0.6167  data: 0.0001  max mem: 15824
[10:31:37.144083] Epoch: [36]  [ 40/345]  eta: 0:03:11  lr: 0.000055  loss: 0.1388 (0.1412)  time: 0.6187  data: 0.0001  max mem: 15824
[10:31:49.538128] Epoch: [36]  [ 60/345]  eta: 0:02:58  lr: 0.000055  loss: 0.1401 (0.1437)  time: 0.6197  data: 0.0001  max mem: 15824
[10:32:01.947762] Epoch: [36]  [ 80/345]  eta: 0:02:45  lr: 0.000054  loss: 0.1424 (0.1456)  time: 0.6204  data: 0.0001  max mem: 15824
[10:32:14.388662] Epoch: [36]  [100/345]  eta: 0:02:32  lr: 0.000054  loss: 0.1488 (0.1467)  time: 0.6220  data: 0.0001  max mem: 15824
[10:32:26.849379] Epoch: [36]  [120/345]  eta: 0:02:20  lr: 0.000054  loss: 0.1433 (0.1465)  time: 0.6230  data: 0.0001  max mem: 15824
[10:32:39.315955] Epoch: [36]  [140/345]  eta: 0:02:07  lr: 0.000053  loss: 0.1381 (0.1453)  time: 0.6233  data: 0.0001  max mem: 15824
[10:32:51.789092] Epoch: [36]  [160/345]  eta: 0:01:55  lr: 0.000053  loss: 0.1370 (0.1457)  time: 0.6236  data: 0.0001  max mem: 15824
[10:33:04.234101] Epoch: [36]  [180/345]  eta: 0:01:42  lr: 0.000053  loss: 0.1575 (0.1470)  time: 0.6222  data: 0.0001  max mem: 15824
[10:33:16.683887] Epoch: [36]  [200/345]  eta: 0:01:30  lr: 0.000052  loss: 0.1420 (0.1464)  time: 0.6224  data: 0.0001  max mem: 15824
[10:33:29.113417] Epoch: [36]  [220/345]  eta: 0:01:17  lr: 0.000052  loss: 0.1344 (0.1456)  time: 0.6214  data: 0.0001  max mem: 15824
[10:33:41.562045] Epoch: [36]  [240/345]  eta: 0:01:05  lr: 0.000051  loss: 0.1442 (0.1455)  time: 0.6224  data: 0.0001  max mem: 15824
[10:33:54.009758] Epoch: [36]  [260/345]  eta: 0:00:52  lr: 0.000051  loss: 0.1487 (0.1458)  time: 0.6223  data: 0.0001  max mem: 15824
[10:34:06.447490] Epoch: [36]  [280/345]  eta: 0:00:40  lr: 0.000051  loss: 0.1494 (0.1460)  time: 0.6218  data: 0.0001  max mem: 15824
[10:34:18.877497] Epoch: [36]  [300/345]  eta: 0:00:28  lr: 0.000050  loss: 0.1317 (0.1454)  time: 0.6215  data: 0.0001  max mem: 15824
[10:34:31.284873] Epoch: [36]  [320/345]  eta: 0:00:15  lr: 0.000050  loss: 0.1523 (0.1460)  time: 0.6203  data: 0.0001  max mem: 15824
[10:34:43.686299] Epoch: [36]  [340/345]  eta: 0:00:03  lr: 0.000050  loss: 0.1450 (0.1459)  time: 0.6200  data: 0.0001  max mem: 15824
[10:34:46.162662] Epoch: [36]  [344/345]  eta: 0:00:00  lr: 0.000050  loss: 0.1495 (0.1461)  time: 0.6197  data: 0.0001  max mem: 15824
[10:34:46.241119] Epoch: [36] Total time: 0:03:34 (0.6227 s / it)
[10:34:46.241479] Averaged stats: lr: 0.000050  loss: 0.1495 (0.1461)
[10:34:46.869295] Test:  [  0/345]  eta: 0:03:34  loss: 0.1234 (0.1234)  time: 0.6229  data: 0.4411  max mem: 15824
[10:34:48.662569] Test:  [ 10/345]  eta: 0:01:13  loss: 0.1468 (0.1482)  time: 0.2196  data: 0.0402  max mem: 15824
[10:34:50.457952] Test:  [ 20/345]  eta: 0:01:05  loss: 0.1468 (0.1433)  time: 0.1794  data: 0.0001  max mem: 15824
[10:34:52.257763] Test:  [ 30/345]  eta: 0:01:01  loss: 0.1310 (0.1399)  time: 0.1797  data: 0.0001  max mem: 15824
[10:34:54.059639] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1310 (0.1420)  time: 0.1800  data: 0.0001  max mem: 15824
[10:34:55.868087] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1409 (0.1418)  time: 0.1804  data: 0.0001  max mem: 15824
[10:34:57.679485] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1372 (0.1411)  time: 0.1809  data: 0.0001  max mem: 15824
[10:34:59.493570] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1337 (0.1409)  time: 0.1812  data: 0.0001  max mem: 15824
[10:35:01.309902] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1342 (0.1402)  time: 0.1815  data: 0.0001  max mem: 15824
[10:35:03.131614] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1342 (0.1401)  time: 0.1818  data: 0.0001  max mem: 15824
[10:35:04.956112] Test:  [100/345]  eta: 0:00:45  loss: 0.1429 (0.1408)  time: 0.1822  data: 0.0001  max mem: 15824
[10:35:06.781784] Test:  [110/345]  eta: 0:00:43  loss: 0.1428 (0.1406)  time: 0.1824  data: 0.0001  max mem: 15824
[10:35:08.613955] Test:  [120/345]  eta: 0:00:41  loss: 0.1378 (0.1407)  time: 0.1828  data: 0.0001  max mem: 15824
[10:35:10.446927] Test:  [130/345]  eta: 0:00:39  loss: 0.1378 (0.1405)  time: 0.1832  data: 0.0001  max mem: 15824
[10:35:12.285028] Test:  [140/345]  eta: 0:00:37  loss: 0.1346 (0.1407)  time: 0.1835  data: 0.0001  max mem: 15824
[10:35:14.126102] Test:  [150/345]  eta: 0:00:35  loss: 0.1444 (0.1410)  time: 0.1839  data: 0.0001  max mem: 15824
[10:35:15.972356] Test:  [160/345]  eta: 0:00:34  loss: 0.1411 (0.1409)  time: 0.1843  data: 0.0001  max mem: 15824
[10:35:17.822630] Test:  [170/345]  eta: 0:00:32  loss: 0.1403 (0.1410)  time: 0.1848  data: 0.0001  max mem: 15824
[10:35:19.676657] Test:  [180/345]  eta: 0:00:30  loss: 0.1302 (0.1403)  time: 0.1852  data: 0.0001  max mem: 15824
[10:35:21.532502] Test:  [190/345]  eta: 0:00:28  loss: 0.1255 (0.1398)  time: 0.1854  data: 0.0001  max mem: 15824
[10:35:23.390844] Test:  [200/345]  eta: 0:00:26  loss: 0.1271 (0.1393)  time: 0.1856  data: 0.0001  max mem: 15824
[10:35:25.253673] Test:  [210/345]  eta: 0:00:24  loss: 0.1311 (0.1393)  time: 0.1860  data: 0.0001  max mem: 15824
[10:35:27.118131] Test:  [220/345]  eta: 0:00:23  loss: 0.1343 (0.1393)  time: 0.1863  data: 0.0001  max mem: 15824
[10:35:28.988926] Test:  [230/345]  eta: 0:00:21  loss: 0.1279 (0.1389)  time: 0.1867  data: 0.0001  max mem: 15824
[10:35:30.862811] Test:  [240/345]  eta: 0:00:19  loss: 0.1259 (0.1391)  time: 0.1872  data: 0.0001  max mem: 15824
[10:35:32.738145] Test:  [250/345]  eta: 0:00:17  loss: 0.1289 (0.1388)  time: 0.1874  data: 0.0001  max mem: 15824
[10:35:34.617971] Test:  [260/345]  eta: 0:00:15  loss: 0.1252 (0.1385)  time: 0.1877  data: 0.0001  max mem: 15824
[10:35:36.500308] Test:  [270/345]  eta: 0:00:13  loss: 0.1228 (0.1385)  time: 0.1880  data: 0.0001  max mem: 15824
[10:35:38.389338] Test:  [280/345]  eta: 0:00:12  loss: 0.1324 (0.1384)  time: 0.1885  data: 0.0001  max mem: 15824
[10:35:40.279592] Test:  [290/345]  eta: 0:00:10  loss: 0.1287 (0.1382)  time: 0.1889  data: 0.0001  max mem: 15824
[10:35:42.176029] Test:  [300/345]  eta: 0:00:08  loss: 0.1271 (0.1381)  time: 0.1893  data: 0.0001  max mem: 15824
[10:35:44.076168] Test:  [310/345]  eta: 0:00:06  loss: 0.1315 (0.1382)  time: 0.1898  data: 0.0001  max mem: 15824
[10:35:45.979350] Test:  [320/345]  eta: 0:00:04  loss: 0.1333 (0.1381)  time: 0.1901  data: 0.0001  max mem: 15824
[10:35:47.880512] Test:  [330/345]  eta: 0:00:02  loss: 0.1357 (0.1382)  time: 0.1902  data: 0.0001  max mem: 15824
[10:35:49.786117] Test:  [340/345]  eta: 0:00:00  loss: 0.1383 (0.1381)  time: 0.1903  data: 0.0001  max mem: 15824
[10:35:50.549420] Test:  [344/345]  eta: 0:00:00  loss: 0.1383 (0.1380)  time: 0.1904  data: 0.0001  max mem: 15824
[10:35:50.618287] Test: Total time: 0:01:04 (0.1866 s / it)
[10:36:01.228612] Test:  [ 0/57]  eta: 0:00:32  loss: 0.5469 (0.5469)  time: 0.5656  data: 0.3839  max mem: 15824
[10:36:03.006150] Test:  [10/57]  eta: 0:00:10  loss: 0.4592 (0.4909)  time: 0.2129  data: 0.0350  max mem: 15824
[10:36:04.786446] Test:  [20/57]  eta: 0:00:07  loss: 0.4592 (0.4798)  time: 0.1778  data: 0.0001  max mem: 15824
[10:36:06.570610] Test:  [30/57]  eta: 0:00:05  loss: 0.3061 (0.4153)  time: 0.1782  data: 0.0001  max mem: 15824
[10:36:08.364796] Test:  [40/57]  eta: 0:00:03  loss: 0.2911 (0.3881)  time: 0.1789  data: 0.0001  max mem: 15824
[10:36:10.157525] Test:  [50/57]  eta: 0:00:01  loss: 0.3240 (0.3932)  time: 0.1793  data: 0.0001  max mem: 15824
[10:36:11.130890] Test:  [56/57]  eta: 0:00:00  loss: 0.3739 (0.4105)  time: 0.1742  data: 0.0000  max mem: 15824
[10:36:11.215385] Test: Total time: 0:00:10 (0.1851 s / it)
[10:36:12.951163] Dice score of the network on the train images: 0.866411, val images: 0.764214
[10:36:12.955346] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:36:13.972001] Epoch: [37]  [  0/345]  eta: 0:05:50  lr: 0.000050  loss: 0.1536 (0.1536)  time: 1.0155  data: 0.3873  max mem: 15824
[10:36:26.334274] Epoch: [37]  [ 20/345]  eta: 0:03:27  lr: 0.000049  loss: 0.1400 (0.1452)  time: 0.6181  data: 0.0001  max mem: 15824
[10:36:38.730746] Epoch: [37]  [ 40/345]  eta: 0:03:11  lr: 0.000049  loss: 0.1366 (0.1438)  time: 0.6198  data: 0.0001  max mem: 15824
[10:36:51.128506] Epoch: [37]  [ 60/345]  eta: 0:02:58  lr: 0.000048  loss: 0.1346 (0.1431)  time: 0.6198  data: 0.0001  max mem: 15824
[10:37:03.535237] Epoch: [37]  [ 80/345]  eta: 0:02:45  lr: 0.000048  loss: 0.1327 (0.1421)  time: 0.6203  data: 0.0001  max mem: 15824
[10:37:15.948510] Epoch: [37]  [100/345]  eta: 0:02:32  lr: 0.000048  loss: 0.1334 (0.1415)  time: 0.6206  data: 0.0001  max mem: 15824
[10:37:28.397490] Epoch: [37]  [120/345]  eta: 0:02:20  lr: 0.000047  loss: 0.1263 (0.1397)  time: 0.6224  data: 0.0001  max mem: 15824
[10:37:40.858375] Epoch: [37]  [140/345]  eta: 0:02:07  lr: 0.000047  loss: 0.1396 (0.1404)  time: 0.6230  data: 0.0001  max mem: 15824
[10:37:53.305479] Epoch: [37]  [160/345]  eta: 0:01:55  lr: 0.000047  loss: 0.1351 (0.1407)  time: 0.6223  data: 0.0001  max mem: 15824
[10:38:05.771234] Epoch: [37]  [180/345]  eta: 0:01:42  lr: 0.000046  loss: 0.1331 (0.1405)  time: 0.6232  data: 0.0001  max mem: 15824
[10:38:18.227046] Epoch: [37]  [200/345]  eta: 0:01:30  lr: 0.000046  loss: 0.1272 (0.1400)  time: 0.6227  data: 0.0001  max mem: 15824
[10:38:30.685929] Epoch: [37]  [220/345]  eta: 0:01:17  lr: 0.000045  loss: 0.1567 (0.1413)  time: 0.6229  data: 0.0001  max mem: 15824
[10:38:43.135343] Epoch: [37]  [240/345]  eta: 0:01:05  lr: 0.000045  loss: 0.1443 (0.1418)  time: 0.6224  data: 0.0001  max mem: 15824
[10:38:55.593401] Epoch: [37]  [260/345]  eta: 0:00:52  lr: 0.000045  loss: 0.1450 (0.1422)  time: 0.6229  data: 0.0001  max mem: 15824
[10:39:08.035452] Epoch: [37]  [280/345]  eta: 0:00:40  lr: 0.000044  loss: 0.1443 (0.1425)  time: 0.6221  data: 0.0001  max mem: 15824
[10:39:20.483902] Epoch: [37]  [300/345]  eta: 0:00:28  lr: 0.000044  loss: 0.1402 (0.1426)  time: 0.6224  data: 0.0001  max mem: 15824
[10:39:33.007842] Epoch: [37]  [320/345]  eta: 0:00:15  lr: 0.000044  loss: 0.1338 (0.1423)  time: 0.6261  data: 0.0001  max mem: 15824
[10:39:45.424167] Epoch: [37]  [340/345]  eta: 0:00:03  lr: 0.000043  loss: 0.1360 (0.1420)  time: 0.6208  data: 0.0001  max mem: 15824
[10:39:47.911020] Epoch: [37]  [344/345]  eta: 0:00:00  lr: 0.000043  loss: 0.1340 (0.1419)  time: 0.6208  data: 0.0001  max mem: 15824
[10:39:47.984641] Epoch: [37] Total time: 0:03:35 (0.6233 s / it)
[10:39:47.984855] Averaged stats: lr: 0.000043  loss: 0.1340 (0.1419)
[10:39:48.593095] Test:  [  0/345]  eta: 0:03:27  loss: 0.1198 (0.1198)  time: 0.6028  data: 0.4205  max mem: 15824
[10:39:50.389747] Test:  [ 10/345]  eta: 0:01:13  loss: 0.1279 (0.1269)  time: 0.2181  data: 0.0383  max mem: 15824
[10:39:52.186793] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1309 (0.1351)  time: 0.1796  data: 0.0001  max mem: 15824
[10:39:53.987224] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1363 (0.1370)  time: 0.1798  data: 0.0001  max mem: 15824
[10:39:55.789310] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1312 (0.1335)  time: 0.1801  data: 0.0001  max mem: 15824
[10:39:57.598377] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1310 (0.1363)  time: 0.1805  data: 0.0001  max mem: 15824
[10:39:59.406458] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1385 (0.1364)  time: 0.1808  data: 0.0001  max mem: 15824
[10:40:01.220281] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1337 (0.1365)  time: 0.1810  data: 0.0001  max mem: 15824
[10:40:03.038130] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1325 (0.1361)  time: 0.1815  data: 0.0001  max mem: 15824
[10:40:04.859872] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1280 (0.1357)  time: 0.1819  data: 0.0001  max mem: 15824
[10:40:06.683703] Test:  [100/345]  eta: 0:00:45  loss: 0.1285 (0.1357)  time: 0.1822  data: 0.0001  max mem: 15824
[10:40:08.510416] Test:  [110/345]  eta: 0:00:43  loss: 0.1291 (0.1355)  time: 0.1825  data: 0.0001  max mem: 15824
[10:40:10.341739] Test:  [120/345]  eta: 0:00:41  loss: 0.1388 (0.1368)  time: 0.1828  data: 0.0001  max mem: 15824
[10:40:12.176916] Test:  [130/345]  eta: 0:00:39  loss: 0.1390 (0.1372)  time: 0.1833  data: 0.0001  max mem: 15824
[10:40:14.015353] Test:  [140/345]  eta: 0:00:37  loss: 0.1343 (0.1370)  time: 0.1836  data: 0.0001  max mem: 15824
[10:40:15.856994] Test:  [150/345]  eta: 0:00:35  loss: 0.1351 (0.1369)  time: 0.1839  data: 0.0001  max mem: 15824
[10:40:17.701713] Test:  [160/345]  eta: 0:00:34  loss: 0.1293 (0.1364)  time: 0.1843  data: 0.0001  max mem: 15824
[10:40:19.548540] Test:  [170/345]  eta: 0:00:32  loss: 0.1219 (0.1361)  time: 0.1845  data: 0.0001  max mem: 15824
[10:40:21.400933] Test:  [180/345]  eta: 0:00:30  loss: 0.1336 (0.1367)  time: 0.1849  data: 0.0001  max mem: 15824
[10:40:23.257420] Test:  [190/345]  eta: 0:00:28  loss: 0.1360 (0.1368)  time: 0.1854  data: 0.0001  max mem: 15824
[10:40:25.116951] Test:  [200/345]  eta: 0:00:26  loss: 0.1325 (0.1364)  time: 0.1857  data: 0.0001  max mem: 15824
[10:40:26.978168] Test:  [210/345]  eta: 0:00:24  loss: 0.1283 (0.1363)  time: 0.1860  data: 0.0001  max mem: 15824
[10:40:28.843164] Test:  [220/345]  eta: 0:00:23  loss: 0.1283 (0.1361)  time: 0.1863  data: 0.0001  max mem: 15824
[10:40:30.711924] Test:  [230/345]  eta: 0:00:21  loss: 0.1314 (0.1358)  time: 0.1866  data: 0.0001  max mem: 15824
[10:40:32.584415] Test:  [240/345]  eta: 0:00:19  loss: 0.1270 (0.1356)  time: 0.1870  data: 0.0001  max mem: 15824
[10:40:34.462317] Test:  [250/345]  eta: 0:00:17  loss: 0.1277 (0.1357)  time: 0.1875  data: 0.0001  max mem: 15824
[10:40:36.342104] Test:  [260/345]  eta: 0:00:15  loss: 0.1295 (0.1355)  time: 0.1878  data: 0.0001  max mem: 15824
[10:40:38.226143] Test:  [270/345]  eta: 0:00:13  loss: 0.1324 (0.1356)  time: 0.1881  data: 0.0001  max mem: 15824
[10:40:40.116881] Test:  [280/345]  eta: 0:00:12  loss: 0.1324 (0.1353)  time: 0.1887  data: 0.0001  max mem: 15824
[10:40:42.006696] Test:  [290/345]  eta: 0:00:10  loss: 0.1296 (0.1352)  time: 0.1890  data: 0.0001  max mem: 15824
[10:40:43.900193] Test:  [300/345]  eta: 0:00:08  loss: 0.1296 (0.1352)  time: 0.1891  data: 0.0001  max mem: 15824
[10:40:45.800884] Test:  [310/345]  eta: 0:00:06  loss: 0.1266 (0.1349)  time: 0.1896  data: 0.0001  max mem: 15824
[10:40:47.701068] Test:  [320/345]  eta: 0:00:04  loss: 0.1242 (0.1347)  time: 0.1900  data: 0.0001  max mem: 15824
[10:40:49.601963] Test:  [330/345]  eta: 0:00:02  loss: 0.1250 (0.1345)  time: 0.1900  data: 0.0001  max mem: 15824
[10:40:51.504269] Test:  [340/345]  eta: 0:00:00  loss: 0.1284 (0.1345)  time: 0.1901  data: 0.0001  max mem: 15824
[10:40:52.268931] Test:  [344/345]  eta: 0:00:00  loss: 0.1284 (0.1344)  time: 0.1903  data: 0.0001  max mem: 15824
[10:40:52.337149] Test: Total time: 0:01:04 (0.1865 s / it)
[10:41:02.933398] Test:  [ 0/57]  eta: 0:00:35  loss: 0.5051 (0.5051)  time: 0.6294  data: 0.4461  max mem: 15824
[10:41:04.709933] Test:  [10/57]  eta: 0:00:10  loss: 0.4668 (0.4733)  time: 0.2186  data: 0.0406  max mem: 15824
[10:41:06.491825] Test:  [20/57]  eta: 0:00:07  loss: 0.4668 (0.4624)  time: 0.1778  data: 0.0001  max mem: 15824
[10:41:08.277348] Test:  [30/57]  eta: 0:00:05  loss: 0.3121 (0.4024)  time: 0.1783  data: 0.0001  max mem: 15824
[10:41:10.066127] Test:  [40/57]  eta: 0:00:03  loss: 0.2753 (0.3772)  time: 0.1787  data: 0.0001  max mem: 15824
[10:41:11.855699] Test:  [50/57]  eta: 0:00:01  loss: 0.3242 (0.3829)  time: 0.1789  data: 0.0001  max mem: 15824
[10:41:12.829869] Test:  [56/57]  eta: 0:00:00  loss: 0.3599 (0.4018)  time: 0.1739  data: 0.0001  max mem: 15824
[10:41:12.901417] Test: Total time: 0:00:10 (0.1859 s / it)
[10:41:14.614215] Dice score of the network on the train images: 0.869431, val images: 0.766918
[10:41:14.618984] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:41:15.641033] Epoch: [38]  [  0/345]  eta: 0:05:52  lr: 0.000043  loss: 0.1395 (0.1395)  time: 1.0209  data: 0.3948  max mem: 15824
[10:41:28.015475] Epoch: [38]  [ 20/345]  eta: 0:03:27  lr: 0.000043  loss: 0.1332 (0.1426)  time: 0.6187  data: 0.0001  max mem: 15824
[10:41:40.390820] Epoch: [38]  [ 40/345]  eta: 0:03:11  lr: 0.000042  loss: 0.1262 (0.1361)  time: 0.6187  data: 0.0001  max mem: 15824
[10:41:52.791188] Epoch: [38]  [ 60/345]  eta: 0:02:58  lr: 0.000042  loss: 0.1315 (0.1376)  time: 0.6200  data: 0.0001  max mem: 15824
[10:42:05.196279] Epoch: [38]  [ 80/345]  eta: 0:02:45  lr: 0.000042  loss: 0.1451 (0.1391)  time: 0.6202  data: 0.0001  max mem: 15824
[10:42:17.640913] Epoch: [38]  [100/345]  eta: 0:02:32  lr: 0.000041  loss: 0.1358 (0.1388)  time: 0.6222  data: 0.0001  max mem: 15824
[10:42:30.104491] Epoch: [38]  [120/345]  eta: 0:02:20  lr: 0.000041  loss: 0.1254 (0.1382)  time: 0.6231  data: 0.0001  max mem: 15824
[10:42:42.570155] Epoch: [38]  [140/345]  eta: 0:02:07  lr: 0.000041  loss: 0.1414 (0.1384)  time: 0.6232  data: 0.0001  max mem: 15824
[10:42:55.040418] Epoch: [38]  [160/345]  eta: 0:01:55  lr: 0.000040  loss: 0.1351 (0.1386)  time: 0.6235  data: 0.0001  max mem: 15824
[10:43:07.515963] Epoch: [38]  [180/345]  eta: 0:01:42  lr: 0.000040  loss: 0.1309 (0.1379)  time: 0.6237  data: 0.0001  max mem: 15824
[10:43:19.992296] Epoch: [38]  [200/345]  eta: 0:01:30  lr: 0.000040  loss: 0.1361 (0.1383)  time: 0.6238  data: 0.0001  max mem: 15824
[10:43:32.469359] Epoch: [38]  [220/345]  eta: 0:01:17  lr: 0.000039  loss: 0.1368 (0.1385)  time: 0.6238  data: 0.0001  max mem: 15824
[10:43:44.935229] Epoch: [38]  [240/345]  eta: 0:01:05  lr: 0.000039  loss: 0.1355 (0.1386)  time: 0.6232  data: 0.0001  max mem: 15824
[10:43:57.399916] Epoch: [38]  [260/345]  eta: 0:00:53  lr: 0.000039  loss: 0.1383 (0.1387)  time: 0.6232  data: 0.0001  max mem: 15824
[10:44:09.856633] Epoch: [38]  [280/345]  eta: 0:00:40  lr: 0.000038  loss: 0.1389 (0.1389)  time: 0.6228  data: 0.0001  max mem: 15824
[10:44:22.320176] Epoch: [38]  [300/345]  eta: 0:00:28  lr: 0.000038  loss: 0.1316 (0.1389)  time: 0.6231  data: 0.0001  max mem: 15824
[10:44:34.758472] Epoch: [38]  [320/345]  eta: 0:00:15  lr: 0.000038  loss: 0.1317 (0.1386)  time: 0.6219  data: 0.0001  max mem: 15824
[10:44:47.181406] Epoch: [38]  [340/345]  eta: 0:00:03  lr: 0.000037  loss: 0.1356 (0.1387)  time: 0.6211  data: 0.0001  max mem: 15824
[10:44:49.665111] Epoch: [38]  [344/345]  eta: 0:00:00  lr: 0.000037  loss: 0.1356 (0.1388)  time: 0.6209  data: 0.0001  max mem: 15824
[10:44:49.736496] Epoch: [38] Total time: 0:03:35 (0.6235 s / it)
[10:44:49.736981] Averaged stats: lr: 0.000037  loss: 0.1356 (0.1388)
[10:44:50.320313] Test:  [  0/345]  eta: 0:03:19  loss: 0.1369 (0.1369)  time: 0.5780  data: 0.3969  max mem: 15824
[10:44:52.117590] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1428 (0.1419)  time: 0.2158  data: 0.0362  max mem: 15824
[10:44:53.919451] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1424 (0.1441)  time: 0.1799  data: 0.0001  max mem: 15824
[10:44:55.719507] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1316 (0.1368)  time: 0.1800  data: 0.0001  max mem: 15824
[10:44:57.526102] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1276 (0.1363)  time: 0.1803  data: 0.0001  max mem: 15824
[10:44:59.332251] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1344 (0.1362)  time: 0.1806  data: 0.0001  max mem: 15824
[10:45:01.146879] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1332 (0.1352)  time: 0.1810  data: 0.0001  max mem: 15824
[10:45:02.961587] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1292 (0.1358)  time: 0.1814  data: 0.0001  max mem: 15824
[10:45:04.779029] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1368 (0.1351)  time: 0.1815  data: 0.0001  max mem: 15824
[10:45:06.602531] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1383 (0.1357)  time: 0.1820  data: 0.0001  max mem: 15824
[10:45:08.429604] Test:  [100/345]  eta: 0:00:45  loss: 0.1410 (0.1353)  time: 0.1825  data: 0.0001  max mem: 15824
[10:45:10.259333] Test:  [110/345]  eta: 0:00:43  loss: 0.1260 (0.1347)  time: 0.1828  data: 0.0001  max mem: 15824
[10:45:12.092689] Test:  [120/345]  eta: 0:00:41  loss: 0.1263 (0.1342)  time: 0.1831  data: 0.0001  max mem: 15824
[10:45:13.928613] Test:  [130/345]  eta: 0:00:39  loss: 0.1239 (0.1332)  time: 0.1834  data: 0.0001  max mem: 15824
[10:45:15.768972] Test:  [140/345]  eta: 0:00:37  loss: 0.1217 (0.1327)  time: 0.1837  data: 0.0001  max mem: 15824
[10:45:17.615474] Test:  [150/345]  eta: 0:00:35  loss: 0.1332 (0.1334)  time: 0.1843  data: 0.0001  max mem: 15824
[10:45:19.465461] Test:  [160/345]  eta: 0:00:34  loss: 0.1273 (0.1330)  time: 0.1848  data: 0.0001  max mem: 15824
[10:45:21.315922] Test:  [170/345]  eta: 0:00:32  loss: 0.1268 (0.1331)  time: 0.1850  data: 0.0001  max mem: 15824
[10:45:23.173094] Test:  [180/345]  eta: 0:00:30  loss: 0.1404 (0.1334)  time: 0.1853  data: 0.0001  max mem: 15824
[10:45:25.030812] Test:  [190/345]  eta: 0:00:28  loss: 0.1413 (0.1336)  time: 0.1857  data: 0.0001  max mem: 15824
[10:45:26.894102] Test:  [200/345]  eta: 0:00:26  loss: 0.1325 (0.1335)  time: 0.1860  data: 0.0001  max mem: 15824
[10:45:28.758156] Test:  [210/345]  eta: 0:00:24  loss: 0.1345 (0.1340)  time: 0.1863  data: 0.0001  max mem: 15824
[10:45:30.626117] Test:  [220/345]  eta: 0:00:23  loss: 0.1289 (0.1334)  time: 0.1865  data: 0.0001  max mem: 15824
[10:45:32.499694] Test:  [230/345]  eta: 0:00:21  loss: 0.1257 (0.1331)  time: 0.1870  data: 0.0001  max mem: 15824
[10:45:34.377639] Test:  [240/345]  eta: 0:00:19  loss: 0.1253 (0.1328)  time: 0.1875  data: 0.0001  max mem: 15824
[10:45:36.254823] Test:  [250/345]  eta: 0:00:17  loss: 0.1192 (0.1323)  time: 0.1877  data: 0.0001  max mem: 15824
[10:45:38.136272] Test:  [260/345]  eta: 0:00:15  loss: 0.1249 (0.1324)  time: 0.1879  data: 0.0001  max mem: 15824
[10:45:40.020832] Test:  [270/345]  eta: 0:00:13  loss: 0.1363 (0.1323)  time: 0.1882  data: 0.0001  max mem: 15824
[10:45:41.907558] Test:  [280/345]  eta: 0:00:12  loss: 0.1262 (0.1321)  time: 0.1885  data: 0.0001  max mem: 15824
[10:45:43.798883] Test:  [290/345]  eta: 0:00:10  loss: 0.1245 (0.1320)  time: 0.1888  data: 0.0001  max mem: 15824
[10:45:45.699207] Test:  [300/345]  eta: 0:00:08  loss: 0.1245 (0.1318)  time: 0.1895  data: 0.0001  max mem: 15824
[10:45:47.599509] Test:  [310/345]  eta: 0:00:06  loss: 0.1284 (0.1319)  time: 0.1900  data: 0.0001  max mem: 15824
[10:45:49.502659] Test:  [320/345]  eta: 0:00:04  loss: 0.1297 (0.1319)  time: 0.1901  data: 0.0001  max mem: 15824
[10:45:51.405190] Test:  [330/345]  eta: 0:00:02  loss: 0.1297 (0.1317)  time: 0.1902  data: 0.0001  max mem: 15824
[10:45:53.311123] Test:  [340/345]  eta: 0:00:00  loss: 0.1231 (0.1315)  time: 0.1904  data: 0.0001  max mem: 15824
[10:45:54.075340] Test:  [344/345]  eta: 0:00:00  loss: 0.1201 (0.1314)  time: 0.1905  data: 0.0001  max mem: 15824
[10:45:54.149251] Test: Total time: 0:01:04 (0.1867 s / it)
[10:46:04.747567] Test:  [ 0/57]  eta: 0:00:33  loss: 0.5051 (0.5051)  time: 0.5872  data: 0.4071  max mem: 15824
[10:46:06.524379] Test:  [10/57]  eta: 0:00:10  loss: 0.4558 (0.4693)  time: 0.2148  data: 0.0371  max mem: 15824
[10:46:08.306790] Test:  [20/57]  eta: 0:00:07  loss: 0.4524 (0.4569)  time: 0.1779  data: 0.0001  max mem: 15824
[10:46:10.091403] Test:  [30/57]  eta: 0:00:05  loss: 0.3101 (0.3983)  time: 0.1783  data: 0.0001  max mem: 15824
[10:46:11.881376] Test:  [40/57]  eta: 0:00:03  loss: 0.2843 (0.3753)  time: 0.1787  data: 0.0001  max mem: 15824
[10:46:13.672174] Test:  [50/57]  eta: 0:00:01  loss: 0.3308 (0.3796)  time: 0.1790  data: 0.0001  max mem: 15824
[10:46:14.649512] Test:  [56/57]  eta: 0:00:00  loss: 0.3605 (0.3969)  time: 0.1741  data: 0.0001  max mem: 15824
[10:46:14.711011] Test: Total time: 0:00:10 (0.1851 s / it)
[10:46:16.436775] Dice score of the network on the train images: 0.869230, val images: 0.767356
[10:46:16.441064] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:46:17.451927] Epoch: [39]  [  0/345]  eta: 0:05:48  lr: 0.000037  loss: 0.1284 (0.1284)  time: 1.0097  data: 0.3832  max mem: 15824
[10:46:29.814469] Epoch: [39]  [ 20/345]  eta: 0:03:26  lr: 0.000037  loss: 0.1346 (0.1366)  time: 0.6181  data: 0.0001  max mem: 15824
[10:46:42.334387] Epoch: [39]  [ 40/345]  eta: 0:03:12  lr: 0.000036  loss: 0.1412 (0.1409)  time: 0.6259  data: 0.0001  max mem: 15824
[10:46:54.733898] Epoch: [39]  [ 60/345]  eta: 0:02:58  lr: 0.000036  loss: 0.1324 (0.1413)  time: 0.6199  data: 0.0001  max mem: 15824
[10:47:07.129861] Epoch: [39]  [ 80/345]  eta: 0:02:45  lr: 0.000036  loss: 0.1285 (0.1389)  time: 0.6198  data: 0.0001  max mem: 15824
[10:47:19.570918] Epoch: [39]  [100/345]  eta: 0:02:33  lr: 0.000035  loss: 0.1341 (0.1375)  time: 0.6220  data: 0.0001  max mem: 15824
[10:47:32.029675] Epoch: [39]  [120/345]  eta: 0:02:20  lr: 0.000035  loss: 0.1391 (0.1377)  time: 0.6229  data: 0.0001  max mem: 15824
[10:47:44.481592] Epoch: [39]  [140/345]  eta: 0:02:07  lr: 0.000035  loss: 0.1255 (0.1369)  time: 0.6226  data: 0.0001  max mem: 15824
[10:47:56.936642] Epoch: [39]  [160/345]  eta: 0:01:55  lr: 0.000034  loss: 0.1358 (0.1374)  time: 0.6227  data: 0.0001  max mem: 15824
[10:48:09.419595] Epoch: [39]  [180/345]  eta: 0:01:42  lr: 0.000034  loss: 0.1332 (0.1381)  time: 0.6241  data: 0.0001  max mem: 15824
[10:48:21.888516] Epoch: [39]  [200/345]  eta: 0:01:30  lr: 0.000034  loss: 0.1361 (0.1381)  time: 0.6234  data: 0.0001  max mem: 15824
[10:48:34.324178] Epoch: [39]  [220/345]  eta: 0:01:17  lr: 0.000033  loss: 0.1365 (0.1378)  time: 0.6217  data: 0.0001  max mem: 15824
[10:48:46.745584] Epoch: [39]  [240/345]  eta: 0:01:05  lr: 0.000033  loss: 0.1252 (0.1371)  time: 0.6210  data: 0.0001  max mem: 15824
[10:48:59.189450] Epoch: [39]  [260/345]  eta: 0:00:52  lr: 0.000033  loss: 0.1316 (0.1370)  time: 0.6221  data: 0.0001  max mem: 15824
[10:49:11.656452] Epoch: [39]  [280/345]  eta: 0:00:40  lr: 0.000032  loss: 0.1321 (0.1368)  time: 0.6233  data: 0.0001  max mem: 15824
[10:49:24.159837] Epoch: [39]  [300/345]  eta: 0:00:28  lr: 0.000032  loss: 0.1275 (0.1363)  time: 0.6251  data: 0.0001  max mem: 15824
[10:49:36.577046] Epoch: [39]  [320/345]  eta: 0:00:15  lr: 0.000032  loss: 0.1377 (0.1363)  time: 0.6208  data: 0.0001  max mem: 15824
[10:49:48.999489] Epoch: [39]  [340/345]  eta: 0:00:03  lr: 0.000031  loss: 0.1323 (0.1360)  time: 0.6211  data: 0.0001  max mem: 15824
[10:49:51.481586] Epoch: [39]  [344/345]  eta: 0:00:00  lr: 0.000031  loss: 0.1364 (0.1361)  time: 0.6207  data: 0.0001  max mem: 15824
[10:49:51.553773] Epoch: [39] Total time: 0:03:35 (0.6235 s / it)
[10:49:51.554277] Averaged stats: lr: 0.000031  loss: 0.1364 (0.1361)
[10:49:52.159027] Test:  [  0/345]  eta: 0:03:26  loss: 0.1219 (0.1219)  time: 0.5971  data: 0.4121  max mem: 15824
[10:49:53.960112] Test:  [ 10/345]  eta: 0:01:13  loss: 0.1200 (0.1291)  time: 0.2179  data: 0.0376  max mem: 15824
[10:49:55.760892] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1330 (0.1365)  time: 0.1800  data: 0.0001  max mem: 15824
[10:49:57.563795] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1317 (0.1323)  time: 0.1801  data: 0.0001  max mem: 15824
[10:49:59.364248] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1199 (0.1304)  time: 0.1801  data: 0.0001  max mem: 15824
[10:50:01.172274] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1300 (0.1323)  time: 0.1804  data: 0.0001  max mem: 15824
[10:50:02.984140] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1285 (0.1312)  time: 0.1809  data: 0.0001  max mem: 15824
[10:50:04.795863] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1213 (0.1307)  time: 0.1811  data: 0.0001  max mem: 15824
[10:50:06.615686] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1213 (0.1304)  time: 0.1815  data: 0.0001  max mem: 15824
[10:50:08.441444] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1237 (0.1304)  time: 0.1822  data: 0.0001  max mem: 15824
[10:50:10.266051] Test:  [100/345]  eta: 0:00:45  loss: 0.1237 (0.1307)  time: 0.1825  data: 0.0001  max mem: 15824
[10:50:12.093826] Test:  [110/345]  eta: 0:00:43  loss: 0.1237 (0.1306)  time: 0.1826  data: 0.0001  max mem: 15824
[10:50:13.925837] Test:  [120/345]  eta: 0:00:41  loss: 0.1289 (0.1307)  time: 0.1829  data: 0.0001  max mem: 15824
[10:50:15.761374] Test:  [130/345]  eta: 0:00:39  loss: 0.1302 (0.1307)  time: 0.1833  data: 0.0001  max mem: 15824
[10:50:17.601664] Test:  [140/345]  eta: 0:00:37  loss: 0.1250 (0.1301)  time: 0.1837  data: 0.0001  max mem: 15824
[10:50:19.443104] Test:  [150/345]  eta: 0:00:35  loss: 0.1243 (0.1301)  time: 0.1840  data: 0.0001  max mem: 15824
[10:50:21.288297] Test:  [160/345]  eta: 0:00:34  loss: 0.1285 (0.1307)  time: 0.1843  data: 0.0001  max mem: 15824
[10:50:23.143335] Test:  [170/345]  eta: 0:00:32  loss: 0.1328 (0.1309)  time: 0.1850  data: 0.0001  max mem: 15824
[10:50:24.997998] Test:  [180/345]  eta: 0:00:30  loss: 0.1311 (0.1308)  time: 0.1854  data: 0.0001  max mem: 15824
[10:50:26.855733] Test:  [190/345]  eta: 0:00:28  loss: 0.1222 (0.1302)  time: 0.1856  data: 0.0001  max mem: 15824
[10:50:28.716340] Test:  [200/345]  eta: 0:00:26  loss: 0.1190 (0.1299)  time: 0.1859  data: 0.0001  max mem: 15824
[10:50:30.583424] Test:  [210/345]  eta: 0:00:24  loss: 0.1194 (0.1294)  time: 0.1863  data: 0.0001  max mem: 15824
[10:50:32.448919] Test:  [220/345]  eta: 0:00:23  loss: 0.1215 (0.1295)  time: 0.1866  data: 0.0001  max mem: 15824
[10:50:34.318361] Test:  [230/345]  eta: 0:00:21  loss: 0.1198 (0.1291)  time: 0.1867  data: 0.0001  max mem: 15824
[10:50:36.193340] Test:  [240/345]  eta: 0:00:19  loss: 0.1229 (0.1290)  time: 0.1872  data: 0.0001  max mem: 15824
[10:50:38.074306] Test:  [250/345]  eta: 0:00:17  loss: 0.1203 (0.1286)  time: 0.1877  data: 0.0001  max mem: 15824
[10:50:39.954598] Test:  [260/345]  eta: 0:00:15  loss: 0.1206 (0.1289)  time: 0.1880  data: 0.0001  max mem: 15824
[10:50:41.837966] Test:  [270/345]  eta: 0:00:13  loss: 0.1303 (0.1291)  time: 0.1881  data: 0.0001  max mem: 15824
[10:50:43.724954] Test:  [280/345]  eta: 0:00:12  loss: 0.1273 (0.1289)  time: 0.1885  data: 0.0001  max mem: 15824
[10:50:45.616585] Test:  [290/345]  eta: 0:00:10  loss: 0.1211 (0.1290)  time: 0.1889  data: 0.0001  max mem: 15824
[10:50:47.511718] Test:  [300/345]  eta: 0:00:08  loss: 0.1265 (0.1290)  time: 0.1893  data: 0.0001  max mem: 15824
[10:50:49.412048] Test:  [310/345]  eta: 0:00:06  loss: 0.1196 (0.1289)  time: 0.1897  data: 0.0001  max mem: 15824
[10:50:51.314229] Test:  [320/345]  eta: 0:00:04  loss: 0.1205 (0.1289)  time: 0.1900  data: 0.0001  max mem: 15824
[10:50:53.217803] Test:  [330/345]  eta: 0:00:02  loss: 0.1322 (0.1294)  time: 0.1902  data: 0.0001  max mem: 15824
[10:50:55.123943] Test:  [340/345]  eta: 0:00:00  loss: 0.1376 (0.1294)  time: 0.1904  data: 0.0001  max mem: 15824
[10:50:55.887035] Test:  [344/345]  eta: 0:00:00  loss: 0.1329 (0.1295)  time: 0.1905  data: 0.0001  max mem: 15824
[10:50:55.955913] Test: Total time: 0:01:04 (0.1867 s / it)
[10:51:06.473536] Test:  [ 0/57]  eta: 0:00:32  loss: 0.5133 (0.5133)  time: 0.5674  data: 0.3876  max mem: 15824
[10:51:08.250550] Test:  [10/57]  eta: 0:00:10  loss: 0.4621 (0.4750)  time: 0.2130  data: 0.0353  max mem: 15824
[10:51:10.031909] Test:  [20/57]  eta: 0:00:07  loss: 0.4544 (0.4583)  time: 0.1778  data: 0.0001  max mem: 15824
[10:51:11.816597] Test:  [30/57]  eta: 0:00:05  loss: 0.3000 (0.3986)  time: 0.1782  data: 0.0001  max mem: 15824
[10:51:13.609384] Test:  [40/57]  eta: 0:00:03  loss: 0.2675 (0.3740)  time: 0.1788  data: 0.0001  max mem: 15824
[10:51:15.399367] Test:  [50/57]  eta: 0:00:01  loss: 0.3207 (0.3796)  time: 0.1791  data: 0.0001  max mem: 15824
[10:51:16.373049] Test:  [56/57]  eta: 0:00:00  loss: 0.3687 (0.3989)  time: 0.1740  data: 0.0001  max mem: 15824
[10:51:16.448786] Test: Total time: 0:00:10 (0.1850 s / it)
[10:51:18.167858] Dice score of the network on the train images: 0.866319, val images: 0.767713
[10:51:18.172026] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:51:19.189493] Epoch: [40]  [  0/345]  eta: 0:05:50  lr: 0.000031  loss: 0.1427 (0.1427)  time: 1.0166  data: 0.3920  max mem: 15824
[10:51:31.547454] Epoch: [40]  [ 20/345]  eta: 0:03:26  lr: 0.000031  loss: 0.1318 (0.1340)  time: 0.6178  data: 0.0001  max mem: 15824

[10:51:43.935467] Epoch: [40]  [ 40/345]  eta: 0:03:11  lr: 0.000031  loss: 0.1352 (0.1351)  time: 0.6194  data: 0.0001  max mem: 15824
[10:51:56.316418] Epoch: [40]  [ 60/345]  eta: 0:02:58  lr: 0.000030  loss: 0.1417 (0.1369)  time: 0.6190  data: 0.0001  max mem: 15824
[10:52:08.709408] Epoch: [40]  [ 80/345]  eta: 0:02:45  lr: 0.000030  loss: 0.1266 (0.1346)  time: 0.6196  data: 0.0001  max mem: 15824
[10:52:21.109539] Epoch: [40]  [100/345]  eta: 0:02:32  lr: 0.000030  loss: 0.1292 (0.1335)  time: 0.6200  data: 0.0001  max mem: 15824
[10:52:33.534305] Epoch: [40]  [120/345]  eta: 0:02:20  lr: 0.000029  loss: 0.1296 (0.1335)  time: 0.6212  data: 0.0001  max mem: 15824

[10:52:45.996496] Epoch: [40]  [140/345]  eta: 0:02:07  lr: 0.000029  loss: 0.1379 (0.1335)  time: 0.6231  data: 0.0001  max mem: 15824
[10:52:58.433673] Epoch: [40]  [160/345]  eta: 0:01:55  lr: 0.000029  loss: 0.1318 (0.1332)  time: 0.6218  data: 0.0001  max mem: 15824
[10:53:10.875734] Epoch: [40]  [180/345]  eta: 0:01:42  lr: 0.000028  loss: 0.1411 (0.1341)  time: 0.6221  data: 0.0001  max mem: 15824
[10:53:23.310870] Epoch: [40]  [200/345]  eta: 0:01:30  lr: 0.000028  loss: 0.1318 (0.1340)  time: 0.6217  data: 0.0001  max mem: 15824
[10:53:35.770873] Epoch: [40]  [220/345]  eta: 0:01:17  lr: 0.000028  loss: 0.1338 (0.1345)  time: 0.6230  data: 0.0001  max mem: 15824
[10:53:48.212125] Epoch: [40]  [240/345]  eta: 0:01:05  lr: 0.000027  loss: 0.1303 (0.1343)  time: 0.6220  data: 0.0001  max mem: 15824
[10:54:00.650607] Epoch: [40]  [260/345]  eta: 0:00:52  lr: 0.000027  loss: 0.1329 (0.1346)  time: 0.6219  data: 0.0001  max mem: 15824
[10:54:13.085716] Epoch: [40]  [280/345]  eta: 0:00:40  lr: 0.000027  loss: 0.1312 (0.1345)  time: 0.6217  data: 0.0001  max mem: 15824
[10:54:25.536346] Epoch: [40]  [300/345]  eta: 0:00:28  lr: 0.000026  loss: 0.1268 (0.1343)  time: 0.6225  data: 0.0001  max mem: 15824
[10:54:37.982899] Epoch: [40]  [320/345]  eta: 0:00:15  lr: 0.000026  loss: 0.1281 (0.1341)  time: 0.6223  data: 0.0001  max mem: 15824
[10:54:50.421874] Epoch: [40]  [340/345]  eta: 0:00:03  lr: 0.000026  loss: 0.1308 (0.1342)  time: 0.6219  data: 0.0001  max mem: 15824
[10:54:52.903086] Epoch: [40]  [344/345]  eta: 0:00:00  lr: 0.000026  loss: 0.1250 (0.1340)  time: 0.6216  data: 0.0001  max mem: 15824
[10:54:52.973010] Epoch: [40] Total time: 0:03:34 (0.6226 s / it)
[10:54:52.973654] Averaged stats: lr: 0.000026  loss: 0.1250 (0.1340)
[10:54:53.558309] Test:  [  0/345]  eta: 0:03:19  loss: 0.1239 (0.1239)  time: 0.5788  data: 0.3967  max mem: 15824
[10:54:55.353827] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1202 (0.1226)  time: 0.2157  data: 0.0362  max mem: 15824
[10:54:57.151362] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1224 (0.1268)  time: 0.1796  data: 0.0001  max mem: 15824
[10:54:58.951242] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1309 (0.1275)  time: 0.1798  data: 0.0001  max mem: 15824
[10:55:00.754531] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1253 (0.1259)  time: 0.1801  data: 0.0001  max mem: 15824
[10:55:02.564234] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1221 (0.1256)  time: 0.1806  data: 0.0001  max mem: 15824
[10:55:04.374492] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1256 (0.1249)  time: 0.1809  data: 0.0001  max mem: 15824
[10:55:06.191541] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1205 (0.1242)  time: 0.1813  data: 0.0001  max mem: 15824
[10:55:08.011014] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1212 (0.1261)  time: 0.1818  data: 0.0001  max mem: 15824
[10:55:09.832396] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1259 (0.1263)  time: 0.1820  data: 0.0001  max mem: 15824
[10:55:11.657716] Test:  [100/345]  eta: 0:00:45  loss: 0.1228 (0.1264)  time: 0.1823  data: 0.0001  max mem: 15824
[10:55:13.485948] Test:  [110/345]  eta: 0:00:43  loss: 0.1202 (0.1259)  time: 0.1826  data: 0.0001  max mem: 15824
[10:55:15.317239] Test:  [120/345]  eta: 0:00:41  loss: 0.1206 (0.1266)  time: 0.1829  data: 0.0001  max mem: 15824
[10:55:17.152718] Test:  [130/345]  eta: 0:00:39  loss: 0.1304 (0.1272)  time: 0.1833  data: 0.0001  max mem: 15824
[10:55:18.992501] Test:  [140/345]  eta: 0:00:37  loss: 0.1267 (0.1275)  time: 0.1837  data: 0.0001  max mem: 15824
[10:55:20.838297] Test:  [150/345]  eta: 0:00:35  loss: 0.1267 (0.1279)  time: 0.1842  data: 0.0001  max mem: 15824
[10:55:22.687701] Test:  [160/345]  eta: 0:00:34  loss: 0.1268 (0.1276)  time: 0.1847  data: 0.0001  max mem: 15824
[10:55:24.539968] Test:  [170/345]  eta: 0:00:32  loss: 0.1262 (0.1276)  time: 0.1850  data: 0.0001  max mem: 15824
[10:55:26.391968] Test:  [180/345]  eta: 0:00:30  loss: 0.1240 (0.1274)  time: 0.1851  data: 0.0001  max mem: 15824
[10:55:28.248411] Test:  [190/345]  eta: 0:00:28  loss: 0.1201 (0.1270)  time: 0.1854  data: 0.0001  max mem: 15824
[10:55:30.109043] Test:  [200/345]  eta: 0:00:26  loss: 0.1183 (0.1270)  time: 0.1858  data: 0.0001  max mem: 15824
[10:55:31.976064] Test:  [210/345]  eta: 0:00:24  loss: 0.1171 (0.1269)  time: 0.1863  data: 0.0001  max mem: 15824
[10:55:33.843227] Test:  [220/345]  eta: 0:00:23  loss: 0.1210 (0.1269)  time: 0.1867  data: 0.0001  max mem: 15824
[10:55:35.714840] Test:  [230/345]  eta: 0:00:21  loss: 0.1279 (0.1270)  time: 0.1869  data: 0.0001  max mem: 15824
[10:55:37.591193] Test:  [240/345]  eta: 0:00:19  loss: 0.1305 (0.1274)  time: 0.1873  data: 0.0001  max mem: 15824
[10:55:39.470539] Test:  [250/345]  eta: 0:00:17  loss: 0.1215 (0.1270)  time: 0.1877  data: 0.0001  max mem: 15824
[10:55:41.354012] Test:  [260/345]  eta: 0:00:15  loss: 0.1188 (0.1268)  time: 0.1881  data: 0.0001  max mem: 15824
[10:55:43.238022] Test:  [270/345]  eta: 0:00:13  loss: 0.1157 (0.1264)  time: 0.1883  data: 0.0001  max mem: 15824
[10:55:45.126418] Test:  [280/345]  eta: 0:00:12  loss: 0.1157 (0.1262)  time: 0.1886  data: 0.0001  max mem: 15824
[10:55:47.016479] Test:  [290/345]  eta: 0:00:10  loss: 0.1264 (0.1267)  time: 0.1889  data: 0.0001  max mem: 15824
[10:55:48.912389] Test:  [300/345]  eta: 0:00:08  loss: 0.1342 (0.1272)  time: 0.1892  data: 0.0001  max mem: 15824
[10:55:50.811715] Test:  [310/345]  eta: 0:00:06  loss: 0.1218 (0.1268)  time: 0.1897  data: 0.0001  max mem: 15824
[10:55:52.716359] Test:  [320/345]  eta: 0:00:04  loss: 0.1217 (0.1269)  time: 0.1901  data: 0.0001  max mem: 15824
[10:55:54.620064] Test:  [330/345]  eta: 0:00:02  loss: 0.1217 (0.1265)  time: 0.1904  data: 0.0001  max mem: 15824
[10:55:56.526132] Test:  [340/345]  eta: 0:00:00  loss: 0.1093 (0.1263)  time: 0.1904  data: 0.0001  max mem: 15824
[10:55:57.289416] Test:  [344/345]  eta: 0:00:00  loss: 0.1162 (0.1263)  time: 0.1906  data: 0.0001  max mem: 15824
[10:55:57.369108] Test: Total time: 0:01:04 (0.1866 s / it)
[10:56:07.884744] Test:  [ 0/57]  eta: 0:00:32  loss: 0.5129 (0.5129)  time: 0.5709  data: 0.3906  max mem: 15824
[10:56:09.661726] Test:  [10/57]  eta: 0:00:10  loss: 0.4588 (0.4741)  time: 0.2134  data: 0.0356  max mem: 15824
[10:56:11.445105] Test:  [20/57]  eta: 0:00:07  loss: 0.4588 (0.4607)  time: 0.1779  data: 0.0001  max mem: 15824
[10:56:13.231721] Test:  [30/57]  eta: 0:00:05  loss: 0.3062 (0.4022)  time: 0.1784  data: 0.0001  max mem: 15824
[10:56:15.021190] Test:  [40/57]  eta: 0:00:03  loss: 0.2875 (0.3792)  time: 0.1787  data: 0.0001  max mem: 15824
[10:56:16.812527] Test:  [50/57]  eta: 0:00:01  loss: 0.3321 (0.3835)  time: 0.1790  data: 0.0001  max mem: 15824
[10:56:17.788087] Test:  [56/57]  eta: 0:00:00  loss: 0.3747 (0.4014)  time: 0.1741  data: 0.0000  max mem: 15824
[10:56:17.857926] Test: Total time: 0:00:10 (0.1850 s / it)
[10:56:19.600349] Dice score of the network on the train images: 0.871968, val images: 0.765422
[10:56:19.604360] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:56:20.624729] Epoch: [41]  [  0/345]  eta: 0:05:51  lr: 0.000026  loss: 0.1581 (0.1581)  time: 1.0194  data: 0.3914  max mem: 15824
[10:56:32.998017] Epoch: [41]  [ 20/345]  eta: 0:03:27  lr: 0.000025  loss: 0.1309 (0.1362)  time: 0.6186  data: 0.0001  max mem: 15824
[10:56:45.402229] Epoch: [41]  [ 40/345]  eta: 0:03:11  lr: 0.000025  loss: 0.1230 (0.1309)  time: 0.6202  data: 0.0001  max mem: 15824
[10:56:57.799942] Epoch: [41]  [ 60/345]  eta: 0:02:58  lr: 0.000025  loss: 0.1255 (0.1294)  time: 0.6198  data: 0.0001  max mem: 15824
[10:57:10.230891] Epoch: [41]  [ 80/345]  eta: 0:02:45  lr: 0.000025  loss: 0.1237 (0.1306)  time: 0.6215  data: 0.0001  max mem: 15824
[10:57:22.678343] Epoch: [41]  [100/345]  eta: 0:02:32  lr: 0.000024  loss: 0.1272 (0.1315)  time: 0.6223  data: 0.0001  max mem: 15824
[10:57:35.140231] Epoch: [41]  [120/345]  eta: 0:02:20  lr: 0.000024  loss: 0.1257 (0.1316)  time: 0.6231  data: 0.0001  max mem: 15824
[10:57:47.615078] Epoch: [41]  [140/345]  eta: 0:02:07  lr: 0.000024  loss: 0.1242 (0.1308)  time: 0.6237  data: 0.0001  max mem: 15824
[10:58:00.092113] Epoch: [41]  [160/345]  eta: 0:01:55  lr: 0.000023  loss: 0.1262 (0.1306)  time: 0.6238  data: 0.0001  max mem: 15824
[10:58:12.560340] Epoch: [41]  [180/345]  eta: 0:01:42  lr: 0.000023  loss: 0.1306 (0.1306)  time: 0.6234  data: 0.0001  max mem: 15824
[10:58:24.988257] Epoch: [41]  [200/345]  eta: 0:01:30  lr: 0.000023  loss: 0.1250 (0.1303)  time: 0.6214  data: 0.0001  max mem: 15824
[10:58:37.422778] Epoch: [41]  [220/345]  eta: 0:01:17  lr: 0.000022  loss: 0.1289 (0.1305)  time: 0.6217  data: 0.0001  max mem: 15824
[10:58:50.002253] Epoch: [41]  [240/345]  eta: 0:01:05  lr: 0.000022  loss: 0.1216 (0.1301)  time: 0.6289  data: 0.0001  max mem: 15824
[10:59:02.471814] Epoch: [41]  [260/345]  eta: 0:00:53  lr: 0.000022  loss: 0.1323 (0.1305)  time: 0.6234  data: 0.0001  max mem: 15824
[10:59:14.931157] Epoch: [41]  [280/345]  eta: 0:00:40  lr: 0.000022  loss: 0.1276 (0.1309)  time: 0.6229  data: 0.0001  max mem: 15824
[10:59:27.369899] Epoch: [41]  [300/345]  eta: 0:00:28  lr: 0.000021  loss: 0.1244 (0.1310)  time: 0.6219  data: 0.0001  max mem: 15824
[10:59:39.803697] Epoch: [41]  [320/345]  eta: 0:00:15  lr: 0.000021  loss: 0.1242 (0.1309)  time: 0.6216  data: 0.0001  max mem: 15824
[10:59:52.231980] Epoch: [41]  [340/345]  eta: 0:00:03  lr: 0.000021  loss: 0.1291 (0.1309)  time: 0.6214  data: 0.0001  max mem: 15824
[10:59:54.716979] Epoch: [41]  [344/345]  eta: 0:00:00  lr: 0.000021  loss: 0.1296 (0.1310)  time: 0.6210  data: 0.0001  max mem: 15824
[10:59:54.793544] Epoch: [41] Total time: 0:03:35 (0.6237 s / it)
[10:59:54.793775] Averaged stats: lr: 0.000021  loss: 0.1296 (0.1310)
[10:59:55.398642] Test:  [  0/345]  eta: 0:03:26  loss: 0.1278 (0.1278)  time: 0.5990  data: 0.4177  max mem: 15824
[10:59:57.191659] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1278 (0.1259)  time: 0.2174  data: 0.0381  max mem: 15824
[10:59:58.988284] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1249 (0.1255)  time: 0.1794  data: 0.0001  max mem: 15824
[11:00:00.786188] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1249 (0.1295)  time: 0.1797  data: 0.0001  max mem: 15824
[11:00:02.589561] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1209 (0.1261)  time: 0.1800  data: 0.0001  max mem: 15824
[11:00:04.396358] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1169 (0.1246)  time: 0.1804  data: 0.0001  max mem: 15824
[11:00:06.208873] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1149 (0.1235)  time: 0.1809  data: 0.0001  max mem: 15824
[11:00:08.022913] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1158 (0.1233)  time: 0.1813  data: 0.0001  max mem: 15824
[11:00:09.840757] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1189 (0.1232)  time: 0.1815  data: 0.0001  max mem: 15824
[11:00:11.662010] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1239 (0.1237)  time: 0.1819  data: 0.0001  max mem: 15824
[11:00:13.486576] Test:  [100/345]  eta: 0:00:45  loss: 0.1138 (0.1224)  time: 0.1822  data: 0.0001  max mem: 15824
[11:00:15.313563] Test:  [110/345]  eta: 0:00:43  loss: 0.1121 (0.1218)  time: 0.1825  data: 0.0001  max mem: 15824
[11:00:17.145377] Test:  [120/345]  eta: 0:00:41  loss: 0.1208 (0.1225)  time: 0.1829  data: 0.0001  max mem: 15824
[11:00:18.979530] Test:  [130/345]  eta: 0:00:39  loss: 0.1250 (0.1225)  time: 0.1832  data: 0.0001  max mem: 15824
[11:00:20.817118] Test:  [140/345]  eta: 0:00:37  loss: 0.1204 (0.1228)  time: 0.1835  data: 0.0001  max mem: 15824
[11:00:22.658747] Test:  [150/345]  eta: 0:00:35  loss: 0.1194 (0.1227)  time: 0.1839  data: 0.0001  max mem: 15824
[11:00:24.505773] Test:  [160/345]  eta: 0:00:34  loss: 0.1128 (0.1225)  time: 0.1844  data: 0.0001  max mem: 15824
[11:00:26.354974] Test:  [170/345]  eta: 0:00:32  loss: 0.1181 (0.1228)  time: 0.1848  data: 0.0001  max mem: 15824
[11:00:28.208788] Test:  [180/345]  eta: 0:00:30  loss: 0.1251 (0.1231)  time: 0.1851  data: 0.0001  max mem: 15824
[11:00:30.065349] Test:  [190/345]  eta: 0:00:28  loss: 0.1263 (0.1231)  time: 0.1855  data: 0.0001  max mem: 15824
[11:00:31.922902] Test:  [200/345]  eta: 0:00:26  loss: 0.1203 (0.1231)  time: 0.1856  data: 0.0001  max mem: 15824
[11:00:33.785661] Test:  [210/345]  eta: 0:00:24  loss: 0.1156 (0.1229)  time: 0.1860  data: 0.0001  max mem: 15824
[11:00:35.651722] Test:  [220/345]  eta: 0:00:23  loss: 0.1242 (0.1229)  time: 0.1864  data: 0.0001  max mem: 15824
[11:00:37.521100] Test:  [230/345]  eta: 0:00:21  loss: 0.1252 (0.1232)  time: 0.1867  data: 0.0001  max mem: 15824
[11:00:39.393291] Test:  [240/345]  eta: 0:00:19  loss: 0.1252 (0.1234)  time: 0.1870  data: 0.0001  max mem: 15824
[11:00:41.269776] Test:  [250/345]  eta: 0:00:17  loss: 0.1194 (0.1233)  time: 0.1874  data: 0.0001  max mem: 15824
[11:00:43.149963] Test:  [260/345]  eta: 0:00:15  loss: 0.1138 (0.1232)  time: 0.1878  data: 0.0001  max mem: 15824
[11:00:45.032283] Test:  [270/345]  eta: 0:00:13  loss: 0.1221 (0.1236)  time: 0.1881  data: 0.0001  max mem: 15824
[11:00:46.918915] Test:  [280/345]  eta: 0:00:12  loss: 0.1223 (0.1235)  time: 0.1884  data: 0.0001  max mem: 15824
[11:00:48.810220] Test:  [290/345]  eta: 0:00:10  loss: 0.1188 (0.1235)  time: 0.1888  data: 0.0001  max mem: 15824
[11:00:50.705346] Test:  [300/345]  eta: 0:00:08  loss: 0.1212 (0.1237)  time: 0.1893  data: 0.0001  max mem: 15824
[11:00:52.604563] Test:  [310/345]  eta: 0:00:06  loss: 0.1280 (0.1238)  time: 0.1897  data: 0.0001  max mem: 15824
[11:00:54.506415] Test:  [320/345]  eta: 0:00:04  loss: 0.1196 (0.1238)  time: 0.1900  data: 0.0001  max mem: 15824
[11:00:56.410271] Test:  [330/345]  eta: 0:00:02  loss: 0.1196 (0.1238)  time: 0.1902  data: 0.0001  max mem: 15824
[11:00:58.316700] Test:  [340/345]  eta: 0:00:00  loss: 0.1216 (0.1239)  time: 0.1905  data: 0.0001  max mem: 15824
[11:00:59.080413] Test:  [344/345]  eta: 0:00:00  loss: 0.1216 (0.1239)  time: 0.1905  data: 0.0001  max mem: 15824
[11:00:59.146281] Test: Total time: 0:01:04 (0.1865 s / it)
[11:01:09.618203] Test:  [ 0/57]  eta: 0:00:31  loss: 0.5080 (0.5080)  time: 0.5581  data: 0.3770  max mem: 15824
[11:01:11.396194] Test:  [10/57]  eta: 0:00:09  loss: 0.4502 (0.4727)  time: 0.2123  data: 0.0344  max mem: 15824
[11:01:13.177950] Test:  [20/57]  eta: 0:00:07  loss: 0.4502 (0.4615)  time: 0.1779  data: 0.0001  max mem: 15824
[11:01:14.961433] Test:  [30/57]  eta: 0:00:05  loss: 0.3023 (0.4021)  time: 0.1782  data: 0.0001  max mem: 15824
[11:01:16.753685] Test:  [40/57]  eta: 0:00:03  loss: 0.2818 (0.3781)  time: 0.1787  data: 0.0001  max mem: 15824
[11:01:18.542729] Test:  [50/57]  eta: 0:00:01  loss: 0.3311 (0.3812)  time: 0.1790  data: 0.0001  max mem: 15824
[11:01:19.517180] Test:  [56/57]  eta: 0:00:00  loss: 0.3668 (0.3989)  time: 0.1740  data: 0.0001  max mem: 15824
[11:01:19.593943] Test: Total time: 0:00:10 (0.1848 s / it)
[11:01:21.341646] Dice score of the network on the train images: 0.873516, val images: 0.768041
[11:01:21.345727] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[11:01:22.348731] Epoch: [42]  [  0/345]  eta: 0:05:45  lr: 0.000021  loss: 0.1502 (0.1502)  time: 1.0020  data: 0.3756  max mem: 15824
[11:01:34.723571] Epoch: [42]  [ 20/345]  eta: 0:03:27  lr: 0.000020  loss: 0.1289 (0.1338)  time: 0.6187  data: 0.0001  max mem: 15824
[11:01:47.107507] Epoch: [42]  [ 40/345]  eta: 0:03:11  lr: 0.000020  loss: 0.1213 (0.1318)  time: 0.6191  data: 0.0001  max mem: 15824
[11:01:59.522945] Epoch: [42]  [ 60/345]  eta: 0:02:58  lr: 0.000020  loss: 0.1224 (0.1315)  time: 0.6207  data: 0.0001  max mem: 15824
[11:02:11.945521] Epoch: [42]  [ 80/345]  eta: 0:02:45  lr: 0.000020  loss: 0.1253 (0.1306)  time: 0.6211  data: 0.0001  max mem: 15824
[11:02:24.381529] Epoch: [42]  [100/345]  eta: 0:02:32  lr: 0.000019  loss: 0.1219 (0.1291)  time: 0.6218  data: 0.0001  max mem: 15824
[11:02:36.964920] Epoch: [42]  [120/345]  eta: 0:02:20  lr: 0.000019  loss: 0.1243 (0.1287)  time: 0.6291  data: 0.0001  max mem: 15824
[11:02:49.418614] Epoch: [42]  [140/345]  eta: 0:02:08  lr: 0.000019  loss: 0.1218 (0.1279)  time: 0.6226  data: 0.0001  max mem: 15824
[11:03:01.885871] Epoch: [42]  [160/345]  eta: 0:01:55  lr: 0.000018  loss: 0.1355 (0.1294)  time: 0.6233  data: 0.0001  max mem: 15824
[11:03:14.368055] Epoch: [42]  [180/345]  eta: 0:01:43  lr: 0.000018  loss: 0.1201 (0.1291)  time: 0.6241  data: 0.0001  max mem: 15824
[11:03:26.840804] Epoch: [42]  [200/345]  eta: 0:01:30  lr: 0.000018  loss: 0.1251 (0.1291)  time: 0.6236  data: 0.0001  max mem: 15824
[11:03:39.317161] Epoch: [42]  [220/345]  eta: 0:01:18  lr: 0.000018  loss: 0.1268 (0.1292)  time: 0.6238  data: 0.0001  max mem: 15824
[11:03:51.783891] Epoch: [42]  [240/345]  eta: 0:01:05  lr: 0.000017  loss: 0.1210 (0.1289)  time: 0.6233  data: 0.0001  max mem: 15824
[11:04:04.232398] Epoch: [42]  [260/345]  eta: 0:00:53  lr: 0.000017  loss: 0.1276 (0.1292)  time: 0.6224  data: 0.0001  max mem: 15824
[11:04:16.682273] Epoch: [42]  [280/345]  eta: 0:00:40  lr: 0.000017  loss: 0.1203 (0.1286)  time: 0.6225  data: 0.0001  max mem: 15824
[11:04:29.115358] Epoch: [42]  [300/345]  eta: 0:00:28  lr: 0.000017  loss: 0.1245 (0.1286)  time: 0.6216  data: 0.0001  max mem: 15824
[11:04:41.550714] Epoch: [42]  [320/345]  eta: 0:00:15  lr: 0.000016  loss: 0.1229 (0.1285)  time: 0.6217  data: 0.0001  max mem: 15824
[11:04:53.980169] Epoch: [42]  [340/345]  eta: 0:00:03  lr: 0.000016  loss: 0.1249 (0.1284)  time: 0.6214  data: 0.0001  max mem: 15824
[11:04:56.462652] Epoch: [42]  [344/345]  eta: 0:00:00  lr: 0.000016  loss: 0.1249 (0.1284)  time: 0.6210  data: 0.0001  max mem: 15824
[11:04:56.533379] Epoch: [42] Total time: 0:03:35 (0.6237 s / it)
[11:04:56.533595] Averaged stats: lr: 0.000016  loss: 0.1249 (0.1284)
[11:04:57.125294] Test:  [  0/345]  eta: 0:03:22  loss: 0.1103 (0.1103)  time: 0.5863  data: 0.4031  max mem: 15824
[11:04:58.925593] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1180 (0.1209)  time: 0.2169  data: 0.0367  max mem: 15824
[11:05:00.725966] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1180 (0.1189)  time: 0.1800  data: 0.0001  max mem: 15824
[11:05:02.527145] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1187 (0.1199)  time: 0.1800  data: 0.0001  max mem: 15824
[11:05:04.329764] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1186 (0.1192)  time: 0.1801  data: 0.0001  max mem: 15824
[11:05:06.136758] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1186 (0.1198)  time: 0.1804  data: 0.0001  max mem: 15824
[11:05:07.950047] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1156 (0.1187)  time: 0.1809  data: 0.0001  max mem: 15824
[11:05:09.764740] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1156 (0.1184)  time: 0.1813  data: 0.0001  max mem: 15824
[11:05:11.582822] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1196 (0.1188)  time: 0.1816  data: 0.0001  max mem: 15824
[11:05:13.402608] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1217 (0.1192)  time: 0.1818  data: 0.0001  max mem: 15824
[11:05:15.227999] Test:  [100/345]  eta: 0:00:45  loss: 0.1128 (0.1183)  time: 0.1822  data: 0.0001  max mem: 15824
[11:05:17.055890] Test:  [110/345]  eta: 0:00:43  loss: 0.1101 (0.1198)  time: 0.1826  data: 0.0001  max mem: 15824
[11:05:18.888047] Test:  [120/345]  eta: 0:00:41  loss: 0.1175 (0.1199)  time: 0.1829  data: 0.0001  max mem: 15824
[11:05:20.723271] Test:  [130/345]  eta: 0:00:39  loss: 0.1178 (0.1206)  time: 0.1833  data: 0.0001  max mem: 15824
[11:05:22.562502] Test:  [140/345]  eta: 0:00:37  loss: 0.1208 (0.1209)  time: 0.1837  data: 0.0001  max mem: 15824
[11:05:24.404198] Test:  [150/345]  eta: 0:00:35  loss: 0.1203 (0.1211)  time: 0.1840  data: 0.0001  max mem: 15824
[11:05:26.251663] Test:  [160/345]  eta: 0:00:34  loss: 0.1201 (0.1212)  time: 0.1844  data: 0.0001  max mem: 15824
[11:05:28.102182] Test:  [170/345]  eta: 0:00:32  loss: 0.1201 (0.1215)  time: 0.1848  data: 0.0001  max mem: 15824
[11:05:29.955895] Test:  [180/345]  eta: 0:00:30  loss: 0.1212 (0.1216)  time: 0.1852  data: 0.0001  max mem: 15824
[11:05:31.812995] Test:  [190/345]  eta: 0:00:28  loss: 0.1161 (0.1212)  time: 0.1855  data: 0.0001  max mem: 15824
[11:05:33.674881] Test:  [200/345]  eta: 0:00:26  loss: 0.1171 (0.1216)  time: 0.1859  data: 0.0001  max mem: 15824
[11:05:35.537767] Test:  [210/345]  eta: 0:00:24  loss: 0.1228 (0.1219)  time: 0.1862  data: 0.0001  max mem: 15824
[11:05:37.405168] Test:  [220/345]  eta: 0:00:23  loss: 0.1205 (0.1219)  time: 0.1865  data: 0.0001  max mem: 15824
[11:05:39.275231] Test:  [230/345]  eta: 0:00:21  loss: 0.1232 (0.1220)  time: 0.1868  data: 0.0001  max mem: 15824
[11:05:41.148698] Test:  [240/345]  eta: 0:00:19  loss: 0.1218 (0.1219)  time: 0.1871  data: 0.0001  max mem: 15824
[11:05:43.024898] Test:  [250/345]  eta: 0:00:17  loss: 0.1218 (0.1220)  time: 0.1874  data: 0.0001  max mem: 15824
[11:05:44.905627] Test:  [260/345]  eta: 0:00:15  loss: 0.1179 (0.1218)  time: 0.1878  data: 0.0001  max mem: 15824
[11:05:46.788860] Test:  [270/345]  eta: 0:00:13  loss: 0.1144 (0.1216)  time: 0.1881  data: 0.0001  max mem: 15824
[11:05:48.677124] Test:  [280/345]  eta: 0:00:12  loss: 0.1129 (0.1213)  time: 0.1885  data: 0.0001  max mem: 15824
[11:05:50.569497] Test:  [290/345]  eta: 0:00:10  loss: 0.1188 (0.1214)  time: 0.1890  data: 0.0001  max mem: 15824
[11:05:52.465396] Test:  [300/345]  eta: 0:00:08  loss: 0.1150 (0.1210)  time: 0.1893  data: 0.0001  max mem: 15824
[11:05:54.366083] Test:  [310/345]  eta: 0:00:06  loss: 0.1171 (0.1212)  time: 0.1898  data: 0.0001  max mem: 15824
[11:05:56.270125] Test:  [320/345]  eta: 0:00:04  loss: 0.1207 (0.1211)  time: 0.1902  data: 0.0001  max mem: 15824
[11:05:58.174856] Test:  [330/345]  eta: 0:00:02  loss: 0.1236 (0.1213)  time: 0.1904  data: 0.0001  max mem: 15824
[11:06:00.082658] Test:  [340/345]  eta: 0:00:00  loss: 0.1236 (0.1215)  time: 0.1906  data: 0.0001  max mem: 15824
[11:06:00.846946] Test:  [344/345]  eta: 0:00:00  loss: 0.1236 (0.1216)  time: 0.1907  data: 0.0001  max mem: 15824
[11:06:00.914640] Test: Total time: 0:01:04 (0.1866 s / it)
[11:06:11.463584] Test:  [ 0/57]  eta: 0:00:31  loss: 0.5077 (0.5077)  time: 0.5535  data: 0.3715  max mem: 15824
[11:06:13.241691] Test:  [10/57]  eta: 0:00:09  loss: 0.4582 (0.4747)  time: 0.2119  data: 0.0339  max mem: 15824
[11:06:15.025405] Test:  [20/57]  eta: 0:00:07  loss: 0.4582 (0.4647)  time: 0.1780  data: 0.0001  max mem: 15824
[11:06:16.813171] Test:  [30/57]  eta: 0:00:05  loss: 0.3029 (0.4030)  time: 0.1785  data: 0.0001  max mem: 15824
[11:06:18.601785] Test:  [40/57]  eta: 0:00:03  loss: 0.2755 (0.3776)  time: 0.1788  data: 0.0001  max mem: 15824
[11:06:20.395824] Test:  [50/57]  eta: 0:00:01  loss: 0.3318 (0.3807)  time: 0.1791  data: 0.0001  max mem: 15824
[11:06:21.372595] Test:  [56/57]  eta: 0:00:00  loss: 0.3675 (0.3977)  time: 0.1742  data: 0.0000  max mem: 15824
[11:06:21.441408] Test: Total time: 0:00:10 (0.1848 s / it)
[11:06:23.203859] Dice score of the network on the train images: 0.872865, val images: 0.771556
[11:06:23.208162] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[11:06:24.229953] Epoch: [43]  [  0/345]  eta: 0:05:52  lr: 0.000016  loss: 0.0990 (0.0990)  time: 1.0207  data: 0.3921  max mem: 15824
[11:06:36.713044] Epoch: [43]  [ 20/345]  eta: 0:03:28  lr: 0.000016  loss: 0.1166 (0.1254)  time: 0.6241  data: 0.0001  max mem: 15824
[11:06:49.101293] Epoch: [43]  [ 40/345]  eta: 0:03:12  lr: 0.000016  loss: 0.1201 (0.1262)  time: 0.6194  data: 0.0001  max mem: 15824
[11:07:01.504437] Epoch: [43]  [ 60/345]  eta: 0:02:58  lr: 0.000015  loss: 0.1221 (0.1263)  time: 0.6201  data: 0.0001  max mem: 15824
[11:07:13.924350] Epoch: [43]  [ 80/345]  eta: 0:02:45  lr: 0.000015  loss: 0.1245 (0.1266)  time: 0.6209  data: 0.0001  max mem: 15824
[11:07:26.375830] Epoch: [43]  [100/345]  eta: 0:02:33  lr: 0.000015  loss: 0.1261 (0.1273)  time: 0.6225  data: 0.0001  max mem: 15824
[11:07:38.837689] Epoch: [43]  [120/345]  eta: 0:02:20  lr: 0.000015  loss: 0.1251 (0.1279)  time: 0.6230  data: 0.0001  max mem: 15824
[11:07:51.308605] Epoch: [43]  [140/345]  eta: 0:02:08  lr: 0.000014  loss: 0.1240 (0.1277)  time: 0.6235  data: 0.0001  max mem: 15824
[11:08:03.796396] Epoch: [43]  [160/345]  eta: 0:01:55  lr: 0.000014  loss: 0.1267 (0.1285)  time: 0.6243  data: 0.0001  max mem: 15824
[11:08:16.271580] Epoch: [43]  [180/345]  eta: 0:01:43  lr: 0.000014  loss: 0.1180 (0.1276)  time: 0.6237  data: 0.0001  max mem: 15824
[11:08:28.747093] Epoch: [43]  [200/345]  eta: 0:01:30  lr: 0.000014  loss: 0.1215 (0.1274)  time: 0.6237  data: 0.0001  max mem: 15824
[11:08:41.211947] Epoch: [43]  [220/345]  eta: 0:01:18  lr: 0.000013  loss: 0.1234 (0.1272)  time: 0.6232  data: 0.0001  max mem: 15824
[11:08:53.668446] Epoch: [43]  [240/345]  eta: 0:01:05  lr: 0.000013  loss: 0.1229 (0.1275)  time: 0.6228  data: 0.0001  max mem: 15824
[11:09:06.121110] Epoch: [43]  [260/345]  eta: 0:00:53  lr: 0.000013  loss: 0.1233 (0.1276)  time: 0.6226  data: 0.0001  max mem: 15824
[11:09:18.563017] Epoch: [43]  [280/345]  eta: 0:00:40  lr: 0.000013  loss: 0.1197 (0.1272)  time: 0.6220  data: 0.0001  max mem: 15824
[11:09:30.998572] Epoch: [43]  [300/345]  eta: 0:00:28  lr: 0.000012  loss: 0.1271 (0.1275)  time: 0.6217  data: 0.0001  max mem: 15824
[11:09:43.450589] Epoch: [43]  [320/345]  eta: 0:00:15  lr: 0.000012  loss: 0.1191 (0.1273)  time: 0.6226  data: 0.0001  max mem: 15824
[11:09:55.894333] Epoch: [43]  [340/345]  eta: 0:00:03  lr: 0.000012  loss: 0.1157 (0.1270)  time: 0.6221  data: 0.0001  max mem: 15824
[11:09:58.379737] Epoch: [43]  [344/345]  eta: 0:00:00  lr: 0.000012  loss: 0.1180 (0.1269)  time: 0.6219  data: 0.0001  max mem: 15824
[11:09:58.453940] Epoch: [43] Total time: 0:03:35 (0.6239 s / it)
[11:09:58.454419] Averaged stats: lr: 0.000012  loss: 0.1180 (0.1269)
[11:09:59.046999] Test:  [  0/345]  eta: 0:03:22  loss: 0.1127 (0.1127)  time: 0.5868  data: 0.4002  max mem: 15824
[11:10:00.846220] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1190 (0.1189)  time: 0.2168  data: 0.0365  max mem: 15824
[11:10:02.649124] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1169 (0.1196)  time: 0.1800  data: 0.0001  max mem: 15824
[11:10:04.450302] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1169 (0.1207)  time: 0.1801  data: 0.0001  max mem: 15824
[11:10:06.256168] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1193 (0.1215)  time: 0.1803  data: 0.0001  max mem: 15824
[11:10:08.068021] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1161 (0.1203)  time: 0.1808  data: 0.0001  max mem: 15824
[11:10:09.879905] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1121 (0.1190)  time: 0.1811  data: 0.0001  max mem: 15824
[11:10:11.695463] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1083 (0.1179)  time: 0.1813  data: 0.0001  max mem: 15824
[11:10:13.513172] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1127 (0.1192)  time: 0.1816  data: 0.0001  max mem: 15824
[11:10:15.334843] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1167 (0.1188)  time: 0.1819  data: 0.0001  max mem: 15824
[11:10:17.161787] Test:  [100/345]  eta: 0:00:45  loss: 0.1130 (0.1185)  time: 0.1824  data: 0.0001  max mem: 15824
[11:10:18.992520] Test:  [110/345]  eta: 0:00:43  loss: 0.1145 (0.1185)  time: 0.1828  data: 0.0001  max mem: 15824
[11:10:20.828556] Test:  [120/345]  eta: 0:00:41  loss: 0.1179 (0.1185)  time: 0.1833  data: 0.0001  max mem: 15824
[11:10:22.665025] Test:  [130/345]  eta: 0:00:39  loss: 0.1133 (0.1180)  time: 0.1836  data: 0.0001  max mem: 15824
[11:10:24.503551] Test:  [140/345]  eta: 0:00:37  loss: 0.1111 (0.1188)  time: 0.1837  data: 0.0001  max mem: 15824
[11:10:26.348008] Test:  [150/345]  eta: 0:00:36  loss: 0.1228 (0.1187)  time: 0.1841  data: 0.0001  max mem: 15824
[11:10:28.194142] Test:  [160/345]  eta: 0:00:34  loss: 0.1160 (0.1191)  time: 0.1845  data: 0.0001  max mem: 15824
[11:10:30.045099] Test:  [170/345]  eta: 0:00:32  loss: 0.1224 (0.1197)  time: 0.1848  data: 0.0001  max mem: 15824
[11:10:31.900060] Test:  [180/345]  eta: 0:00:30  loss: 0.1224 (0.1199)  time: 0.1852  data: 0.0001  max mem: 15824
[11:10:33.759058] Test:  [190/345]  eta: 0:00:28  loss: 0.1219 (0.1199)  time: 0.1856  data: 0.0001  max mem: 15824
[11:10:35.623016] Test:  [200/345]  eta: 0:00:26  loss: 0.1200 (0.1202)  time: 0.1860  data: 0.0001  max mem: 15824
[11:10:37.491658] Test:  [210/345]  eta: 0:00:24  loss: 0.1256 (0.1211)  time: 0.1865  data: 0.0001  max mem: 15824
[11:10:39.359203] Test:  [220/345]  eta: 0:00:23  loss: 0.1256 (0.1213)  time: 0.1867  data: 0.0001  max mem: 15824
[11:10:41.230238] Test:  [230/345]  eta: 0:00:21  loss: 0.1179 (0.1209)  time: 0.1869  data: 0.0001  max mem: 15824
[11:10:43.104914] Test:  [240/345]  eta: 0:00:19  loss: 0.1130 (0.1208)  time: 0.1872  data: 0.0001  max mem: 15824
[11:10:44.981128] Test:  [250/345]  eta: 0:00:17  loss: 0.1148 (0.1209)  time: 0.1875  data: 0.0001  max mem: 15824
[11:10:46.860476] Test:  [260/345]  eta: 0:00:15  loss: 0.1190 (0.1209)  time: 0.1877  data: 0.0001  max mem: 15824
[11:10:48.748725] Test:  [270/345]  eta: 0:00:13  loss: 0.1229 (0.1209)  time: 0.1883  data: 0.0001  max mem: 15824
[11:10:50.638857] Test:  [280/345]  eta: 0:00:12  loss: 0.1229 (0.1208)  time: 0.1889  data: 0.0001  max mem: 15824
[11:10:52.529992] Test:  [290/345]  eta: 0:00:10  loss: 0.1185 (0.1208)  time: 0.1890  data: 0.0001  max mem: 15824
[11:10:54.429334] Test:  [300/345]  eta: 0:00:08  loss: 0.1141 (0.1205)  time: 0.1895  data: 0.0001  max mem: 15824
[11:10:56.332546] Test:  [310/345]  eta: 0:00:06  loss: 0.1107 (0.1204)  time: 0.1901  data: 0.0001  max mem: 15824
[11:10:58.235196] Test:  [320/345]  eta: 0:00:04  loss: 0.1147 (0.1204)  time: 0.1902  data: 0.0001  max mem: 15824
[11:11:00.142696] Test:  [330/345]  eta: 0:00:02  loss: 0.1164 (0.1201)  time: 0.1904  data: 0.0001  max mem: 15824
[11:11:02.048996] Test:  [340/345]  eta: 0:00:00  loss: 0.1166 (0.1203)  time: 0.1906  data: 0.0001  max mem: 15824
[11:11:02.812656] Test:  [344/345]  eta: 0:00:00  loss: 0.1166 (0.1203)  time: 0.1906  data: 0.0001  max mem: 15824
[11:11:02.872626] Test: Total time: 0:01:04 (0.1867 s / it)
[11:11:13.421712] Test:  [ 0/57]  eta: 0:00:31  loss: 0.5149 (0.5149)  time: 0.5590  data: 0.3790  max mem: 15824
[11:11:15.200150] Test:  [10/57]  eta: 0:00:09  loss: 0.4594 (0.4761)  time: 0.2124  data: 0.0345  max mem: 15824
[11:11:16.982685] Test:  [20/57]  eta: 0:00:07  loss: 0.4536 (0.4639)  time: 0.1780  data: 0.0001  max mem: 15824
[11:11:18.768964] Test:  [30/57]  eta: 0:00:05  loss: 0.3016 (0.4030)  time: 0.1784  data: 0.0001  max mem: 15824
[11:11:20.559235] Test:  [40/57]  eta: 0:00:03  loss: 0.2820 (0.3780)  time: 0.1788  data: 0.0001  max mem: 15824
[11:11:22.350544] Test:  [50/57]  eta: 0:00:01  loss: 0.3299 (0.3809)  time: 0.1790  data: 0.0001  max mem: 15824
[11:11:23.323804] Test:  [56/57]  eta: 0:00:00  loss: 0.3700 (0.3984)  time: 0.1741  data: 0.0001  max mem: 15824
[11:11:23.406687] Test: Total time: 0:00:10 (0.1850 s / it)
[11:11:25.176151] Dice score of the network on the train images: 0.872642, val images: 0.771201
[11:11:25.180139] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[11:11:26.192276] Epoch: [44]  [  0/345]  eta: 0:05:48  lr: 0.000012  loss: 0.1195 (0.1195)  time: 1.0111  data: 0.3862  max mem: 15824
[11:11:38.544721] Epoch: [44]  [ 20/345]  eta: 0:03:26  lr: 0.000012  loss: 0.1206 (0.1225)  time: 0.6176  data: 0.0001  max mem: 15824
[11:11:50.945543] Epoch: [44]  [ 40/345]  eta: 0:03:11  lr: 0.000011  loss: 0.1228 (0.1233)  time: 0.6200  data: 0.0001  max mem: 15824
[11:12:03.350965] Epoch: [44]  [ 60/345]  eta: 0:02:58  lr: 0.000011  loss: 0.1227 (0.1252)  time: 0.6202  data: 0.0001  max mem: 15824
[11:12:15.772978] Epoch: [44]  [ 80/345]  eta: 0:02:45  lr: 0.000011  loss: 0.1221 (0.1249)  time: 0.6211  data: 0.0001  max mem: 15824
[11:12:28.213817] Epoch: [44]  [100/345]  eta: 0:02:32  lr: 0.000011  loss: 0.1176 (0.1246)  time: 0.6220  data: 0.0001  max mem: 15824
[11:12:40.679373] Epoch: [44]  [120/345]  eta: 0:02:20  lr: 0.000011  loss: 0.1163 (0.1234)  time: 0.6232  data: 0.0001  max mem: 15824
[11:12:53.147725] Epoch: [44]  [140/345]  eta: 0:02:07  lr: 0.000010  loss: 0.1320 (0.1243)  time: 0.6234  data: 0.0001  max mem: 15824
[11:13:05.604227] Epoch: [44]  [160/345]  eta: 0:01:55  lr: 0.000010  loss: 0.1325 (0.1252)  time: 0.6228  data: 0.0001  max mem: 15824
[11:13:18.075695] Epoch: [44]  [180/345]  eta: 0:01:42  lr: 0.000010  loss: 0.1208 (0.1253)  time: 0.6235  data: 0.0001  max mem: 15824
[11:13:30.555307] Epoch: [44]  [200/345]  eta: 0:01:30  lr: 0.000010  loss: 0.1147 (0.1251)  time: 0.6239  data: 0.0001  max mem: 15824
[11:13:43.015853] Epoch: [44]  [220/345]  eta: 0:01:17  lr: 0.000010  loss: 0.1171 (0.1246)  time: 0.6230  data: 0.0001  max mem: 15824
[11:13:55.481702] Epoch: [44]  [240/345]  eta: 0:01:05  lr: 0.000009  loss: 0.1167 (0.1245)  time: 0.6232  data: 0.0001  max mem: 15824
[11:14:07.940777] Epoch: [44]  [260/345]  eta: 0:00:53  lr: 0.000009  loss: 0.1231 (0.1248)  time: 0.6229  data: 0.0001  max mem: 15824
[11:14:20.471706] Epoch: [44]  [280/345]  eta: 0:00:40  lr: 0.000009  loss: 0.1246 (0.1252)  time: 0.6265  data: 0.0001  max mem: 15824
[11:14:32.938704] Epoch: [44]  [300/345]  eta: 0:00:28  lr: 0.000009  loss: 0.1259 (0.1256)  time: 0.6233  data: 0.0001  max mem: 15824
[11:14:45.355945] Epoch: [44]  [320/345]  eta: 0:00:15  lr: 0.000009  loss: 0.1170 (0.1255)  time: 0.6208  data: 0.0001  max mem: 15824
[11:14:57.788371] Epoch: [44]  [340/345]  eta: 0:00:03  lr: 0.000008  loss: 0.1187 (0.1253)  time: 0.6216  data: 0.0001  max mem: 15824
[11:15:00.267848] Epoch: [44]  [344/345]  eta: 0:00:00  lr: 0.000008  loss: 0.1264 (0.1254)  time: 0.6212  data: 0.0001  max mem: 15824
[11:15:00.348389] Epoch: [44] Total time: 0:03:35 (0.6237 s / it)
[11:15:00.348774] Averaged stats: lr: 0.000008  loss: 0.1264 (0.1254)
[11:15:00.938465] Test:  [  0/345]  eta: 0:03:21  loss: 0.1385 (0.1385)  time: 0.5843  data: 0.4001  max mem: 15824
[11:15:02.734354] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1116 (0.1161)  time: 0.2163  data: 0.0365  max mem: 15824
[11:15:04.531432] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1058 (0.1125)  time: 0.1796  data: 0.0001  max mem: 15824
[11:15:06.331570] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1058 (0.1137)  time: 0.1798  data: 0.0001  max mem: 15824
[11:15:08.137605] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1124 (0.1145)  time: 0.1802  data: 0.0001  max mem: 15824
[11:15:09.948022] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1137 (0.1144)  time: 0.1808  data: 0.0001  max mem: 15824
[11:15:11.757187] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1105 (0.1149)  time: 0.1809  data: 0.0001  max mem: 15824
[11:15:13.572507] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1146 (0.1152)  time: 0.1812  data: 0.0001  max mem: 15824
[11:15:15.391128] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1154 (0.1149)  time: 0.1816  data: 0.0001  max mem: 15824
[11:15:17.210774] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1187 (0.1159)  time: 0.1819  data: 0.0001  max mem: 15824
[11:15:19.034356] Test:  [100/345]  eta: 0:00:45  loss: 0.1211 (0.1167)  time: 0.1821  data: 0.0001  max mem: 15824
[11:15:20.861952] Test:  [110/345]  eta: 0:00:43  loss: 0.1233 (0.1174)  time: 0.1825  data: 0.0001  max mem: 15824
[11:15:22.695374] Test:  [120/345]  eta: 0:00:41  loss: 0.1233 (0.1183)  time: 0.1830  data: 0.0001  max mem: 15824
[11:15:24.531180] Test:  [130/345]  eta: 0:00:39  loss: 0.1121 (0.1183)  time: 0.1834  data: 0.0001  max mem: 15824
[11:15:26.372085] Test:  [140/345]  eta: 0:00:37  loss: 0.1140 (0.1183)  time: 0.1838  data: 0.0001  max mem: 15824
[11:15:28.216520] Test:  [150/345]  eta: 0:00:35  loss: 0.1164 (0.1189)  time: 0.1842  data: 0.0001  max mem: 15824
[11:15:30.065618] Test:  [160/345]  eta: 0:00:34  loss: 0.1156 (0.1190)  time: 0.1846  data: 0.0001  max mem: 15824
[11:15:31.914572] Test:  [170/345]  eta: 0:00:32  loss: 0.1156 (0.1195)  time: 0.1848  data: 0.0001  max mem: 15824
[11:15:33.767713] Test:  [180/345]  eta: 0:00:30  loss: 0.1180 (0.1193)  time: 0.1850  data: 0.0001  max mem: 15824
[11:15:35.623548] Test:  [190/345]  eta: 0:00:28  loss: 0.1159 (0.1196)  time: 0.1854  data: 0.0001  max mem: 15824
[11:15:37.483294] Test:  [200/345]  eta: 0:00:26  loss: 0.1159 (0.1195)  time: 0.1857  data: 0.0001  max mem: 15824
[11:15:39.346442] Test:  [210/345]  eta: 0:00:24  loss: 0.1112 (0.1193)  time: 0.1861  data: 0.0001  max mem: 15824
[11:15:41.212943] Test:  [220/345]  eta: 0:00:23  loss: 0.1110 (0.1191)  time: 0.1864  data: 0.0001  max mem: 15824
[11:15:43.082476] Test:  [230/345]  eta: 0:00:21  loss: 0.1136 (0.1190)  time: 0.1867  data: 0.0001  max mem: 15824
[11:15:44.960579] Test:  [240/345]  eta: 0:00:19  loss: 0.1215 (0.1195)  time: 0.1873  data: 0.0001  max mem: 15824
[11:15:46.837715] Test:  [250/345]  eta: 0:00:17  loss: 0.1248 (0.1195)  time: 0.1877  data: 0.0001  max mem: 15824
[11:15:48.718279] Test:  [260/345]  eta: 0:00:15  loss: 0.1287 (0.1200)  time: 0.1878  data: 0.0001  max mem: 15824
[11:15:50.601951] Test:  [270/345]  eta: 0:00:13  loss: 0.1133 (0.1196)  time: 0.1882  data: 0.0001  max mem: 15824
[11:15:52.489936] Test:  [280/345]  eta: 0:00:12  loss: 0.1108 (0.1195)  time: 0.1885  data: 0.0001  max mem: 15824
[11:15:54.381929] Test:  [290/345]  eta: 0:00:10  loss: 0.1101 (0.1194)  time: 0.1889  data: 0.0001  max mem: 15824
[11:15:56.280434] Test:  [300/345]  eta: 0:00:08  loss: 0.1099 (0.1191)  time: 0.1895  data: 0.0001  max mem: 15824
[11:15:58.181872] Test:  [310/345]  eta: 0:00:06  loss: 0.1092 (0.1191)  time: 0.1899  data: 0.0001  max mem: 15824
[11:16:00.085670] Test:  [320/345]  eta: 0:00:04  loss: 0.1092 (0.1189)  time: 0.1902  data: 0.0001  max mem: 15824
[11:16:01.990063] Test:  [330/345]  eta: 0:00:02  loss: 0.1111 (0.1187)  time: 0.1903  data: 0.0001  max mem: 15824
[11:16:03.895627] Test:  [340/345]  eta: 0:00:00  loss: 0.1167 (0.1189)  time: 0.1904  data: 0.0001  max mem: 15824
[11:16:04.660855] Test:  [344/345]  eta: 0:00:00  loss: 0.1167 (0.1189)  time: 0.1907  data: 0.0001  max mem: 15824
[11:16:04.731240] Test: Total time: 0:01:04 (0.1866 s / it)
[11:16:15.405876] Test:  [ 0/57]  eta: 0:00:35  loss: 0.5081 (0.5081)  time: 0.6203  data: 0.4391  max mem: 15824
[11:16:17.187734] Test:  [10/57]  eta: 0:00:10  loss: 0.4612 (0.4760)  time: 0.2183  data: 0.0400  max mem: 15824
[11:16:18.969446] Test:  [20/57]  eta: 0:00:07  loss: 0.4612 (0.4645)  time: 0.1781  data: 0.0001  max mem: 15824
[11:16:20.758608] Test:  [30/57]  eta: 0:00:05  loss: 0.3032 (0.4033)  time: 0.1785  data: 0.0001  max mem: 15824
[11:16:22.550668] Test:  [40/57]  eta: 0:00:03  loss: 0.2776 (0.3786)  time: 0.1790  data: 0.0001  max mem: 15824
[11:16:24.341828] Test:  [50/57]  eta: 0:00:01  loss: 0.3358 (0.3819)  time: 0.1791  data: 0.0001  max mem: 15824
[11:16:25.318798] Test:  [56/57]  eta: 0:00:00  loss: 0.3713 (0.3994)  time: 0.1742  data: 0.0000  max mem: 15824
[11:16:25.389851] Test: Total time: 0:00:10 (0.1860 s / it)
[11:16:27.120574] Dice score of the network on the train images: 0.874312, val images: 0.770505
[11:16:27.124630] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[11:16:28.170308] Epoch: [45]  [  0/345]  eta: 0:06:00  lr: 0.000008  loss: 0.1210 (0.1210)  time: 1.0446  data: 0.4177  max mem: 15824
[11:16:40.528631] Epoch: [45]  [ 20/345]  eta: 0:03:27  lr: 0.000008  loss: 0.1277 (0.1298)  time: 0.6179  data: 0.0001  max mem: 15824
[11:16:52.923435] Epoch: [45]  [ 40/345]  eta: 0:03:11  lr: 0.000008  loss: 0.1190 (0.1250)  time: 0.6197  data: 0.0001  max mem: 15824
[11:17:05.328348] Epoch: [45]  [ 60/345]  eta: 0:02:58  lr: 0.000008  loss: 0.1195 (0.1235)  time: 0.6202  data: 0.0001  max mem: 15824
[11:17:17.769072] Epoch: [45]  [ 80/345]  eta: 0:02:45  lr: 0.000008  loss: 0.1166 (0.1234)  time: 0.6220  data: 0.0001  max mem: 15824
[11:17:30.209434] Epoch: [45]  [100/345]  eta: 0:02:33  lr: 0.000007  loss: 0.1180 (0.1237)  time: 0.6220  data: 0.0001  max mem: 15824
[11:17:42.625217] Epoch: [45]  [120/345]  eta: 0:02:20  lr: 0.000007  loss: 0.1289 (0.1246)  time: 0.6207  data: 0.0001  max mem: 15824
[11:17:55.064304] Epoch: [45]  [140/345]  eta: 0:02:07  lr: 0.000007  loss: 0.1185 (0.1237)  time: 0.6219  data: 0.0001  max mem: 15824
[11:18:07.534399] Epoch: [45]  [160/345]  eta: 0:01:55  lr: 0.000007  loss: 0.1180 (0.1236)  time: 0.6235  data: 0.0001  max mem: 15824
[11:18:19.985972] Epoch: [45]  [180/345]  eta: 0:01:42  lr: 0.000007  loss: 0.1305 (0.1236)  time: 0.6225  data: 0.0001  max mem: 15824
[11:18:32.455850] Epoch: [45]  [200/345]  eta: 0:01:30  lr: 0.000007  loss: 0.1258 (0.1240)  time: 0.6234  data: 0.0001  max mem: 15824
[11:18:44.915569] Epoch: [45]  [220/345]  eta: 0:01:17  lr: 0.000006  loss: 0.1187 (0.1234)  time: 0.6229  data: 0.0001  max mem: 15824
[11:18:57.349460] Epoch: [45]  [240/345]  eta: 0:01:05  lr: 0.000006  loss: 0.1202 (0.1233)  time: 0.6217  data: 0.0001  max mem: 15824
[11:19:09.776009] Epoch: [45]  [260/345]  eta: 0:00:52  lr: 0.000006  loss: 0.1271 (0.1233)  time: 0.6213  data: 0.0001  max mem: 15824
[11:19:22.224150] Epoch: [45]  [280/345]  eta: 0:00:40  lr: 0.000006  loss: 0.1299 (0.1240)  time: 0.6224  data: 0.0001  max mem: 15824
[11:19:34.643682] Epoch: [45]  [300/345]  eta: 0:00:28  lr: 0.000006  loss: 0.1148 (0.1240)  time: 0.6209  data: 0.0001  max mem: 15824
[11:19:47.068875] Epoch: [45]  [320/345]  eta: 0:00:15  lr: 0.000006  loss: 0.1166 (0.1237)  time: 0.6212  data: 0.0001  max mem: 15824
[11:19:59.483170] Epoch: [45]  [340/345]  eta: 0:00:03  lr: 0.000005  loss: 0.1223 (0.1238)  time: 0.6207  data: 0.0001  max mem: 15824
[11:20:01.964852] Epoch: [45]  [344/345]  eta: 0:00:00  lr: 0.000005  loss: 0.1207 (0.1238)  time: 0.6204  data: 0.0001  max mem: 15824
[11:20:02.034586] Epoch: [45] Total time: 0:03:34 (0.6229 s / it)
[11:20:02.034809] Averaged stats: lr: 0.000005  loss: 0.1207 (0.1238)
[11:20:02.701328] Test:  [  0/345]  eta: 0:03:48  loss: 0.1351 (0.1351)  time: 0.6611  data: 0.4795  max mem: 15824
[11:20:04.495464] Test:  [ 10/345]  eta: 0:01:14  loss: 0.1161 (0.1183)  time: 0.2231  data: 0.0437  max mem: 15824
[11:20:06.292207] Test:  [ 20/345]  eta: 0:01:05  loss: 0.1124 (0.1184)  time: 0.1795  data: 0.0001  max mem: 15824
[11:20:08.092847] Test:  [ 30/345]  eta: 0:01:01  loss: 0.1146 (0.1168)  time: 0.1798  data: 0.0001  max mem: 15824
[11:20:09.893628] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1175 (0.1188)  time: 0.1800  data: 0.0001  max mem: 15824
[11:20:11.698276] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1191 (0.1194)  time: 0.1802  data: 0.0001  max mem: 15824
[11:20:13.510804] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1170 (0.1211)  time: 0.1808  data: 0.0001  max mem: 15824
[11:20:15.322344] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1146 (0.1204)  time: 0.1811  data: 0.0001  max mem: 15824
[11:20:17.138983] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1146 (0.1207)  time: 0.1813  data: 0.0001  max mem: 15824
[11:20:18.960818] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1168 (0.1204)  time: 0.1819  data: 0.0001  max mem: 15824
[11:20:20.784224] Test:  [100/345]  eta: 0:00:45  loss: 0.1154 (0.1198)  time: 0.1822  data: 0.0001  max mem: 15824
[11:20:22.611828] Test:  [110/345]  eta: 0:00:43  loss: 0.1154 (0.1197)  time: 0.1825  data: 0.0001  max mem: 15824
[11:20:24.441767] Test:  [120/345]  eta: 0:00:41  loss: 0.1138 (0.1197)  time: 0.1828  data: 0.0001  max mem: 15824
[11:20:26.277502] Test:  [130/345]  eta: 0:00:39  loss: 0.1119 (0.1192)  time: 0.1832  data: 0.0001  max mem: 15824
[11:20:28.116511] Test:  [140/345]  eta: 0:00:37  loss: 0.1067 (0.1190)  time: 0.1837  data: 0.0001  max mem: 15824
[11:20:29.957820] Test:  [150/345]  eta: 0:00:36  loss: 0.1158 (0.1185)  time: 0.1840  data: 0.0001  max mem: 15824
[11:20:31.802783] Test:  [160/345]  eta: 0:00:34  loss: 0.1160 (0.1185)  time: 0.1843  data: 0.0001  max mem: 15824
[11:20:33.651984] Test:  [170/345]  eta: 0:00:32  loss: 0.1166 (0.1185)  time: 0.1846  data: 0.0001  max mem: 15824
[11:20:35.505198] Test:  [180/345]  eta: 0:00:30  loss: 0.1136 (0.1185)  time: 0.1851  data: 0.0001  max mem: 15824
[11:20:37.361303] Test:  [190/345]  eta: 0:00:28  loss: 0.1152 (0.1182)  time: 0.1854  data: 0.0001  max mem: 15824
[11:20:39.219037] Test:  [200/345]  eta: 0:00:26  loss: 0.1097 (0.1181)  time: 0.1856  data: 0.0001  max mem: 15824
[11:20:41.081316] Test:  [210/345]  eta: 0:00:24  loss: 0.1153 (0.1181)  time: 0.1859  data: 0.0001  max mem: 15824
[11:20:42.951628] Test:  [220/345]  eta: 0:00:23  loss: 0.1173 (0.1181)  time: 0.1866  data: 0.0001  max mem: 15824
[11:20:44.820164] Test:  [230/345]  eta: 0:00:21  loss: 0.1175 (0.1183)  time: 0.1869  data: 0.0001  max mem: 15824
[11:20:46.693677] Test:  [240/345]  eta: 0:00:19  loss: 0.1189 (0.1184)  time: 0.1870  data: 0.0001  max mem: 15824
[11:20:48.571255] Test:  [250/345]  eta: 0:00:17  loss: 0.1189 (0.1185)  time: 0.1875  data: 0.0001  max mem: 15824
[11:20:50.452942] Test:  [260/345]  eta: 0:00:15  loss: 0.1141 (0.1184)  time: 0.1879  data: 0.0001  max mem: 15824
[11:20:52.338690] Test:  [270/345]  eta: 0:00:13  loss: 0.1139 (0.1183)  time: 0.1883  data: 0.0001  max mem: 15824
[11:20:54.226120] Test:  [280/345]  eta: 0:00:12  loss: 0.1164 (0.1185)  time: 0.1886  data: 0.0001  max mem: 15824
[11:20:56.118431] Test:  [290/345]  eta: 0:00:10  loss: 0.1170 (0.1183)  time: 0.1889  data: 0.0001  max mem: 15824
[11:20:58.014681] Test:  [300/345]  eta: 0:00:08  loss: 0.1122 (0.1183)  time: 0.1894  data: 0.0001  max mem: 15824
[11:20:59.912924] Test:  [310/345]  eta: 0:00:06  loss: 0.1225 (0.1184)  time: 0.1897  data: 0.0001  max mem: 15824
[11:21:01.814903] Test:  [320/345]  eta: 0:00:04  loss: 0.1172 (0.1182)  time: 0.1900  data: 0.0001  max mem: 15824
[11:21:03.719537] Test:  [330/345]  eta: 0:00:02  loss: 0.1129 (0.1181)  time: 0.1903  data: 0.0001  max mem: 15824
[11:21:05.626049] Test:  [340/345]  eta: 0:00:00  loss: 0.1151 (0.1180)  time: 0.1905  data: 0.0001  max mem: 15824
[11:21:06.389481] Test:  [344/345]  eta: 0:00:00  loss: 0.1151 (0.1180)  time: 0.1906  data: 0.0001  max mem: 15824
[11:21:06.464233] Test: Total time: 0:01:04 (0.1867 s / it)
[11:21:16.941908] Test:  [ 0/57]  eta: 0:00:31  loss: 0.5125 (0.5125)  time: 0.5514  data: 0.3703  max mem: 15824
[11:21:18.721248] Test:  [10/57]  eta: 0:00:09  loss: 0.4634 (0.4769)  time: 0.2118  data: 0.0338  max mem: 15824
[11:21:20.503299] Test:  [20/57]  eta: 0:00:07  loss: 0.4634 (0.4653)  time: 0.1780  data: 0.0001  max mem: 15824
[11:21:22.293636] Test:  [30/57]  eta: 0:00:05  loss: 0.3033 (0.4045)  time: 0.1786  data: 0.0001  max mem: 15824
[11:21:24.085432] Test:  [40/57]  eta: 0:00:03  loss: 0.2825 (0.3797)  time: 0.1790  data: 0.0001  max mem: 15824
[11:21:25.879634] Test:  [50/57]  eta: 0:00:01  loss: 0.3341 (0.3828)  time: 0.1792  data: 0.0001  max mem: 15824
[11:21:26.857174] Test:  [56/57]  eta: 0:00:00  loss: 0.3722 (0.4006)  time: 0.1744  data: 0.0000  max mem: 15824
[11:21:26.926590] Test: Total time: 0:00:10 (0.1849 s / it)
[11:21:28.661686] Dice score of the network on the train images: 0.874799, val images: 0.770080
[11:21:28.666028] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[11:21:29.666970] Epoch: [46]  [  0/345]  eta: 0:05:44  lr: 0.000005  loss: 0.1223 (0.1223)  time: 0.9998  data: 0.3744  max mem: 15824
[11:21:41.989493] Epoch: [46]  [ 20/345]  eta: 0:03:26  lr: 0.000005  loss: 0.1224 (0.1273)  time: 0.6161  data: 0.0001  max mem: 15824
[11:21:54.349300] Epoch: [46]  [ 40/345]  eta: 0:03:11  lr: 0.000005  loss: 0.1251 (0.1249)  time: 0.6179  data: 0.0001  max mem: 15824
[11:22:06.724690] Epoch: [46]  [ 60/345]  eta: 0:02:57  lr: 0.000005  loss: 0.1129 (0.1221)  time: 0.6187  data: 0.0001  max mem: 15824
[11:22:19.131708] Epoch: [46]  [ 80/345]  eta: 0:02:45  lr: 0.000005  loss: 0.1283 (0.1232)  time: 0.6203  data: 0.0001  max mem: 15824
[11:22:31.575386] Epoch: [46]  [100/345]  eta: 0:02:32  lr: 0.000005  loss: 0.1286 (0.1238)  time: 0.6221  data: 0.0001  max mem: 15824
[11:22:44.026603] Epoch: [46]  [120/345]  eta: 0:02:20  lr: 0.000005  loss: 0.1164 (0.1236)  time: 0.6225  data: 0.0001  max mem: 15824
[11:22:56.499837] Epoch: [46]  [140/345]  eta: 0:02:07  lr: 0.000004  loss: 0.1133 (0.1229)  time: 0.6236  data: 0.0001  max mem: 15824
[11:23:08.970056] Epoch: [46]  [160/345]  eta: 0:01:55  lr: 0.000004  loss: 0.1204 (0.1232)  time: 0.6235  data: 0.0001  max mem: 15824
[11:23:21.453177] Epoch: [46]  [180/345]  eta: 0:01:42  lr: 0.000004  loss: 0.1143 (0.1228)  time: 0.6241  data: 0.0001  max mem: 15824
[11:23:33.921035] Epoch: [46]  [200/345]  eta: 0:01:30  lr: 0.000004  loss: 0.1134 (0.1230)  time: 0.6233  data: 0.0001  max mem: 15824
[11:23:46.393527] Epoch: [46]  [220/345]  eta: 0:01:17  lr: 0.000004  loss: 0.1185 (0.1228)  time: 0.6236  data: 0.0001  max mem: 15824
[11:23:58.851635] Epoch: [46]  [240/345]  eta: 0:01:05  lr: 0.000004  loss: 0.1198 (0.1228)  time: 0.6229  data: 0.0001  max mem: 15824
[11:24:11.403589] Epoch: [46]  [260/345]  eta: 0:00:52  lr: 0.000004  loss: 0.1192 (0.1226)  time: 0.6275  data: 0.0001  max mem: 15824
[11:24:23.850691] Epoch: [46]  [280/345]  eta: 0:00:40  lr: 0.000003  loss: 0.1136 (0.1227)  time: 0.6223  data: 0.0001  max mem: 15824
[11:24:36.296401] Epoch: [46]  [300/345]  eta: 0:00:28  lr: 0.000003  loss: 0.1160 (0.1230)  time: 0.6222  data: 0.0001  max mem: 15824
[11:24:48.735239] Epoch: [46]  [320/345]  eta: 0:00:15  lr: 0.000003  loss: 0.1226 (0.1230)  time: 0.6219  data: 0.0001  max mem: 15824
[11:25:01.173215] Epoch: [46]  [340/345]  eta: 0:00:03  lr: 0.000003  loss: 0.1211 (0.1233)  time: 0.6219  data: 0.0001  max mem: 15824
[11:25:03.661908] Epoch: [46]  [344/345]  eta: 0:00:00  lr: 0.000003  loss: 0.1211 (0.1233)  time: 0.6217  data: 0.0001  max mem: 15824
[11:25:03.739568] Epoch: [46] Total time: 0:03:35 (0.6234 s / it)
[11:25:03.739815] Averaged stats: lr: 0.000003  loss: 0.1211 (0.1233)
[11:25:04.336718] Test:  [  0/345]  eta: 0:03:23  loss: 0.1071 (0.1071)  time: 0.5913  data: 0.4086  max mem: 15824
[11:25:06.131984] Test:  [ 10/345]  eta: 0:01:12  loss: 0.1125 (0.1214)  time: 0.2169  data: 0.0372  max mem: 15824
[11:25:07.930880] Test:  [ 20/345]  eta: 0:01:04  loss: 0.1123 (0.1176)  time: 0.1796  data: 0.0001  max mem: 15824
[11:25:09.733227] Test:  [ 30/345]  eta: 0:01:00  loss: 0.1185 (0.1196)  time: 0.1800  data: 0.0001  max mem: 15824
[11:25:11.534567] Test:  [ 40/345]  eta: 0:00:57  loss: 0.1203 (0.1188)  time: 0.1801  data: 0.0001  max mem: 15824
[11:25:13.340828] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1125 (0.1182)  time: 0.1803  data: 0.0001  max mem: 15824
[11:25:15.152425] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1104 (0.1164)  time: 0.1808  data: 0.0001  max mem: 15824
[11:25:16.964187] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1066 (0.1162)  time: 0.1811  data: 0.0001  max mem: 15824
[11:25:18.780876] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1106 (0.1154)  time: 0.1814  data: 0.0001  max mem: 15824
[11:25:20.604207] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1060 (0.1147)  time: 0.1819  data: 0.0001  max mem: 15824
[11:25:22.429016] Test:  [100/345]  eta: 0:00:45  loss: 0.1098 (0.1148)  time: 0.1823  data: 0.0001  max mem: 15824
[11:25:24.261196] Test:  [110/345]  eta: 0:00:43  loss: 0.1145 (0.1151)  time: 0.1828  data: 0.0001  max mem: 15824
[11:25:26.096305] Test:  [120/345]  eta: 0:00:41  loss: 0.1131 (0.1155)  time: 0.1833  data: 0.0001  max mem: 15824
[11:25:27.931459] Test:  [130/345]  eta: 0:00:39  loss: 0.1138 (0.1152)  time: 0.1835  data: 0.0001  max mem: 15824
[11:25:29.771679] Test:  [140/345]  eta: 0:00:37  loss: 0.1148 (0.1156)  time: 0.1837  data: 0.0001  max mem: 15824
[11:25:31.617778] Test:  [150/345]  eta: 0:00:35  loss: 0.1171 (0.1160)  time: 0.1843  data: 0.0001  max mem: 15824
[11:25:33.468328] Test:  [160/345]  eta: 0:00:34  loss: 0.1105 (0.1156)  time: 0.1848  data: 0.0001  max mem: 15824
[11:25:35.318782] Test:  [170/345]  eta: 0:00:32  loss: 0.1072 (0.1158)  time: 0.1850  data: 0.0001  max mem: 15824
[11:25:37.171420] Test:  [180/345]  eta: 0:00:30  loss: 0.1105 (0.1158)  time: 0.1851  data: 0.0001  max mem: 15824
[11:25:39.027223] Test:  [190/345]  eta: 0:00:28  loss: 0.1101 (0.1158)  time: 0.1854  data: 0.0001  max mem: 15824
[11:25:40.888743] Test:  [200/345]  eta: 0:00:26  loss: 0.1168 (0.1160)  time: 0.1858  data: 0.0001  max mem: 15824
[11:25:42.754774] Test:  [210/345]  eta: 0:00:24  loss: 0.1168 (0.1159)  time: 0.1863  data: 0.0001  max mem: 15824
[11:25:44.620812] Test:  [220/345]  eta: 0:00:23  loss: 0.1126 (0.1159)  time: 0.1865  data: 0.0001  max mem: 15824
[11:25:46.493072] Test:  [230/345]  eta: 0:00:21  loss: 0.1094 (0.1157)  time: 0.1869  data: 0.0001  max mem: 15824
[11:25:48.368056] Test:  [240/345]  eta: 0:00:19  loss: 0.1088 (0.1158)  time: 0.1873  data: 0.0001  max mem: 15824
[11:25:50.245181] Test:  [250/345]  eta: 0:00:17  loss: 0.1118 (0.1158)  time: 0.1875  data: 0.0001  max mem: 15824
[11:25:52.125922] Test:  [260/345]  eta: 0:00:15  loss: 0.1136 (0.1157)  time: 0.1878  data: 0.0001  max mem: 15824
[11:25:54.010200] Test:  [270/345]  eta: 0:00:13  loss: 0.1209 (0.1162)  time: 0.1882  data: 0.0001  max mem: 15824
[11:25:55.896205] Test:  [280/345]  eta: 0:00:12  loss: 0.1259 (0.1164)  time: 0.1885  data: 0.0001  max mem: 15824
[11:25:57.787705] Test:  [290/345]  eta: 0:00:10  loss: 0.1184 (0.1166)  time: 0.1888  data: 0.0001  max mem: 15824
[11:25:59.685899] Test:  [300/345]  eta: 0:00:08  loss: 0.1177 (0.1168)  time: 0.1894  data: 0.0001  max mem: 15824
[11:26:01.586585] Test:  [310/345]  eta: 0:00:06  loss: 0.1156 (0.1167)  time: 0.1899  data: 0.0001  max mem: 15824
[11:26:03.489197] Test:  [320/345]  eta: 0:00:04  loss: 0.1153 (0.1168)  time: 0.1901  data: 0.0001  max mem: 15824
[11:26:05.394045] Test:  [330/345]  eta: 0:00:02  loss: 0.1226 (0.1170)  time: 0.1903  data: 0.0001  max mem: 15824
[11:26:07.299589] Test:  [340/345]  eta: 0:00:00  loss: 0.1199 (0.1171)  time: 0.1905  data: 0.0001  max mem: 15824
[11:26:08.063858] Test:  [344/345]  eta: 0:00:00  loss: 0.1293 (0.1172)  time: 0.1906  data: 0.0001  max mem: 15824
[11:26:08.141421] Test: Total time: 0:01:04 (0.1867 s / it)
[11:26:18.795073] Test:  [ 0/57]  eta: 0:00:35  loss: 0.5102 (0.5102)  time: 0.6233  data: 0.4421  max mem: 15824
[11:26:20.571226] Test:  [10/57]  eta: 0:00:10  loss: 0.4615 (0.4767)  time: 0.2180  data: 0.0403  max mem: 15824
[11:26:22.352691] Test:  [20/57]  eta: 0:00:07  loss: 0.4615 (0.4657)  time: 0.1778  data: 0.0001  max mem: 15824
[11:26:24.139542] Test:  [30/57]  eta: 0:00:05  loss: 0.3048 (0.4044)  time: 0.1784  data: 0.0001  max mem: 15824
[11:26:25.934393] Test:  [40/57]  eta: 0:00:03  loss: 0.2784 (0.3794)  time: 0.1790  data: 0.0001  max mem: 15824
[11:26:27.725869] Test:  [50/57]  eta: 0:00:01  loss: 0.3355 (0.3823)  time: 0.1793  data: 0.0001  max mem: 15824
[11:26:28.702063] Test:  [56/57]  eta: 0:00:00  loss: 0.3698 (0.3997)  time: 0.1743  data: 0.0001  max mem: 15824
[11:26:28.774350] Test: Total time: 0:00:10 (0.1860 s / it)
[11:26:30.515139] Dice score of the network on the train images: 0.875542, val images: 0.770514
[11:26:30.519061] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[11:26:31.526101] Epoch: [47]  [  0/345]  eta: 0:05:47  lr: 0.000003  loss: 0.1216 (0.1216)  time: 1.0058  data: 0.3790  max mem: 15824
[11:26:43.922730] Epoch: [47]  [ 20/345]  eta: 0:03:27  lr: 0.000003  loss: 0.1175 (0.1188)  time: 0.6198  data: 0.0001  max mem: 15824
[11:26:56.330065] Epoch: [47]  [ 40/345]  eta: 0:03:11  lr: 0.000003  loss: 0.1246 (0.1233)  time: 0.6203  data: 0.0001  max mem: 15824
[11:27:08.742316] Epoch: [47]  [ 60/345]  eta: 0:02:58  lr: 0.000003  loss: 0.1229 (0.1222)  time: 0.6206  data: 0.0001  max mem: 15824
[11:27:21.176251] Epoch: [47]  [ 80/345]  eta: 0:02:45  lr: 0.000003  loss: 0.1158 (0.1221)  time: 0.6217  data: 0.0001  max mem: 15824
[11:27:33.626987] Epoch: [47]  [100/345]  eta: 0:02:33  lr: 0.000003  loss: 0.1181 (0.1218)  time: 0.6225  data: 0.0001  max mem: 15824
[11:27:46.092600] Epoch: [47]  [120/345]  eta: 0:02:20  lr: 0.000002  loss: 0.1141 (0.1208)  time: 0.6232  data: 0.0001  max mem: 15824
[11:27:58.571109] Epoch: [47]  [140/345]  eta: 0:02:08  lr: 0.000002  loss: 0.1166 (0.1206)  time: 0.6239  data: 0.0001  max mem: 15824
[11:28:11.058741] Epoch: [47]  [160/345]  eta: 0:01:55  lr: 0.000002  loss: 0.1197 (0.1206)  time: 0.6243  data: 0.0001  max mem: 15824
[11:28:23.545827] Epoch: [47]  [180/345]  eta: 0:01:43  lr: 0.000002  loss: 0.1178 (0.1211)  time: 0.6243  data: 0.0001  max mem: 15824
[11:28:36.024930] Epoch: [47]  [200/345]  eta: 0:01:30  lr: 0.000002  loss: 0.1167 (0.1211)  time: 0.6239  data: 0.0001  max mem: 15824
[11:28:48.500509] Epoch: [47]  [220/345]  eta: 0:01:18  lr: 0.000002  loss: 0.1199 (0.1212)  time: 0.6237  data: 0.0001  max mem: 15824
[11:29:00.980454] Epoch: [47]  [240/345]  eta: 0:01:05  lr: 0.000002  loss: 0.1160 (0.1211)  time: 0.6239  data: 0.0001  max mem: 15824
[11:29:13.446966] Epoch: [47]  [260/345]  eta: 0:00:53  lr: 0.000002  loss: 0.1245 (0.1215)  time: 0.6233  data: 0.0001  max mem: 15824
[11:29:25.915978] Epoch: [47]  [280/345]  eta: 0:00:40  lr: 0.000002  loss: 0.1162 (0.1217)  time: 0.6234  data: 0.0001  max mem: 15824
[11:29:38.379003] Epoch: [47]  [300/345]  eta: 0:00:28  lr: 0.000002  loss: 0.1297 (0.1223)  time: 0.6231  data: 0.0001  max mem: 15824
[11:29:50.832119] Epoch: [47]  [320/345]  eta: 0:00:15  lr: 0.000001  loss: 0.1226 (0.1224)  time: 0.6226  data: 0.0001  max mem: 15824
[11:30:03.266459] Epoch: [47]  [340/345]  eta: 0:00:03  lr: 0.000001  loss: 0.1224 (0.1225)  time: 0.6217  data: 0.0001  max mem: 15824
[11:30:05.750343] Epoch: [47]  [344/345]  eta: 0:00:00  lr: 0.000001  loss: 0.1195 (0.1225)  time: 0.6214  data: 0.0001  max mem: 15824
[11:30:05.828151] Epoch: [47] Total time: 0:03:35 (0.6241 s / it)
[11:30:05.828295] Averaged stats: lr: 0.000001  loss: 0.1195 (0.1225)
[11:30:06.498419] Test:  [  0/345]  eta: 0:03:49  loss: 0.1141 (0.1141)  time: 0.6656  data: 0.4836  max mem: 15824
[11:30:08.293133] Test:  [ 10/345]  eta: 0:01:14  loss: 0.1141 (0.1213)  time: 0.2236  data: 0.0441  max mem: 15824
[11:30:10.094371] Test:  [ 20/345]  eta: 0:01:05  loss: 0.1128 (0.1178)  time: 0.1797  data: 0.0001  max mem: 15824
[11:30:11.895205] Test:  [ 30/345]  eta: 0:01:01  loss: 0.1116 (0.1155)  time: 0.1800  data: 0.0001  max mem: 15824
[11:30:13.697258] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1132 (0.1153)  time: 0.1801  data: 0.0001  max mem: 15824
[11:30:15.504858] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1147 (0.1165)  time: 0.1804  data: 0.0001  max mem: 15824
[11:30:17.316707] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1148 (0.1156)  time: 0.1809  data: 0.0001  max mem: 15824
[11:30:19.132709] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1112 (0.1148)  time: 0.1813  data: 0.0001  max mem: 15824
[11:30:20.949585] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1116 (0.1149)  time: 0.1816  data: 0.0001  max mem: 15824
[11:30:22.770778] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1163 (0.1152)  time: 0.1818  data: 0.0001  max mem: 15824
[11:30:24.596797] Test:  [100/345]  eta: 0:00:45  loss: 0.1123 (0.1149)  time: 0.1823  data: 0.0001  max mem: 15824
[11:30:26.425996] Test:  [110/345]  eta: 0:00:43  loss: 0.1123 (0.1148)  time: 0.1827  data: 0.0001  max mem: 15824
[11:30:28.258838] Test:  [120/345]  eta: 0:00:41  loss: 0.1148 (0.1148)  time: 0.1830  data: 0.0001  max mem: 15824
[11:30:30.095605] Test:  [130/345]  eta: 0:00:39  loss: 0.1141 (0.1147)  time: 0.1834  data: 0.0001  max mem: 15824
[11:30:31.935572] Test:  [140/345]  eta: 0:00:37  loss: 0.1134 (0.1151)  time: 0.1838  data: 0.0001  max mem: 15824
[11:30:33.778195] Test:  [150/345]  eta: 0:00:36  loss: 0.1083 (0.1148)  time: 0.1841  data: 0.0001  max mem: 15824
[11:30:35.626792] Test:  [160/345]  eta: 0:00:34  loss: 0.1083 (0.1151)  time: 0.1845  data: 0.0001  max mem: 15824
[11:30:37.479892] Test:  [170/345]  eta: 0:00:32  loss: 0.1190 (0.1156)  time: 0.1850  data: 0.0001  max mem: 15824
[11:30:39.335244] Test:  [180/345]  eta: 0:00:30  loss: 0.1101 (0.1155)  time: 0.1854  data: 0.0001  max mem: 15824
[11:30:41.191206] Test:  [190/345]  eta: 0:00:28  loss: 0.1101 (0.1156)  time: 0.1855  data: 0.0001  max mem: 15824
[11:30:43.051217] Test:  [200/345]  eta: 0:00:26  loss: 0.1181 (0.1157)  time: 0.1857  data: 0.0001  max mem: 15824
[11:30:44.915768] Test:  [210/345]  eta: 0:00:24  loss: 0.1086 (0.1153)  time: 0.1862  data: 0.0001  max mem: 15824
[11:30:46.782536] Test:  [220/345]  eta: 0:00:23  loss: 0.1135 (0.1154)  time: 0.1865  data: 0.0001  max mem: 15824
[11:30:48.653753] Test:  [230/345]  eta: 0:00:21  loss: 0.1173 (0.1157)  time: 0.1868  data: 0.0001  max mem: 15824
[11:30:50.526903] Test:  [240/345]  eta: 0:00:19  loss: 0.1198 (0.1162)  time: 0.1872  data: 0.0001  max mem: 15824
[11:30:52.405082] Test:  [250/345]  eta: 0:00:17  loss: 0.1162 (0.1162)  time: 0.1875  data: 0.0001  max mem: 15824
[11:30:54.284853] Test:  [260/345]  eta: 0:00:15  loss: 0.1111 (0.1158)  time: 0.1878  data: 0.0001  max mem: 15824
[11:30:56.168811] Test:  [270/345]  eta: 0:00:13  loss: 0.1111 (0.1159)  time: 0.1881  data: 0.0001  max mem: 15824
[11:30:58.059057] Test:  [280/345]  eta: 0:00:12  loss: 0.1072 (0.1157)  time: 0.1886  data: 0.0001  max mem: 15824
[11:30:59.949580] Test:  [290/345]  eta: 0:00:10  loss: 0.1090 (0.1159)  time: 0.1890  data: 0.0001  max mem: 15824
[11:31:01.846317] Test:  [300/345]  eta: 0:00:08  loss: 0.1122 (0.1160)  time: 0.1893  data: 0.0001  max mem: 15824
[11:31:03.746996] Test:  [310/345]  eta: 0:00:06  loss: 0.1162 (0.1161)  time: 0.1898  data: 0.0001  max mem: 15824
[11:31:05.647254] Test:  [320/345]  eta: 0:00:04  loss: 0.1191 (0.1161)  time: 0.1900  data: 0.0001  max mem: 15824
[11:31:07.550847] Test:  [330/345]  eta: 0:00:02  loss: 0.1132 (0.1160)  time: 0.1901  data: 0.0001  max mem: 15824
[11:31:09.456508] Test:  [340/345]  eta: 0:00:00  loss: 0.1156 (0.1164)  time: 0.1904  data: 0.0001  max mem: 15824
[11:31:10.220315] Test:  [344/345]  eta: 0:00:00  loss: 0.1157 (0.1164)  time: 0.1905  data: 0.0001  max mem: 15824
[11:31:10.308718] Test: Total time: 0:01:04 (0.1869 s / it)
[11:31:20.766136] Test:  [ 0/57]  eta: 0:00:32  loss: 0.5134 (0.5134)  time: 0.5657  data: 0.3846  max mem: 15824
[11:31:22.545895] Test:  [10/57]  eta: 0:00:10  loss: 0.4660 (0.4794)  time: 0.2131  data: 0.0350  max mem: 15824
[11:31:24.327956] Test:  [20/57]  eta: 0:00:07  loss: 0.4660 (0.4680)  time: 0.1780  data: 0.0001  max mem: 15824
[11:31:26.113105] Test:  [30/57]  eta: 0:00:05  loss: 0.3060 (0.4063)  time: 0.1783  data: 0.0001  max mem: 15824
[11:31:27.904047] Test:  [40/57]  eta: 0:00:03  loss: 0.2774 (0.3810)  time: 0.1787  data: 0.0001  max mem: 15824
[11:31:29.694981] Test:  [50/57]  eta: 0:00:01  loss: 0.3380 (0.3843)  time: 0.1790  data: 0.0001  max mem: 15824
[11:31:30.668320] Test:  [56/57]  eta: 0:00:00  loss: 0.3716 (0.4023)  time: 0.1740  data: 0.0000  max mem: 15824
[11:31:30.741343] Test: Total time: 0:00:10 (0.1849 s / it)
[11:31:32.462648] Dice score of the network on the train images: 0.876963, val images: 0.769239
[11:31:32.466750] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[11:31:33.481647] Epoch: [48]  [  0/345]  eta: 0:05:49  lr: 0.000001  loss: 0.1060 (0.1060)  time: 1.0138  data: 0.3881  max mem: 15824
[11:31:45.849248] Epoch: [48]  [ 20/345]  eta: 0:03:27  lr: 0.000001  loss: 0.1206 (0.1179)  time: 0.6183  data: 0.0001  max mem: 15824
[11:31:58.256075] Epoch: [48]  [ 40/345]  eta: 0:03:11  lr: 0.000001  loss: 0.1247 (0.1205)  time: 0.6203  data: 0.0001  max mem: 15824
[11:32:10.664408] Epoch: [48]  [ 60/345]  eta: 0:02:58  lr: 0.000001  loss: 0.1107 (0.1188)  time: 0.6204  data: 0.0001  max mem: 15824
[11:32:23.081555] Epoch: [48]  [ 80/345]  eta: 0:02:45  lr: 0.000001  loss: 0.1178 (0.1193)  time: 0.6208  data: 0.0001  max mem: 15824
[11:32:35.529134] Epoch: [48]  [100/345]  eta: 0:02:32  lr: 0.000001  loss: 0.1224 (0.1202)  time: 0.6223  data: 0.0001  max mem: 15824
[11:32:47.970500] Epoch: [48]  [120/345]  eta: 0:02:20  lr: 0.000001  loss: 0.1149 (0.1204)  time: 0.6220  data: 0.0001  max mem: 15824
[11:33:00.428066] Epoch: [48]  [140/345]  eta: 0:02:07  lr: 0.000001  loss: 0.1239 (0.1213)  time: 0.6228  data: 0.0001  max mem: 15824
[11:33:12.911623] Epoch: [48]  [160/345]  eta: 0:01:55  lr: 0.000001  loss: 0.1146 (0.1210)  time: 0.6241  data: 0.0001  max mem: 15824
[11:33:25.512919] Epoch: [48]  [180/345]  eta: 0:01:43  lr: 0.000001  loss: 0.1189 (0.1210)  time: 0.6300  data: 0.0001  max mem: 15824
[11:33:37.995464] Epoch: [48]  [200/345]  eta: 0:01:30  lr: 0.000001  loss: 0.1116 (0.1208)  time: 0.6241  data: 0.0001  max mem: 15824
[11:33:50.486976] Epoch: [48]  [220/345]  eta: 0:01:18  lr: 0.000001  loss: 0.1163 (0.1206)  time: 0.6245  data: 0.0001  max mem: 15824
[11:34:02.970709] Epoch: [48]  [240/345]  eta: 0:01:05  lr: 0.000001  loss: 0.1141 (0.1201)  time: 0.6241  data: 0.0001  max mem: 15824
[11:34:15.442928] Epoch: [48]  [260/345]  eta: 0:00:53  lr: 0.000001  loss: 0.1223 (0.1206)  time: 0.6236  data: 0.0001  max mem: 15824
[11:34:27.908345] Epoch: [48]  [280/345]  eta: 0:00:40  lr: 0.000000  loss: 0.1212 (0.1209)  time: 0.6232  data: 0.0001  max mem: 15824
[11:34:40.358988] Epoch: [48]  [300/345]  eta: 0:00:28  lr: 0.000000  loss: 0.1198 (0.1212)  time: 0.6225  data: 0.0001  max mem: 15824
[11:34:52.805897] Epoch: [48]  [320/345]  eta: 0:00:15  lr: 0.000000  loss: 0.1234 (0.1215)  time: 0.6223  data: 0.0001  max mem: 15824
[11:35:05.229837] Epoch: [48]  [340/345]  eta: 0:00:03  lr: 0.000000  loss: 0.1353 (0.1222)  time: 0.6212  data: 0.0001  max mem: 15824
[11:35:07.716002] Epoch: [48]  [344/345]  eta: 0:00:00  lr: 0.000000  loss: 0.1362 (0.1222)  time: 0.6207  data: 0.0001  max mem: 15824
[11:35:07.777189] Epoch: [48] Total time: 0:03:35 (0.6241 s / it)
[11:35:07.777531] Averaged stats: lr: 0.000000  loss: 0.1362 (0.1222)
[11:35:08.404920] Test:  [  0/345]  eta: 0:03:34  loss: 0.0991 (0.0991)  time: 0.6221  data: 0.4399  max mem: 15824
[11:35:10.198261] Test:  [ 10/345]  eta: 0:01:13  loss: 0.1093 (0.1119)  time: 0.2195  data: 0.0401  max mem: 15824
[11:35:11.995435] Test:  [ 20/345]  eta: 0:01:05  loss: 0.1105 (0.1134)  time: 0.1795  data: 0.0001  max mem: 15824
[11:35:13.795930] Test:  [ 30/345]  eta: 0:01:01  loss: 0.1109 (0.1136)  time: 0.1798  data: 0.0001  max mem: 15824
[11:35:15.597167] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1110 (0.1137)  time: 0.1800  data: 0.0001  max mem: 15824
[11:35:17.401689] Test:  [ 50/345]  eta: 0:00:55  loss: 0.1157 (0.1144)  time: 0.1802  data: 0.0001  max mem: 15824
[11:35:19.212795] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1273 (0.1162)  time: 0.1807  data: 0.0001  max mem: 15824
[11:35:21.024171] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1239 (0.1171)  time: 0.1811  data: 0.0001  max mem: 15824
[11:35:22.842346] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1140 (0.1166)  time: 0.1814  data: 0.0001  max mem: 15824
[11:35:24.664309] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1093 (0.1162)  time: 0.1819  data: 0.0001  max mem: 15824
[11:35:26.489017] Test:  [100/345]  eta: 0:00:45  loss: 0.1096 (0.1160)  time: 0.1823  data: 0.0001  max mem: 15824
[11:35:28.318757] Test:  [110/345]  eta: 0:00:43  loss: 0.1096 (0.1153)  time: 0.1827  data: 0.0001  max mem: 15824
[11:35:30.152855] Test:  [120/345]  eta: 0:00:41  loss: 0.1112 (0.1150)  time: 0.1831  data: 0.0001  max mem: 15824
[11:35:31.988695] Test:  [130/345]  eta: 0:00:39  loss: 0.1127 (0.1147)  time: 0.1834  data: 0.0001  max mem: 15824
[11:35:33.826693] Test:  [140/345]  eta: 0:00:37  loss: 0.1161 (0.1149)  time: 0.1836  data: 0.0001  max mem: 15824
[11:35:35.670724] Test:  [150/345]  eta: 0:00:36  loss: 0.1188 (0.1153)  time: 0.1840  data: 0.0001  max mem: 15824
[11:35:37.515773] Test:  [160/345]  eta: 0:00:34  loss: 0.1118 (0.1149)  time: 0.1844  data: 0.0001  max mem: 15824
[11:35:39.364934] Test:  [170/345]  eta: 0:00:32  loss: 0.1171 (0.1153)  time: 0.1846  data: 0.0001  max mem: 15824
[11:35:41.216529] Test:  [180/345]  eta: 0:00:30  loss: 0.1227 (0.1157)  time: 0.1850  data: 0.0001  max mem: 15824
[11:35:43.072960] Test:  [190/345]  eta: 0:00:28  loss: 0.1095 (0.1154)  time: 0.1853  data: 0.0001  max mem: 15824
[11:35:44.935139] Test:  [200/345]  eta: 0:00:26  loss: 0.1090 (0.1153)  time: 0.1859  data: 0.0001  max mem: 15824
[11:35:46.800324] Test:  [210/345]  eta: 0:00:24  loss: 0.1102 (0.1151)  time: 0.1863  data: 0.0001  max mem: 15824
[11:35:48.668491] Test:  [220/345]  eta: 0:00:23  loss: 0.1145 (0.1152)  time: 0.1866  data: 0.0001  max mem: 15824
[11:35:50.540764] Test:  [230/345]  eta: 0:00:21  loss: 0.1159 (0.1157)  time: 0.1870  data: 0.0001  max mem: 15824
[11:35:52.415194] Test:  [240/345]  eta: 0:00:19  loss: 0.1178 (0.1159)  time: 0.1873  data: 0.0001  max mem: 15824
[11:35:54.291571] Test:  [250/345]  eta: 0:00:17  loss: 0.1177 (0.1158)  time: 0.1875  data: 0.0001  max mem: 15824
[11:35:56.171603] Test:  [260/345]  eta: 0:00:15  loss: 0.1132 (0.1159)  time: 0.1877  data: 0.0001  max mem: 15824
[11:35:58.054435] Test:  [270/345]  eta: 0:00:13  loss: 0.1132 (0.1159)  time: 0.1881  data: 0.0001  max mem: 15824
[11:35:59.942470] Test:  [280/345]  eta: 0:00:12  loss: 0.1068 (0.1157)  time: 0.1885  data: 0.0001  max mem: 15824
[11:36:01.832916] Test:  [290/345]  eta: 0:00:10  loss: 0.1099 (0.1158)  time: 0.1889  data: 0.0001  max mem: 15824
[11:36:03.725925] Test:  [300/345]  eta: 0:00:08  loss: 0.1192 (0.1158)  time: 0.1891  data: 0.0001  max mem: 15824
[11:36:05.624524] Test:  [310/345]  eta: 0:00:06  loss: 0.1150 (0.1159)  time: 0.1895  data: 0.0001  max mem: 15824
[11:36:07.529183] Test:  [320/345]  eta: 0:00:04  loss: 0.1141 (0.1160)  time: 0.1901  data: 0.0001  max mem: 15824
[11:36:09.437968] Test:  [330/345]  eta: 0:00:02  loss: 0.1174 (0.1160)  time: 0.1906  data: 0.0001  max mem: 15824
[11:36:11.342311] Test:  [340/345]  eta: 0:00:00  loss: 0.1108 (0.1161)  time: 0.1906  data: 0.0001  max mem: 15824
[11:36:12.106586] Test:  [344/345]  eta: 0:00:00  loss: 0.1174 (0.1163)  time: 0.1906  data: 0.0001  max mem: 15824
[11:36:12.184647] Test: Total time: 0:01:04 (0.1867 s / it)
[11:36:22.616521] Test:  [ 0/57]  eta: 0:00:31  loss: 0.5128 (0.5128)  time: 0.5551  data: 0.3746  max mem: 15824
[11:36:24.392141] Test:  [10/57]  eta: 0:00:09  loss: 0.4662 (0.4796)  time: 0.2118  data: 0.0341  max mem: 15824
[11:36:26.173364] Test:  [20/57]  eta: 0:00:07  loss: 0.4662 (0.4682)  time: 0.1778  data: 0.0001  max mem: 15824
[11:36:27.961818] Test:  [30/57]  eta: 0:00:05  loss: 0.3079 (0.4064)  time: 0.1784  data: 0.0001  max mem: 15824
[11:36:29.750555] Test:  [40/57]  eta: 0:00:03  loss: 0.2747 (0.3813)  time: 0.1788  data: 0.0001  max mem: 15824
[11:36:31.542387] Test:  [50/57]  eta: 0:00:01  loss: 0.3379 (0.3847)  time: 0.1790  data: 0.0001  max mem: 15824
[11:36:32.518502] Test:  [56/57]  eta: 0:00:00  loss: 0.3715 (0.4027)  time: 0.1741  data: 0.0001  max mem: 15824
[11:36:32.601972] Test: Total time: 0:00:10 (0.1849 s / it)
[11:36:34.307492] Dice score of the network on the train images: 0.876990, val images: 0.768749
[11:36:34.311576] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[11:36:35.367441] Epoch: [49]  [  0/345]  eta: 0:06:03  lr: 0.000000  loss: 0.1331 (0.1331)  time: 1.0548  data: 0.4293  max mem: 15824
[11:36:47.734369] Epoch: [49]  [ 20/345]  eta: 0:03:27  lr: 0.000000  loss: 0.1234 (0.1251)  time: 0.6183  data: 0.0001  max mem: 15824
[11:37:00.133655] Epoch: [49]  [ 40/345]  eta: 0:03:12  lr: 0.000000  loss: 0.1176 (0.1227)  time: 0.6199  data: 0.0001  max mem: 15824
[11:37:12.530486] Epoch: [49]  [ 60/345]  eta: 0:02:58  lr: 0.000000  loss: 0.1163 (0.1211)  time: 0.6198  data: 0.0001  max mem: 15824
[11:37:24.922860] Epoch: [49]  [ 80/345]  eta: 0:02:45  lr: 0.000000  loss: 0.1157 (0.1207)  time: 0.6196  data: 0.0001  max mem: 15824
[11:37:37.322114] Epoch: [49]  [100/345]  eta: 0:02:32  lr: 0.000000  loss: 0.1293 (0.1221)  time: 0.6199  data: 0.0001  max mem: 15824
[11:37:49.746953] Epoch: [49]  [120/345]  eta: 0:02:20  lr: 0.000000  loss: 0.1197 (0.1225)  time: 0.6212  data: 0.0001  max mem: 15824
[11:38:02.213199] Epoch: [49]  [140/345]  eta: 0:02:07  lr: 0.000000  loss: 0.1239 (0.1233)  time: 0.6233  data: 0.0001  max mem: 15824
[11:38:14.690153] Epoch: [49]  [160/345]  eta: 0:01:55  lr: 0.000000  loss: 0.1213 (0.1232)  time: 0.6238  data: 0.0001  max mem: 15824
[11:38:27.160557] Epoch: [49]  [180/345]  eta: 0:01:42  lr: 0.000000  loss: 0.1218 (0.1233)  time: 0.6235  data: 0.0001  max mem: 15824
[11:38:39.623006] Epoch: [49]  [200/345]  eta: 0:01:30  lr: 0.000000  loss: 0.1204 (0.1232)  time: 0.6231  data: 0.0001  max mem: 15824
[11:38:52.099455] Epoch: [49]  [220/345]  eta: 0:01:17  lr: 0.000000  loss: 0.1170 (0.1227)  time: 0.6238  data: 0.0001  max mem: 15824
[11:39:04.571902] Epoch: [49]  [240/345]  eta: 0:01:05  lr: 0.000000  loss: 0.1207 (0.1230)  time: 0.6236  data: 0.0001  max mem: 15824
[11:39:17.020147] Epoch: [49]  [260/345]  eta: 0:00:52  lr: 0.000000  loss: 0.1170 (0.1228)  time: 0.6224  data: 0.0001  max mem: 15824
[11:39:29.442044] Epoch: [49]  [280/345]  eta: 0:00:40  lr: 0.000000  loss: 0.1202 (0.1231)  time: 0.6211  data: 0.0001  max mem: 15824

[11:39:41.849974] Epoch: [49]  [300/345]  eta: 0:00:28  lr: 0.000000  loss: 0.1137 (0.1229)  time: 0.6204  data: 0.0001  max mem: 15824
[11:39:54.292547] Epoch: [49]  [320/345]  eta: 0:00:15  lr: 0.000000  loss: 0.1088 (0.1226)  time: 0.6221  data: 0.0001  max mem: 15824
[11:40:06.695049] Epoch: [49]  [340/345]  eta: 0:00:03  lr: 0.000000  loss: 0.1086 (0.1222)  time: 0.6201  data: 0.0001  max mem: 15824
[11:40:09.175645] Epoch: [49]  [344/345]  eta: 0:00:00  lr: 0.000000  loss: 0.1079 (0.1220)  time: 0.6198  data: 0.0001  max mem: 15824
[11:40:09.254802] Epoch: [49] Total time: 0:03:34 (0.6230 s / it)
[11:40:09.255031] Averaged stats: lr: 0.000000  loss: 0.1079 (0.1220)
[11:40:09.937149] Test:  [  0/345]  eta: 0:03:53  loss: 0.1074 (0.1074)  time: 0.6765  data: 0.4948  max mem: 15824
[11:40:11.734896] Test:  [ 10/345]  eta: 0:01:15  loss: 0.1125 (0.1231)  time: 0.2249  data: 0.0451  max mem: 15824
[11:40:13.535711] Test:  [ 20/345]  eta: 0:01:06  loss: 0.1125 (0.1197)  time: 0.1799  data: 0.0001  max mem: 15824
[11:40:15.339435] Test:  [ 30/345]  eta: 0:01:01  loss: 0.1123 (0.1181)  time: 0.1802  data: 0.0001  max mem: 15824
[11:40:17.141865] Test:  [ 40/345]  eta: 0:00:58  loss: 0.1123 (0.1165)  time: 0.1802  data: 0.0001  max mem: 15824
[11:40:18.948169] Test:  [ 50/345]  eta: 0:00:56  loss: 0.1122 (0.1160)  time: 0.1804  data: 0.0001  max mem: 15824
[11:40:20.760466] Test:  [ 60/345]  eta: 0:00:53  loss: 0.1134 (0.1160)  time: 0.1809  data: 0.0001  max mem: 15824
[11:40:22.575081] Test:  [ 70/345]  eta: 0:00:51  loss: 0.1130 (0.1157)  time: 0.1813  data: 0.0001  max mem: 15824
[11:40:24.394319] Test:  [ 80/345]  eta: 0:00:49  loss: 0.1111 (0.1160)  time: 0.1816  data: 0.0001  max mem: 15824
[11:40:26.215139] Test:  [ 90/345]  eta: 0:00:47  loss: 0.1147 (0.1167)  time: 0.1819  data: 0.0001  max mem: 15824
[11:40:28.037320] Test:  [100/345]  eta: 0:00:45  loss: 0.1096 (0.1157)  time: 0.1821  data: 0.0001  max mem: 15824
[11:40:29.864691] Test:  [110/345]  eta: 0:00:43  loss: 0.1058 (0.1156)  time: 0.1824  data: 0.0001  max mem: 15824
[11:40:31.696255] Test:  [120/345]  eta: 0:00:41  loss: 0.1126 (0.1154)  time: 0.1829  data: 0.0001  max mem: 15824
[11:40:33.529923] Test:  [130/345]  eta: 0:00:39  loss: 0.1126 (0.1153)  time: 0.1832  data: 0.0001  max mem: 15824
[11:40:35.365850] Test:  [140/345]  eta: 0:00:37  loss: 0.1084 (0.1153)  time: 0.1834  data: 0.0001  max mem: 15824
[11:40:37.206799] Test:  [150/345]  eta: 0:00:36  loss: 0.1071 (0.1148)  time: 0.1838  data: 0.0001  max mem: 15824
[11:40:39.054312] Test:  [160/345]  eta: 0:00:34  loss: 0.1062 (0.1146)  time: 0.1844  data: 0.0001  max mem: 15824
[11:40:40.902564] Test:  [170/345]  eta: 0:00:32  loss: 0.1141 (0.1151)  time: 0.1847  data: 0.0001  max mem: 15824
[11:40:42.754509] Test:  [180/345]  eta: 0:00:30  loss: 0.1142 (0.1152)  time: 0.1849  data: 0.0001  max mem: 15824
[11:40:44.607929] Test:  [190/345]  eta: 0:00:28  loss: 0.1169 (0.1156)  time: 0.1852  data: 0.0001  max mem: 15824
[11:40:46.468177] Test:  [200/345]  eta: 0:00:26  loss: 0.1184 (0.1157)  time: 0.1856  data: 0.0001  max mem: 15824
[11:40:48.333027] Test:  [210/345]  eta: 0:00:24  loss: 0.1176 (0.1159)  time: 0.1862  data: 0.0001  max mem: 15824
[11:40:50.202173] Test:  [220/345]  eta: 0:00:23  loss: 0.1169 (0.1159)  time: 0.1866  data: 0.0001  max mem: 15824
[11:40:52.074878] Test:  [230/345]  eta: 0:00:21  loss: 0.1163 (0.1160)  time: 0.1870  data: 0.0001  max mem: 15824
[11:40:53.950312] Test:  [240/345]  eta: 0:00:19  loss: 0.1149 (0.1158)  time: 0.1873  data: 0.0001  max mem: 15824
[11:40:55.829107] Test:  [250/345]  eta: 0:00:17  loss: 0.1149 (0.1161)  time: 0.1876  data: 0.0001  max mem: 15824
[11:40:57.711287] Test:  [260/345]  eta: 0:00:15  loss: 0.1137 (0.1159)  time: 0.1880  data: 0.0001  max mem: 15824
[11:40:59.597559] Test:  [270/345]  eta: 0:00:13  loss: 0.1085 (0.1158)  time: 0.1883  data: 0.0001  max mem: 15824
[11:41:01.487320] Test:  [280/345]  eta: 0:00:12  loss: 0.1211 (0.1164)  time: 0.1887  data: 0.0001  max mem: 15824
[11:41:03.380479] Test:  [290/345]  eta: 0:00:10  loss: 0.1095 (0.1161)  time: 0.1891  data: 0.0001  max mem: 15824
[11:41:05.278472] Test:  [300/345]  eta: 0:00:08  loss: 0.1084 (0.1161)  time: 0.1895  data: 0.0001  max mem: 15824
[11:41:07.178995] Test:  [310/345]  eta: 0:00:06  loss: 0.1129 (0.1163)  time: 0.1899  data: 0.0001  max mem: 15824
[11:41:09.080231] Test:  [320/345]  eta: 0:00:04  loss: 0.1216 (0.1164)  time: 0.1900  data: 0.0001  max mem: 15824
[11:41:10.984600] Test:  [330/345]  eta: 0:00:02  loss: 0.1181 (0.1164)  time: 0.1902  data: 0.0001  max mem: 15824
[11:41:12.892735] Test:  [340/345]  eta: 0:00:00  loss: 0.1181 (0.1165)  time: 0.1906  data: 0.0001  max mem: 15824
[11:41:13.656680] Test:  [344/345]  eta: 0:00:00  loss: 0.1164 (0.1164)  time: 0.1907  data: 0.0001  max mem: 15824
[11:41:13.724403] Test: Total time: 0:01:04 (0.1869 s / it)
[11:41:24.229023] Test:  [ 0/57]  eta: 0:00:33  loss: 0.5126 (0.5126)  time: 0.5839  data: 0.4035  max mem: 15824
[11:41:26.006742] Test:  [10/57]  eta: 0:00:10  loss: 0.4659 (0.4796)  time: 0.2146  data: 0.0368  max mem: 15824
[11:41:27.788787] Test:  [20/57]  eta: 0:00:07  loss: 0.4659 (0.4681)  time: 0.1779  data: 0.0001  max mem: 15824
[11:41:29.574796] Test:  [30/57]  eta: 0:00:05  loss: 0.3081 (0.4064)  time: 0.1783  data: 0.0001  max mem: 15824
[11:41:31.366196] Test:  [40/57]  eta: 0:00:03  loss: 0.2752 (0.3813)  time: 0.1788  data: 0.0001  max mem: 15824
[11:41:33.155945] Test:  [50/57]  eta: 0:00:01  loss: 0.3381 (0.3847)  time: 0.1790  data: 0.0001  max mem: 15824
[11:41:34.130062] Test:  [56/57]  eta: 0:00:00  loss: 0.3712 (0.4027)  time: 0.1740  data: 0.0000  max mem: 15824
[11:41:34.196748] Test: Total time: 0:00:10 (0.1851 s / it)
[11:41:35.952313] Dice score of the network on the train images: 0.876807, val images: 0.768771
[11:41:35.954157] Training time 4:11:37
[11:41:37.975948] Load pre-trained checkpoint from: /root/seg_framework/MS-Mamba/output_dir/mslesseg/checkpoints_0/checkpoint-best_dice_model_0.pth
[11:41:37.998353] <All keys matched successfully>
[11:41:41.144214] Test:  [  0/246]  eta: 0:12:35    time: 3.0715  data: 0.5210  max mem: 15824

[11:41:43.995073] Test:  [ 10/246]  eta: 0:02:07    time: 0.5383  data: 0.0475  max mem: 15824
[11:41:50.101115] ---------------------------------
[11:41:50.101343] Patient 1:
[11:41:50.101419]       precision: 0.3903812766075134
[11:41:50.101484]       recall: 0.5931534171104431
[11:41:50.101548]       dice_score: 0.4708649218082428
[11:41:50.104612] Test:  [ 20/246]  eta: 0:02:09    time: 0.4480  data: 0.0001  max mem: 15824
[11:41:52.948696] Test:  [ 30/246]  eta: 0:01:43    time: 0.4476  data: 0.0001  max mem: 15824
[11:41:58.968034] ---------------------------------
[11:41:58.968246] Patient 2:
[11:41:58.968324]       precision: 0.48446744680404663
[11:41:58.968387]       recall: 0.6612821817398071
[11:41:58.968445]       dice_score: 0.5592315793037415
[11:41:58.969004] Test:  [ 40/246]  eta: 0:01:44    time: 0.4432  data: 0.0001  max mem: 15824
[11:42:01.812928] Test:  [ 50/246]  eta: 0:01:31    time: 0.4432  data: 0.0001  max mem: 15824
[11:42:04.666181] Test:  [ 60/246]  eta: 0:01:21    time: 0.2848  data: 0.0001  max mem: 15824
[11:42:08.101388] ---------------------------------
[11:42:08.101611] Patient 3:
[11:42:08.101690]       precision: 0.2786659300327301
[11:42:08.101754]       recall: 0.5545803904533386
[11:42:08.101811]       dice_score: 0.3709411025047302
[11:42:10.673932] Test:  [ 70/246]  eta: 0:01:20    time: 0.4430  data: 0.0001  max mem: 15824
[11:42:13.526103] Test:  [ 80/246]  eta: 0:01:12    time: 0.4429  data: 0.0001  max mem: 15824
[11:42:17.033968] ---------------------------------
[11:42:17.034195] Patient 4:
[11:42:17.034284]       precision: 0.5457660555839539
[11:42:17.034347]       recall: 0.5628470182418823
[11:42:17.034408]       dice_score: 0.5541749596595764
[11:42:19.591973] Test:  [ 90/246]  eta: 0:01:11    time: 0.4458  data: 0.0001  max mem: 15824
[11:42:22.443469] Test:  [100/246]  eta: 0:01:04    time: 0.4458  data: 0.0001  max mem: 15824
[11:42:26.188999] ---------------------------------
[11:42:26.189203] Patient 5:
[11:42:26.189299]       precision: 0.30039656162261963
[11:42:26.189365]       recall: 0.4986286461353302
[11:42:26.189426]       dice_score: 0.37492266297340393
[11:42:28.464975] Test:  [110/246]  eta: 0:01:01    time: 0.4436  data: 0.0001  max mem: 15824
[11:42:31.320691] Test:  [120/246]  eta: 0:00:55    time: 0.4438  data: 0.0001  max mem: 15824
[11:42:35.058070] ---------------------------------
[11:42:35.058303] Patient 6:
[11:42:35.058386]       precision: 0.3170126974582672
[11:42:35.058450]       recall: 0.5611628890037537
[11:42:35.058510]       dice_score: 0.4051485061645508
[11:42:37.336658] Test:  [130/246]  eta: 0:00:52    time: 0.4435  data: 0.0001  max mem: 15824
[11:42:40.192237] Test:  [140/246]  eta: 0:00:46    time: 0.4435  data: 0.0001  max mem: 15824
[11:42:44.239101] ---------------------------------
[11:42:44.239327] Patient 7:
[11:42:44.239403]       precision: 0.7532758712768555
[11:42:44.239465]       recall: 0.8117306232452393
[11:42:44.239521]       dice_score: 0.7814115881919861
[11:42:46.231264] Test:  [150/246]  eta: 0:00:43    time: 0.4447  data: 0.0001  max mem: 15824
[11:42:49.082821] Test:  [160/246]  eta: 0:00:37    time: 0.4445  data: 0.0001  max mem: 15824
[11:42:53.112593] ---------------------------------
[11:42:53.112820] Patient 8:
[11:42:53.112899]       precision: 0.8290913701057434
[11:42:53.112964]       recall: 0.5873554944992065
[11:42:53.113022]       dice_score: 0.6875956654548645
[11:42:55.100949] Test:  [170/246]  eta: 0:00:34    time: 0.4434  data: 0.0001  max mem: 15824
[11:42:57.951879] Test:  [180/246]  eta: 0:00:29    time: 0.4434  data: 0.0001  max mem: 15824
[11:43:01.976415] ---------------------------------
[11:43:01.976639] Patient 9:
[11:43:01.976718]       precision: 0.7076939344406128
[11:43:01.976784]       recall: 0.8147653341293335
[11:43:01.976845]       dice_score: 0.757464587688446
[11:43:03.968193] Test:  [190/246]  eta: 0:00:25    time: 0.4433  data: 0.0001  max mem: 15824
[11:43:06.821840] Test:  [200/246]  eta: 0:00:20    time: 0.4434  data: 0.0001  max mem: 15824
[11:43:11.132738] ---------------------------------
[11:43:11.132950] Patient 10:
[11:43:11.133029]       precision: 0.6339815258979797
[11:43:11.133092]       recall: 0.8828243613243103
[11:43:11.133152]       dice_score: 0.7379907369613647
[11:43:12.840963] Test:  [210/246]  eta: 0:00:16    time: 0.4436  data: 0.0001  max mem: 15824
[11:43:15.694051] Test:  [220/246]  eta: 0:00:11    time: 0.4436  data: 0.0001  max mem: 15824
[11:43:20.012262] ---------------------------------
[11:43:20.012483] Patient 11:
[11:43:20.012559]       precision: 0.8748537302017212
[11:43:20.012621]       recall: 0.7562236785888672
[11:43:20.012679]       dice_score: 0.811224639415741
[11:43:21.718321] Test:  [230/246]  eta: 0:00:07    time: 0.4438  data: 0.0001  max mem: 15824
[11:43:24.563780] Test:  [240/246]  eta: 0:00:02    time: 0.4434  data: 0.0001  max mem: 15824
[11:43:29.761762] ---------------------------------
[11:43:29.761980] Patient 12:
[11:43:29.762057]       precision: 0.5972080230712891
[11:43:29.762118]       recall: 0.790802538394928
[11:43:29.762178]       dice_score: 0.6805043816566467
[11:43:29.767774] Test:  [245/246]  eta: 0:00:00    time: 0.4737  data: 0.0000  max mem: 15824
[11:43:29.854409] Test: Total time: 0:01:51 (0.4544 s / it)
[11:43:29.854611] ================================
[11:43:29.854674] Averaged over all patients:
[11:43:29.854975]       precision: 0.5594 ± 0.1995
[11:43:29.855159]       recall: 0.6729 ± 0.1249
[11:43:29.855310]       dice_score: 0.5993 ± 0.1573
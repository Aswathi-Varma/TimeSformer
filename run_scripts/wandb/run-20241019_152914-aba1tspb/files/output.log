Not using distributed mode
[15:29:16.589102] job dir: /root/seg_framework/MS-Mamba/run_scripts
[15:29:16.589247] Namespace(accum_iter=1,
model='SegFormer3D',
in_channels=1,
lr=None,
blr=0.001,
min_lr=0,
dist_on_itp=False,
warmup_epochs=20,
device='cuda:0',
seed=42,
layer_decay=0.75,
clip_grad=None,
num_workers=8,
pin_mem=True,
resume='',
mask_mode='concatenate to image',
world_size=1,
embed_dim=1,
local_rank=-1,
dist_url='env://',
nb_classes=2,
data_dir='/root/MSLesSeg24/data',
datalist=None,
preprocess=True,
dim=2,
distributed=False)
[15:29:16.590098] Starting for fold 0
[15:29:16.592272] Preprocessing files...
[15:29:16.592360] Processing patient P9
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/fold_0/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/fold_0/val_ft
[15:29:40.236804] Processing patient P11
[15:29:56.106590] Processing patient P28
[15:30:11.760066] Processing patient P20
[15:30:35.190722] Processing patient P10
[15:30:50.877945] Processing patient P49
[15:31:06.522094] Processing patient P33
[15:31:22.342374] Processing patient P19
[15:31:53.600137] Processing patient P22
[15:32:09.255899] Processing patient P14
[15:32:40.781529] Processing patient P1
[15:33:04.255612] Processing patient P12
[15:33:35.589610] Processing patient P53
[15:33:51.394657] Processing patient P5
[15:34:07.099682] Processing patient P52
[15:34:22.868583] Processing patient P2
[15:34:54.042948] Files preprocessed.
[15:34:54.232471] Elements in data_dir_paths: 11052
[15:34:54.234377] Preprocessing files...
[15:34:54.234499] Processing patient P4
[15:35:17.587036] Processing patient P50

[15:35:33.241407] Processing patient P31
[15:35:49.372923] Processing patient P13
[15:36:05.062922] Files preprocessed.
[15:36:05.097010] Elements in data_dir_paths: 1803
[15:36:05.097224] Preprocessing files...
[15:36:05.097288] Processing patient P3
[15:36:36.455789] Processing patient P51
[15:36:52.382552] Processing patient P7
[15:37:08.077629] Processing patient P8
[15:37:23.727052] Processing patient P6
[15:37:47.186786] Files preprocessed.
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[15:37:49.162251] number of params: 59620439
[15:37:49.162488] model: Vivim2D(
  (encoder): mamba_block(
    (downsample_layers): SegformerEncoder(
      (patch_embeddings): ModuleList(
        (0): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (3): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (block): ModuleList(
        (0): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): Identity()
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.003703703870996833)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.007407407741993666)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.011111111380159855)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.014814815483987331)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.018518518656492233)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.02222222276031971)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.025925926864147186)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.029629630967974663)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03333333507180214)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03703703731298447)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04074074327945709)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04444444552063942)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.048148151487112045)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.051851850003004074)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0555555559694767)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.05925925821065903)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06296296417713165)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06666667014360428)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07037036865949631)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07407407462596893)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07777778059244156)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08148147910833359)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08518518507480621)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08888889104127884)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.09259259700775146)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0962962955236435)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.10000000149011612)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (layer_norm): ModuleList(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (2): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (3): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (decoder): SegformerDecodeHead(
    (linear_c): ModuleList(
      (0): SegformerMLP(
        (proj): Linear(in_features=64, out_features=768, bias=True)
      )
      (1): SegformerMLP(
        (proj): Linear(in_features=128, out_features=768, bias=True)
      )
      (2): SegformerMLP(
        (proj): Linear(in_features=320, out_features=768, bias=True)
      )
      (3): SegformerMLP(
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
    )
    (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Conv2d(768, 150, kernel_size=(1, 1), stride=(1, 1))
  )
  (out): Conv2d(768, 1, kernel_size=(1, 1), stride=(1, 1))
)
[15:37:49.165491] base lr: 1.00e-03
[15:37:49.165549] actual lr: 1.25e-04
[15:37:49.165598] accumulate grad iterations: 1
[15:37:49.165649] effective batch size: 32
[15:37:49.167077] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.000125
    maximize: False
    weight_decay: 0.01
)
[15:37:49.169176] Start training for 50 epochs
[15:37:49.169273] Number of samples in train dataloader:  345
[15:37:49.171032] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[15:37:59.074162] Epoch: [0]  [  0/345]  eta: 0:56:56  lr: 0.000000  loss: 1.7560 (1.7560)  time: 9.9021  data: 0.2984  max mem: 15821
[15:38:10.708880] Epoch: [0]  [ 20/345]  eta: 0:05:33  lr: 0.000000  loss: 1.7554 (1.7560)  time: 0.5817  data: 0.0001  max mem: 15821
[15:38:22.427797] Epoch: [0]  [ 40/345]  eta: 0:04:07  lr: 0.000001  loss: 1.7397 (1.7483)  time: 0.5859  data: 0.0001  max mem: 15821
[15:38:34.225001] Epoch: [0]  [ 60/345]  eta: 0:03:30  lr: 0.000001  loss: 1.7334 (1.7437)  time: 0.5898  data: 0.0001  max mem: 15821
[15:38:46.091305] Epoch: [0]  [ 80/345]  eta: 0:03:06  lr: 0.000001  loss: 1.7277 (1.7400)  time: 0.5933  data: 0.0001  max mem: 15821
[15:38:58.020170] Epoch: [0]  [100/345]  eta: 0:02:46  lr: 0.000002  loss: 1.7063 (1.7339)  time: 0.5964  data: 0.0001  max mem: 15821
[15:39:09.973535] Epoch: [0]  [120/345]  eta: 0:02:30  lr: 0.000002  loss: 1.6928 (1.7269)  time: 0.5976  data: 0.0001  max mem: 15821
[15:39:21.964014] Epoch: [0]  [140/345]  eta: 0:02:14  lr: 0.000003  loss: 1.6674 (1.7186)  time: 0.5995  data: 0.0001  max mem: 15821
[15:39:33.966320] Epoch: [0]  [160/345]  eta: 0:02:00  lr: 0.000003  loss: 1.6377 (1.7087)  time: 0.6001  data: 0.0001  max mem: 15821
[15:39:45.994410] Epoch: [0]  [180/345]  eta: 0:01:46  lr: 0.000003  loss: 1.6056 (1.6972)  time: 0.6014  data: 0.0001  max mem: 15821
[15:39:58.051434] Epoch: [0]  [200/345]  eta: 0:01:32  lr: 0.000004  loss: 1.5544 (1.6831)  time: 0.6028  data: 0.0001  max mem: 15821
[15:40:10.115713] Epoch: [0]  [220/345]  eta: 0:01:19  lr: 0.000004  loss: 1.4982 (1.6668)  time: 0.6032  data: 0.0001  max mem: 15821
[15:40:22.188370] Epoch: [0]  [240/345]  eta: 0:01:06  lr: 0.000004  loss: 1.4323 (1.6477)  time: 0.6036  data: 0.0001  max mem: 15821
[15:40:34.255117] Epoch: [0]  [260/345]  eta: 0:00:53  lr: 0.000005  loss: 1.3519 (1.6256)  time: 0.6033  data: 0.0001  max mem: 15821
[15:40:46.315016] Epoch: [0]  [280/345]  eta: 0:00:40  lr: 0.000005  loss: 1.2943 (1.6018)  time: 0.6029  data: 0.0001  max mem: 15821
[15:40:58.382393] Epoch: [0]  [300/345]  eta: 0:00:28  lr: 0.000005  loss: 1.2342 (1.5776)  time: 0.6033  data: 0.0001  max mem: 15821
[15:41:10.558298] Epoch: [0]  [320/345]  eta: 0:00:15  lr: 0.000006  loss: 1.1928 (1.5539)  time: 0.6087  data: 0.0001  max mem: 15821
[15:41:22.662008] Epoch: [0]  [340/345]  eta: 0:00:03  lr: 0.000006  loss: 1.1657 (1.5310)  time: 0.6051  data: 0.0001  max mem: 15821
[15:41:25.079884] Epoch: [0]  [344/345]  eta: 0:00:00  lr: 0.000006  loss: 1.1610 (1.5266)  time: 0.6050  data: 0.0001  max mem: 15821
[15:41:25.149398] Epoch: [0] Total time: 0:03:35 (0.6260 s / it)
[15:41:25.149760] Averaged stats: lr: 0.000006  loss: 1.1610 (1.5266)
[15:41:25.656912] Test:  [  0/345]  eta: 0:02:52  loss: 1.1594 (1.1594)  time: 0.5007  data: 0.3350  max mem: 15821
[15:41:27.326687] Test:  [ 10/345]  eta: 0:01:06  loss: 1.1616 (1.1611)  time: 0.1972  data: 0.0306  max mem: 15821
[15:41:28.999423] Test:  [ 20/345]  eta: 0:00:59  loss: 1.1616 (1.1615)  time: 0.1670  data: 0.0001  max mem: 15821
[15:41:30.673919] Test:  [ 30/345]  eta: 0:00:56  loss: 1.1606 (1.1611)  time: 0.1673  data: 0.0001  max mem: 15821
[15:41:32.353960] Test:  [ 40/345]  eta: 0:00:53  loss: 1.1606 (1.1612)  time: 0.1677  data: 0.0001  max mem: 15821
[15:41:34.037799] Test:  [ 50/345]  eta: 0:00:51  loss: 1.1608 (1.1611)  time: 0.1681  data: 0.0001  max mem: 15821
[15:41:35.723407] Test:  [ 60/345]  eta: 0:00:49  loss: 1.1619 (1.1615)  time: 0.1684  data: 0.0001  max mem: 15821
[15:41:37.412464] Test:  [ 70/345]  eta: 0:00:47  loss: 1.1619 (1.1614)  time: 0.1687  data: 0.0001  max mem: 15821
[15:41:39.106095] Test:  [ 80/345]  eta: 0:00:45  loss: 1.1611 (1.1614)  time: 0.1691  data: 0.0001  max mem: 15821
[15:41:40.803192] Test:  [ 90/345]  eta: 0:00:43  loss: 1.1610 (1.1614)  time: 0.1695  data: 0.0001  max mem: 15821
[15:41:42.503043] Test:  [100/345]  eta: 0:00:42  loss: 1.1609 (1.1616)  time: 0.1698  data: 0.0001  max mem: 15821
[15:41:44.204898] Test:  [110/345]  eta: 0:00:40  loss: 1.1624 (1.1616)  time: 0.1700  data: 0.0001  max mem: 15821
[15:41:45.912295] Test:  [120/345]  eta: 0:00:38  loss: 1.1618 (1.1615)  time: 0.1704  data: 0.0001  max mem: 15821
[15:41:47.623259] Test:  [130/345]  eta: 0:00:36  loss: 1.1621 (1.1616)  time: 0.1709  data: 0.0001  max mem: 15821
[15:41:49.336606] Test:  [140/345]  eta: 0:00:35  loss: 1.1625 (1.1617)  time: 0.1712  data: 0.0001  max mem: 15821
[15:41:51.052569] Test:  [150/345]  eta: 0:00:33  loss: 1.1622 (1.1616)  time: 0.1714  data: 0.0001  max mem: 15821
[15:41:52.773155] Test:  [160/345]  eta: 0:00:31  loss: 1.1621 (1.1616)  time: 0.1718  data: 0.0001  max mem: 15821
[15:41:54.495830] Test:  [170/345]  eta: 0:00:30  loss: 1.1613 (1.1616)  time: 0.1721  data: 0.0001  max mem: 15821
[15:41:56.221862] Test:  [180/345]  eta: 0:00:28  loss: 1.1613 (1.1616)  time: 0.1724  data: 0.0001  max mem: 15821
[15:41:57.953754] Test:  [190/345]  eta: 0:00:26  loss: 1.1610 (1.1616)  time: 0.1728  data: 0.0001  max mem: 15821
[15:41:59.688028] Test:  [200/345]  eta: 0:00:24  loss: 1.1603 (1.1616)  time: 0.1732  data: 0.0001  max mem: 15821
[15:42:03.564227] Test:  [220/345]  eta: 0:00:21  loss: 1.1603 (1.1614)  time: 0.1937  data: 0.0001  max mem: 15821
[15:42:05.445241] Test:  [230/345]  eta: 0:00:20  loss: 1.1616 (1.1614)  time: 0.1816  data: 0.0001  max mem: 15821
[15:42:07.196864] Test:  [240/345]  eta: 0:00:18  loss: 1.1616 (1.1614)  time: 0.1816  data: 0.0001  max mem: 15821
[15:42:09.180266] Test:  [250/345]  eta: 0:00:16  loss: 1.1603 (1.1614)  time: 0.1867  data: 0.0001  max mem: 15821
[15:42:11.173486] Test:  [260/345]  eta: 0:00:14  loss: 1.1611 (1.1614)  time: 0.1988  data: 0.0001  max mem: 15821
[15:42:13.081193] Test:  [270/345]  eta: 0:00:13  loss: 1.1620 (1.1615)  time: 0.1950  data: 0.0001  max mem: 15821
[15:42:14.868916] Test:  [280/345]  eta: 0:00:11  loss: 1.1629 (1.1615)  time: 0.1847  data: 0.0001  max mem: 15821
[15:42:16.820790] Test:  [290/345]  eta: 0:00:09  loss: 1.1617 (1.1616)  time: 0.1869  data: 0.0001  max mem: 15821
[15:42:18.841855] Test:  [300/345]  eta: 0:00:08  loss: 1.1613 (1.1616)  time: 0.1986  data: 0.0001  max mem: 15821
[15:42:20.837595] Test:  [310/345]  eta: 0:00:06  loss: 1.1618 (1.1616)  time: 0.2008  data: 0.0001  max mem: 15821
[15:42:22.854429] Test:  [320/345]  eta: 0:00:04  loss: 1.1616 (1.1616)  time: 0.2006  data: 0.0001  max mem: 15821
[15:42:24.867005] Test:  [330/345]  eta: 0:00:02  loss: 1.1618 (1.1616)  time: 0.2014  data: 0.0001  max mem: 15821
[15:42:26.670751] Test:  [340/345]  eta: 0:00:00  loss: 1.1615 (1.1616)  time: 0.1907  data: 0.0001  max mem: 15821
[15:42:27.709303] Test:  [344/345]  eta: 0:00:00  loss: 1.1618 (1.1616)  time: 0.2061  data: 0.0001  max mem: 15821
[15:42:27.773149] Test: Total time: 0:01:02 (0.1815 s / it)
[15:42:38.309364] Test:  [ 0/57]  eta: 0:00:27  loss: 1.1738 (1.1738)  time: 0.4848  data: 0.3230  max mem: 15821
[15:42:39.955862] Test:  [10/57]  eta: 0:00:09  loss: 1.1683 (1.1660)  time: 0.1937  data: 0.0294  max mem: 15821
[15:42:41.607232] Test:  [20/57]  eta: 0:00:06  loss: 1.1643 (1.1637)  time: 0.1648  data: 0.0001  max mem: 15821
[15:42:43.262448] Test:  [30/57]  eta: 0:00:04  loss: 1.1618 (1.1580)  time: 0.1653  data: 0.0001  max mem: 15821
[15:42:44.923468] Test:  [40/57]  eta: 0:00:02  loss: 1.1449 (1.1544)  time: 0.1658  data: 0.0001  max mem: 15821
[15:42:46.588019] Test:  [50/57]  eta: 0:00:01  loss: 1.1415 (1.1528)  time: 0.1662  data: 0.0001  max mem: 15821
[15:42:48.271155] Test:  [56/57]  eta: 0:00:00  loss: 1.1491 (1.1525)  time: 0.2006  data: 0.0000  max mem: 15821
[15:42:48.325409] Test: Total time: 0:00:10 (0.1842 s / it)
[15:42:50.042236] Dice score of the network on the train images: 0.000000, val images: 0.000000
[15:42:50.042490] saving best_dice_model_0 @ epoch 0
[15:42:50.751197] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[15:42:51.692314] Epoch: [1]  [  0/345]  eta: 0:05:24  lr: 0.000006  loss: 1.1488 (1.1488)  time: 0.9399  data: 0.3380  max mem: 15821
[15:43:03.688439] Epoch: [1]  [ 20/345]  eta: 0:03:20  lr: 0.000007  loss: 1.1355 (1.1356)  time: 0.5998  data: 0.0001  max mem: 15821
[15:43:15.713440] Epoch: [1]  [ 40/345]  eta: 0:03:05  lr: 0.000007  loss: 1.1143 (1.1253)  time: 0.6012  data: 0.0001  max mem: 15821
[15:43:27.747274] Epoch: [1]  [ 60/345]  eta: 0:02:52  lr: 0.000007  loss: 1.0983 (1.1165)  time: 0.6016  data: 0.0001  max mem: 15821
[15:43:39.806860] Epoch: [1]  [ 80/345]  eta: 0:02:40  lr: 0.000008  loss: 1.0853 (1.1086)  time: 0.6029  data: 0.0001  max mem: 15821
[15:43:51.885339] Epoch: [1]  [100/345]  eta: 0:02:28  lr: 0.000008  loss: 1.0753 (1.1017)  time: 0.6039  data: 0.0001  max mem: 15821
[15:44:03.995529] Epoch: [1]  [120/345]  eta: 0:02:16  lr: 0.000008  loss: 1.0639 (1.0955)  time: 0.6055  data: 0.0001  max mem: 15821
[15:44:16.101365] Epoch: [1]  [140/345]  eta: 0:02:04  lr: 0.000009  loss: 1.0556 (1.0901)  time: 0.6052  data: 0.0001  max mem: 15821
[15:44:28.198793] Epoch: [1]  [160/345]  eta: 0:01:51  lr: 0.000009  loss: 1.0494 (1.0851)  time: 0.6048  data: 0.0001  max mem: 15821
[15:44:40.286665] Epoch: [1]  [180/345]  eta: 0:01:39  lr: 0.000010  loss: 1.0466 (1.0806)  time: 0.6043  data: 0.0001  max mem: 15821
[15:44:52.400469] Epoch: [1]  [200/345]  eta: 0:01:27  lr: 0.000010  loss: 1.0259 (1.0753)  time: 0.6056  data: 0.0001  max mem: 15821
[15:45:04.490960] Epoch: [1]  [220/345]  eta: 0:01:15  lr: 0.000010  loss: 1.0200 (1.0703)  time: 0.6045  data: 0.0001  max mem: 15821
[15:45:16.561417] Epoch: [1]  [240/345]  eta: 0:01:03  lr: 0.000011  loss: 1.0107 (1.0654)  time: 0.6035  data: 0.0001  max mem: 15821
[15:45:28.647033] Epoch: [1]  [260/345]  eta: 0:00:51  lr: 0.000011  loss: 0.9981 (1.0605)  time: 0.6042  data: 0.0001  max mem: 15821
[15:45:40.724202] Epoch: [1]  [280/345]  eta: 0:00:39  lr: 0.000011  loss: 0.9927 (1.0554)  time: 0.6038  data: 0.0001  max mem: 15821
[15:45:52.787817] Epoch: [1]  [300/345]  eta: 0:00:27  lr: 0.000012  loss: 0.9827 (1.0509)  time: 0.6031  data: 0.0001  max mem: 15821
[15:46:04.869622] Epoch: [1]  [320/345]  eta: 0:00:15  lr: 0.000012  loss: 0.9815 (1.0465)  time: 0.6040  data: 0.0001  max mem: 15821
[15:46:16.948080] Epoch: [1]  [340/345]  eta: 0:00:03  lr: 0.000012  loss: 0.9780 (1.0425)  time: 0.6039  data: 0.0001  max mem: 15821
[15:46:19.367949] Epoch: [1]  [344/345]  eta: 0:00:00  lr: 0.000012  loss: 0.9795 (1.0418)  time: 0.6039  data: 0.0001  max mem: 15821
[15:46:19.433828] Epoch: [1] Total time: 0:03:28 (0.6049 s / it)
[15:46:19.434513] Averaged stats: lr: 0.000012  loss: 0.9795 (1.0418)
[15:46:19.925232] Test:  [  0/345]  eta: 0:02:47  loss: 0.9507 (0.9507)  time: 0.4848  data: 0.3207  max mem: 15821
[15:46:21.593026] Test:  [ 10/345]  eta: 0:01:05  loss: 0.9507 (0.9498)  time: 0.1956  data: 0.0293  max mem: 15821
[15:46:23.263009] Test:  [ 20/345]  eta: 0:00:59  loss: 0.9551 (0.9566)  time: 0.1668  data: 0.0001  max mem: 15821
[15:46:24.934976] Test:  [ 30/345]  eta: 0:00:55  loss: 0.9616 (0.9613)  time: 0.1670  data: 0.0001  max mem: 15821
[15:46:26.612635] Test:  [ 40/345]  eta: 0:00:53  loss: 0.9632 (0.9625)  time: 0.1674  data: 0.0001  max mem: 15821
[15:46:28.292561] Test:  [ 50/345]  eta: 0:00:51  loss: 0.9728 (0.9659)  time: 0.1678  data: 0.0001  max mem: 15821
[15:46:29.976691] Test:  [ 60/345]  eta: 0:00:49  loss: 0.9703 (0.9649)  time: 0.1681  data: 0.0001  max mem: 15821
[15:46:31.663483] Test:  [ 70/345]  eta: 0:00:47  loss: 0.9445 (0.9626)  time: 0.1685  data: 0.0001  max mem: 15821
[15:46:33.354142] Test:  [ 80/345]  eta: 0:00:45  loss: 0.9532 (0.9628)  time: 0.1688  data: 0.0001  max mem: 15821
[15:46:35.048366] Test:  [ 90/345]  eta: 0:00:43  loss: 0.9612 (0.9622)  time: 0.1692  data: 0.0001  max mem: 15821
[15:46:36.745958] Test:  [100/345]  eta: 0:00:41  loss: 0.9541 (0.9612)  time: 0.1695  data: 0.0001  max mem: 15821
[15:46:38.446246] Test:  [110/345]  eta: 0:00:40  loss: 0.9566 (0.9616)  time: 0.1698  data: 0.0001  max mem: 15821
[15:46:40.150411] Test:  [120/345]  eta: 0:00:38  loss: 0.9587 (0.9615)  time: 0.1702  data: 0.0001  max mem: 15821
[15:46:41.859124] Test:  [130/345]  eta: 0:00:36  loss: 0.9576 (0.9610)  time: 0.1706  data: 0.0001  max mem: 15821
[15:46:43.570031] Test:  [140/345]  eta: 0:00:35  loss: 0.9576 (0.9611)  time: 0.1709  data: 0.0001  max mem: 15821
[15:46:45.283859] Test:  [150/345]  eta: 0:00:33  loss: 0.9647 (0.9621)  time: 0.1712  data: 0.0001  max mem: 15821
[15:46:47.002059] Test:  [160/345]  eta: 0:00:31  loss: 0.9847 (0.9632)  time: 0.1715  data: 0.0001  max mem: 15821
[15:46:48.722945] Test:  [170/345]  eta: 0:00:29  loss: 0.9684 (0.9627)  time: 0.1719  data: 0.0001  max mem: 15821
[15:46:50.446728] Test:  [180/345]  eta: 0:00:28  loss: 0.9660 (0.9628)  time: 0.1722  data: 0.0001  max mem: 15821
[15:46:52.174043] Test:  [190/345]  eta: 0:00:26  loss: 0.9542 (0.9625)  time: 0.1725  data: 0.0001  max mem: 15821
[15:46:53.908007] Test:  [200/345]  eta: 0:00:24  loss: 0.9491 (0.9625)  time: 0.1730  data: 0.0001  max mem: 15821
[15:46:55.642626] Test:  [210/345]  eta: 0:00:23  loss: 0.9533 (0.9624)  time: 0.1734  data: 0.0001  max mem: 15821
[15:46:57.744838] Test:  [220/345]  eta: 0:00:21  loss: 0.9611 (0.9623)  time: 0.1918  data: 0.0001  max mem: 15821
[15:46:59.486004] Test:  [230/345]  eta: 0:00:19  loss: 0.9610 (0.9620)  time: 0.1921  data: 0.0001  max mem: 15821
[15:47:01.394676] Test:  [240/345]  eta: 0:00:18  loss: 0.9506 (0.9617)  time: 0.1824  data: 0.0001  max mem: 15821
[15:47:03.162920] Test:  [250/345]  eta: 0:00:16  loss: 0.9510 (0.9620)  time: 0.1838  data: 0.0001  max mem: 15821
[15:47:05.118943] Test:  [260/345]  eta: 0:00:14  loss: 0.9653 (0.9622)  time: 0.1861  data: 0.0001  max mem: 15821
[15:47:06.901335] Test:  [270/345]  eta: 0:00:13  loss: 0.9699 (0.9627)  time: 0.1868  data: 0.0001  max mem: 15821
[15:47:08.799863] Test:  [280/345]  eta: 0:00:11  loss: 0.9596 (0.9621)  time: 0.1840  data: 0.0001  max mem: 15821
[15:47:10.587687] Test:  [290/345]  eta: 0:00:09  loss: 0.9474 (0.9618)  time: 0.1842  data: 0.0001  max mem: 15821
[15:47:12.529345] Test:  [300/345]  eta: 0:00:07  loss: 0.9548 (0.9617)  time: 0.1864  data: 0.0001  max mem: 15821
[15:47:14.315156] Test:  [310/345]  eta: 0:00:06  loss: 0.9573 (0.9615)  time: 0.1863  data: 0.0001  max mem: 15821
[15:47:16.345436] Test:  [320/345]  eta: 0:00:04  loss: 0.9565 (0.9611)  time: 0.1907  data: 0.0001  max mem: 15821
[15:47:18.124098] Test:  [330/345]  eta: 0:00:02  loss: 0.9608 (0.9613)  time: 0.1904  data: 0.0001  max mem: 15821
[15:47:20.013466] Test:  [340/345]  eta: 0:00:00  loss: 0.9620 (0.9614)  time: 0.1833  data: 0.0001  max mem: 15821
[15:47:20.726814] Test:  [344/345]  eta: 0:00:00  loss: 0.9614 (0.9614)  time: 0.1833  data: 0.0001  max mem: 15821
[15:47:20.795369] Test: Total time: 0:01:01 (0.1778 s / it)
[15:47:31.113534] Test:  [ 0/57]  eta: 0:00:27  loss: 1.0757 (1.0757)  time: 0.4908  data: 0.3286  max mem: 15821
[15:47:32.756805] Test:  [10/57]  eta: 0:00:09  loss: 1.0138 (0.9956)  time: 0.1939  data: 0.0300  max mem: 15821
[15:47:34.408431] Test:  [20/57]  eta: 0:00:06  loss: 0.9935 (0.9825)  time: 0.1647  data: 0.0001  max mem: 15821
[15:47:36.063970] Test:  [30/57]  eta: 0:00:04  loss: 0.9466 (0.9387)  time: 0.1653  data: 0.0001  max mem: 15821
[15:47:37.721585] Test:  [40/57]  eta: 0:00:02  loss: 0.8363 (0.9108)  time: 0.1656  data: 0.0001  max mem: 15821
[15:47:39.387333] Test:  [50/57]  eta: 0:00:01  loss: 0.7921 (0.9017)  time: 0.1661  data: 0.0001  max mem: 15821
[15:47:40.285757] Test:  [56/57]  eta: 0:00:00  loss: 0.9110 (0.9059)  time: 0.1613  data: 0.0001  max mem: 15821
[15:47:40.351857] Test: Total time: 0:00:09 (0.1707 s / it)
[15:47:42.058586] Dice score of the network on the train images: 0.398455, val images: 0.515114
[15:47:42.058765] saving best_prec_model_0 @ epoch 1
[15:47:42.775801] saving best_rec_model_0 @ epoch 1
[15:47:43.468672] saving best_dice_model_0 @ epoch 1
[15:47:44.636263] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[15:47:45.611279] Epoch: [2]  [  0/345]  eta: 0:05:36  lr: 0.000013  loss: 0.9592 (0.9592)  time: 0.9740  data: 0.3744  max mem: 15821
[15:47:57.588585] Epoch: [2]  [ 20/345]  eta: 0:03:20  lr: 0.000013  loss: 0.9616 (0.9621)  time: 0.5988  data: 0.0001  max mem: 15821
[15:48:09.617036] Epoch: [2]  [ 40/345]  eta: 0:03:05  lr: 0.000013  loss: 0.9545 (0.9610)  time: 0.6014  data: 0.0001  max mem: 15821
[15:48:21.664881] Epoch: [2]  [ 60/345]  eta: 0:02:52  lr: 0.000014  loss: 0.9543 (0.9590)  time: 0.6023  data: 0.0001  max mem: 15821
[15:48:33.735398] Epoch: [2]  [ 80/345]  eta: 0:02:40  lr: 0.000014  loss: 0.9528 (0.9575)  time: 0.6035  data: 0.0001  max mem: 15821
[15:48:45.834994] Epoch: [2]  [100/345]  eta: 0:02:28  lr: 0.000014  loss: 0.9395 (0.9549)  time: 0.6049  data: 0.0001  max mem: 15821
[15:48:57.966074] Epoch: [2]  [120/345]  eta: 0:02:16  lr: 0.000015  loss: 0.9326 (0.9509)  time: 0.6065  data: 0.0001  max mem: 15821
[15:49:10.093352] Epoch: [2]  [140/345]  eta: 0:02:04  lr: 0.000015  loss: 0.9275 (0.9470)  time: 0.6063  data: 0.0001  max mem: 15821
[15:49:22.215362] Epoch: [2]  [160/345]  eta: 0:01:52  lr: 0.000015  loss: 0.9236 (0.9441)  time: 0.6060  data: 0.0001  max mem: 15821
[15:49:34.316377] Epoch: [2]  [180/345]  eta: 0:01:39  lr: 0.000016  loss: 0.9098 (0.9402)  time: 0.6050  data: 0.0001  max mem: 15821
[15:49:46.434068] Epoch: [2]  [200/345]  eta: 0:01:27  lr: 0.000016  loss: 0.8942 (0.9360)  time: 0.6058  data: 0.0001  max mem: 15821
[15:49:58.539911] Epoch: [2]  [220/345]  eta: 0:01:15  lr: 0.000016  loss: 0.8833 (0.9320)  time: 0.6052  data: 0.0001  max mem: 15821
[15:50:10.639679] Epoch: [2]  [240/345]  eta: 0:01:03  lr: 0.000017  loss: 0.8795 (0.9282)  time: 0.6049  data: 0.0001  max mem: 15821
[15:50:22.740117] Epoch: [2]  [260/345]  eta: 0:00:51  lr: 0.000017  loss: 0.8875 (0.9248)  time: 0.6050  data: 0.0001  max mem: 15821
[15:50:34.837205] Epoch: [2]  [280/345]  eta: 0:00:39  lr: 0.000018  loss: 0.8831 (0.9218)  time: 0.6048  data: 0.0001  max mem: 15821
[15:50:46.926145] Epoch: [2]  [300/345]  eta: 0:00:27  lr: 0.000018  loss: 0.8649 (0.9180)  time: 0.6044  data: 0.0001  max mem: 15821
[15:50:59.014773] Epoch: [2]  [320/345]  eta: 0:00:15  lr: 0.000018  loss: 0.8590 (0.9148)  time: 0.6044  data: 0.0001  max mem: 15821
[15:51:11.095755] Epoch: [2]  [340/345]  eta: 0:00:03  lr: 0.000019  loss: 0.8467 (0.9109)  time: 0.6040  data: 0.0001  max mem: 15821
[15:51:13.509566] Epoch: [2]  [344/345]  eta: 0:00:00  lr: 0.000019  loss: 0.8433 (0.9103)  time: 0.6038  data: 0.0001  max mem: 15821
[15:51:13.574060] Epoch: [2] Total time: 0:03:28 (0.6056 s / it)
[15:51:13.574503] Averaged stats: lr: 0.000019  loss: 0.8433 (0.9103)
[15:51:14.144862] Test:  [  0/345]  eta: 0:03:14  loss: 0.7840 (0.7840)  time: 0.5648  data: 0.4007  max mem: 15821
[15:51:15.811318] Test:  [ 10/345]  eta: 0:01:07  loss: 0.8118 (0.8226)  time: 0.2028  data: 0.0365  max mem: 15821
[15:51:17.480406] Test:  [ 20/345]  eta: 0:01:00  loss: 0.8118 (0.8132)  time: 0.1667  data: 0.0001  max mem: 15821
[15:51:19.152828] Test:  [ 30/345]  eta: 0:00:56  loss: 0.8001 (0.8049)  time: 0.1670  data: 0.0001  max mem: 15821
[15:51:20.828826] Test:  [ 40/345]  eta: 0:00:53  loss: 0.7988 (0.8060)  time: 0.1674  data: 0.0001  max mem: 15821
[15:51:22.507885] Test:  [ 50/345]  eta: 0:00:51  loss: 0.7960 (0.8038)  time: 0.1677  data: 0.0001  max mem: 15821
[15:51:24.189591] Test:  [ 60/345]  eta: 0:00:49  loss: 0.7827 (0.8002)  time: 0.1680  data: 0.0001  max mem: 15821
[15:51:25.875022] Test:  [ 70/345]  eta: 0:00:47  loss: 0.7755 (0.8008)  time: 0.1683  data: 0.0001  max mem: 15821
[15:51:27.563670] Test:  [ 80/345]  eta: 0:00:45  loss: 0.8012 (0.8026)  time: 0.1686  data: 0.0001  max mem: 15821
[15:51:29.256195] Test:  [ 90/345]  eta: 0:00:43  loss: 0.7941 (0.8012)  time: 0.1690  data: 0.0001  max mem: 15821
[15:51:30.952705] Test:  [100/345]  eta: 0:00:42  loss: 0.8007 (0.8026)  time: 0.1694  data: 0.0001  max mem: 15821
[15:51:32.651456] Test:  [110/345]  eta: 0:00:40  loss: 0.8148 (0.8031)  time: 0.1697  data: 0.0001  max mem: 15821
[15:51:34.354514] Test:  [120/345]  eta: 0:00:38  loss: 0.7864 (0.8029)  time: 0.1700  data: 0.0001  max mem: 15821
[15:51:36.060473] Test:  [130/345]  eta: 0:00:36  loss: 0.8128 (0.8040)  time: 0.1704  data: 0.0001  max mem: 15821
[15:51:37.770591] Test:  [140/345]  eta: 0:00:35  loss: 0.8297 (0.8048)  time: 0.1707  data: 0.0001  max mem: 15821
[15:51:39.483611] Test:  [150/345]  eta: 0:00:33  loss: 0.7853 (0.8037)  time: 0.1711  data: 0.0001  max mem: 15821
[15:51:41.200019] Test:  [160/345]  eta: 0:00:31  loss: 0.7759 (0.8030)  time: 0.1714  data: 0.0001  max mem: 15821
[15:51:42.921020] Test:  [170/345]  eta: 0:00:30  loss: 0.7959 (0.8031)  time: 0.1718  data: 0.0001  max mem: 15821
[15:51:44.645509] Test:  [180/345]  eta: 0:00:28  loss: 0.7971 (0.8022)  time: 0.1722  data: 0.0001  max mem: 15821
[15:51:46.373288] Test:  [190/345]  eta: 0:00:26  loss: 0.7913 (0.8016)  time: 0.1725  data: 0.0001  max mem: 15821
[15:51:48.104235] Test:  [200/345]  eta: 0:00:24  loss: 0.7883 (0.8012)  time: 0.1729  data: 0.0001  max mem: 15821
[15:51:49.837463] Test:  [210/345]  eta: 0:00:23  loss: 0.7883 (0.8014)  time: 0.1731  data: 0.0001  max mem: 15821
[15:51:51.575151] Test:  [220/345]  eta: 0:00:21  loss: 0.7838 (0.8006)  time: 0.1735  data: 0.0001  max mem: 15821
[15:51:53.315545] Test:  [230/345]  eta: 0:00:19  loss: 0.7913 (0.8011)  time: 0.1738  data: 0.0001  max mem: 15821
[15:51:55.060474] Test:  [240/345]  eta: 0:00:18  loss: 0.7913 (0.8006)  time: 0.1742  data: 0.0001  max mem: 15821
[15:51:56.808523] Test:  [250/345]  eta: 0:00:16  loss: 0.8028 (0.8009)  time: 0.1746  data: 0.0001  max mem: 15821
[15:51:58.559918] Test:  [260/345]  eta: 0:00:14  loss: 0.8160 (0.8018)  time: 0.1749  data: 0.0001  max mem: 15821
[15:52:00.314591] Test:  [270/345]  eta: 0:00:12  loss: 0.8049 (0.8009)  time: 0.1752  data: 0.0001  max mem: 15821
[15:52:02.072236] Test:  [280/345]  eta: 0:00:11  loss: 0.7959 (0.8011)  time: 0.1756  data: 0.0001  max mem: 15821
[15:52:03.833175] Test:  [290/345]  eta: 0:00:09  loss: 0.8141 (0.8012)  time: 0.1759  data: 0.0001  max mem: 15821
[15:52:05.596429] Test:  [300/345]  eta: 0:00:07  loss: 0.8039 (0.8010)  time: 0.1761  data: 0.0001  max mem: 15821
[15:52:07.365543] Test:  [310/345]  eta: 0:00:06  loss: 0.7893 (0.8002)  time: 0.1766  data: 0.0001  max mem: 15821
[15:52:09.138527] Test:  [320/345]  eta: 0:00:04  loss: 0.7893 (0.8001)  time: 0.1770  data: 0.0001  max mem: 15821
[15:52:10.913699] Test:  [330/345]  eta: 0:00:02  loss: 0.7997 (0.8005)  time: 0.1773  data: 0.0001  max mem: 15821
[15:52:12.691707] Test:  [340/345]  eta: 0:00:00  loss: 0.7933 (0.8002)  time: 0.1776  data: 0.0001  max mem: 15821
[15:52:13.404362] Test:  [344/345]  eta: 0:00:00  loss: 0.7944 (0.8002)  time: 0.1777  data: 0.0001  max mem: 15821
[15:52:13.466759] Test: Total time: 0:00:59 (0.1736 s / it)
[15:52:23.824768] Test:  [ 0/57]  eta: 0:00:28  loss: 0.9894 (0.9894)  time: 0.5041  data: 0.3418  max mem: 15821
[15:52:25.469453] Test:  [10/57]  eta: 0:00:09  loss: 0.8580 (0.8316)  time: 0.1952  data: 0.0312  max mem: 15821
[15:52:27.119312] Test:  [20/57]  eta: 0:00:06  loss: 0.7798 (0.8070)  time: 0.1646  data: 0.0001  max mem: 15821
[15:52:28.773599] Test:  [30/57]  eta: 0:00:04  loss: 0.7037 (0.7565)  time: 0.1651  data: 0.0001  max mem: 15821
[15:52:30.430924] Test:  [40/57]  eta: 0:00:02  loss: 0.5589 (0.7245)  time: 0.1655  data: 0.0001  max mem: 15821
[15:52:32.095890] Test:  [50/57]  eta: 0:00:01  loss: 0.5621 (0.7152)  time: 0.1661  data: 0.0001  max mem: 15821
[15:52:32.994778] Test:  [56/57]  eta: 0:00:00  loss: 0.6840 (0.7229)  time: 0.1613  data: 0.0001  max mem: 15821
[15:52:33.061626] Test: Total time: 0:00:09 (0.1709 s / it)
[15:52:34.754374] Dice score of the network on the train images: 0.531202, val images: 0.651267
[15:52:34.754608] saving best_prec_model_0 @ epoch 2
[15:52:35.869466] saving best_rec_model_0 @ epoch 2
[15:52:37.240269] saving best_dice_model_0 @ epoch 2
[15:52:38.367831] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[15:52:39.299174] Epoch: [3]  [  0/345]  eta: 0:05:20  lr: 0.000019  loss: 0.8493 (0.8493)  time: 0.9300  data: 0.3291  max mem: 15821
[15:52:51.263296] Epoch: [3]  [ 20/345]  eta: 0:03:19  lr: 0.000019  loss: 0.8434 (0.8406)  time: 0.5982  data: 0.0001  max mem: 15821
[15:53:03.286223] Epoch: [3]  [ 40/345]  eta: 0:03:05  lr: 0.000019  loss: 0.8205 (0.8354)  time: 0.6011  data: 0.0001  max mem: 15821
[15:53:15.330627] Epoch: [3]  [ 60/345]  eta: 0:02:52  lr: 0.000020  loss: 0.8247 (0.8310)  time: 0.6022  data: 0.0001  max mem: 15821
[15:53:27.394153] Epoch: [3]  [ 80/345]  eta: 0:02:40  lr: 0.000020  loss: 0.8138 (0.8291)  time: 0.6031  data: 0.0001  max mem: 15821
[15:53:39.484116] Epoch: [3]  [100/345]  eta: 0:02:28  lr: 0.000021  loss: 0.8194 (0.8291)  time: 0.6045  data: 0.0001  max mem: 15821
[15:53:51.604238] Epoch: [3]  [120/345]  eta: 0:02:16  lr: 0.000021  loss: 0.8040 (0.8246)  time: 0.6060  data: 0.0001  max mem: 15821
[15:54:03.725282] Epoch: [3]  [140/345]  eta: 0:02:04  lr: 0.000021  loss: 0.7904 (0.8209)  time: 0.6060  data: 0.0001  max mem: 15821
[15:54:15.837680] Epoch: [3]  [160/345]  eta: 0:01:51  lr: 0.000022  loss: 0.7720 (0.8151)  time: 0.6056  data: 0.0001  max mem: 15821
[15:54:27.942187] Epoch: [3]  [180/345]  eta: 0:01:39  lr: 0.000022  loss: 0.7773 (0.8110)  time: 0.6052  data: 0.0001  max mem: 15821
[15:54:40.052232] Epoch: [3]  [200/345]  eta: 0:01:27  lr: 0.000022  loss: 0.7726 (0.8077)  time: 0.6055  data: 0.0001  max mem: 15821
[15:54:52.155941] Epoch: [3]  [220/345]  eta: 0:01:15  lr: 0.000023  loss: 0.7725 (0.8043)  time: 0.6051  data: 0.0001  max mem: 15821
[15:55:04.255174] Epoch: [3]  [240/345]  eta: 0:01:03  lr: 0.000023  loss: 0.7454 (0.8002)  time: 0.6049  data: 0.0001  max mem: 15821
[15:55:16.356844] Epoch: [3]  [260/345]  eta: 0:00:51  lr: 0.000023  loss: 0.7423 (0.7963)  time: 0.6050  data: 0.0001  max mem: 15821
[15:55:28.448620] Epoch: [3]  [280/345]  eta: 0:00:39  lr: 0.000024  loss: 0.7526 (0.7937)  time: 0.6046  data: 0.0001  max mem: 15821
[15:55:40.549556] Epoch: [3]  [300/345]  eta: 0:00:27  lr: 0.000024  loss: 0.7326 (0.7898)  time: 0.6050  data: 0.0001  max mem: 15821
[15:55:52.648744] Epoch: [3]  [320/345]  eta: 0:00:15  lr: 0.000025  loss: 0.7338 (0.7856)  time: 0.6049  data: 0.0001  max mem: 15821
[15:56:04.728975] Epoch: [3]  [340/345]  eta: 0:00:03  lr: 0.000025  loss: 0.7119 (0.7813)  time: 0.6040  data: 0.0001  max mem: 15821
[15:56:07.146538] Epoch: [3]  [344/345]  eta: 0:00:00  lr: 0.000025  loss: 0.7039 (0.7804)  time: 0.6040  data: 0.0001  max mem: 15821
[15:56:07.211298] Epoch: [3] Total time: 0:03:28 (0.6053 s / it)
[15:56:07.211845] Averaged stats: lr: 0.000025  loss: 0.7039 (0.7804)
[15:56:07.712591] Test:  [  0/345]  eta: 0:02:50  loss: 0.7022 (0.7022)  time: 0.4950  data: 0.3315  max mem: 15821
[15:56:09.377230] Test:  [ 10/345]  eta: 0:01:05  loss: 0.6767 (0.6768)  time: 0.1962  data: 0.0302  max mem: 15821
[15:56:11.044014] Test:  [ 20/345]  eta: 0:00:59  loss: 0.6696 (0.6829)  time: 0.1665  data: 0.0001  max mem: 15821
[15:56:12.713900] Test:  [ 30/345]  eta: 0:00:55  loss: 0.6515 (0.6781)  time: 0.1668  data: 0.0001  max mem: 15821
[15:56:14.385695] Test:  [ 40/345]  eta: 0:00:53  loss: 0.6932 (0.6864)  time: 0.1670  data: 0.0001  max mem: 15821
[15:56:16.060398] Test:  [ 50/345]  eta: 0:00:51  loss: 0.7012 (0.6880)  time: 0.1673  data: 0.0001  max mem: 15821
[15:56:17.740388] Test:  [ 60/345]  eta: 0:00:49  loss: 0.6717 (0.6876)  time: 0.1677  data: 0.0001  max mem: 15821
[15:56:19.422809] Test:  [ 70/345]  eta: 0:00:47  loss: 0.6924 (0.6896)  time: 0.1681  data: 0.0001  max mem: 15821
[15:56:21.109376] Test:  [ 80/345]  eta: 0:00:45  loss: 0.6646 (0.6862)  time: 0.1684  data: 0.0001  max mem: 15821
[15:56:22.799052] Test:  [ 90/345]  eta: 0:00:43  loss: 0.6744 (0.6857)  time: 0.1687  data: 0.0001  max mem: 15821
[15:56:24.492555] Test:  [100/345]  eta: 0:00:41  loss: 0.6915 (0.6872)  time: 0.1691  data: 0.0001  max mem: 15821
[15:56:26.188934] Test:  [110/345]  eta: 0:00:40  loss: 0.6868 (0.6854)  time: 0.1694  data: 0.0001  max mem: 15821
[15:56:27.887651] Test:  [120/345]  eta: 0:00:38  loss: 0.6809 (0.6849)  time: 0.1697  data: 0.0001  max mem: 15821
[15:56:29.590791] Test:  [130/345]  eta: 0:00:36  loss: 0.6857 (0.6851)  time: 0.1700  data: 0.0001  max mem: 15821
[15:56:31.298131] Test:  [140/345]  eta: 0:00:34  loss: 0.6843 (0.6845)  time: 0.1705  data: 0.0001  max mem: 15821
[15:56:33.008149] Test:  [150/345]  eta: 0:00:33  loss: 0.6843 (0.6855)  time: 0.1708  data: 0.0001  max mem: 15821
[15:56:34.721107] Test:  [160/345]  eta: 0:00:31  loss: 0.7122 (0.6879)  time: 0.1711  data: 0.0001  max mem: 15821
[15:56:36.437807] Test:  [170/345]  eta: 0:00:29  loss: 0.6868 (0.6859)  time: 0.1714  data: 0.0001  max mem: 15821
[15:56:38.158593] Test:  [180/345]  eta: 0:00:28  loss: 0.6636 (0.6854)  time: 0.1718  data: 0.0001  max mem: 15821
[15:56:39.881775] Test:  [190/345]  eta: 0:00:26  loss: 0.6768 (0.6874)  time: 0.1721  data: 0.0001  max mem: 15821
[15:56:41.609526] Test:  [200/345]  eta: 0:00:24  loss: 0.6972 (0.6863)  time: 0.1725  data: 0.0001  max mem: 15821
[15:56:43.339773] Test:  [210/345]  eta: 0:00:23  loss: 0.6874 (0.6868)  time: 0.1728  data: 0.0001  max mem: 15821
[15:56:45.073904] Test:  [220/345]  eta: 0:00:21  loss: 0.6904 (0.6876)  time: 0.1732  data: 0.0001  max mem: 15821
[15:56:46.810292] Test:  [230/345]  eta: 0:00:19  loss: 0.6959 (0.6881)  time: 0.1735  data: 0.0001  max mem: 15821
[15:56:48.551044] Test:  [240/345]  eta: 0:00:18  loss: 0.6723 (0.6870)  time: 0.1738  data: 0.0001  max mem: 15821
[15:56:50.294822] Test:  [250/345]  eta: 0:00:16  loss: 0.6532 (0.6862)  time: 0.1742  data: 0.0001  max mem: 15821
[15:56:52.042065] Test:  [260/345]  eta: 0:00:14  loss: 0.6871 (0.6867)  time: 0.1745  data: 0.0001  max mem: 15821
[15:56:53.793923] Test:  [270/345]  eta: 0:00:12  loss: 0.6881 (0.6866)  time: 0.1749  data: 0.0001  max mem: 15821
[15:56:55.549146] Test:  [280/345]  eta: 0:00:11  loss: 0.6880 (0.6881)  time: 0.1753  data: 0.0001  max mem: 15821
[15:56:57.307471] Test:  [290/345]  eta: 0:00:09  loss: 0.6964 (0.6882)  time: 0.1756  data: 0.0001  max mem: 15821
[15:56:59.068969] Test:  [300/345]  eta: 0:00:07  loss: 0.6964 (0.6886)  time: 0.1759  data: 0.0001  max mem: 15821
[15:57:00.834584] Test:  [310/345]  eta: 0:00:06  loss: 0.6995 (0.6890)  time: 0.1763  data: 0.0001  max mem: 15821
[15:57:02.604802] Test:  [320/345]  eta: 0:00:04  loss: 0.6995 (0.6896)  time: 0.1767  data: 0.0001  max mem: 15821
[15:57:04.375627] Test:  [330/345]  eta: 0:00:02  loss: 0.7034 (0.6899)  time: 0.1770  data: 0.0001  max mem: 15821
[15:57:06.151674] Test:  [340/345]  eta: 0:00:00  loss: 0.7191 (0.6899)  time: 0.1773  data: 0.0001  max mem: 15821
[15:57:06.863889] Test:  [344/345]  eta: 0:00:00  loss: 0.7184 (0.6896)  time: 0.1775  data: 0.0001  max mem: 15821
[15:57:06.926909] Test: Total time: 0:00:59 (0.1731 s / it)
[15:57:17.252959] Test:  [ 0/57]  eta: 0:00:28  loss: 0.9056 (0.9056)  time: 0.4917  data: 0.3296  max mem: 15821
[15:57:18.897485] Test:  [10/57]  eta: 0:00:09  loss: 0.7647 (0.7234)  time: 0.1941  data: 0.0300  max mem: 15821
[15:57:20.548299] Test:  [20/57]  eta: 0:00:06  loss: 0.6154 (0.6882)  time: 0.1647  data: 0.0001  max mem: 15821
[15:57:22.202449] Test:  [30/57]  eta: 0:00:04  loss: 0.5848 (0.6394)  time: 0.1652  data: 0.0001  max mem: 15821
[15:57:23.860161] Test:  [40/57]  eta: 0:00:02  loss: 0.4247 (0.6136)  time: 0.1655  data: 0.0001  max mem: 15821
[15:57:25.524717] Test:  [50/57]  eta: 0:00:01  loss: 0.4573 (0.5975)  time: 0.1661  data: 0.0001  max mem: 15821
[15:57:26.422960] Test:  [56/57]  eta: 0:00:00  loss: 0.4915 (0.5929)  time: 0.1613  data: 0.0001  max mem: 15821
[15:57:26.495331] Test: Total time: 0:00:09 (0.1708 s / it)
[15:57:28.159154] Dice score of the network on the train images: 0.605201, val images: 0.720491
[15:57:28.159396] saving best_prec_model_0 @ epoch 3
[15:57:29.177789] saving best_rec_model_0 @ epoch 3
[15:57:30.151080] saving best_dice_model_0 @ epoch 3
[15:57:31.502735] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[15:57:32.429543] Epoch: [4]  [  0/345]  eta: 0:05:19  lr: 0.000025  loss: 0.7394 (0.7394)  time: 0.9255  data: 0.3248  max mem: 15821
[15:57:44.413555] Epoch: [4]  [ 20/345]  eta: 0:03:19  lr: 0.000025  loss: 0.6885 (0.7079)  time: 0.5991  data: 0.0001  max mem: 15821
[15:57:56.553439] Epoch: [4]  [ 40/345]  eta: 0:03:06  lr: 0.000026  loss: 0.7030 (0.7107)  time: 0.6069  data: 0.0001  max mem: 15821
[15:58:08.604322] Epoch: [4]  [ 60/345]  eta: 0:02:53  lr: 0.000026  loss: 0.6900 (0.7088)  time: 0.6025  data: 0.0001  max mem: 15821
[15:58:20.669948] Epoch: [4]  [ 80/345]  eta: 0:02:40  lr: 0.000026  loss: 0.6942 (0.7060)  time: 0.6032  data: 0.0001  max mem: 15821
[15:58:32.775297] Epoch: [4]  [100/345]  eta: 0:02:28  lr: 0.000027  loss: 0.6825 (0.7013)  time: 0.6052  data: 0.0001  max mem: 15821
[15:58:44.892733] Epoch: [4]  [120/345]  eta: 0:02:16  lr: 0.000027  loss: 0.6805 (0.6977)  time: 0.6058  data: 0.0001  max mem: 15821
[15:58:57.009665] Epoch: [4]  [140/345]  eta: 0:02:04  lr: 0.000028  loss: 0.6924 (0.6984)  time: 0.6058  data: 0.0001  max mem: 15821
[15:59:09.128290] Epoch: [4]  [160/345]  eta: 0:01:52  lr: 0.000028  loss: 0.6737 (0.6942)  time: 0.6059  data: 0.0001  max mem: 15821
[15:59:21.241593] Epoch: [4]  [180/345]  eta: 0:01:40  lr: 0.000028  loss: 0.6803 (0.6927)  time: 0.6056  data: 0.0001  max mem: 15821
[15:59:33.346934] Epoch: [4]  [200/345]  eta: 0:01:27  lr: 0.000029  loss: 0.6692 (0.6910)  time: 0.6052  data: 0.0001  max mem: 15821
[15:59:45.457032] Epoch: [4]  [220/345]  eta: 0:01:15  lr: 0.000029  loss: 0.6413 (0.6871)  time: 0.6055  data: 0.0001  max mem: 15821
[15:59:57.562970] Epoch: [4]  [240/345]  eta: 0:01:03  lr: 0.000029  loss: 0.6634 (0.6856)  time: 0.6052  data: 0.0001  max mem: 15821
[16:00:09.650852] Epoch: [4]  [260/345]  eta: 0:00:51  lr: 0.000030  loss: 0.6417 (0.6823)  time: 0.6043  data: 0.0001  max mem: 15821
[16:00:21.740440] Epoch: [4]  [280/345]  eta: 0:00:39  lr: 0.000030  loss: 0.6349 (0.6794)  time: 0.6044  data: 0.0001  max mem: 15821
[16:00:33.829822] Epoch: [4]  [300/345]  eta: 0:00:27  lr: 0.000030  loss: 0.6497 (0.6767)  time: 0.6044  data: 0.0001  max mem: 15821
[16:00:45.917096] Epoch: [4]  [320/345]  eta: 0:00:15  lr: 0.000031  loss: 0.6431 (0.6740)  time: 0.6043  data: 0.0001  max mem: 15821
[16:00:58.005797] Epoch: [4]  [340/345]  eta: 0:00:03  lr: 0.000031  loss: 0.6208 (0.6714)  time: 0.6044  data: 0.0001  max mem: 15821
[16:01:00.423638] Epoch: [4]  [344/345]  eta: 0:00:00  lr: 0.000031  loss: 0.6198 (0.6707)  time: 0.6044  data: 0.0001  max mem: 15821
[16:01:00.494724] Epoch: [4] Total time: 0:03:28 (0.6058 s / it)
[16:01:00.495386] Averaged stats: lr: 0.000031  loss: 0.6198 (0.6707)
[16:01:01.020480] Test:  [  0/345]  eta: 0:02:58  loss: 0.6408 (0.6408)  time: 0.5182  data: 0.3545  max mem: 15821
[16:01:02.686505] Test:  [ 10/345]  eta: 0:01:06  loss: 0.6264 (0.6248)  time: 0.1985  data: 0.0323  max mem: 15821
[16:01:04.354903] Test:  [ 20/345]  eta: 0:00:59  loss: 0.6291 (0.6305)  time: 0.1666  data: 0.0001  max mem: 15821
[16:01:06.026596] Test:  [ 30/345]  eta: 0:00:56  loss: 0.6436 (0.6273)  time: 0.1669  data: 0.0001  max mem: 15821
[16:01:07.702396] Test:  [ 40/345]  eta: 0:00:53  loss: 0.6250 (0.6309)  time: 0.1673  data: 0.0001  max mem: 15821
[16:01:09.380582] Test:  [ 50/345]  eta: 0:00:51  loss: 0.6452 (0.6312)  time: 0.1676  data: 0.0001  max mem: 15821
[16:01:11.063394] Test:  [ 60/345]  eta: 0:00:49  loss: 0.6452 (0.6299)  time: 0.1680  data: 0.0001  max mem: 15821
[16:01:12.747786] Test:  [ 70/345]  eta: 0:00:47  loss: 0.6180 (0.6303)  time: 0.1683  data: 0.0001  max mem: 15821
[16:01:14.437554] Test:  [ 80/345]  eta: 0:00:45  loss: 0.6157 (0.6304)  time: 0.1686  data: 0.0001  max mem: 15821
[16:01:16.129523] Test:  [ 90/345]  eta: 0:00:43  loss: 0.6168 (0.6295)  time: 0.1690  data: 0.0001  max mem: 15821
[16:01:17.825634] Test:  [100/345]  eta: 0:00:42  loss: 0.6168 (0.6283)  time: 0.1693  data: 0.0001  max mem: 15821
[16:01:19.524843] Test:  [110/345]  eta: 0:00:40  loss: 0.6137 (0.6270)  time: 0.1697  data: 0.0001  max mem: 15821
[16:01:21.227083] Test:  [120/345]  eta: 0:00:38  loss: 0.6206 (0.6276)  time: 0.1700  data: 0.0001  max mem: 15821
[16:01:22.932979] Test:  [130/345]  eta: 0:00:36  loss: 0.6329 (0.6289)  time: 0.1703  data: 0.0001  max mem: 15821
[16:01:24.642853] Test:  [140/345]  eta: 0:00:35  loss: 0.6195 (0.6282)  time: 0.1707  data: 0.0001  max mem: 15821
[16:01:26.355356] Test:  [150/345]  eta: 0:00:33  loss: 0.6170 (0.6277)  time: 0.1711  data: 0.0001  max mem: 15821
[16:01:28.072451] Test:  [160/345]  eta: 0:00:31  loss: 0.6170 (0.6267)  time: 0.1714  data: 0.0001  max mem: 15821
[16:01:29.792092] Test:  [170/345]  eta: 0:00:29  loss: 0.6059 (0.6254)  time: 0.1718  data: 0.0001  max mem: 15821
[16:01:31.516309] Test:  [180/345]  eta: 0:00:28  loss: 0.6131 (0.6245)  time: 0.1721  data: 0.0001  max mem: 15821
[16:01:33.243827] Test:  [190/345]  eta: 0:00:26  loss: 0.6244 (0.6250)  time: 0.1725  data: 0.0001  max mem: 15821
[16:01:34.976671] Test:  [200/345]  eta: 0:00:24  loss: 0.6296 (0.6243)  time: 0.1730  data: 0.0001  max mem: 15821
[16:01:36.710829] Test:  [210/345]  eta: 0:00:23  loss: 0.6139 (0.6249)  time: 0.1733  data: 0.0001  max mem: 15821
[16:01:38.448040] Test:  [220/345]  eta: 0:00:21  loss: 0.6081 (0.6241)  time: 0.1735  data: 0.0001  max mem: 15821
[16:01:40.189383] Test:  [230/345]  eta: 0:00:19  loss: 0.5999 (0.6245)  time: 0.1739  data: 0.0001  max mem: 15821
[16:01:41.934006] Test:  [240/345]  eta: 0:00:18  loss: 0.6194 (0.6244)  time: 0.1742  data: 0.0001  max mem: 15821
[16:01:43.680699] Test:  [250/345]  eta: 0:00:16  loss: 0.6389 (0.6255)  time: 0.1745  data: 0.0001  max mem: 15821
[16:01:45.432377] Test:  [260/345]  eta: 0:00:14  loss: 0.6389 (0.6264)  time: 0.1749  data: 0.0001  max mem: 15821
[16:01:47.185918] Test:  [270/345]  eta: 0:00:12  loss: 0.6301 (0.6259)  time: 0.1752  data: 0.0001  max mem: 15821
[16:01:48.944562] Test:  [280/345]  eta: 0:00:11  loss: 0.6194 (0.6257)  time: 0.1755  data: 0.0001  max mem: 15821
[16:01:50.705660] Test:  [290/345]  eta: 0:00:09  loss: 0.6439 (0.6258)  time: 0.1759  data: 0.0001  max mem: 15821
[16:01:52.470444] Test:  [300/345]  eta: 0:00:07  loss: 0.6064 (0.6258)  time: 0.1762  data: 0.0001  max mem: 15821
[16:01:54.239763] Test:  [310/345]  eta: 0:00:06  loss: 0.6174 (0.6264)  time: 0.1766  data: 0.0001  max mem: 15821
[16:01:56.012314] Test:  [320/345]  eta: 0:00:04  loss: 0.6352 (0.6268)  time: 0.1770  data: 0.0001  max mem: 15821
[16:01:57.787589] Test:  [330/345]  eta: 0:00:02  loss: 0.6345 (0.6267)  time: 0.1773  data: 0.0001  max mem: 15821
[16:01:59.566916] Test:  [340/345]  eta: 0:00:00  loss: 0.6317 (0.6272)  time: 0.1777  data: 0.0001  max mem: 15821
[16:02:00.279466] Test:  [344/345]  eta: 0:00:00  loss: 0.6304 (0.6271)  time: 0.1778  data: 0.0001  max mem: 15821
[16:02:00.343087] Test: Total time: 0:00:59 (0.1735 s / it)
[16:02:10.664068] Test:  [ 0/57]  eta: 0:00:26  loss: 0.8462 (0.8462)  time: 0.4727  data: 0.3105  max mem: 15821
[16:02:12.308648] Test:  [10/57]  eta: 0:00:09  loss: 0.6757 (0.6578)  time: 0.1924  data: 0.0283  max mem: 15821
[16:02:13.959631] Test:  [20/57]  eta: 0:00:06  loss: 0.5487 (0.6268)  time: 0.1647  data: 0.0001  max mem: 15821
[16:02:15.614238] Test:  [30/57]  eta: 0:00:04  loss: 0.5198 (0.5815)  time: 0.1652  data: 0.0001  max mem: 15821
[16:02:17.272566] Test:  [40/57]  eta: 0:00:02  loss: 0.3708 (0.5585)  time: 0.1656  data: 0.0001  max mem: 15821
[16:02:18.937057] Test:  [50/57]  eta: 0:00:01  loss: 0.4242 (0.5448)  time: 0.1661  data: 0.0001  max mem: 15821
[16:02:19.834938] Test:  [56/57]  eta: 0:00:00  loss: 0.4723 (0.5429)  time: 0.1613  data: 0.0001  max mem: 15821
[16:02:19.901140] Test: Total time: 0:00:09 (0.1704 s / it)
[16:02:21.611787] Dice score of the network on the train images: 0.655616, val images: 0.734889
[16:02:21.611960] saving best_prec_model_0 @ epoch 4
[16:02:22.669236] saving best_dice_model_0 @ epoch 4
[16:02:23.662430] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[16:02:24.589387] Epoch: [5]  [  0/345]  eta: 0:05:19  lr: 0.000031  loss: 0.5844 (0.5844)  time: 0.9257  data: 0.3239  max mem: 15821
[16:02:36.574821] Epoch: [5]  [ 20/345]  eta: 0:03:19  lr: 0.000032  loss: 0.6091 (0.6109)  time: 0.5992  data: 0.0001  max mem: 15821
[16:02:48.594658] Epoch: [5]  [ 40/345]  eta: 0:03:05  lr: 0.000032  loss: 0.6244 (0.6175)  time: 0.6009  data: 0.0001  max mem: 15821
[16:03:00.639333] Epoch: [5]  [ 60/345]  eta: 0:02:52  lr: 0.000032  loss: 0.6030 (0.6173)  time: 0.6022  data: 0.0001  max mem: 15821
[16:03:12.693673] Epoch: [5]  [ 80/345]  eta: 0:02:40  lr: 0.000033  loss: 0.5973 (0.6144)  time: 0.6027  data: 0.0001  max mem: 15821
[16:03:24.762871] Epoch: [5]  [100/345]  eta: 0:02:28  lr: 0.000033  loss: 0.6220 (0.6157)  time: 0.6034  data: 0.0001  max mem: 15821
[16:03:36.871733] Epoch: [5]  [120/345]  eta: 0:02:16  lr: 0.000033  loss: 0.6180 (0.6148)  time: 0.6054  data: 0.0001  max mem: 15821
[16:03:48.982510] Epoch: [5]  [140/345]  eta: 0:02:04  lr: 0.000034  loss: 0.6148 (0.6155)  time: 0.6055  data: 0.0001  max mem: 15821
[16:04:01.095439] Epoch: [5]  [160/345]  eta: 0:01:51  lr: 0.000034  loss: 0.5924 (0.6140)  time: 0.6056  data: 0.0001  max mem: 15821
[16:04:13.196974] Epoch: [5]  [180/345]  eta: 0:01:39  lr: 0.000035  loss: 0.6069 (0.6141)  time: 0.6050  data: 0.0001  max mem: 15821
[16:04:25.294056] Epoch: [5]  [200/345]  eta: 0:01:27  lr: 0.000035  loss: 0.5698 (0.6110)  time: 0.6048  data: 0.0001  max mem: 15821
[16:04:37.383910] Epoch: [5]  [220/345]  eta: 0:01:15  lr: 0.000035  loss: 0.5952 (0.6100)  time: 0.6045  data: 0.0001  max mem: 15821

[16:04:49.478021] Epoch: [5]  [240/345]  eta: 0:01:03  lr: 0.000036  loss: 0.5810 (0.6086)  time: 0.6047  data: 0.0001  max mem: 15821
[16:05:01.577737] Epoch: [5]  [260/345]  eta: 0:00:51  lr: 0.000036  loss: 0.5820 (0.6075)  time: 0.6049  data: 0.0001  max mem: 15821
[16:05:13.672421] Epoch: [5]  [280/345]  eta: 0:00:39  lr: 0.000036  loss: 0.5860 (0.6070)  time: 0.6047  data: 0.0001  max mem: 15821
[16:05:25.760604] Epoch: [5]  [300/345]  eta: 0:00:27  lr: 0.000037  loss: 0.5789 (0.6054)  time: 0.6044  data: 0.0001  max mem: 15821
[16:05:37.844879] Epoch: [5]  [320/345]  eta: 0:00:15  lr: 0.000037  loss: 0.5873 (0.6051)  time: 0.6042  data: 0.0001  max mem: 15821
[16:05:49.921175] Epoch: [5]  [340/345]  eta: 0:00:03  lr: 0.000037  loss: 0.5715 (0.6036)  time: 0.6038  data: 0.0001  max mem: 15821
[16:05:52.343771] Epoch: [5]  [344/345]  eta: 0:00:00  lr: 0.000037  loss: 0.5677 (0.6030)  time: 0.6041  data: 0.0001  max mem: 15821
[16:05:52.411050] Epoch: [5] Total time: 0:03:28 (0.6051 s / it)
[16:05:52.411663] Averaged stats: lr: 0.000037  loss: 0.5677 (0.6030)
[16:05:52.931945] Test:  [  0/345]  eta: 0:02:57  loss: 0.5730 (0.5730)  time: 0.5141  data: 0.3494  max mem: 15821
[16:05:54.600102] Test:  [ 10/345]  eta: 0:01:06  loss: 0.5747 (0.5676)  time: 0.1983  data: 0.0319  max mem: 15821
[16:05:56.270104] Test:  [ 20/345]  eta: 0:00:59  loss: 0.5747 (0.5716)  time: 0.1668  data: 0.0001  max mem: 15821
[16:05:57.943906] Test:  [ 30/345]  eta: 0:00:56  loss: 0.5666 (0.5715)  time: 0.1671  data: 0.0001  max mem: 15821
[16:05:59.619775] Test:  [ 40/345]  eta: 0:00:53  loss: 0.5768 (0.5768)  time: 0.1674  data: 0.0001  max mem: 15821
[16:06:01.298234] Test:  [ 50/345]  eta: 0:00:51  loss: 0.5893 (0.5783)  time: 0.1677  data: 0.0001  max mem: 15821
[16:06:02.980649] Test:  [ 60/345]  eta: 0:00:49  loss: 0.5513 (0.5754)  time: 0.1680  data: 0.0001  max mem: 15821
[16:06:04.666479] Test:  [ 70/345]  eta: 0:00:47  loss: 0.5513 (0.5744)  time: 0.1683  data: 0.0001  max mem: 15821
[16:06:06.356519] Test:  [ 80/345]  eta: 0:00:45  loss: 0.5578 (0.5758)  time: 0.1687  data: 0.0001  max mem: 15821
[16:06:08.049096] Test:  [ 90/345]  eta: 0:00:43  loss: 0.5904 (0.5766)  time: 0.1691  data: 0.0001  max mem: 15821
[16:06:09.746186] Test:  [100/345]  eta: 0:00:42  loss: 0.5644 (0.5747)  time: 0.1694  data: 0.0001  max mem: 15821
[16:06:11.446698] Test:  [110/345]  eta: 0:00:40  loss: 0.5611 (0.5738)  time: 0.1698  data: 0.0001  max mem: 15821
[16:06:13.148726] Test:  [120/345]  eta: 0:00:38  loss: 0.5717 (0.5754)  time: 0.1701  data: 0.0001  max mem: 15821
[16:06:14.856045] Test:  [130/345]  eta: 0:00:36  loss: 0.5848 (0.5757)  time: 0.1704  data: 0.0001  max mem: 15821
[16:06:16.568395] Test:  [140/345]  eta: 0:00:35  loss: 0.5824 (0.5748)  time: 0.1708  data: 0.0001  max mem: 15821
[16:06:18.281661] Test:  [150/345]  eta: 0:00:33  loss: 0.5453 (0.5733)  time: 0.1711  data: 0.0001  max mem: 15821
[16:06:19.999079] Test:  [160/345]  eta: 0:00:31  loss: 0.5630 (0.5727)  time: 0.1715  data: 0.0001  max mem: 15821
[16:06:21.720226] Test:  [170/345]  eta: 0:00:29  loss: 0.5630 (0.5722)  time: 0.1718  data: 0.0001  max mem: 15821
[16:06:23.443872] Test:  [180/345]  eta: 0:00:28  loss: 0.5506 (0.5718)  time: 0.1722  data: 0.0001  max mem: 15821
[16:06:25.170853] Test:  [190/345]  eta: 0:00:26  loss: 0.5604 (0.5706)  time: 0.1725  data: 0.0001  max mem: 15821
[16:06:26.901860] Test:  [200/345]  eta: 0:00:24  loss: 0.5541 (0.5705)  time: 0.1728  data: 0.0001  max mem: 15821
[16:06:28.636519] Test:  [210/345]  eta: 0:00:23  loss: 0.5541 (0.5699)  time: 0.1732  data: 0.0001  max mem: 15821
[16:06:30.373349] Test:  [220/345]  eta: 0:00:21  loss: 0.5743 (0.5710)  time: 0.1735  data: 0.0001  max mem: 15821
[16:06:32.114167] Test:  [230/345]  eta: 0:00:19  loss: 0.5764 (0.5714)  time: 0.1738  data: 0.0001  max mem: 15821
[16:06:33.858345] Test:  [240/345]  eta: 0:00:18  loss: 0.5555 (0.5700)  time: 0.1742  data: 0.0001  max mem: 15821
[16:06:35.605723] Test:  [250/345]  eta: 0:00:16  loss: 0.5578 (0.5708)  time: 0.1745  data: 0.0001  max mem: 15821
[16:06:37.356680] Test:  [260/345]  eta: 0:00:14  loss: 0.5753 (0.5705)  time: 0.1749  data: 0.0001  max mem: 15821
[16:06:39.110379] Test:  [270/345]  eta: 0:00:12  loss: 0.5814 (0.5710)  time: 0.1752  data: 0.0001  max mem: 15821
[16:06:40.868926] Test:  [280/345]  eta: 0:00:11  loss: 0.5662 (0.5705)  time: 0.1755  data: 0.0001  max mem: 15821
[16:06:42.630329] Test:  [290/345]  eta: 0:00:09  loss: 0.5576 (0.5701)  time: 0.1759  data: 0.0001  max mem: 15821
[16:06:44.395082] Test:  [300/345]  eta: 0:00:07  loss: 0.5632 (0.5700)  time: 0.1762  data: 0.0001  max mem: 15821
[16:06:46.164218] Test:  [310/345]  eta: 0:00:06  loss: 0.5576 (0.5699)  time: 0.1766  data: 0.0001  max mem: 15821
[16:06:47.936971] Test:  [320/345]  eta: 0:00:04  loss: 0.5573 (0.5700)  time: 0.1770  data: 0.0001  max mem: 15821
[16:06:49.713655] Test:  [330/345]  eta: 0:00:02  loss: 0.5489 (0.5695)  time: 0.1774  data: 0.0001  max mem: 15821
[16:06:51.491364] Test:  [340/345]  eta: 0:00:00  loss: 0.5605 (0.5695)  time: 0.1777  data: 0.0001  max mem: 15821
[16:06:52.203828] Test:  [344/345]  eta: 0:00:00  loss: 0.5610 (0.5697)  time: 0.1778  data: 0.0001  max mem: 15821
[16:06:52.270658] Test: Total time: 0:00:59 (0.1735 s / it)
[16:07:02.494604] Test:  [ 0/57]  eta: 0:00:26  loss: 0.8014 (0.8014)  time: 0.4709  data: 0.3091  max mem: 15821
[16:07:04.139499] Test:  [10/57]  eta: 0:00:09  loss: 0.6120 (0.6021)  time: 0.1923  data: 0.0282  max mem: 15821
[16:07:05.791221] Test:  [20/57]  eta: 0:00:06  loss: 0.5364 (0.5851)  time: 0.1647  data: 0.0001  max mem: 15821
[16:07:07.445655] Test:  [30/57]  eta: 0:00:04  loss: 0.4729 (0.5396)  time: 0.1652  data: 0.0001  max mem: 15821
[16:07:09.106773] Test:  [40/57]  eta: 0:00:02  loss: 0.3350 (0.5152)  time: 0.1657  data: 0.0001  max mem: 15821
[16:07:10.771550] Test:  [50/57]  eta: 0:00:01  loss: 0.3451 (0.4994)  time: 0.1662  data: 0.0001  max mem: 15821
[16:07:11.670812] Test:  [56/57]  eta: 0:00:00  loss: 0.3982 (0.4940)  time: 0.1614  data: 0.0001  max mem: 15821
[16:07:11.730261] Test: Total time: 0:00:09 (0.1703 s / it)
[16:07:13.399058] Dice score of the network on the train images: 0.669846, val images: 0.755007
[16:07:13.399289] saving best_prec_model_0 @ epoch 5
[16:07:14.645999] saving best_rec_model_0 @ epoch 5
[16:07:15.614127] saving best_dice_model_0 @ epoch 5
[16:07:16.590621] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[16:07:17.530627] Epoch: [6]  [  0/345]  eta: 0:05:23  lr: 0.000038  loss: 0.6084 (0.6084)  time: 0.9389  data: 0.3398  max mem: 15821
[16:07:29.509506] Epoch: [6]  [ 20/345]  eta: 0:03:19  lr: 0.000038  loss: 0.5617 (0.5817)  time: 0.5989  data: 0.0001  max mem: 15821
[16:07:41.529384] Epoch: [6]  [ 40/345]  eta: 0:03:05  lr: 0.000038  loss: 0.5874 (0.5768)  time: 0.6009  data: 0.0001  max mem: 15821
[16:07:53.572783] Epoch: [6]  [ 60/345]  eta: 0:02:52  lr: 0.000039  loss: 0.5757 (0.5786)  time: 0.6021  data: 0.0001  max mem: 15821
[16:08:05.634903] Epoch: [6]  [ 80/345]  eta: 0:02:40  lr: 0.000039  loss: 0.5537 (0.5735)  time: 0.6031  data: 0.0001  max mem: 15821

[16:08:17.720391] Epoch: [6]  [100/345]  eta: 0:02:28  lr: 0.000039  loss: 0.5229 (0.5668)  time: 0.6042  data: 0.0001  max mem: 15821
[16:08:29.840832] Epoch: [6]  [120/345]  eta: 0:02:16  lr: 0.000040  loss: 0.5806 (0.5700)  time: 0.6060  data: 0.0001  max mem: 15821
[16:08:41.964262] Epoch: [6]  [140/345]  eta: 0:02:04  lr: 0.000040  loss: 0.5723 (0.5701)  time: 0.6061  data: 0.0001  max mem: 15821
[16:08:54.098932] Epoch: [6]  [160/345]  eta: 0:01:52  lr: 0.000040  loss: 0.6038 (0.5733)  time: 0.6067  data: 0.0001  max mem: 15821
[16:09:06.207652] Epoch: [6]  [180/345]  eta: 0:01:39  lr: 0.000041  loss: 0.5791 (0.5728)  time: 0.6054  data: 0.0001  max mem: 15821
[16:09:18.309131] Epoch: [6]  [200/345]  eta: 0:01:27  lr: 0.000041  loss: 0.5371 (0.5694)  time: 0.6050  data: 0.0001  max mem: 15821
[16:09:30.407884] Epoch: [6]  [220/345]  eta: 0:01:15  lr: 0.000041  loss: 0.5583 (0.5674)  time: 0.6049  data: 0.0001  max mem: 15821
[16:09:42.492880] Epoch: [6]  [240/345]  eta: 0:01:03  lr: 0.000042  loss: 0.5238 (0.5657)  time: 0.6042  data: 0.0001  max mem: 15821
[16:09:54.578446] Epoch: [6]  [260/345]  eta: 0:00:51  lr: 0.000042  loss: 0.5342 (0.5642)  time: 0.6042  data: 0.0001  max mem: 15821
[16:10:06.672486] Epoch: [6]  [280/345]  eta: 0:00:39  lr: 0.000043  loss: 0.5622 (0.5650)  time: 0.6047  data: 0.0001  max mem: 15821
[16:10:18.760727] Epoch: [6]  [300/345]  eta: 0:00:27  lr: 0.000043  loss: 0.5484 (0.5638)  time: 0.6044  data: 0.0001  max mem: 15821
[16:10:30.845745] Epoch: [6]  [320/345]  eta: 0:00:15  lr: 0.000043  loss: 0.5550 (0.5632)  time: 0.6042  data: 0.0001  max mem: 15821
[16:10:42.944051] Epoch: [6]  [340/345]  eta: 0:00:03  lr: 0.000044  loss: 0.5398 (0.5628)  time: 0.6049  data: 0.0001  max mem: 15821
[16:10:45.361033] Epoch: [6]  [344/345]  eta: 0:00:00  lr: 0.000044  loss: 0.5439 (0.5627)  time: 0.6046  data: 0.0001  max mem: 15821
[16:10:45.427799] Epoch: [6] Total time: 0:03:28 (0.6053 s / it)
[16:10:45.428160] Averaged stats: lr: 0.000044  loss: 0.5439 (0.5627)
[16:10:45.933220] Test:  [  0/345]  eta: 0:02:52  loss: 0.6333 (0.6333)  time: 0.4995  data: 0.3354  max mem: 15821
[16:10:47.600447] Test:  [ 10/345]  eta: 0:01:05  loss: 0.5400 (0.5413)  time: 0.1969  data: 0.0306  max mem: 15821
[16:10:49.268764] Test:  [ 20/345]  eta: 0:00:59  loss: 0.5400 (0.5485)  time: 0.1667  data: 0.0001  max mem: 15821
[16:10:50.942514] Test:  [ 30/345]  eta: 0:00:55  loss: 0.5584 (0.5660)  time: 0.1670  data: 0.0001  max mem: 15821
[16:10:52.618489] Test:  [ 40/345]  eta: 0:00:53  loss: 0.5564 (0.5592)  time: 0.1674  data: 0.0001  max mem: 15821
[16:10:54.297271] Test:  [ 50/345]  eta: 0:00:51  loss: 0.5388 (0.5567)  time: 0.1677  data: 0.0001  max mem: 15821
[16:10:55.979678] Test:  [ 60/345]  eta: 0:00:49  loss: 0.5381 (0.5554)  time: 0.1680  data: 0.0001  max mem: 15821
[16:10:57.664240] Test:  [ 70/345]  eta: 0:00:47  loss: 0.5399 (0.5527)  time: 0.1683  data: 0.0001  max mem: 15821
[16:10:59.353218] Test:  [ 80/345]  eta: 0:00:45  loss: 0.5298 (0.5494)  time: 0.1686  data: 0.0001  max mem: 15821
[16:11:01.046640] Test:  [ 90/345]  eta: 0:00:43  loss: 0.5321 (0.5477)  time: 0.1691  data: 0.0001  max mem: 15821
[16:11:02.742478] Test:  [100/345]  eta: 0:00:41  loss: 0.5321 (0.5445)  time: 0.1694  data: 0.0001  max mem: 15821
[16:11:04.441868] Test:  [110/345]  eta: 0:00:40  loss: 0.5282 (0.5433)  time: 0.1697  data: 0.0001  max mem: 15821
[16:11:06.144944] Test:  [120/345]  eta: 0:00:38  loss: 0.5519 (0.5446)  time: 0.1701  data: 0.0001  max mem: 15821
[16:11:07.850676] Test:  [130/345]  eta: 0:00:36  loss: 0.5300 (0.5426)  time: 0.1704  data: 0.0001  max mem: 15821
[16:11:09.560378] Test:  [140/345]  eta: 0:00:35  loss: 0.5165 (0.5424)  time: 0.1707  data: 0.0001  max mem: 15821
[16:11:11.272635] Test:  [150/345]  eta: 0:00:33  loss: 0.5506 (0.5434)  time: 0.1710  data: 0.0001  max mem: 15821
[16:11:12.989753] Test:  [160/345]  eta: 0:00:31  loss: 0.5400 (0.5424)  time: 0.1714  data: 0.0001  max mem: 15821
[16:11:14.709696] Test:  [170/345]  eta: 0:00:29  loss: 0.5299 (0.5422)  time: 0.1718  data: 0.0001  max mem: 15821
[16:11:16.432774] Test:  [180/345]  eta: 0:00:28  loss: 0.5420 (0.5434)  time: 0.1721  data: 0.0001  max mem: 15821
[16:11:18.160036] Test:  [190/345]  eta: 0:00:26  loss: 0.5584 (0.5426)  time: 0.1725  data: 0.0001  max mem: 15821
[16:11:19.891991] Test:  [200/345]  eta: 0:00:24  loss: 0.5426 (0.5437)  time: 0.1729  data: 0.0001  max mem: 15821
[16:11:21.626575] Test:  [210/345]  eta: 0:00:23  loss: 0.5426 (0.5436)  time: 0.1733  data: 0.0001  max mem: 15821
[16:11:23.364063] Test:  [220/345]  eta: 0:00:21  loss: 0.5386 (0.5436)  time: 0.1735  data: 0.0001  max mem: 15821
[16:11:25.103898] Test:  [230/345]  eta: 0:00:19  loss: 0.5386 (0.5428)  time: 0.1738  data: 0.0001  max mem: 15821
[16:11:26.847523] Test:  [240/345]  eta: 0:00:18  loss: 0.5219 (0.5420)  time: 0.1741  data: 0.0001  max mem: 15821
[16:11:28.595309] Test:  [250/345]  eta: 0:00:16  loss: 0.5304 (0.5408)  time: 0.1745  data: 0.0001  max mem: 15821
[16:11:30.347066] Test:  [260/345]  eta: 0:00:14  loss: 0.5404 (0.5410)  time: 0.1749  data: 0.0001  max mem: 15821
[16:11:32.100371] Test:  [270/345]  eta: 0:00:12  loss: 0.5460 (0.5409)  time: 0.1752  data: 0.0001  max mem: 15821
[16:11:33.859234] Test:  [280/345]  eta: 0:00:11  loss: 0.5275 (0.5397)  time: 0.1755  data: 0.0001  max mem: 15821
[16:11:35.619838] Test:  [290/345]  eta: 0:00:09  loss: 0.5222 (0.5393)  time: 0.1759  data: 0.0001  max mem: 15821
[16:11:37.385058] Test:  [300/345]  eta: 0:00:07  loss: 0.5145 (0.5384)  time: 0.1762  data: 0.0001  max mem: 15821
[16:11:39.153902] Test:  [310/345]  eta: 0:00:06  loss: 0.5301 (0.5385)  time: 0.1766  data: 0.0001  max mem: 15821
[16:11:40.927120] Test:  [320/345]  eta: 0:00:04  loss: 0.5472 (0.5390)  time: 0.1770  data: 0.0001  max mem: 15821
[16:11:42.704332] Test:  [330/345]  eta: 0:00:02  loss: 0.5288 (0.5383)  time: 0.1775  data: 0.0001  max mem: 15821
[16:11:44.483031] Test:  [340/345]  eta: 0:00:00  loss: 0.5315 (0.5380)  time: 0.1777  data: 0.0001  max mem: 15821
[16:11:45.195591] Test:  [344/345]  eta: 0:00:00  loss: 0.5315 (0.5378)  time: 0.1778  data: 0.0001  max mem: 15821
[16:11:45.261758] Test: Total time: 0:00:59 (0.1734 s / it)
[16:11:55.643168] Test:  [ 0/57]  eta: 0:00:27  loss: 0.7774 (0.7774)  time: 0.4780  data: 0.3159  max mem: 15821
[16:11:57.287585] Test:  [10/57]  eta: 0:00:09  loss: 0.5850 (0.5766)  time: 0.1928  data: 0.0288  max mem: 15821
[16:11:58.938342] Test:  [20/57]  eta: 0:00:06  loss: 0.4997 (0.5637)  time: 0.1647  data: 0.0001  max mem: 15821
[16:12:00.593305] Test:  [30/57]  eta: 0:00:04  loss: 0.4558 (0.5232)  time: 0.1652  data: 0.0001  max mem: 15821
[16:12:02.252758] Test:  [40/57]  eta: 0:00:02  loss: 0.3396 (0.5018)  time: 0.1657  data: 0.0001  max mem: 15821
[16:12:03.918911] Test:  [50/57]  eta: 0:00:01  loss: 0.3604 (0.4937)  time: 0.1662  data: 0.0001  max mem: 15821
[16:12:04.816613] Test:  [56/57]  eta: 0:00:00  loss: 0.4193 (0.4944)  time: 0.1614  data: 0.0001  max mem: 15821
[16:12:04.880397] Test: Total time: 0:00:09 (0.1705 s / it)
[16:12:06.604656] Dice score of the network on the train images: 0.710397, val images: 0.744321
[16:12:06.604875] saving best_prec_model_0 @ epoch 6
[16:12:07.815081] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[16:12:08.743513] Epoch: [7]  [  0/345]  eta: 0:05:19  lr: 0.000044  loss: 0.5556 (0.5556)  time: 0.9272  data: 0.3284  max mem: 15821
[16:12:20.722608] Epoch: [7]  [ 20/345]  eta: 0:03:19  lr: 0.000044  loss: 0.5333 (0.5295)  time: 0.5989  data: 0.0001  max mem: 15821
[16:12:32.757940] Epoch: [7]  [ 40/345]  eta: 0:03:05  lr: 0.000044  loss: 0.5266 (0.5186)  time: 0.6017  data: 0.0001  max mem: 15821
[16:12:44.802741] Epoch: [7]  [ 60/345]  eta: 0:02:52  lr: 0.000045  loss: 0.4644 (0.5016)  time: 0.6022  data: 0.0001  max mem: 15821
[16:12:56.856728] Epoch: [7]  [ 80/345]  eta: 0:02:40  lr: 0.000045  loss: 0.4765 (0.5036)  time: 0.6026  data: 0.0001  max mem: 15821
[16:13:08.929921] Epoch: [7]  [100/345]  eta: 0:02:28  lr: 0.000046  loss: 0.4460 (0.4920)  time: 0.6036  data: 0.0001  max mem: 15821
[16:13:21.016366] Epoch: [7]  [120/345]  eta: 0:02:16  lr: 0.000046  loss: 0.4494 (0.4843)  time: 0.6043  data: 0.0001  max mem: 15821
[16:13:33.127854] Epoch: [7]  [140/345]  eta: 0:02:04  lr: 0.000046  loss: 0.4099 (0.4737)  time: 0.6055  data: 0.0001  max mem: 15821
[16:13:45.240354] Epoch: [7]  [160/345]  eta: 0:01:51  lr: 0.000047  loss: 0.3751 (0.4623)  time: 0.6056  data: 0.0001  max mem: 15821
[16:13:57.339084] Epoch: [7]  [180/345]  eta: 0:01:39  lr: 0.000047  loss: 0.3968 (0.4570)  time: 0.6049  data: 0.0001  max mem: 15821
[16:14:09.438872] Epoch: [7]  [200/345]  eta: 0:01:27  lr: 0.000047  loss: 0.4111 (0.4535)  time: 0.6049  data: 0.0001  max mem: 15821
[16:14:21.542857] Epoch: [7]  [220/345]  eta: 0:01:15  lr: 0.000048  loss: 0.3626 (0.4452)  time: 0.6051  data: 0.0001  max mem: 15821
[16:14:33.615336] Epoch: [7]  [240/345]  eta: 0:01:03  lr: 0.000048  loss: 0.3697 (0.4395)  time: 0.6036  data: 0.0001  max mem: 15821
[16:14:45.700612] Epoch: [7]  [260/345]  eta: 0:00:51  lr: 0.000048  loss: 0.3455 (0.4335)  time: 0.6042  data: 0.0001  max mem: 15821
[16:14:57.777022] Epoch: [7]  [280/345]  eta: 0:00:39  lr: 0.000049  loss: 0.3923 (0.4312)  time: 0.6038  data: 0.0001  max mem: 15821
[16:15:09.858786] Epoch: [7]  [300/345]  eta: 0:00:27  lr: 0.000049  loss: 0.3849 (0.4276)  time: 0.6040  data: 0.0001  max mem: 15821
[16:15:21.935684] Epoch: [7]  [320/345]  eta: 0:00:15  lr: 0.000050  loss: 0.3963 (0.4265)  time: 0.6038  data: 0.0001  max mem: 15821
[16:15:34.000658] Epoch: [7]  [340/345]  eta: 0:00:03  lr: 0.000050  loss: 0.3947 (0.4244)  time: 0.6032  data: 0.0001  max mem: 15821
[16:15:36.413044] Epoch: [7]  [344/345]  eta: 0:00:00  lr: 0.000050  loss: 0.3810 (0.4240)  time: 0.6031  data: 0.0001  max mem: 15821
[16:15:36.476393] Epoch: [7] Total time: 0:03:28 (0.6048 s / it)
[16:15:36.476764] Averaged stats: lr: 0.000050  loss: 0.3810 (0.4240)
[16:15:37.051476] Test:  [  0/345]  eta: 0:03:16  loss: 0.4563 (0.4563)  time: 0.5688  data: 0.4046  max mem: 15821
[16:15:38.717095] Test:  [ 10/345]  eta: 0:01:08  loss: 0.3961 (0.3919)  time: 0.2031  data: 0.0369  max mem: 15821
[16:15:40.385670] Test:  [ 20/345]  eta: 0:01:00  loss: 0.3925 (0.3988)  time: 0.1666  data: 0.0001  max mem: 15821
[16:15:42.057682] Test:  [ 30/345]  eta: 0:00:56  loss: 0.3925 (0.4044)  time: 0.1670  data: 0.0001  max mem: 15821
[16:15:43.733069] Test:  [ 40/345]  eta: 0:00:53  loss: 0.4044 (0.4026)  time: 0.1673  data: 0.0001  max mem: 15821
[16:15:45.412260] Test:  [ 50/345]  eta: 0:00:51  loss: 0.4003 (0.4055)  time: 0.1677  data: 0.0001  max mem: 15821
[16:15:47.094500] Test:  [ 60/345]  eta: 0:00:49  loss: 0.4013 (0.4026)  time: 0.1680  data: 0.0001  max mem: 15821
[16:15:48.781154] Test:  [ 70/345]  eta: 0:00:47  loss: 0.4022 (0.4045)  time: 0.1684  data: 0.0001  max mem: 15821
[16:15:50.469492] Test:  [ 80/345]  eta: 0:00:45  loss: 0.3961 (0.4033)  time: 0.1687  data: 0.0001  max mem: 15821
[16:15:52.161953] Test:  [ 90/345]  eta: 0:00:43  loss: 0.4131 (0.4080)  time: 0.1690  data: 0.0001  max mem: 15821
[16:15:53.857986] Test:  [100/345]  eta: 0:00:42  loss: 0.4155 (0.4082)  time: 0.1694  data: 0.0001  max mem: 15821
[16:15:55.556824] Test:  [110/345]  eta: 0:00:40  loss: 0.3956 (0.4062)  time: 0.1697  data: 0.0001  max mem: 15821
[16:15:57.259683] Test:  [120/345]  eta: 0:00:38  loss: 0.3989 (0.4065)  time: 0.1700  data: 0.0001  max mem: 15821
[16:15:58.967124] Test:  [130/345]  eta: 0:00:36  loss: 0.4086 (0.4078)  time: 0.1704  data: 0.0001  max mem: 15821
[16:16:00.677020] Test:  [140/345]  eta: 0:00:35  loss: 0.4048 (0.4060)  time: 0.1708  data: 0.0001  max mem: 15821
[16:16:02.390560] Test:  [150/345]  eta: 0:00:33  loss: 0.4048 (0.4060)  time: 0.1711  data: 0.0001  max mem: 15821
[16:16:04.107390] Test:  [160/345]  eta: 0:00:31  loss: 0.4197 (0.4065)  time: 0.1715  data: 0.0001  max mem: 15821
[16:16:05.827895] Test:  [170/345]  eta: 0:00:30  loss: 0.3782 (0.4050)  time: 0.1718  data: 0.0001  max mem: 15821
[16:16:07.552493] Test:  [180/345]  eta: 0:00:28  loss: 0.3782 (0.4056)  time: 0.1722  data: 0.0001  max mem: 15821
[16:16:09.279217] Test:  [190/345]  eta: 0:00:26  loss: 0.4037 (0.4048)  time: 0.1725  data: 0.0001  max mem: 15821
[16:16:11.010337] Test:  [200/345]  eta: 0:00:24  loss: 0.3926 (0.4051)  time: 0.1728  data: 0.0001  max mem: 15821
[16:16:12.744747] Test:  [210/345]  eta: 0:00:23  loss: 0.3948 (0.4048)  time: 0.1732  data: 0.0001  max mem: 15821
[16:16:14.480787] Test:  [220/345]  eta: 0:00:21  loss: 0.3966 (0.4052)  time: 0.1735  data: 0.0001  max mem: 15821
[16:16:16.220909] Test:  [230/345]  eta: 0:00:19  loss: 0.4190 (0.4063)  time: 0.1737  data: 0.0001  max mem: 15821
[16:16:17.964806] Test:  [240/345]  eta: 0:00:18  loss: 0.4106 (0.4066)  time: 0.1741  data: 0.0001  max mem: 15821
[16:16:19.711947] Test:  [250/345]  eta: 0:00:16  loss: 0.3993 (0.4061)  time: 0.1745  data: 0.0001  max mem: 15821
[16:16:21.463485] Test:  [260/345]  eta: 0:00:14  loss: 0.3869 (0.4063)  time: 0.1749  data: 0.0001  max mem: 15821
[16:16:23.217084] Test:  [270/345]  eta: 0:00:12  loss: 0.3883 (0.4060)  time: 0.1752  data: 0.0001  max mem: 15821
[16:16:24.973912] Test:  [280/345]  eta: 0:00:11  loss: 0.4066 (0.4058)  time: 0.1755  data: 0.0001  max mem: 15821
[16:16:26.734582] Test:  [290/345]  eta: 0:00:09  loss: 0.4140 (0.4055)  time: 0.1758  data: 0.0001  max mem: 15821
[16:16:28.498645] Test:  [300/345]  eta: 0:00:07  loss: 0.3811 (0.4046)  time: 0.1762  data: 0.0001  max mem: 15821
[16:16:30.267699] Test:  [310/345]  eta: 0:00:06  loss: 0.3773 (0.4046)  time: 0.1766  data: 0.0001  max mem: 15821
[16:16:32.040188] Test:  [320/345]  eta: 0:00:04  loss: 0.3834 (0.4045)  time: 0.1770  data: 0.0001  max mem: 15821
[16:16:33.816114] Test:  [330/345]  eta: 0:00:02  loss: 0.3934 (0.4042)  time: 0.1774  data: 0.0001  max mem: 15821
[16:16:35.595822] Test:  [340/345]  eta: 0:00:00  loss: 0.3971 (0.4049)  time: 0.1777  data: 0.0001  max mem: 15821
[16:16:36.308608] Test:  [344/345]  eta: 0:00:00  loss: 0.4103 (0.4051)  time: 0.1779  data: 0.0001  max mem: 15821
[16:16:36.376613] Test: Total time: 0:00:59 (0.1736 s / it)
[16:16:46.684998] Test:  [ 0/57]  eta: 0:00:27  loss: 0.3988 (0.3988)  time: 0.4776  data: 0.3152  max mem: 15821
[16:16:48.330216] Test:  [10/57]  eta: 0:00:09  loss: 0.4322 (0.4434)  time: 0.1929  data: 0.0287  max mem: 15821
[16:16:49.982591] Test:  [20/57]  eta: 0:00:06  loss: 0.4221 (0.4205)  time: 0.1648  data: 0.0001  max mem: 15821
[16:16:51.637848] Test:  [30/57]  eta: 0:00:04  loss: 0.3385 (0.3879)  time: 0.1653  data: 0.0001  max mem: 15821
[16:16:53.299416] Test:  [40/57]  eta: 0:00:02  loss: 0.3005 (0.3635)  time: 0.1658  data: 0.0001  max mem: 15821
[16:16:54.963701] Test:  [50/57]  eta: 0:00:01  loss: 0.3005 (0.3590)  time: 0.1662  data: 0.0001  max mem: 15821
[16:16:55.861777] Test:  [56/57]  eta: 0:00:00  loss: 0.3454 (0.3709)  time: 0.1613  data: 0.0001  max mem: 15821
[16:16:55.916924] Test: Total time: 0:00:09 (0.1704 s / it)
[16:16:57.614129] Dice score of the network on the train images: 0.688788, val images: 0.750985
[16:16:57.618409] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[16:16:58.609261] Epoch: [8]  [  0/345]  eta: 0:05:41  lr: 0.000050  loss: 0.4557 (0.4557)  time: 0.9899  data: 0.3885  max mem: 15821
[16:17:10.606194] Epoch: [8]  [ 20/345]  eta: 0:03:20  lr: 0.000050  loss: 0.3417 (0.3673)  time: 0.5998  data: 0.0001  max mem: 15821
[16:17:22.620738] Epoch: [8]  [ 40/345]  eta: 0:03:05  lr: 0.000051  loss: 0.3911 (0.3793)  time: 0.6007  data: 0.0001  max mem: 15821
[16:17:34.645723] Epoch: [8]  [ 60/345]  eta: 0:02:52  lr: 0.000051  loss: 0.3954 (0.3808)  time: 0.6012  data: 0.0001  max mem: 15821
[16:17:46.690773] Epoch: [8]  [ 80/345]  eta: 0:02:40  lr: 0.000051  loss: 0.3929 (0.3804)  time: 0.6022  data: 0.0001  max mem: 15821
[16:17:58.749262] Epoch: [8]  [100/345]  eta: 0:02:28  lr: 0.000052  loss: 0.3496 (0.3738)  time: 0.6029  data: 0.0001  max mem: 15821
[16:18:10.950474] Epoch: [8]  [120/345]  eta: 0:02:16  lr: 0.000052  loss: 0.3584 (0.3716)  time: 0.6100  data: 0.0001  max mem: 15821
[16:18:23.050488] Epoch: [8]  [140/345]  eta: 0:02:04  lr: 0.000053  loss: 0.3619 (0.3717)  time: 0.6050  data: 0.0001  max mem: 15821
[16:18:35.157543] Epoch: [8]  [160/345]  eta: 0:01:52  lr: 0.000053  loss: 0.3561 (0.3696)  time: 0.6053  data: 0.0001  max mem: 15821
[16:18:47.251811] Epoch: [8]  [180/345]  eta: 0:01:39  lr: 0.000053  loss: 0.3362 (0.3673)  time: 0.6047  data: 0.0001  max mem: 15821
[16:18:59.355174] Epoch: [8]  [200/345]  eta: 0:01:27  lr: 0.000054  loss: 0.3722 (0.3673)  time: 0.6051  data: 0.0001  max mem: 15821
[16:19:11.455215] Epoch: [8]  [220/345]  eta: 0:01:15  lr: 0.000054  loss: 0.3275 (0.3647)  time: 0.6050  data: 0.0001  max mem: 15821
[16:19:23.554053] Epoch: [8]  [240/345]  eta: 0:01:03  lr: 0.000054  loss: 0.3342 (0.3633)  time: 0.6049  data: 0.0001  max mem: 15821
[16:19:35.641841] Epoch: [8]  [260/345]  eta: 0:00:51  lr: 0.000055  loss: 0.3344 (0.3614)  time: 0.6043  data: 0.0001  max mem: 15821
[16:19:47.729612] Epoch: [8]  [280/345]  eta: 0:00:39  lr: 0.000055  loss: 0.3350 (0.3596)  time: 0.6043  data: 0.0001  max mem: 15821
[16:19:59.802604] Epoch: [8]  [300/345]  eta: 0:00:27  lr: 0.000055  loss: 0.3545 (0.3602)  time: 0.6036  data: 0.0001  max mem: 15821
[16:20:11.866230] Epoch: [8]  [320/345]  eta: 0:00:15  lr: 0.000056  loss: 0.3235 (0.3586)  time: 0.6031  data: 0.0001  max mem: 15821
[16:20:23.933394] Epoch: [8]  [340/345]  eta: 0:00:03  lr: 0.000056  loss: 0.3434 (0.3580)  time: 0.6033  data: 0.0001  max mem: 15821
[16:20:26.347870] Epoch: [8]  [344/345]  eta: 0:00:00  lr: 0.000056  loss: 0.3296 (0.3577)  time: 0.6034  data: 0.0001  max mem: 15821
[16:20:26.420029] Epoch: [8] Total time: 0:03:28 (0.6052 s / it)
[16:20:26.420371] Averaged stats: lr: 0.000056  loss: 0.3296 (0.3577)
[16:20:26.931246] Test:  [  0/345]  eta: 0:02:54  loss: 0.3722 (0.3722)  time: 0.5051  data: 0.3407  max mem: 15821
[16:20:28.598895] Test:  [ 10/345]  eta: 0:01:06  loss: 0.3123 (0.3158)  time: 0.1974  data: 0.0311  max mem: 15821
[16:20:30.268463] Test:  [ 20/345]  eta: 0:00:59  loss: 0.3205 (0.3250)  time: 0.1668  data: 0.0001  max mem: 15821
[16:20:31.940452] Test:  [ 30/345]  eta: 0:00:56  loss: 0.3205 (0.3207)  time: 0.1670  data: 0.0001  max mem: 15821
[16:20:33.616927] Test:  [ 40/345]  eta: 0:00:53  loss: 0.3189 (0.3244)  time: 0.1673  data: 0.0001  max mem: 15821
[16:20:35.295104] Test:  [ 50/345]  eta: 0:00:51  loss: 0.3285 (0.3238)  time: 0.1677  data: 0.0001  max mem: 15821
[16:20:36.976907] Test:  [ 60/345]  eta: 0:00:49  loss: 0.3285 (0.3294)  time: 0.1679  data: 0.0001  max mem: 15821
[16:20:38.662161] Test:  [ 70/345]  eta: 0:00:47  loss: 0.3250 (0.3267)  time: 0.1683  data: 0.0001  max mem: 15821
[16:20:40.352237] Test:  [ 80/345]  eta: 0:00:45  loss: 0.3292 (0.3305)  time: 0.1687  data: 0.0001  max mem: 15821
[16:20:42.045288] Test:  [ 90/345]  eta: 0:00:43  loss: 0.3233 (0.3277)  time: 0.1691  data: 0.0001  max mem: 15821
[16:20:43.742130] Test:  [100/345]  eta: 0:00:41  loss: 0.3301 (0.3296)  time: 0.1694  data: 0.0001  max mem: 15821
[16:20:45.442170] Test:  [110/345]  eta: 0:00:40  loss: 0.3122 (0.3257)  time: 0.1698  data: 0.0001  max mem: 15821
[16:20:47.144738] Test:  [120/345]  eta: 0:00:38  loss: 0.2948 (0.3263)  time: 0.1701  data: 0.0001  max mem: 15821
[16:20:48.850752] Test:  [130/345]  eta: 0:00:36  loss: 0.3257 (0.3271)  time: 0.1704  data: 0.0001  max mem: 15821
[16:20:50.559532] Test:  [140/345]  eta: 0:00:35  loss: 0.3257 (0.3262)  time: 0.1707  data: 0.0001  max mem: 15821
[16:20:52.271966] Test:  [150/345]  eta: 0:00:33  loss: 0.3160 (0.3271)  time: 0.1710  data: 0.0001  max mem: 15821
[16:20:53.989159] Test:  [160/345]  eta: 0:00:31  loss: 0.3230 (0.3274)  time: 0.1714  data: 0.0001  max mem: 15821
[16:20:55.711410] Test:  [170/345]  eta: 0:00:29  loss: 0.3216 (0.3267)  time: 0.1719  data: 0.0001  max mem: 15821
[16:20:57.433900] Test:  [180/345]  eta: 0:00:28  loss: 0.2947 (0.3255)  time: 0.1722  data: 0.0001  max mem: 15821
[16:20:59.160492] Test:  [190/345]  eta: 0:00:26  loss: 0.3025 (0.3254)  time: 0.1724  data: 0.0001  max mem: 15821
[16:21:00.891377] Test:  [200/345]  eta: 0:00:24  loss: 0.3257 (0.3246)  time: 0.1728  data: 0.0001  max mem: 15821
[16:21:02.625915] Test:  [210/345]  eta: 0:00:23  loss: 0.3138 (0.3238)  time: 0.1732  data: 0.0001  max mem: 15821
[16:21:04.361855] Test:  [220/345]  eta: 0:00:21  loss: 0.3020 (0.3235)  time: 0.1735  data: 0.0001  max mem: 15821
[16:21:06.103680] Test:  [230/345]  eta: 0:00:19  loss: 0.3114 (0.3236)  time: 0.1738  data: 0.0001  max mem: 15821
[16:21:07.846521] Test:  [240/345]  eta: 0:00:18  loss: 0.3095 (0.3230)  time: 0.1742  data: 0.0001  max mem: 15821
[16:21:09.594690] Test:  [250/345]  eta: 0:00:16  loss: 0.3086 (0.3225)  time: 0.1745  data: 0.0001  max mem: 15821
[16:21:11.345547] Test:  [260/345]  eta: 0:00:14  loss: 0.3160 (0.3228)  time: 0.1749  data: 0.0001  max mem: 15821
[16:21:13.099652] Test:  [270/345]  eta: 0:00:12  loss: 0.3378 (0.3236)  time: 0.1752  data: 0.0001  max mem: 15821
[16:21:14.857980] Test:  [280/345]  eta: 0:00:11  loss: 0.3305 (0.3239)  time: 0.1756  data: 0.0001  max mem: 15821
[16:21:16.619696] Test:  [290/345]  eta: 0:00:09  loss: 0.3272 (0.3242)  time: 0.1759  data: 0.0001  max mem: 15821
[16:21:18.384532] Test:  [300/345]  eta: 0:00:07  loss: 0.3513 (0.3256)  time: 0.1763  data: 0.0001  max mem: 15821
[16:21:20.154359] Test:  [310/345]  eta: 0:00:06  loss: 0.3504 (0.3258)  time: 0.1767  data: 0.0001  max mem: 15821
[16:21:21.925853] Test:  [320/345]  eta: 0:00:04  loss: 0.3117 (0.3259)  time: 0.1770  data: 0.0001  max mem: 15821
[16:21:23.702998] Test:  [330/345]  eta: 0:00:02  loss: 0.3117 (0.3255)  time: 0.1774  data: 0.0001  max mem: 15821
[16:21:25.481656] Test:  [340/345]  eta: 0:00:00  loss: 0.3110 (0.3251)  time: 0.1777  data: 0.0001  max mem: 15821
[16:21:26.195124] Test:  [344/345]  eta: 0:00:00  loss: 0.3115 (0.3253)  time: 0.1778  data: 0.0001  max mem: 15821
[16:21:26.247133] Test: Total time: 0:00:59 (0.1734 s / it)
[16:21:36.486415] Test:  [ 0/57]  eta: 0:00:27  loss: 0.3518 (0.3518)  time: 0.4767  data: 0.3145  max mem: 15821
[16:21:38.132325] Test:  [10/57]  eta: 0:00:09  loss: 0.3941 (0.4050)  time: 0.1929  data: 0.0287  max mem: 15821
[16:21:39.782614] Test:  [20/57]  eta: 0:00:06  loss: 0.3726 (0.3769)  time: 0.1647  data: 0.0001  max mem: 15821
[16:21:41.437717] Test:  [30/57]  eta: 0:00:04  loss: 0.2919 (0.3425)  time: 0.1652  data: 0.0001  max mem: 15821
[16:21:43.098112] Test:  [40/57]  eta: 0:00:02  loss: 0.2703 (0.3247)  time: 0.1657  data: 0.0001  max mem: 15821
[16:21:44.764831] Test:  [50/57]  eta: 0:00:01  loss: 0.2821 (0.3252)  time: 0.1663  data: 0.0001  max mem: 15821
[16:21:45.663943] Test:  [56/57]  eta: 0:00:00  loss: 0.3156 (0.3362)  time: 0.1615  data: 0.0001  max mem: 15821
[16:21:45.715801] Test: Total time: 0:00:09 (0.1703 s / it)
[16:21:47.406808] Dice score of the network on the train images: 0.705685, val images: 0.763757
[16:21:47.407030] saving best_dice_model_0 @ epoch 8
[16:21:48.499367] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[16:21:49.484447] Epoch: [9]  [  0/345]  eta: 0:05:39  lr: 0.000056  loss: 0.3416 (0.3416)  time: 0.9841  data: 0.3839  max mem: 15821
[16:22:01.471978] Epoch: [9]  [ 20/345]  eta: 0:03:20  lr: 0.000057  loss: 0.3531 (0.3493)  time: 0.5993  data: 0.0001  max mem: 15821
[16:22:13.485341] Epoch: [9]  [ 40/345]  eta: 0:03:05  lr: 0.000057  loss: 0.3606 (0.3459)  time: 0.6006  data: 0.0001  max mem: 15821
[16:22:25.510258] Epoch: [9]  [ 60/345]  eta: 0:02:52  lr: 0.000057  loss: 0.3443 (0.3479)  time: 0.6012  data: 0.0001  max mem: 15821
[16:22:37.555838] Epoch: [9]  [ 80/345]  eta: 0:02:40  lr: 0.000058  loss: 0.3339 (0.3439)  time: 0.6022  data: 0.0001  max mem: 15821
[16:22:49.620392] Epoch: [9]  [100/345]  eta: 0:02:28  lr: 0.000058  loss: 0.3229 (0.3412)  time: 0.6032  data: 0.0001  max mem: 15821
[16:23:01.700165] Epoch: [9]  [120/345]  eta: 0:02:16  lr: 0.000058  loss: 0.3444 (0.3401)  time: 0.6039  data: 0.0001  max mem: 15821
[16:23:13.792589] Epoch: [9]  [140/345]  eta: 0:02:03  lr: 0.000059  loss: 0.3147 (0.3381)  time: 0.6046  data: 0.0001  max mem: 15821
[16:23:25.885947] Epoch: [9]  [160/345]  eta: 0:01:51  lr: 0.000059  loss: 0.3488 (0.3390)  time: 0.6046  data: 0.0001  max mem: 15821
[16:23:37.981175] Epoch: [9]  [180/345]  eta: 0:01:39  lr: 0.000060  loss: 0.3453 (0.3396)  time: 0.6047  data: 0.0001  max mem: 15821
[16:23:50.078137] Epoch: [9]  [200/345]  eta: 0:01:27  lr: 0.000060  loss: 0.3132 (0.3362)  time: 0.6048  data: 0.0001  max mem: 15821
[16:24:02.169628] Epoch: [9]  [220/345]  eta: 0:01:15  lr: 0.000060  loss: 0.3040 (0.3339)  time: 0.6045  data: 0.0001  max mem: 15821
[16:24:14.255072] Epoch: [9]  [240/345]  eta: 0:01:03  lr: 0.000061  loss: 0.3424 (0.3345)  time: 0.6042  data: 0.0001  max mem: 15821
[16:24:26.345037] Epoch: [9]  [260/345]  eta: 0:00:51  lr: 0.000061  loss: 0.3399 (0.3357)  time: 0.6044  data: 0.0001  max mem: 15821
[16:24:38.425939] Epoch: [9]  [280/345]  eta: 0:00:39  lr: 0.000061  loss: 0.3325 (0.3359)  time: 0.6040  data: 0.0001  max mem: 15821
[16:24:50.487883] Epoch: [9]  [300/345]  eta: 0:00:27  lr: 0.000062  loss: 0.3285 (0.3352)  time: 0.6030  data: 0.0001  max mem: 15821
[16:25:02.560000] Epoch: [9]  [320/345]  eta: 0:00:15  lr: 0.000062  loss: 0.3308 (0.3353)  time: 0.6036  data: 0.0001  max mem: 15821
[16:25:14.614684] Epoch: [9]  [340/345]  eta: 0:00:03  lr: 0.000062  loss: 0.3212 (0.3346)  time: 0.6027  data: 0.0001  max mem: 15821
[16:25:17.030230] Epoch: [9]  [344/345]  eta: 0:00:00  lr: 0.000062  loss: 0.3252 (0.3346)  time: 0.6027  data: 0.0001  max mem: 15821
[16:25:17.100970] Epoch: [9] Total time: 0:03:28 (0.6046 s / it)
[16:25:17.101181] Averaged stats: lr: 0.000062  loss: 0.3252 (0.3346)
[16:25:17.609457] Test:  [  0/345]  eta: 0:02:53  loss: 0.3721 (0.3721)  time: 0.5027  data: 0.3386  max mem: 15821
[16:25:19.276286] Test:  [ 10/345]  eta: 0:01:06  loss: 0.3690 (0.3696)  time: 0.1971  data: 0.0309  max mem: 15821
[16:25:20.946032] Test:  [ 20/345]  eta: 0:00:59  loss: 0.3511 (0.3586)  time: 0.1667  data: 0.0001  max mem: 15821
[16:25:22.619031] Test:  [ 30/345]  eta: 0:00:55  loss: 0.3384 (0.3566)  time: 0.1671  data: 0.0001  max mem: 15821
[16:25:24.295393] Test:  [ 40/345]  eta: 0:00:53  loss: 0.3290 (0.3509)  time: 0.1674  data: 0.0001  max mem: 15821
[16:25:25.974994] Test:  [ 50/345]  eta: 0:00:51  loss: 0.3354 (0.3451)  time: 0.1677  data: 0.0001  max mem: 15821
[16:25:27.658678] Test:  [ 60/345]  eta: 0:00:49  loss: 0.3189 (0.3440)  time: 0.1681  data: 0.0001  max mem: 15821
[16:25:29.345281] Test:  [ 70/345]  eta: 0:00:47  loss: 0.3221 (0.3423)  time: 0.1685  data: 0.0001  max mem: 15821
[16:25:31.034846] Test:  [ 80/345]  eta: 0:00:45  loss: 0.3221 (0.3389)  time: 0.1687  data: 0.0001  max mem: 15821
[16:25:32.728256] Test:  [ 90/345]  eta: 0:00:43  loss: 0.3274 (0.3391)  time: 0.1691  data: 0.0001  max mem: 15821
[16:25:34.425668] Test:  [100/345]  eta: 0:00:41  loss: 0.3028 (0.3358)  time: 0.1695  data: 0.0001  max mem: 15821
[16:25:36.125743] Test:  [110/345]  eta: 0:00:40  loss: 0.3246 (0.3363)  time: 0.1698  data: 0.0001  max mem: 15821
[16:25:37.829022] Test:  [120/345]  eta: 0:00:38  loss: 0.3151 (0.3344)  time: 0.1701  data: 0.0001  max mem: 15821
[16:25:39.535988] Test:  [130/345]  eta: 0:00:36  loss: 0.3151 (0.3336)  time: 0.1704  data: 0.0001  max mem: 15821
[16:25:41.246639] Test:  [140/345]  eta: 0:00:35  loss: 0.3181 (0.3315)  time: 0.1708  data: 0.0001  max mem: 15821
[16:25:42.961133] Test:  [150/345]  eta: 0:00:33  loss: 0.3181 (0.3323)  time: 0.1712  data: 0.0001  max mem: 15821
[16:25:44.677833] Test:  [160/345]  eta: 0:00:31  loss: 0.3456 (0.3331)  time: 0.1715  data: 0.0001  max mem: 15821
[16:25:46.399523] Test:  [170/345]  eta: 0:00:29  loss: 0.3373 (0.3318)  time: 0.1719  data: 0.0001  max mem: 15821
[16:25:48.123951] Test:  [180/345]  eta: 0:00:28  loss: 0.3206 (0.3306)  time: 0.1722  data: 0.0001  max mem: 15821
[16:25:49.851217] Test:  [190/345]  eta: 0:00:26  loss: 0.3275 (0.3313)  time: 0.1725  data: 0.0001  max mem: 15821
[16:25:51.581881] Test:  [200/345]  eta: 0:00:24  loss: 0.3237 (0.3318)  time: 0.1728  data: 0.0001  max mem: 15821
[16:25:53.314425] Test:  [210/345]  eta: 0:00:23  loss: 0.3317 (0.3319)  time: 0.1731  data: 0.0001  max mem: 15821
[16:25:55.052575] Test:  [220/345]  eta: 0:00:21  loss: 0.3362 (0.3320)  time: 0.1735  data: 0.0001  max mem: 15821
[16:25:56.792602] Test:  [230/345]  eta: 0:00:19  loss: 0.3311 (0.3309)  time: 0.1738  data: 0.0001  max mem: 15821
[16:25:58.537270] Test:  [240/345]  eta: 0:00:18  loss: 0.3354 (0.3316)  time: 0.1742  data: 0.0001  max mem: 15821
[16:26:00.285727] Test:  [250/345]  eta: 0:00:16  loss: 0.3343 (0.3314)  time: 0.1746  data: 0.0001  max mem: 15821
[16:26:02.037519] Test:  [260/345]  eta: 0:00:14  loss: 0.3134 (0.3315)  time: 0.1750  data: 0.0001  max mem: 15821
[16:26:03.792101] Test:  [270/345]  eta: 0:00:12  loss: 0.3482 (0.3323)  time: 0.1753  data: 0.0001  max mem: 15821
[16:26:05.550290] Test:  [280/345]  eta: 0:00:11  loss: 0.3381 (0.3323)  time: 0.1756  data: 0.0001  max mem: 15821
[16:26:07.310692] Test:  [290/345]  eta: 0:00:09  loss: 0.3375 (0.3326)  time: 0.1759  data: 0.0001  max mem: 15821
[16:26:09.077238] Test:  [300/345]  eta: 0:00:07  loss: 0.3388 (0.3326)  time: 0.1763  data: 0.0001  max mem: 15821
[16:26:10.846738] Test:  [310/345]  eta: 0:00:06  loss: 0.3216 (0.3325)  time: 0.1767  data: 0.0001  max mem: 15821
[16:26:12.621045] Test:  [320/345]  eta: 0:00:04  loss: 0.3373 (0.3330)  time: 0.1771  data: 0.0001  max mem: 15821
[16:26:14.397711] Test:  [330/345]  eta: 0:00:02  loss: 0.3839 (0.3346)  time: 0.1775  data: 0.0001  max mem: 15821
[16:26:16.176827] Test:  [340/345]  eta: 0:00:00  loss: 0.3683 (0.3346)  time: 0.1777  data: 0.0001  max mem: 15821
[16:26:16.889364] Test:  [344/345]  eta: 0:00:00  loss: 0.3351 (0.3347)  time: 0.1779  data: 0.0001  max mem: 15821
[16:26:16.959031] Test: Total time: 0:00:59 (0.1735 s / it)
[16:26:27.271609] Test:  [ 0/57]  eta: 0:00:29  loss: 0.4333 (0.4333)  time: 0.5180  data: 0.3559  max mem: 15821
[16:26:28.918138] Test:  [10/57]  eta: 0:00:09  loss: 0.4333 (0.4372)  time: 0.1967  data: 0.0324  max mem: 15821
[16:26:30.569577] Test:  [20/57]  eta: 0:00:06  loss: 0.4017 (0.4192)  time: 0.1648  data: 0.0001  max mem: 15821
[16:26:32.225318] Test:  [30/57]  eta: 0:00:04  loss: 0.3220 (0.3849)  time: 0.1653  data: 0.0001  max mem: 15821
[16:26:33.883535] Test:  [40/57]  eta: 0:00:02  loss: 0.3112 (0.3690)  time: 0.1656  data: 0.0001  max mem: 15821
[16:26:35.549636] Test:  [50/57]  eta: 0:00:01  loss: 0.3261 (0.3749)  time: 0.1662  data: 0.0001  max mem: 15821
[16:26:36.447438] Test:  [56/57]  eta: 0:00:00  loss: 0.3969 (0.3882)  time: 0.1613  data: 0.0001  max mem: 15821
[16:26:36.504170] Test: Total time: 0:00:09 (0.1711 s / it)
[16:26:38.240673] Dice score of the network on the train images: 0.740128, val images: 0.738220
[16:26:38.240891] saving best_prec_model_0 @ epoch 9
[16:26:39.340426] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[16:26:40.281602] Epoch: [10]  [  0/345]  eta: 0:05:24  lr: 0.000063  loss: 0.2807 (0.2807)  time: 0.9400  data: 0.3398  max mem: 15821
[16:26:52.260768] Epoch: [10]  [ 20/345]  eta: 0:03:19  lr: 0.000063  loss: 0.3395 (0.3382)  time: 0.5989  data: 0.0001  max mem: 15821
[16:27:04.273521] Epoch: [10]  [ 40/345]  eta: 0:03:05  lr: 0.000063  loss: 0.3231 (0.3364)  time: 0.6006  data: 0.0001  max mem: 15821
[16:27:16.298810] Epoch: [10]  [ 60/345]  eta: 0:02:52  lr: 0.000064  loss: 0.3050 (0.3246)  time: 0.6012  data: 0.0001  max mem: 15821
[16:27:28.316030] Epoch: [10]  [ 80/345]  eta: 0:02:40  lr: 0.000064  loss: 0.3140 (0.3213)  time: 0.6008  data: 0.0001  max mem: 15821
[16:27:40.364546] Epoch: [10]  [100/345]  eta: 0:02:28  lr: 0.000064  loss: 0.2922 (0.3174)  time: 0.6024  data: 0.0001  max mem: 15821
[16:27:52.450379] Epoch: [10]  [120/345]  eta: 0:02:15  lr: 0.000065  loss: 0.3272 (0.3198)  time: 0.6042  data: 0.0001  max mem: 15821
[16:28:04.559232] Epoch: [10]  [140/345]  eta: 0:02:03  lr: 0.000065  loss: 0.3178 (0.3202)  time: 0.6054  data: 0.0001  max mem: 15821
[16:28:16.671901] Epoch: [10]  [160/345]  eta: 0:01:51  lr: 0.000065  loss: 0.3251 (0.3214)  time: 0.6056  data: 0.0001  max mem: 15821
[16:28:28.770169] Epoch: [10]  [180/345]  eta: 0:01:39  lr: 0.000066  loss: 0.3270 (0.3221)  time: 0.6049  data: 0.0001  max mem: 15821
[16:28:40.865272] Epoch: [10]  [200/345]  eta: 0:01:27  lr: 0.000066  loss: 0.3118 (0.3219)  time: 0.6047  data: 0.0001  max mem: 15821
[16:28:52.955278] Epoch: [10]  [220/345]  eta: 0:01:15  lr: 0.000066  loss: 0.3129 (0.3214)  time: 0.6045  data: 0.0001  max mem: 15821
[16:29:05.046183] Epoch: [10]  [240/345]  eta: 0:01:03  lr: 0.000067  loss: 0.3215 (0.3217)  time: 0.6045  data: 0.0001  max mem: 15821
[16:29:17.134303] Epoch: [10]  [260/345]  eta: 0:00:51  lr: 0.000067  loss: 0.3035 (0.3208)  time: 0.6044  data: 0.0001  max mem: 15821
[16:29:29.217359] Epoch: [10]  [280/345]  eta: 0:00:39  lr: 0.000068  loss: 0.3111 (0.3196)  time: 0.6041  data: 0.0001  max mem: 15821
[16:29:41.303056] Epoch: [10]  [300/345]  eta: 0:00:27  lr: 0.000068  loss: 0.2948 (0.3186)  time: 0.6042  data: 0.0001  max mem: 15821
[16:29:53.386283] Epoch: [10]  [320/345]  eta: 0:00:15  lr: 0.000068  loss: 0.3097 (0.3185)  time: 0.6041  data: 0.0001  max mem: 15821
[16:30:05.463114] Epoch: [10]  [340/345]  eta: 0:00:03  lr: 0.000069  loss: 0.2986 (0.3178)  time: 0.6038  data: 0.0001  max mem: 15821
[16:30:07.880579] Epoch: [10]  [344/345]  eta: 0:00:00  lr: 0.000069  loss: 0.3048 (0.3177)  time: 0.6039  data: 0.0001  max mem: 15821
[16:30:07.945015] Epoch: [10] Total time: 0:03:28 (0.6047 s / it)
[16:30:07.945523] Averaged stats: lr: 0.000069  loss: 0.3048 (0.3177)
[16:30:08.480612] Test:  [  0/345]  eta: 0:03:02  loss: 0.3169 (0.3169)  time: 0.5301  data: 0.3657  max mem: 15821
[16:30:10.148648] Test:  [ 10/345]  eta: 0:01:06  loss: 0.3169 (0.3108)  time: 0.1997  data: 0.0333  max mem: 15821
[16:30:11.818937] Test:  [ 20/345]  eta: 0:00:59  loss: 0.3095 (0.3129)  time: 0.1668  data: 0.0001  max mem: 15821
[16:30:13.492891] Test:  [ 30/345]  eta: 0:00:56  loss: 0.3095 (0.3077)  time: 0.1671  data: 0.0001  max mem: 15821
[16:30:15.168941] Test:  [ 40/345]  eta: 0:00:53  loss: 0.3273 (0.3132)  time: 0.1674  data: 0.0001  max mem: 15821
[16:30:16.848466] Test:  [ 50/345]  eta: 0:00:51  loss: 0.3273 (0.3145)  time: 0.1677  data: 0.0001  max mem: 15821
[16:30:18.531783] Test:  [ 60/345]  eta: 0:00:49  loss: 0.3057 (0.3114)  time: 0.1681  data: 0.0001  max mem: 15821
[16:30:20.217913] Test:  [ 70/345]  eta: 0:00:47  loss: 0.3057 (0.3099)  time: 0.1684  data: 0.0001  max mem: 15821
[16:30:21.908030] Test:  [ 80/345]  eta: 0:00:45  loss: 0.3042 (0.3084)  time: 0.1687  data: 0.0001  max mem: 15821
[16:30:23.601067] Test:  [ 90/345]  eta: 0:00:43  loss: 0.3068 (0.3083)  time: 0.1691  data: 0.0001  max mem: 15821
[16:30:25.298982] Test:  [100/345]  eta: 0:00:42  loss: 0.2821 (0.3064)  time: 0.1695  data: 0.0001  max mem: 15821
[16:30:26.998758] Test:  [110/345]  eta: 0:00:40  loss: 0.2821 (0.3044)  time: 0.1698  data: 0.0001  max mem: 15821
[16:30:28.702696] Test:  [120/345]  eta: 0:00:38  loss: 0.2902 (0.3041)  time: 0.1701  data: 0.0001  max mem: 15821
[16:30:30.409642] Test:  [130/345]  eta: 0:00:36  loss: 0.2974 (0.3035)  time: 0.1705  data: 0.0001  max mem: 15821
[16:30:32.119701] Test:  [140/345]  eta: 0:00:35  loss: 0.2963 (0.3037)  time: 0.1708  data: 0.0001  max mem: 15821
[16:30:33.833433] Test:  [150/345]  eta: 0:00:33  loss: 0.2968 (0.3039)  time: 0.1711  data: 0.0001  max mem: 15821
[16:30:35.550434] Test:  [160/345]  eta: 0:00:31  loss: 0.3105 (0.3055)  time: 0.1715  data: 0.0001  max mem: 15821
[16:30:37.271511] Test:  [170/345]  eta: 0:00:29  loss: 0.3071 (0.3047)  time: 0.1718  data: 0.0001  max mem: 15821
[16:30:38.996404] Test:  [180/345]  eta: 0:00:28  loss: 0.2956 (0.3043)  time: 0.1722  data: 0.0001  max mem: 15821
[16:30:40.724192] Test:  [190/345]  eta: 0:00:26  loss: 0.2956 (0.3045)  time: 0.1726  data: 0.0001  max mem: 15821
[16:30:42.455593] Test:  [200/345]  eta: 0:00:24  loss: 0.3109 (0.3051)  time: 0.1729  data: 0.0001  max mem: 15821
[16:30:44.190692] Test:  [210/345]  eta: 0:00:23  loss: 0.3158 (0.3054)  time: 0.1732  data: 0.0001  max mem: 15821
[16:30:45.928056] Test:  [220/345]  eta: 0:00:21  loss: 0.2975 (0.3052)  time: 0.1736  data: 0.0001  max mem: 15821
[16:30:47.669874] Test:  [230/345]  eta: 0:00:19  loss: 0.3037 (0.3056)  time: 0.1739  data: 0.0001  max mem: 15821
[16:30:49.412922] Test:  [240/345]  eta: 0:00:18  loss: 0.3228 (0.3060)  time: 0.1742  data: 0.0001  max mem: 15821
[16:30:51.160351] Test:  [250/345]  eta: 0:00:16  loss: 0.3205 (0.3063)  time: 0.1745  data: 0.0001  max mem: 15821
[16:30:52.911701] Test:  [260/345]  eta: 0:00:14  loss: 0.3205 (0.3065)  time: 0.1749  data: 0.0001  max mem: 15821
[16:30:54.667307] Test:  [270/345]  eta: 0:00:12  loss: 0.3235 (0.3073)  time: 0.1753  data: 0.0001  max mem: 15821
[16:30:56.426574] Test:  [280/345]  eta: 0:00:11  loss: 0.3252 (0.3081)  time: 0.1757  data: 0.0001  max mem: 15821
[16:30:58.188224] Test:  [290/345]  eta: 0:00:09  loss: 0.3091 (0.3080)  time: 0.1760  data: 0.0001  max mem: 15821
[16:30:59.952447] Test:  [300/345]  eta: 0:00:07  loss: 0.2932 (0.3079)  time: 0.1762  data: 0.0001  max mem: 15821
[16:31:01.722361] Test:  [310/345]  eta: 0:00:06  loss: 0.3067 (0.3081)  time: 0.1766  data: 0.0001  max mem: 15821
[16:31:03.495056] Test:  [320/345]  eta: 0:00:04  loss: 0.3092 (0.3085)  time: 0.1771  data: 0.0001  max mem: 15821
[16:31:05.271274] Test:  [330/345]  eta: 0:00:02  loss: 0.3176 (0.3090)  time: 0.1774  data: 0.0001  max mem: 15821
[16:31:07.050743] Test:  [340/345]  eta: 0:00:00  loss: 0.3176 (0.3090)  time: 0.1777  data: 0.0001  max mem: 15821
[16:31:07.763533] Test:  [344/345]  eta: 0:00:00  loss: 0.2995 (0.3088)  time: 0.1778  data: 0.0001  max mem: 15821
[16:31:07.821385] Test: Total time: 0:00:59 (0.1735 s / it)
[16:31:18.113057] Test:  [ 0/57]  eta: 0:00:29  loss: 0.3158 (0.3158)  time: 0.5168  data: 0.3548  max mem: 15821
[16:31:19.759274] Test:  [10/57]  eta: 0:00:09  loss: 0.3815 (0.3786)  time: 0.1965  data: 0.0323  max mem: 15821
[16:31:21.410347] Test:  [20/57]  eta: 0:00:06  loss: 0.3793 (0.3755)  time: 0.1648  data: 0.0001  max mem: 15821
[16:31:23.064144] Test:  [30/57]  eta: 0:00:04  loss: 0.3416 (0.3512)  time: 0.1652  data: 0.0001  max mem: 15821
[16:31:24.723849] Test:  [40/57]  eta: 0:00:02  loss: 0.2830 (0.3347)  time: 0.1656  data: 0.0001  max mem: 15821
[16:31:26.390438] Test:  [50/57]  eta: 0:00:01  loss: 0.2887 (0.3310)  time: 0.1663  data: 0.0001  max mem: 15821
[16:31:27.288537] Test:  [56/57]  eta: 0:00:00  loss: 0.3146 (0.3421)  time: 0.1614  data: 0.0001  max mem: 15821
[16:31:27.354170] Test: Total time: 0:00:09 (0.1712 s / it)
[16:31:29.053874] Dice score of the network on the train images: 0.723084, val images: 0.752308
[16:31:29.058188] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[16:31:29.979163] Epoch: [11]  [  0/345]  eta: 0:05:17  lr: 0.000069  loss: 0.2471 (0.2471)  time: 0.9199  data: 0.3196  max mem: 15821
[16:31:41.949693] Epoch: [11]  [ 20/345]  eta: 0:03:19  lr: 0.000069  loss: 0.2868 (0.3106)  time: 0.5985  data: 0.0001  max mem: 15821
[16:31:53.954470] Epoch: [11]  [ 40/345]  eta: 0:03:05  lr: 0.000069  loss: 0.3157 (0.3120)  time: 0.6002  data: 0.0001  max mem: 15821
[16:32:05.984348] Epoch: [11]  [ 60/345]  eta: 0:02:52  lr: 0.000070  loss: 0.2944 (0.3098)  time: 0.6014  data: 0.0001  max mem: 15821
[16:32:18.035688] Epoch: [11]  [ 80/345]  eta: 0:02:40  lr: 0.000070  loss: 0.3042 (0.3090)  time: 0.6025  data: 0.0001  max mem: 15821
[16:32:30.097721] Epoch: [11]  [100/345]  eta: 0:02:28  lr: 0.000071  loss: 0.3159 (0.3114)  time: 0.6031  data: 0.0001  max mem: 15821
[16:32:42.179662] Epoch: [11]  [120/345]  eta: 0:02:15  lr: 0.000071  loss: 0.2914 (0.3082)  time: 0.6040  data: 0.0001  max mem: 15821
[16:32:54.289419] Epoch: [11]  [140/345]  eta: 0:02:03  lr: 0.000071  loss: 0.3048 (0.3073)  time: 0.6054  data: 0.0001  max mem: 15821
[16:33:06.390265] Epoch: [11]  [160/345]  eta: 0:01:51  lr: 0.000072  loss: 0.2957 (0.3055)  time: 0.6050  data: 0.0001  max mem: 15821
[16:33:18.495555] Epoch: [11]  [180/345]  eta: 0:01:39  lr: 0.000072  loss: 0.2917 (0.3058)  time: 0.6052  data: 0.0001  max mem: 15821
[16:33:30.604206] Epoch: [11]  [200/345]  eta: 0:01:27  lr: 0.000072  loss: 0.2988 (0.3056)  time: 0.6054  data: 0.0001  max mem: 15821
[16:33:42.706538] Epoch: [11]  [220/345]  eta: 0:01:15  lr: 0.000073  loss: 0.2974 (0.3052)  time: 0.6051  data: 0.0001  max mem: 15821
[16:33:54.796581] Epoch: [11]  [240/345]  eta: 0:01:03  lr: 0.000073  loss: 0.2610 (0.3021)  time: 0.6045  data: 0.0001  max mem: 15821
[16:34:06.858268] Epoch: [11]  [260/345]  eta: 0:00:51  lr: 0.000073  loss: 0.2827 (0.3017)  time: 0.6030  data: 0.0001  max mem: 15821
[16:34:18.938863] Epoch: [11]  [280/345]  eta: 0:00:39  lr: 0.000074  loss: 0.3148 (0.3023)  time: 0.6040  data: 0.0001  max mem: 15821
[16:34:31.026931] Epoch: [11]  [300/345]  eta: 0:00:27  lr: 0.000074  loss: 0.3630 (0.3078)  time: 0.6044  data: 0.0001  max mem: 15821
[16:34:43.113254] Epoch: [11]  [320/345]  eta: 0:00:15  lr: 0.000075  loss: 0.3429 (0.3105)  time: 0.6043  data: 0.0001  max mem: 15821
[16:34:55.176813] Epoch: [11]  [340/345]  eta: 0:00:03  lr: 0.000075  loss: 0.3302 (0.3116)  time: 0.6031  data: 0.0001  max mem: 15821
[16:34:57.589425] Epoch: [11]  [344/345]  eta: 0:00:00  lr: 0.000075  loss: 0.3316 (0.3119)  time: 0.6029  data: 0.0001  max mem: 15821
[16:34:57.651033] Epoch: [11] Total time: 0:03:28 (0.6046 s / it)
[16:34:57.651261] Averaged stats: lr: 0.000075  loss: 0.3316 (0.3119)
[16:34:58.154106] Test:  [  0/345]  eta: 0:02:51  loss: 0.3414 (0.3414)  time: 0.4975  data: 0.3339  max mem: 15821
[16:34:59.822482] Test:  [ 10/345]  eta: 0:01:05  loss: 0.3242 (0.3122)  time: 0.1968  data: 0.0305  max mem: 15821
[16:35:01.493230] Test:  [ 20/345]  eta: 0:00:59  loss: 0.3242 (0.3225)  time: 0.1669  data: 0.0001  max mem: 15821
[16:35:03.166297] Test:  [ 30/345]  eta: 0:00:55  loss: 0.3155 (0.3163)  time: 0.1671  data: 0.0001  max mem: 15821
[16:35:04.843889] Test:  [ 40/345]  eta: 0:00:53  loss: 0.3041 (0.3156)  time: 0.1675  data: 0.0001  max mem: 15821
[16:35:06.524163] Test:  [ 50/345]  eta: 0:00:51  loss: 0.3041 (0.3182)  time: 0.1678  data: 0.0001  max mem: 15821
[16:35:08.208651] Test:  [ 60/345]  eta: 0:00:49  loss: 0.3157 (0.3179)  time: 0.1682  data: 0.0001  max mem: 15821
[16:35:09.894777] Test:  [ 70/345]  eta: 0:00:47  loss: 0.3270 (0.3211)  time: 0.1685  data: 0.0001  max mem: 15821
[16:35:11.585275] Test:  [ 80/345]  eta: 0:00:45  loss: 0.3333 (0.3213)  time: 0.1688  data: 0.0001  max mem: 15821
[16:35:13.279053] Test:  [ 90/345]  eta: 0:00:43  loss: 0.3079 (0.3187)  time: 0.1691  data: 0.0001  max mem: 15821
[16:35:14.976882] Test:  [100/345]  eta: 0:00:41  loss: 0.2960 (0.3167)  time: 0.1695  data: 0.0001  max mem: 15821
[16:35:16.677957] Test:  [110/345]  eta: 0:00:40  loss: 0.2992 (0.3162)  time: 0.1699  data: 0.0001  max mem: 15821
[16:35:18.381230] Test:  [120/345]  eta: 0:00:38  loss: 0.3072 (0.3167)  time: 0.1701  data: 0.0001  max mem: 15821
[16:35:20.088107] Test:  [130/345]  eta: 0:00:36  loss: 0.3299 (0.3174)  time: 0.1704  data: 0.0001  max mem: 15821
[16:35:21.800394] Test:  [140/345]  eta: 0:00:35  loss: 0.3197 (0.3165)  time: 0.1709  data: 0.0001  max mem: 15821
[16:35:23.514472] Test:  [150/345]  eta: 0:00:33  loss: 0.2996 (0.3151)  time: 0.1712  data: 0.0001  max mem: 15821
[16:35:25.231414] Test:  [160/345]  eta: 0:00:31  loss: 0.2916 (0.3143)  time: 0.1714  data: 0.0001  max mem: 15821
[16:35:26.952574] Test:  [170/345]  eta: 0:00:29  loss: 0.2945 (0.3128)  time: 0.1718  data: 0.0001  max mem: 15821
[16:35:28.676634] Test:  [180/345]  eta: 0:00:28  loss: 0.3063 (0.3129)  time: 0.1722  data: 0.0001  max mem: 15821
[16:35:30.403613] Test:  [190/345]  eta: 0:00:26  loss: 0.2995 (0.3112)  time: 0.1725  data: 0.0001  max mem: 15821
[16:35:32.134876] Test:  [200/345]  eta: 0:00:24  loss: 0.2934 (0.3114)  time: 0.1728  data: 0.0001  max mem: 15821
[16:35:33.870214] Test:  [210/345]  eta: 0:00:23  loss: 0.2968 (0.3104)  time: 0.1733  data: 0.0001  max mem: 15821
[16:35:35.607437] Test:  [220/345]  eta: 0:00:21  loss: 0.3066 (0.3113)  time: 0.1736  data: 0.0001  max mem: 15821
[16:35:37.348639] Test:  [230/345]  eta: 0:00:19  loss: 0.3325 (0.3122)  time: 0.1738  data: 0.0001  max mem: 15821
[16:35:39.094144] Test:  [240/345]  eta: 0:00:18  loss: 0.3262 (0.3116)  time: 0.1743  data: 0.0001  max mem: 15821
[16:35:40.841173] Test:  [250/345]  eta: 0:00:16  loss: 0.3097 (0.3113)  time: 0.1746  data: 0.0001  max mem: 15821
[16:35:42.592786] Test:  [260/345]  eta: 0:00:14  loss: 0.3108 (0.3120)  time: 0.1749  data: 0.0001  max mem: 15821
[16:35:44.347959] Test:  [270/345]  eta: 0:00:12  loss: 0.3254 (0.3126)  time: 0.1753  data: 0.0001  max mem: 15821
[16:35:46.107289] Test:  [280/345]  eta: 0:00:11  loss: 0.3372 (0.3133)  time: 0.1757  data: 0.0001  max mem: 15821
[16:35:47.870036] Test:  [290/345]  eta: 0:00:09  loss: 0.3278 (0.3134)  time: 0.1760  data: 0.0001  max mem: 15821
[16:35:49.635695] Test:  [300/345]  eta: 0:00:07  loss: 0.3250 (0.3135)  time: 0.1764  data: 0.0001  max mem: 15821
[16:35:51.405443] Test:  [310/345]  eta: 0:00:06  loss: 0.3250 (0.3138)  time: 0.1767  data: 0.0001  max mem: 15821
[16:35:53.179517] Test:  [320/345]  eta: 0:00:04  loss: 0.3088 (0.3135)  time: 0.1771  data: 0.0001  max mem: 15821
[16:35:54.956380] Test:  [330/345]  eta: 0:00:02  loss: 0.2865 (0.3129)  time: 0.1775  data: 0.0001  max mem: 15821
[16:35:56.734848] Test:  [340/345]  eta: 0:00:00  loss: 0.3114 (0.3136)  time: 0.1777  data: 0.0001  max mem: 15821
[16:35:57.447703] Test:  [344/345]  eta: 0:00:00  loss: 0.3143 (0.3137)  time: 0.1778  data: 0.0001  max mem: 15821
[16:35:57.512648] Test: Total time: 0:00:59 (0.1735 s / it)
[16:36:08.006092] Test:  [ 0/57]  eta: 0:00:30  loss: 0.3828 (0.3828)  time: 0.5296  data: 0.3678  max mem: 15821
[16:36:09.652370] Test:  [10/57]  eta: 0:00:09  loss: 0.3828 (0.4183)  time: 0.1977  data: 0.0335  max mem: 15821
[16:36:11.303503] Test:  [20/57]  eta: 0:00:06  loss: 0.3813 (0.3998)  time: 0.1648  data: 0.0001  max mem: 15821
[16:36:12.958542] Test:  [30/57]  eta: 0:00:04  loss: 0.3356 (0.3680)  time: 0.1652  data: 0.0001  max mem: 15821
[16:36:14.617001] Test:  [40/57]  eta: 0:00:02  loss: 0.3085 (0.3551)  time: 0.1656  data: 0.0001  max mem: 15821
[16:36:16.282057] Test:  [50/57]  eta: 0:00:01  loss: 0.3163 (0.3638)  time: 0.1661  data: 0.0001  max mem: 15821
[16:36:17.181184] Test:  [56/57]  eta: 0:00:00  loss: 0.3725 (0.3682)  time: 0.1613  data: 0.0001  max mem: 15821
[16:36:17.241629] Test: Total time: 0:00:09 (0.1713 s / it)
[16:36:18.950998] Dice score of the network on the train images: 0.750562, val images: 0.757238
[16:36:18.951223] saving best_prec_model_0 @ epoch 11
[16:36:20.137935] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[16:36:21.067834] Epoch: [12]  [  0/345]  eta: 0:05:20  lr: 0.000075  loss: 0.2757 (0.2757)  time: 0.9289  data: 0.3262  max mem: 15821
[16:36:33.046641] Epoch: [12]  [ 20/345]  eta: 0:03:19  lr: 0.000075  loss: 0.2895 (0.2927)  time: 0.5989  data: 0.0001  max mem: 15821
[16:36:45.058335] Epoch: [12]  [ 40/345]  eta: 0:03:05  lr: 0.000076  loss: 0.3233 (0.3137)  time: 0.6005  data: 0.0001  max mem: 15821
[16:36:57.095028] Epoch: [12]  [ 60/345]  eta: 0:02:52  lr: 0.000076  loss: 0.2963 (0.3119)  time: 0.6018  data: 0.0001  max mem: 15821
[16:37:09.144877] Epoch: [12]  [ 80/345]  eta: 0:02:40  lr: 0.000076  loss: 0.2890 (0.3091)  time: 0.6024  data: 0.0001  max mem: 15821
[16:37:21.208261] Epoch: [12]  [100/345]  eta: 0:02:28  lr: 0.000077  loss: 0.3110 (0.3127)  time: 0.6031  data: 0.0001  max mem: 15821
[16:37:33.281383] Epoch: [12]  [120/345]  eta: 0:02:15  lr: 0.000077  loss: 0.3693 (0.3261)  time: 0.6036  data: 0.0001  max mem: 15821
[16:37:45.378505] Epoch: [12]  [140/345]  eta: 0:02:03  lr: 0.000078  loss: 0.3147 (0.3273)  time: 0.6048  data: 0.0001  max mem: 15821
[16:37:57.482604] Epoch: [12]  [160/345]  eta: 0:01:51  lr: 0.000078  loss: 0.3272 (0.3288)  time: 0.6052  data: 0.0001  max mem: 15821
[16:38:09.579765] Epoch: [12]  [180/345]  eta: 0:01:39  lr: 0.000078  loss: 0.3229 (0.3289)  time: 0.6048  data: 0.0001  max mem: 15821
[16:38:21.676654] Epoch: [12]  [200/345]  eta: 0:01:27  lr: 0.000079  loss: 0.3372 (0.3291)  time: 0.6048  data: 0.0001  max mem: 15821
[16:38:33.768751] Epoch: [12]  [220/345]  eta: 0:01:15  lr: 0.000079  loss: 0.3019 (0.3272)  time: 0.6046  data: 0.0001  max mem: 15821
[16:38:45.876227] Epoch: [12]  [240/345]  eta: 0:01:03  lr: 0.000079  loss: 0.2983 (0.3254)  time: 0.6053  data: 0.0001  max mem: 15821
[16:38:58.088294] Epoch: [12]  [260/345]  eta: 0:00:51  lr: 0.000080  loss: 0.3206 (0.3245)  time: 0.6106  data: 0.0001  max mem: 15821
[16:39:10.178722] Epoch: [12]  [280/345]  eta: 0:00:39  lr: 0.000080  loss: 0.2752 (0.3218)  time: 0.6045  data: 0.0001  max mem: 15821
[16:39:22.270229] Epoch: [12]  [300/345]  eta: 0:00:27  lr: 0.000080  loss: 0.2941 (0.3191)  time: 0.6045  data: 0.0001  max mem: 15821
[16:39:34.342936] Epoch: [12]  [320/345]  eta: 0:00:15  lr: 0.000081  loss: 0.2751 (0.3170)  time: 0.6036  data: 0.0001  max mem: 15821
[16:39:46.411626] Epoch: [12]  [340/345]  eta: 0:00:03  lr: 0.000081  loss: 0.3050 (0.3161)  time: 0.6034  data: 0.0001  max mem: 15821
[16:39:48.824405] Epoch: [12]  [344/345]  eta: 0:00:00  lr: 0.000081  loss: 0.3006 (0.3157)  time: 0.6033  data: 0.0001  max mem: 15821
[16:39:48.893318] Epoch: [12] Total time: 0:03:28 (0.6051 s / it)
[16:39:48.893974] Averaged stats: lr: 0.000081  loss: 0.3006 (0.3157)
[16:39:49.405496] Test:  [  0/345]  eta: 0:02:54  loss: 0.3347 (0.3347)  time: 0.5055  data: 0.3414  max mem: 15821
[16:39:51.072572] Test:  [ 10/345]  eta: 0:01:06  loss: 0.3347 (0.3310)  time: 0.1974  data: 0.0311  max mem: 15821
[16:39:52.742276] Test:  [ 20/345]  eta: 0:00:59  loss: 0.3236 (0.3239)  time: 0.1668  data: 0.0001  max mem: 15821
[16:39:54.416303] Test:  [ 30/345]  eta: 0:00:56  loss: 0.3236 (0.3214)  time: 0.1671  data: 0.0001  max mem: 15821
[16:39:56.092527] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2936 (0.3111)  time: 0.1674  data: 0.0001  max mem: 15821
[16:39:57.772057] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2835 (0.3036)  time: 0.1677  data: 0.0001  max mem: 15821
[16:39:59.454708] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2797 (0.3009)  time: 0.1680  data: 0.0001  max mem: 15821
[16:40:01.140859] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2884 (0.2992)  time: 0.1684  data: 0.0001  max mem: 15821
[16:40:02.829869] Test:  [ 80/345]  eta: 0:00:45  loss: 0.3079 (0.3023)  time: 0.1687  data: 0.0001  max mem: 15821
[16:40:04.523853] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2905 (0.2992)  time: 0.1691  data: 0.0001  max mem: 15821
[16:40:06.221069] Test:  [100/345]  eta: 0:00:41  loss: 0.2789 (0.3009)  time: 0.1695  data: 0.0001  max mem: 15821
[16:40:07.921500] Test:  [110/345]  eta: 0:00:40  loss: 0.2954 (0.3010)  time: 0.1698  data: 0.0001  max mem: 15821
[16:40:09.625069] Test:  [120/345]  eta: 0:00:38  loss: 0.3173 (0.3026)  time: 0.1701  data: 0.0001  max mem: 15821
[16:40:11.331352] Test:  [130/345]  eta: 0:00:36  loss: 0.3061 (0.3007)  time: 0.1704  data: 0.0001  max mem: 15821
[16:40:13.041576] Test:  [140/345]  eta: 0:00:35  loss: 0.2864 (0.3030)  time: 0.1708  data: 0.0001  max mem: 15821
[16:40:14.754054] Test:  [150/345]  eta: 0:00:33  loss: 0.3172 (0.3034)  time: 0.1711  data: 0.0001  max mem: 15821
[16:40:16.471305] Test:  [160/345]  eta: 0:00:31  loss: 0.2921 (0.3028)  time: 0.1714  data: 0.0001  max mem: 15821
[16:40:18.192493] Test:  [170/345]  eta: 0:00:29  loss: 0.2974 (0.3035)  time: 0.1719  data: 0.0001  max mem: 15821
[16:40:19.917432] Test:  [180/345]  eta: 0:00:28  loss: 0.2945 (0.3026)  time: 0.1722  data: 0.0001  max mem: 15821
[16:40:21.643766] Test:  [190/345]  eta: 0:00:26  loss: 0.2909 (0.3028)  time: 0.1725  data: 0.0001  max mem: 15821
[16:40:23.375785] Test:  [200/345]  eta: 0:00:24  loss: 0.3126 (0.3044)  time: 0.1729  data: 0.0001  max mem: 15821
[16:40:25.110350] Test:  [210/345]  eta: 0:00:23  loss: 0.3126 (0.3048)  time: 0.1733  data: 0.0001  max mem: 15821
[16:40:26.846335] Test:  [220/345]  eta: 0:00:21  loss: 0.3225 (0.3059)  time: 0.1735  data: 0.0001  max mem: 15821
[16:40:28.586777] Test:  [230/345]  eta: 0:00:19  loss: 0.3251 (0.3056)  time: 0.1738  data: 0.0001  max mem: 15821
[16:40:30.331425] Test:  [240/345]  eta: 0:00:18  loss: 0.3032 (0.3066)  time: 0.1742  data: 0.0001  max mem: 15821
[16:40:32.080011] Test:  [250/345]  eta: 0:00:16  loss: 0.2990 (0.3061)  time: 0.1746  data: 0.0001  max mem: 15821
[16:40:33.831206] Test:  [260/345]  eta: 0:00:14  loss: 0.2703 (0.3051)  time: 0.1749  data: 0.0001  max mem: 15821
[16:40:35.585421] Test:  [270/345]  eta: 0:00:12  loss: 0.2949 (0.3049)  time: 0.1752  data: 0.0001  max mem: 15821
[16:40:37.343421] Test:  [280/345]  eta: 0:00:11  loss: 0.3010 (0.3046)  time: 0.1755  data: 0.0001  max mem: 15821
[16:40:39.104244] Test:  [290/345]  eta: 0:00:09  loss: 0.2953 (0.3041)  time: 0.1759  data: 0.0001  max mem: 15821
[16:40:40.868677] Test:  [300/345]  eta: 0:00:07  loss: 0.2855 (0.3047)  time: 0.1762  data: 0.0001  max mem: 15821
[16:40:42.638484] Test:  [310/345]  eta: 0:00:06  loss: 0.2890 (0.3047)  time: 0.1767  data: 0.0001  max mem: 15821
[16:40:44.410541] Test:  [320/345]  eta: 0:00:04  loss: 0.3134 (0.3053)  time: 0.1770  data: 0.0001  max mem: 15821
[16:40:46.186952] Test:  [330/345]  eta: 0:00:02  loss: 0.2864 (0.3048)  time: 0.1774  data: 0.0001  max mem: 15821
[16:40:47.966174] Test:  [340/345]  eta: 0:00:00  loss: 0.2684 (0.3044)  time: 0.1777  data: 0.0001  max mem: 15821
[16:40:48.679030] Test:  [344/345]  eta: 0:00:00  loss: 0.2684 (0.3043)  time: 0.1778  data: 0.0001  max mem: 15821
[16:40:48.743557] Test: Total time: 0:00:59 (0.1735 s / it)
[16:40:59.005490] Test:  [ 0/57]  eta: 0:00:27  loss: 0.4038 (0.4038)  time: 0.4888  data: 0.3265  max mem: 15821
[16:41:00.652346] Test:  [10/57]  eta: 0:00:09  loss: 0.4038 (0.4233)  time: 0.1941  data: 0.0298  max mem: 15821
[16:41:02.303638] Test:  [20/57]  eta: 0:00:06  loss: 0.3850 (0.4086)  time: 0.1648  data: 0.0001  max mem: 15821
[16:41:03.959605] Test:  [30/57]  eta: 0:00:04  loss: 0.3270 (0.3726)  time: 0.1653  data: 0.0001  max mem: 15821
[16:41:05.617612] Test:  [40/57]  eta: 0:00:02  loss: 0.2868 (0.3520)  time: 0.1656  data: 0.0001  max mem: 15821
[16:41:07.282740] Test:  [50/57]  eta: 0:00:01  loss: 0.2913 (0.3455)  time: 0.1661  data: 0.0001  max mem: 15821
[16:41:08.180990] Test:  [56/57]  eta: 0:00:00  loss: 0.3179 (0.3541)  time: 0.1613  data: 0.0000  max mem: 15821
[16:41:08.244522] Test: Total time: 0:00:09 (0.1707 s / it)
[16:41:09.989478] Dice score of the network on the train images: 0.752730, val images: 0.769668
[16:41:09.989711] saving best_dice_model_0 @ epoch 12
[16:41:11.033588] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[16:41:11.957717] Epoch: [13]  [  0/345]  eta: 0:05:18  lr: 0.000081  loss: 0.2831 (0.2831)  time: 0.9230  data: 0.3242  max mem: 15821
[16:41:23.937437] Epoch: [13]  [ 20/345]  eta: 0:03:19  lr: 0.000082  loss: 0.2914 (0.2983)  time: 0.5989  data: 0.0001  max mem: 15821
[16:41:35.930689] Epoch: [13]  [ 40/345]  eta: 0:03:05  lr: 0.000082  loss: 0.2789 (0.2949)  time: 0.5996  data: 0.0001  max mem: 15821
[16:41:47.954367] Epoch: [13]  [ 60/345]  eta: 0:02:52  lr: 0.000082  loss: 0.3007 (0.2938)  time: 0.6011  data: 0.0001  max mem: 15821
[16:41:59.998445] Epoch: [13]  [ 80/345]  eta: 0:02:40  lr: 0.000083  loss: 0.3320 (0.3026)  time: 0.6022  data: 0.0001  max mem: 15821
[16:42:12.169418] Epoch: [13]  [100/345]  eta: 0:02:28  lr: 0.000083  loss: 0.2785 (0.2983)  time: 0.6085  data: 0.0001  max mem: 15821
[16:42:24.236427] Epoch: [13]  [120/345]  eta: 0:02:16  lr: 0.000083  loss: 0.2904 (0.2999)  time: 0.6033  data: 0.0001  max mem: 15821
[16:42:36.319673] Epoch: [13]  [140/345]  eta: 0:02:03  lr: 0.000084  loss: 0.2996 (0.3004)  time: 0.6041  data: 0.0001  max mem: 15821
[16:42:48.416229] Epoch: [13]  [160/345]  eta: 0:01:51  lr: 0.000084  loss: 0.2980 (0.2995)  time: 0.6048  data: 0.0001  max mem: 15821
[16:43:00.525497] Epoch: [13]  [180/345]  eta: 0:01:39  lr: 0.000085  loss: 0.2869 (0.2985)  time: 0.6054  data: 0.0001  max mem: 15821
[16:43:12.629831] Epoch: [13]  [200/345]  eta: 0:01:27  lr: 0.000085  loss: 0.2745 (0.2967)  time: 0.6052  data: 0.0001  max mem: 15821
[16:43:24.738548] Epoch: [13]  [220/345]  eta: 0:01:15  lr: 0.000085  loss: 0.2709 (0.2944)  time: 0.6054  data: 0.0001  max mem: 15821
[16:43:36.822051] Epoch: [13]  [240/345]  eta: 0:01:03  lr: 0.000086  loss: 0.2750 (0.2933)  time: 0.6041  data: 0.0001  max mem: 15821
[16:43:48.897312] Epoch: [13]  [260/345]  eta: 0:00:51  lr: 0.000086  loss: 0.2651 (0.2917)  time: 0.6037  data: 0.0001  max mem: 15821
[16:44:00.964647] Epoch: [13]  [280/345]  eta: 0:00:39  lr: 0.000086  loss: 0.2805 (0.2908)  time: 0.6033  data: 0.0001  max mem: 15821
[16:44:13.047188] Epoch: [13]  [300/345]  eta: 0:00:27  lr: 0.000087  loss: 0.2560 (0.2886)  time: 0.6041  data: 0.0001  max mem: 15821
[16:44:25.136994] Epoch: [13]  [320/345]  eta: 0:00:15  lr: 0.000087  loss: 0.2611 (0.2873)  time: 0.6044  data: 0.0001  max mem: 15821
[16:44:37.220348] Epoch: [13]  [340/345]  eta: 0:00:03  lr: 0.000087  loss: 0.2782 (0.2873)  time: 0.6041  data: 0.0001  max mem: 15821
[16:44:39.631991] Epoch: [13]  [344/345]  eta: 0:00:00  lr: 0.000087  loss: 0.2780 (0.2869)  time: 0.6037  data: 0.0001  max mem: 15821
[16:44:39.698127] Epoch: [13] Total time: 0:03:28 (0.6048 s / it)
[16:44:39.698335] Averaged stats: lr: 0.000087  loss: 0.2780 (0.2869)
[16:44:40.207708] Test:  [  0/345]  eta: 0:02:53  loss: 0.3048 (0.3048)  time: 0.5036  data: 0.3394  max mem: 15821
[16:44:41.874541] Test:  [ 10/345]  eta: 0:01:06  loss: 0.2641 (0.2725)  time: 0.1972  data: 0.0310  max mem: 15821
[16:44:43.543607] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2641 (0.2713)  time: 0.1667  data: 0.0001  max mem: 15821
[16:44:45.216376] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2659 (0.2732)  time: 0.1670  data: 0.0001  max mem: 15821
[16:44:46.893191] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2682 (0.2781)  time: 0.1674  data: 0.0001  max mem: 15821
[16:44:48.573597] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2782 (0.2780)  time: 0.1678  data: 0.0001  max mem: 15821
[16:44:50.256514] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2971 (0.2854)  time: 0.1681  data: 0.0001  max mem: 15821
[16:44:51.943144] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2952 (0.2830)  time: 0.1684  data: 0.0001  max mem: 15821
[16:44:53.632596] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2672 (0.2807)  time: 0.1687  data: 0.0001  max mem: 15821
[16:44:55.326623] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2637 (0.2790)  time: 0.1691  data: 0.0001  max mem: 15821
[16:44:57.024113] Test:  [100/345]  eta: 0:00:41  loss: 0.2658 (0.2791)  time: 0.1695  data: 0.0001  max mem: 15821
[16:44:58.724220] Test:  [110/345]  eta: 0:00:40  loss: 0.2683 (0.2782)  time: 0.1698  data: 0.0001  max mem: 15821
[16:45:00.428338] Test:  [120/345]  eta: 0:00:38  loss: 0.2579 (0.2766)  time: 0.1701  data: 0.0001  max mem: 15821
[16:45:02.134813] Test:  [130/345]  eta: 0:00:36  loss: 0.2511 (0.2757)  time: 0.1705  data: 0.0001  max mem: 15821
[16:45:03.845287] Test:  [140/345]  eta: 0:00:35  loss: 0.2511 (0.2754)  time: 0.1708  data: 0.0001  max mem: 15821
[16:45:05.558847] Test:  [150/345]  eta: 0:00:33  loss: 0.2483 (0.2745)  time: 0.1711  data: 0.0001  max mem: 15821
[16:45:07.275764] Test:  [160/345]  eta: 0:00:31  loss: 0.2823 (0.2764)  time: 0.1715  data: 0.0001  max mem: 15821
[16:45:08.997418] Test:  [170/345]  eta: 0:00:29  loss: 0.2903 (0.2762)  time: 0.1719  data: 0.0001  max mem: 15821
[16:45:10.721643] Test:  [180/345]  eta: 0:00:28  loss: 0.2790 (0.2774)  time: 0.1722  data: 0.0001  max mem: 15821
[16:45:12.450133] Test:  [190/345]  eta: 0:00:26  loss: 0.2680 (0.2768)  time: 0.1726  data: 0.0001  max mem: 15821
[16:45:14.181714] Test:  [200/345]  eta: 0:00:24  loss: 0.2842 (0.2779)  time: 0.1729  data: 0.0001  max mem: 15821
[16:45:15.916268] Test:  [210/345]  eta: 0:00:23  loss: 0.2920 (0.2786)  time: 0.1732  data: 0.0001  max mem: 15821
[16:45:17.654669] Test:  [220/345]  eta: 0:00:21  loss: 0.2920 (0.2788)  time: 0.1736  data: 0.0001  max mem: 15821
[16:45:19.395825] Test:  [230/345]  eta: 0:00:19  loss: 0.2692 (0.2786)  time: 0.1739  data: 0.0001  max mem: 15821
[16:45:21.140767] Test:  [240/345]  eta: 0:00:18  loss: 0.2843 (0.2790)  time: 0.1742  data: 0.0001  max mem: 15821
[16:45:22.888737] Test:  [250/345]  eta: 0:00:16  loss: 0.3011 (0.2792)  time: 0.1746  data: 0.0001  max mem: 15821
[16:45:24.640855] Test:  [260/345]  eta: 0:00:14  loss: 0.2719 (0.2795)  time: 0.1749  data: 0.0001  max mem: 15821
[16:45:26.394882] Test:  [270/345]  eta: 0:00:12  loss: 0.2629 (0.2794)  time: 0.1752  data: 0.0001  max mem: 15821
[16:45:28.153146] Test:  [280/345]  eta: 0:00:11  loss: 0.2512 (0.2792)  time: 0.1755  data: 0.0001  max mem: 15821
[16:45:29.914890] Test:  [290/345]  eta: 0:00:09  loss: 0.2665 (0.2791)  time: 0.1759  data: 0.0001  max mem: 15821
[16:45:31.681166] Test:  [300/345]  eta: 0:00:07  loss: 0.2709 (0.2794)  time: 0.1763  data: 0.0001  max mem: 15821
[16:45:33.451260] Test:  [310/345]  eta: 0:00:06  loss: 0.2650 (0.2787)  time: 0.1768  data: 0.0001  max mem: 15821
[16:45:35.225024] Test:  [320/345]  eta: 0:00:04  loss: 0.2649 (0.2782)  time: 0.1771  data: 0.0001  max mem: 15821
[16:45:37.001490] Test:  [330/345]  eta: 0:00:02  loss: 0.2712 (0.2785)  time: 0.1774  data: 0.0001  max mem: 15821
[16:45:38.781443] Test:  [340/345]  eta: 0:00:00  loss: 0.2918 (0.2790)  time: 0.1778  data: 0.0001  max mem: 15821
[16:45:39.493467] Test:  [344/345]  eta: 0:00:00  loss: 0.2709 (0.2787)  time: 0.1779  data: 0.0001  max mem: 15821
[16:45:39.557506] Test: Total time: 0:00:59 (0.1735 s / it)
[16:45:49.767938] Test:  [ 0/57]  eta: 0:00:28  loss: 0.3776 (0.3776)  time: 0.5018  data: 0.3393  max mem: 15821
[16:45:51.414406] Test:  [10/57]  eta: 0:00:09  loss: 0.3776 (0.4028)  time: 0.1952  data: 0.0310  max mem: 15821
[16:45:53.066297] Test:  [20/57]  eta: 0:00:06  loss: 0.3649 (0.3838)  time: 0.1648  data: 0.0001  max mem: 15821
[16:45:54.721609] Test:  [30/57]  eta: 0:00:04  loss: 0.3387 (0.3553)  time: 0.1653  data: 0.0001  max mem: 15821
[16:45:56.382110] Test:  [40/57]  eta: 0:00:02  loss: 0.3089 (0.3468)  time: 0.1657  data: 0.0001  max mem: 15821
[16:45:58.046933] Test:  [50/57]  eta: 0:00:01  loss: 0.3221 (0.3548)  time: 0.1662  data: 0.0001  max mem: 15821
[16:45:58.945134] Test:  [56/57]  eta: 0:00:00  loss: 0.3826 (0.3720)  time: 0.1613  data: 0.0001  max mem: 15821
[16:45:59.011766] Test: Total time: 0:00:09 (0.1710 s / it)
[16:46:00.726958] Dice score of the network on the train images: 0.754666, val images: 0.736084
[16:46:00.731056] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[16:46:01.649289] Epoch: [14]  [  0/345]  eta: 0:05:16  lr: 0.000087  loss: 0.2998 (0.2998)  time: 0.9172  data: 0.3179  max mem: 15821
[16:46:13.627562] Epoch: [14]  [ 20/345]  eta: 0:03:19  lr: 0.000088  loss: 0.2819 (0.2862)  time: 0.5989  data: 0.0001  max mem: 15821
[16:46:25.642556] Epoch: [14]  [ 40/345]  eta: 0:03:05  lr: 0.000088  loss: 0.2904 (0.2831)  time: 0.6007  data: 0.0001  max mem: 15821
[16:46:37.672726] Epoch: [14]  [ 60/345]  eta: 0:02:52  lr: 0.000089  loss: 0.2680 (0.2823)  time: 0.6015  data: 0.0001  max mem: 15821
[16:46:49.699540] Epoch: [14]  [ 80/345]  eta: 0:02:40  lr: 0.000089  loss: 0.2716 (0.2790)  time: 0.6013  data: 0.0001  max mem: 15821
[16:47:01.752365] Epoch: [14]  [100/345]  eta: 0:02:28  lr: 0.000089  loss: 0.3001 (0.2812)  time: 0.6026  data: 0.0001  max mem: 15821
[16:47:13.808285] Epoch: [14]  [120/345]  eta: 0:02:15  lr: 0.000090  loss: 0.3220 (0.2862)  time: 0.6027  data: 0.0001  max mem: 15821
[16:47:25.888805] Epoch: [14]  [140/345]  eta: 0:02:03  lr: 0.000090  loss: 0.3150 (0.2898)  time: 0.6040  data: 0.0001  max mem: 15821
[16:47:37.963923] Epoch: [14]  [160/345]  eta: 0:01:51  lr: 0.000090  loss: 0.3314 (0.2946)  time: 0.6037  data: 0.0001  max mem: 15821
[16:47:50.038455] Epoch: [14]  [180/345]  eta: 0:01:39  lr: 0.000091  loss: 0.3526 (0.3003)  time: 0.6037  data: 0.0001  max mem: 15821
[16:48:02.225894] Epoch: [14]  [200/345]  eta: 0:01:27  lr: 0.000091  loss: 0.3035 (0.3012)  time: 0.6093  data: 0.0001  max mem: 15821
[16:48:14.296768] Epoch: [14]  [220/345]  eta: 0:01:15  lr: 0.000091  loss: 0.3051 (0.3024)  time: 0.6035  data: 0.0001  max mem: 15821
[16:48:26.383736] Epoch: [14]  [240/345]  eta: 0:01:03  lr: 0.000092  loss: 0.2589 (0.2998)  time: 0.6043  data: 0.0001  max mem: 15821
[16:48:38.445207] Epoch: [14]  [260/345]  eta: 0:00:51  lr: 0.000092  loss: 0.2789 (0.2992)  time: 0.6030  data: 0.0001  max mem: 15821
[16:48:50.522394] Epoch: [14]  [280/345]  eta: 0:00:39  lr: 0.000093  loss: 0.2677 (0.2976)  time: 0.6038  data: 0.0001  max mem: 15821
[16:49:02.602229] Epoch: [14]  [300/345]  eta: 0:00:27  lr: 0.000093  loss: 0.2923 (0.2976)  time: 0.6039  data: 0.0001  max mem: 15821
[16:49:14.679173] Epoch: [14]  [320/345]  eta: 0:00:15  lr: 0.000093  loss: 0.2708 (0.2959)  time: 0.6038  data: 0.0001  max mem: 15821
[16:49:26.741653] Epoch: [14]  [340/345]  eta: 0:00:03  lr: 0.000094  loss: 0.2932 (0.2955)  time: 0.6031  data: 0.0001  max mem: 15821
[16:49:29.151623] Epoch: [14]  [344/345]  eta: 0:00:00  lr: 0.000094  loss: 0.2894 (0.2951)  time: 0.6029  data: 0.0001  max mem: 15821
[16:49:29.217978] Epoch: [14] Total time: 0:03:28 (0.6043 s / it)
[16:49:29.218429] Averaged stats: lr: 0.000094  loss: 0.2894 (0.2951)
[16:49:29.725648] Test:  [  0/345]  eta: 0:02:53  loss: 0.3168 (0.3168)  time: 0.5015  data: 0.3375  max mem: 15821
[16:49:31.389922] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2713 (0.2674)  time: 0.1968  data: 0.0308  max mem: 15821
[16:49:33.058081] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2713 (0.2709)  time: 0.1665  data: 0.0001  max mem: 15821
[16:49:34.729735] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2675 (0.2697)  time: 0.1669  data: 0.0001  max mem: 15821
[16:49:36.403221] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2608 (0.2677)  time: 0.1672  data: 0.0001  max mem: 15821
[16:49:38.080323] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2608 (0.2648)  time: 0.1675  data: 0.0001  max mem: 15821
[16:49:39.761330] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2555 (0.2623)  time: 0.1678  data: 0.0001  max mem: 15821
[16:49:41.445354] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2665 (0.2649)  time: 0.1682  data: 0.0001  max mem: 15821
[16:49:43.133477] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2673 (0.2665)  time: 0.1685  data: 0.0001  max mem: 15821
[16:49:44.824473] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2655 (0.2674)  time: 0.1689  data: 0.0001  max mem: 15821
[16:49:46.519037] Test:  [100/345]  eta: 0:00:41  loss: 0.2655 (0.2665)  time: 0.1692  data: 0.0001  max mem: 15821
[16:49:48.216616] Test:  [110/345]  eta: 0:00:40  loss: 0.2552 (0.2651)  time: 0.1695  data: 0.0001  max mem: 15821
[16:49:49.918344] Test:  [120/345]  eta: 0:00:38  loss: 0.2380 (0.2642)  time: 0.1699  data: 0.0001  max mem: 15821
[16:49:51.624083] Test:  [130/345]  eta: 0:00:36  loss: 0.2380 (0.2638)  time: 0.1703  data: 0.0001  max mem: 15821
[16:49:53.332813] Test:  [140/345]  eta: 0:00:35  loss: 0.2508 (0.2634)  time: 0.1707  data: 0.0001  max mem: 15821
[16:49:55.043182] Test:  [150/345]  eta: 0:00:33  loss: 0.2602 (0.2641)  time: 0.1709  data: 0.0001  max mem: 15821
[16:49:56.758444] Test:  [160/345]  eta: 0:00:31  loss: 0.2721 (0.2643)  time: 0.1712  data: 0.0001  max mem: 15821
[16:49:58.477910] Test:  [170/345]  eta: 0:00:29  loss: 0.2813 (0.2661)  time: 0.1717  data: 0.0001  max mem: 15821
[16:50:00.200854] Test:  [180/345]  eta: 0:00:28  loss: 0.2867 (0.2670)  time: 0.1721  data: 0.0001  max mem: 15821
[16:50:01.927843] Test:  [190/345]  eta: 0:00:26  loss: 0.2673 (0.2672)  time: 0.1724  data: 0.0001  max mem: 15821
[16:50:03.658259] Test:  [200/345]  eta: 0:00:24  loss: 0.2457 (0.2672)  time: 0.1728  data: 0.0001  max mem: 15821
[16:50:05.392749] Test:  [210/345]  eta: 0:00:23  loss: 0.2450 (0.2671)  time: 0.1732  data: 0.0001  max mem: 15821
[16:50:07.128139] Test:  [220/345]  eta: 0:00:21  loss: 0.2509 (0.2665)  time: 0.1734  data: 0.0001  max mem: 15821
[16:50:08.867763] Test:  [230/345]  eta: 0:00:19  loss: 0.2585 (0.2674)  time: 0.1737  data: 0.0001  max mem: 15821
[16:50:10.611616] Test:  [240/345]  eta: 0:00:18  loss: 0.2861 (0.2678)  time: 0.1741  data: 0.0001  max mem: 15821
[16:50:12.357288] Test:  [250/345]  eta: 0:00:16  loss: 0.2708 (0.2672)  time: 0.1744  data: 0.0001  max mem: 15821
[16:50:14.106519] Test:  [260/345]  eta: 0:00:14  loss: 0.2415 (0.2658)  time: 0.1747  data: 0.0001  max mem: 15821
[16:50:15.858732] Test:  [270/345]  eta: 0:00:12  loss: 0.2432 (0.2654)  time: 0.1750  data: 0.0001  max mem: 15821
[16:50:17.615503] Test:  [280/345]  eta: 0:00:11  loss: 0.2567 (0.2655)  time: 0.1754  data: 0.0001  max mem: 15821
[16:50:19.376614] Test:  [290/345]  eta: 0:00:09  loss: 0.2605 (0.2654)  time: 0.1758  data: 0.0001  max mem: 15821
[16:50:21.140228] Test:  [300/345]  eta: 0:00:07  loss: 0.2667 (0.2654)  time: 0.1762  data: 0.0001  max mem: 15821
[16:50:22.908133] Test:  [310/345]  eta: 0:00:06  loss: 0.2634 (0.2651)  time: 0.1765  data: 0.0001  max mem: 15821
[16:50:24.679150] Test:  [320/345]  eta: 0:00:04  loss: 0.2634 (0.2655)  time: 0.1769  data: 0.0001  max mem: 15821
[16:50:26.454096] Test:  [330/345]  eta: 0:00:02  loss: 0.2879 (0.2659)  time: 0.1772  data: 0.0001  max mem: 15821
[16:50:28.232052] Test:  [340/345]  eta: 0:00:00  loss: 0.2788 (0.2662)  time: 0.1776  data: 0.0001  max mem: 15821
[16:50:28.944652] Test:  [344/345]  eta: 0:00:00  loss: 0.2879 (0.2669)  time: 0.1777  data: 0.0001  max mem: 15821
[16:50:29.006775] Test: Total time: 0:00:59 (0.1733 s / it)
[16:50:39.573866] Test:  [ 0/57]  eta: 0:00:27  loss: 0.3367 (0.3367)  time: 0.4779  data: 0.3155  max mem: 15821
[16:50:41.219660] Test:  [10/57]  eta: 0:00:09  loss: 0.3741 (0.3852)  time: 0.1930  data: 0.0288  max mem: 15821
[16:50:42.871126] Test:  [20/57]  eta: 0:00:06  loss: 0.3741 (0.3775)  time: 0.1648  data: 0.0001  max mem: 15821
[16:50:44.526839] Test:  [30/57]  eta: 0:00:04  loss: 0.3020 (0.3395)  time: 0.1653  data: 0.0001  max mem: 15821
[16:50:46.185151] Test:  [40/57]  eta: 0:00:02  loss: 0.2544 (0.3138)  time: 0.1656  data: 0.0001  max mem: 15821
[16:50:47.849652] Test:  [50/57]  eta: 0:00:01  loss: 0.2600 (0.3198)  time: 0.1661  data: 0.0001  max mem: 15821
[16:50:48.747413] Test:  [56/57]  eta: 0:00:00  loss: 0.3155 (0.3383)  time: 0.1612  data: 0.0001  max mem: 15821
[16:50:48.815070] Test: Total time: 0:00:09 (0.1705 s / it)
[16:50:50.521518] Dice score of the network on the train images: 0.746759, val images: 0.770945
[16:50:50.521745] saving best_dice_model_0 @ epoch 14
[16:50:51.636624] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[16:50:52.574565] Epoch: [15]  [  0/345]  eta: 0:05:23  lr: 0.000094  loss: 0.2244 (0.2244)  time: 0.9368  data: 0.3370  max mem: 15821
[16:51:04.547332] Epoch: [15]  [ 20/345]  eta: 0:03:19  lr: 0.000094  loss: 0.2409 (0.2492)  time: 0.5986  data: 0.0001  max mem: 15821
[16:51:16.560984] Epoch: [15]  [ 40/345]  eta: 0:03:05  lr: 0.000094  loss: 0.2516 (0.2563)  time: 0.6006  data: 0.0001  max mem: 15821
[16:51:28.592342] Epoch: [15]  [ 60/345]  eta: 0:02:52  lr: 0.000095  loss: 0.2606 (0.2586)  time: 0.6015  data: 0.0001  max mem: 15821
[16:51:40.646155] Epoch: [15]  [ 80/345]  eta: 0:02:40  lr: 0.000095  loss: 0.2612 (0.2645)  time: 0.6026  data: 0.0001  max mem: 15821
[16:51:52.723149] Epoch: [15]  [100/345]  eta: 0:02:28  lr: 0.000096  loss: 0.2706 (0.2687)  time: 0.6038  data: 0.0001  max mem: 15821
[16:52:04.812699] Epoch: [15]  [120/345]  eta: 0:02:16  lr: 0.000096  loss: 0.2707 (0.2703)  time: 0.6044  data: 0.0001  max mem: 15821
[16:52:16.911830] Epoch: [15]  [140/345]  eta: 0:02:03  lr: 0.000096  loss: 0.2573 (0.2711)  time: 0.6049  data: 0.0001  max mem: 15821
[16:52:29.007406] Epoch: [15]  [160/345]  eta: 0:01:51  lr: 0.000097  loss: 0.2640 (0.2713)  time: 0.6047  data: 0.0001  max mem: 15821
[16:52:41.104208] Epoch: [15]  [180/345]  eta: 0:01:39  lr: 0.000097  loss: 0.3131 (0.2763)  time: 0.6048  data: 0.0001  max mem: 15821
[16:52:53.195775] Epoch: [15]  [200/345]  eta: 0:01:27  lr: 0.000097  loss: 0.2838 (0.2783)  time: 0.6045  data: 0.0001  max mem: 15821
[16:53:05.274341] Epoch: [15]  [220/345]  eta: 0:01:15  lr: 0.000098  loss: 0.2611 (0.2777)  time: 0.6039  data: 0.0001  max mem: 15821
[16:53:17.351016] Epoch: [15]  [240/345]  eta: 0:01:03  lr: 0.000098  loss: 0.2764 (0.2778)  time: 0.6038  data: 0.0001  max mem: 15821
[16:53:29.438252] Epoch: [15]  [260/345]  eta: 0:00:51  lr: 0.000098  loss: 0.2713 (0.2778)  time: 0.6043  data: 0.0001  max mem: 15821
[16:53:41.494479] Epoch: [15]  [280/345]  eta: 0:00:39  lr: 0.000099  loss: 0.2671 (0.2775)  time: 0.6028  data: 0.0001  max mem: 15821
[16:53:53.538860] Epoch: [15]  [300/345]  eta: 0:00:27  lr: 0.000099  loss: 0.2868 (0.2782)  time: 0.6022  data: 0.0001  max mem: 15821
[16:54:05.586001] Epoch: [15]  [320/345]  eta: 0:00:15  lr: 0.000100  loss: 0.2285 (0.2753)  time: 0.6023  data: 0.0001  max mem: 15821
[16:54:17.645747] Epoch: [15]  [340/345]  eta: 0:00:03  lr: 0.000100  loss: 0.2782 (0.2755)  time: 0.6029  data: 0.0001  max mem: 15821
[16:54:20.053828] Epoch: [15]  [344/345]  eta: 0:00:00  lr: 0.000100  loss: 0.2782 (0.2750)  time: 0.6028  data: 0.0001  max mem: 15821
[16:54:20.118027] Epoch: [15] Total time: 0:03:28 (0.6043 s / it)
[16:54:20.118369] Averaged stats: lr: 0.000100  loss: 0.2782 (0.2750)
[16:54:20.633670] Test:  [  0/345]  eta: 0:02:55  loss: 0.2530 (0.2530)  time: 0.5095  data: 0.3458  max mem: 15821
[16:54:22.299550] Test:  [ 10/345]  eta: 0:01:06  loss: 0.2707 (0.2513)  time: 0.1976  data: 0.0316  max mem: 15821
[16:54:23.966871] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2456 (0.2462)  time: 0.1666  data: 0.0001  max mem: 15821
[16:54:25.637814] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2453 (0.2515)  time: 0.1668  data: 0.0001  max mem: 15821
[16:54:27.311902] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2453 (0.2488)  time: 0.1672  data: 0.0001  max mem: 15821
[16:54:28.988709] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2504 (0.2523)  time: 0.1675  data: 0.0001  max mem: 15821
[16:54:30.669377] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2636 (0.2544)  time: 0.1678  data: 0.0001  max mem: 15821
[16:54:32.352565] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2624 (0.2547)  time: 0.1681  data: 0.0001  max mem: 15821
[16:54:34.039396] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2596 (0.2554)  time: 0.1684  data: 0.0001  max mem: 15821
[16:54:35.729432] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2643 (0.2541)  time: 0.1688  data: 0.0001  max mem: 15821
[16:54:37.424532] Test:  [100/345]  eta: 0:00:41  loss: 0.2595 (0.2523)  time: 0.1692  data: 0.0001  max mem: 15821
[16:54:39.123466] Test:  [110/345]  eta: 0:00:40  loss: 0.2460 (0.2519)  time: 0.1696  data: 0.0001  max mem: 15821
[16:54:40.825462] Test:  [120/345]  eta: 0:00:38  loss: 0.2460 (0.2518)  time: 0.1700  data: 0.0001  max mem: 15821
[16:54:42.529089] Test:  [130/345]  eta: 0:00:36  loss: 0.2577 (0.2525)  time: 0.1702  data: 0.0001  max mem: 15821
[16:54:44.238700] Test:  [140/345]  eta: 0:00:35  loss: 0.2629 (0.2531)  time: 0.1706  data: 0.0001  max mem: 15821
[16:54:45.950006] Test:  [150/345]  eta: 0:00:33  loss: 0.2567 (0.2523)  time: 0.1710  data: 0.0001  max mem: 15821
[16:54:47.665355] Test:  [160/345]  eta: 0:00:31  loss: 0.2397 (0.2520)  time: 0.1713  data: 0.0001  max mem: 15821
[16:54:49.384054] Test:  [170/345]  eta: 0:00:29  loss: 0.2479 (0.2519)  time: 0.1716  data: 0.0001  max mem: 15821
[16:54:51.105589] Test:  [180/345]  eta: 0:00:28  loss: 0.2479 (0.2518)  time: 0.1719  data: 0.0001  max mem: 15821
[16:54:52.830942] Test:  [190/345]  eta: 0:00:26  loss: 0.2418 (0.2518)  time: 0.1723  data: 0.0001  max mem: 15821
[16:54:54.559731] Test:  [200/345]  eta: 0:00:24  loss: 0.2589 (0.2538)  time: 0.1726  data: 0.0001  max mem: 15821
[16:54:56.292954] Test:  [210/345]  eta: 0:00:23  loss: 0.2873 (0.2552)  time: 0.1730  data: 0.0001  max mem: 15821
[16:54:58.027206] Test:  [220/345]  eta: 0:00:21  loss: 0.2741 (0.2553)  time: 0.1733  data: 0.0001  max mem: 15821
[16:54:59.765700] Test:  [230/345]  eta: 0:00:19  loss: 0.2467 (0.2551)  time: 0.1736  data: 0.0001  max mem: 15821
[16:55:01.507890] Test:  [240/345]  eta: 0:00:18  loss: 0.2251 (0.2545)  time: 0.1740  data: 0.0001  max mem: 15821
[16:55:03.253557] Test:  [250/345]  eta: 0:00:16  loss: 0.2444 (0.2544)  time: 0.1743  data: 0.0001  max mem: 15821
[16:55:05.002175] Test:  [260/345]  eta: 0:00:14  loss: 0.2444 (0.2535)  time: 0.1747  data: 0.0001  max mem: 15821
[16:55:06.755088] Test:  [270/345]  eta: 0:00:12  loss: 0.2374 (0.2539)  time: 0.1750  data: 0.0001  max mem: 15821
[16:55:08.511181] Test:  [280/345]  eta: 0:00:11  loss: 0.2542 (0.2544)  time: 0.1754  data: 0.0001  max mem: 15821
[16:55:10.269679] Test:  [290/345]  eta: 0:00:09  loss: 0.2586 (0.2541)  time: 0.1757  data: 0.0001  max mem: 15821
[16:55:12.033241] Test:  [300/345]  eta: 0:00:07  loss: 0.2586 (0.2548)  time: 0.1760  data: 0.0001  max mem: 15821
[16:55:13.800781] Test:  [310/345]  eta: 0:00:06  loss: 0.2451 (0.2541)  time: 0.1765  data: 0.0001  max mem: 15821
[16:55:15.570994] Test:  [320/345]  eta: 0:00:04  loss: 0.2443 (0.2540)  time: 0.1768  data: 0.0001  max mem: 15821
[16:55:17.343686] Test:  [330/345]  eta: 0:00:02  loss: 0.2535 (0.2544)  time: 0.1771  data: 0.0001  max mem: 15821
[16:55:19.121100] Test:  [340/345]  eta: 0:00:00  loss: 0.2606 (0.2547)  time: 0.1774  data: 0.0001  max mem: 15821
[16:55:19.834176] Test:  [344/345]  eta: 0:00:00  loss: 0.2637 (0.2549)  time: 0.1776  data: 0.0001  max mem: 15821
[16:55:19.898183] Test: Total time: 0:00:59 (0.1733 s / it)
[16:55:30.205502] Test:  [ 0/57]  eta: 0:00:28  loss: 0.3218 (0.3218)  time: 0.5052  data: 0.3427  max mem: 15821
[16:55:31.852234] Test:  [10/57]  eta: 0:00:09  loss: 0.3667 (0.3730)  time: 0.1955  data: 0.0313  max mem: 15821
[16:55:33.504334] Test:  [20/57]  eta: 0:00:06  loss: 0.3884 (0.3826)  time: 0.1649  data: 0.0001  max mem: 15821
[16:55:35.159641] Test:  [30/57]  eta: 0:00:04  loss: 0.3138 (0.3472)  time: 0.1653  data: 0.0001  max mem: 15821
[16:55:36.818590] Test:  [40/57]  eta: 0:00:02  loss: 0.2592 (0.3263)  time: 0.1656  data: 0.0001  max mem: 15821
[16:55:38.481974] Test:  [50/57]  eta: 0:00:01  loss: 0.2822 (0.3266)  time: 0.1661  data: 0.0001  max mem: 15821
[16:55:39.378565] Test:  [56/57]  eta: 0:00:00  loss: 0.3317 (0.3398)  time: 0.1611  data: 0.0001  max mem: 15821
[16:55:39.448635] Test: Total time: 0:00:09 (0.1710 s / it)
[16:55:41.159315] Dice score of the network on the train images: 0.761043, val images: 0.772126
[16:55:41.159520] saving best_dice_model_0 @ epoch 15
[16:55:42.268232] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft

[16:55:43.195981] Epoch: [16]  [  0/345]  eta: 0:05:19  lr: 0.000100  loss: 0.2308 (0.2308)  time: 0.9265  data: 0.3262  max mem: 15821
[16:55:55.158313] Epoch: [16]  [ 20/345]  eta: 0:03:19  lr: 0.000100  loss: 0.2465 (0.2639)  time: 0.5981  data: 0.0001  max mem: 15821
[16:56:07.162726] Epoch: [16]  [ 40/345]  eta: 0:03:05  lr: 0.000101  loss: 0.2912 (0.2796)  time: 0.6002  data: 0.0001  max mem: 15821
[16:56:19.177165] Epoch: [16]  [ 60/345]  eta: 0:02:52  lr: 0.000101  loss: 0.2671 (0.2758)  time: 0.6007  data: 0.0001  max mem: 15821
[16:56:31.212977] Epoch: [16]  [ 80/345]  eta: 0:02:40  lr: 0.000101  loss: 0.2637 (0.2755)  time: 0.6017  data: 0.0001  max mem: 15821

[16:56:43.269891] Epoch: [16]  [100/345]  eta: 0:02:27  lr: 0.000102  loss: 0.2908 (0.2789)  time: 0.6028  data: 0.0001  max mem: 15821
[16:56:55.344229] Epoch: [16]  [120/345]  eta: 0:02:15  lr: 0.000102  loss: 0.2672 (0.2770)  time: 0.6037  data: 0.0001  max mem: 15821
[16:57:07.428378] Epoch: [16]  [140/345]  eta: 0:02:03  lr: 0.000103  loss: 0.2744 (0.2766)  time: 0.6042  data: 0.0001  max mem: 15821
[16:57:19.642813] Epoch: [16]  [160/345]  eta: 0:01:51  lr: 0.000103  loss: 0.2613 (0.2756)  time: 0.6107  data: 0.0001  max mem: 15821
[16:57:31.716809] Epoch: [16]  [180/345]  eta: 0:01:39  lr: 0.000103  loss: 0.2696 (0.2754)  time: 0.6037  data: 0.0001  max mem: 15821
[16:57:43.785415] Epoch: [16]  [200/345]  eta: 0:01:27  lr: 0.000104  loss: 0.2573 (0.2743)  time: 0.6034  data: 0.0001  max mem: 15821
[16:57:55.838923] Epoch: [16]  [220/345]  eta: 0:01:15  lr: 0.000104  loss: 0.2612 (0.2729)  time: 0.6026  data: 0.0001  max mem: 15821
[16:58:07.916477] Epoch: [16]  [240/345]  eta: 0:01:03  lr: 0.000104  loss: 0.2292 (0.2700)  time: 0.6038  data: 0.0001  max mem: 15821
[16:58:19.990956] Epoch: [16]  [260/345]  eta: 0:00:51  lr: 0.000105  loss: 0.2543 (0.2701)  time: 0.6037  data: 0.0001  max mem: 15821
[16:58:32.063720] Epoch: [16]  [280/345]  eta: 0:00:39  lr: 0.000105  loss: 0.2963 (0.2716)  time: 0.6036  data: 0.0001  max mem: 15821
[16:58:44.130992] Epoch: [16]  [300/345]  eta: 0:00:27  lr: 0.000105  loss: 0.2633 (0.2714)  time: 0.6033  data: 0.0001  max mem: 15821
[16:58:56.184259] Epoch: [16]  [320/345]  eta: 0:00:15  lr: 0.000106  loss: 0.2858 (0.2721)  time: 0.6026  data: 0.0001  max mem: 15821
[16:59:08.233859] Epoch: [16]  [340/345]  eta: 0:00:03  lr: 0.000106  loss: 0.2619 (0.2715)  time: 0.6024  data: 0.0001  max mem: 15821
[16:59:10.645461] Epoch: [16]  [344/345]  eta: 0:00:00  lr: 0.000106  loss: 0.2619 (0.2715)  time: 0.6023  data: 0.0001  max mem: 15821
[16:59:10.711204] Epoch: [16] Total time: 0:03:28 (0.6042 s / it)
[16:59:10.711432] Averaged stats: lr: 0.000106  loss: 0.2619 (0.2715)
[16:59:11.259448] Test:  [  0/345]  eta: 0:03:07  loss: 0.3213 (0.3213)  time: 0.5424  data: 0.3786  max mem: 15821
[16:59:12.924429] Test:  [ 10/345]  eta: 0:01:07  loss: 0.2626 (0.2620)  time: 0.2006  data: 0.0345  max mem: 15821
[16:59:14.592128] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2749 (0.2696)  time: 0.1665  data: 0.0001  max mem: 15821
[16:59:16.263407] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2704 (0.2630)  time: 0.1669  data: 0.0001  max mem: 15821
[16:59:17.937110] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2660 (0.2640)  time: 0.1672  data: 0.0001  max mem: 15821
[16:59:19.614102] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2437 (0.2568)  time: 0.1675  data: 0.0001  max mem: 15821
[16:59:21.294124] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2502 (0.2584)  time: 0.1678  data: 0.0001  max mem: 15821
[16:59:22.978582] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2537 (0.2586)  time: 0.1682  data: 0.0001  max mem: 15821
[16:59:24.665880] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2502 (0.2580)  time: 0.1685  data: 0.0001  max mem: 15821
[16:59:26.356611] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2570 (0.2580)  time: 0.1688  data: 0.0001  max mem: 15821
[16:59:28.050826] Test:  [100/345]  eta: 0:00:42  loss: 0.2545 (0.2577)  time: 0.1692  data: 0.0001  max mem: 15821
[16:59:29.748199] Test:  [110/345]  eta: 0:00:40  loss: 0.2551 (0.2593)  time: 0.1695  data: 0.0001  max mem: 15821
[16:59:31.450311] Test:  [120/345]  eta: 0:00:38  loss: 0.2851 (0.2612)  time: 0.1699  data: 0.0001  max mem: 15821
[16:59:33.154735] Test:  [130/345]  eta: 0:00:36  loss: 0.2399 (0.2590)  time: 0.1703  data: 0.0001  max mem: 15821
[16:59:34.862933] Test:  [140/345]  eta: 0:00:35  loss: 0.2270 (0.2584)  time: 0.1706  data: 0.0001  max mem: 15821
[16:59:36.573895] Test:  [150/345]  eta: 0:00:33  loss: 0.2490 (0.2581)  time: 0.1709  data: 0.0001  max mem: 15821
[16:59:38.289216] Test:  [160/345]  eta: 0:00:31  loss: 0.2494 (0.2577)  time: 0.1712  data: 0.0001  max mem: 15821
[16:59:40.009626] Test:  [170/345]  eta: 0:00:29  loss: 0.2564 (0.2582)  time: 0.1717  data: 0.0001  max mem: 15821
[16:59:41.729999] Test:  [180/345]  eta: 0:00:28  loss: 0.2446 (0.2568)  time: 0.1720  data: 0.0001  max mem: 15821
[16:59:43.455094] Test:  [190/345]  eta: 0:00:26  loss: 0.2264 (0.2568)  time: 0.1722  data: 0.0001  max mem: 15821
[16:59:45.184179] Test:  [200/345]  eta: 0:00:24  loss: 0.2473 (0.2562)  time: 0.1726  data: 0.0001  max mem: 15821
[16:59:46.914532] Test:  [210/345]  eta: 0:00:23  loss: 0.2473 (0.2558)  time: 0.1729  data: 0.0001  max mem: 15821
[16:59:48.648343] Test:  [220/345]  eta: 0:00:21  loss: 0.2561 (0.2556)  time: 0.1731  data: 0.0001  max mem: 15821
[16:59:50.386220] Test:  [230/345]  eta: 0:00:19  loss: 0.2561 (0.2554)  time: 0.1735  data: 0.0001  max mem: 15821
[16:59:52.128188] Test:  [240/345]  eta: 0:00:18  loss: 0.2529 (0.2552)  time: 0.1739  data: 0.0001  max mem: 15821
[16:59:53.874754] Test:  [250/345]  eta: 0:00:16  loss: 0.2439 (0.2548)  time: 0.1744  data: 0.0001  max mem: 15821
[16:59:55.623470] Test:  [260/345]  eta: 0:00:14  loss: 0.2364 (0.2538)  time: 0.1747  data: 0.0001  max mem: 15821
[16:59:57.375327] Test:  [270/345]  eta: 0:00:12  loss: 0.2426 (0.2543)  time: 0.1750  data: 0.0001  max mem: 15821
[16:59:59.131803] Test:  [280/345]  eta: 0:00:11  loss: 0.2595 (0.2550)  time: 0.1753  data: 0.0001  max mem: 15821
[17:00:00.891189] Test:  [290/345]  eta: 0:00:09  loss: 0.2595 (0.2556)  time: 0.1757  data: 0.0001  max mem: 15821
[17:00:02.653231] Test:  [300/345]  eta: 0:00:07  loss: 0.2483 (0.2557)  time: 0.1760  data: 0.0001  max mem: 15821
[17:00:04.421713] Test:  [310/345]  eta: 0:00:06  loss: 0.2484 (0.2563)  time: 0.1765  data: 0.0001  max mem: 15821
[17:00:06.192318] Test:  [320/345]  eta: 0:00:04  loss: 0.2576 (0.2567)  time: 0.1769  data: 0.0001  max mem: 15821
[17:00:07.967392] Test:  [330/345]  eta: 0:00:02  loss: 0.2541 (0.2568)  time: 0.1772  data: 0.0001  max mem: 15821
[17:00:09.744939] Test:  [340/345]  eta: 0:00:00  loss: 0.2587 (0.2570)  time: 0.1776  data: 0.0001  max mem: 15821
[17:00:10.457494] Test:  [344/345]  eta: 0:00:00  loss: 0.2492 (0.2566)  time: 0.1777  data: 0.0001  max mem: 15821
[17:00:10.511420] Test: Total time: 0:00:59 (0.1733 s / it)
[17:00:21.103691] Test:  [ 0/57]  eta: 0:00:27  loss: 0.3274 (0.3274)  time: 0.4816  data: 0.3193  max mem: 15821
[17:00:22.749055] Test:  [10/57]  eta: 0:00:09  loss: 0.3807 (0.3871)  time: 0.1933  data: 0.0291  max mem: 15821
[17:00:24.401588] Test:  [20/57]  eta: 0:00:06  loss: 0.3458 (0.3622)  time: 0.1648  data: 0.0001  max mem: 15821
[17:00:26.056668] Test:  [30/57]  eta: 0:00:04  loss: 0.2959 (0.3325)  time: 0.1653  data: 0.0001  max mem: 15821
[17:00:27.716414] Test:  [40/57]  eta: 0:00:02  loss: 0.2632 (0.3155)  time: 0.1656  data: 0.0001  max mem: 15821
[17:00:29.379754] Test:  [50/57]  eta: 0:00:01  loss: 0.2752 (0.3238)  time: 0.1661  data: 0.0001  max mem: 15821
[17:00:30.277418] Test:  [56/57]  eta: 0:00:00  loss: 0.3214 (0.3491)  time: 0.1612  data: 0.0001  max mem: 15821
[17:00:30.342806] Test: Total time: 0:00:09 (0.1706 s / it)
[17:00:32.055773] Dice score of the network on the train images: 0.743599, val images: 0.739770
[17:00:32.059373] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[17:00:33.024773] Epoch: [17]  [  0/345]  eta: 0:05:32  lr: 0.000106  loss: 0.1996 (0.1996)  time: 0.9642  data: 0.3617  max mem: 15821
[17:00:45.012873] Epoch: [17]  [ 20/345]  eta: 0:03:20  lr: 0.000107  loss: 0.2578 (0.2596)  time: 0.5993  data: 0.0001  max mem: 15821
[17:00:57.159890] Epoch: [17]  [ 40/345]  eta: 0:03:06  lr: 0.000107  loss: 0.2750 (0.2673)  time: 0.6073  data: 0.0001  max mem: 15821
[17:01:09.183009] Epoch: [17]  [ 60/345]  eta: 0:02:53  lr: 0.000107  loss: 0.2760 (0.2708)  time: 0.6011  data: 0.0001  max mem: 15821
[17:01:21.199696] Epoch: [17]  [ 80/345]  eta: 0:02:40  lr: 0.000108  loss: 0.2650 (0.2699)  time: 0.6008  data: 0.0001  max mem: 15821
[17:01:33.240788] Epoch: [17]  [100/345]  eta: 0:02:28  lr: 0.000108  loss: 0.2367 (0.2662)  time: 0.6020  data: 0.0001  max mem: 15821
[17:01:45.294188] Epoch: [17]  [120/345]  eta: 0:02:16  lr: 0.000108  loss: 0.2353 (0.2643)  time: 0.6026  data: 0.0001  max mem: 15821
[17:01:57.355303] Epoch: [17]  [140/345]  eta: 0:02:04  lr: 0.000109  loss: 0.2503 (0.2629)  time: 0.6030  data: 0.0001  max mem: 15821
[17:02:09.423826] Epoch: [17]  [160/345]  eta: 0:01:51  lr: 0.000109  loss: 0.2544 (0.2628)  time: 0.6034  data: 0.0001  max mem: 15821
[17:02:21.496106] Epoch: [17]  [180/345]  eta: 0:01:39  lr: 0.000110  loss: 0.2537 (0.2615)  time: 0.6036  data: 0.0001  max mem: 15821
[17:02:33.577102] Epoch: [17]  [200/345]  eta: 0:01:27  lr: 0.000110  loss: 0.2451 (0.2607)  time: 0.6040  data: 0.0001  max mem: 15821
[17:02:45.642856] Epoch: [17]  [220/345]  eta: 0:01:15  lr: 0.000110  loss: 0.2608 (0.2608)  time: 0.6032  data: 0.0001  max mem: 15821

[17:02:57.694356] Epoch: [17]  [240/345]  eta: 0:01:03  lr: 0.000111  loss: 0.2411 (0.2595)  time: 0.6025  data: 0.0001  max mem: 15821
[17:03:09.749870] Epoch: [17]  [260/345]  eta: 0:00:51  lr: 0.000111  loss: 0.2448 (0.2582)  time: 0.6027  data: 0.0001  max mem: 15821
[17:03:21.885356] Epoch: [17]  [280/345]  eta: 0:00:39  lr: 0.000111  loss: 0.2566 (0.2583)  time: 0.6067  data: 0.0001  max mem: 15821
[17:03:33.937280] Epoch: [17]  [300/345]  eta: 0:00:27  lr: 0.000112  loss: 0.2531 (0.2586)  time: 0.6026  data: 0.0001  max mem: 15821
[17:03:45.992104] Epoch: [17]  [320/345]  eta: 0:00:15  lr: 0.000112  loss: 0.2679 (0.2595)  time: 0.6027  data: 0.0001  max mem: 15821
[17:03:58.044910] Epoch: [17]  [340/345]  eta: 0:00:03  lr: 0.000112  loss: 0.2598 (0.2597)  time: 0.6026  data: 0.0001  max mem: 15821
[17:04:00.457128] Epoch: [17]  [344/345]  eta: 0:00:00  lr: 0.000112  loss: 0.2717 (0.2599)  time: 0.6025  data: 0.0001  max mem: 15821
[17:04:00.531114] Epoch: [17] Total time: 0:03:28 (0.6043 s / it)
[17:04:00.531584] Averaged stats: lr: 0.000112  loss: 0.2717 (0.2599)
[17:04:01.046961] Test:  [  0/345]  eta: 0:02:55  loss: 0.2917 (0.2917)  time: 0.5097  data: 0.3461  max mem: 15821
[17:04:02.711682] Test:  [ 10/345]  eta: 0:01:06  loss: 0.2618 (0.2667)  time: 0.1976  data: 0.0316  max mem: 15821
[17:04:04.378175] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2652 (0.2804)  time: 0.1665  data: 0.0001  max mem: 15821
[17:04:06.049665] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2714 (0.2755)  time: 0.1668  data: 0.0001  max mem: 15821
[17:04:07.723570] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2606 (0.2733)  time: 0.1672  data: 0.0001  max mem: 15821
[17:04:09.400740] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2569 (0.2676)  time: 0.1675  data: 0.0001  max mem: 15821
[17:04:11.081761] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2569 (0.2672)  time: 0.1678  data: 0.0001  max mem: 15821
[17:04:12.765493] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2691 (0.2701)  time: 0.1682  data: 0.0001  max mem: 15821
[17:04:14.451944] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2685 (0.2698)  time: 0.1684  data: 0.0001  max mem: 15821
[17:04:16.142481] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2513 (0.2696)  time: 0.1688  data: 0.0001  max mem: 15821
[17:04:17.836842] Test:  [100/345]  eta: 0:00:41  loss: 0.2466 (0.2648)  time: 0.1692  data: 0.0001  max mem: 15821
[17:04:19.533739] Test:  [110/345]  eta: 0:00:40  loss: 0.2331 (0.2637)  time: 0.1695  data: 0.0001  max mem: 15821
[17:04:21.232979] Test:  [120/345]  eta: 0:00:38  loss: 0.2421 (0.2628)  time: 0.1697  data: 0.0001  max mem: 15821
[17:04:22.936849] Test:  [130/345]  eta: 0:00:36  loss: 0.2469 (0.2618)  time: 0.1701  data: 0.0001  max mem: 15821
[17:04:24.645133] Test:  [140/345]  eta: 0:00:35  loss: 0.2518 (0.2615)  time: 0.1705  data: 0.0001  max mem: 15821
[17:04:26.355740] Test:  [150/345]  eta: 0:00:33  loss: 0.2616 (0.2622)  time: 0.1709  data: 0.0001  max mem: 15821
[17:04:28.071050] Test:  [160/345]  eta: 0:00:31  loss: 0.2685 (0.2631)  time: 0.1712  data: 0.0001  max mem: 15821
[17:04:29.790315] Test:  [170/345]  eta: 0:00:29  loss: 0.2842 (0.2652)  time: 0.1717  data: 0.0001  max mem: 15821
[17:04:31.512921] Test:  [180/345]  eta: 0:00:28  loss: 0.2809 (0.2649)  time: 0.1720  data: 0.0001  max mem: 15821
[17:04:33.239009] Test:  [190/345]  eta: 0:00:26  loss: 0.2682 (0.2653)  time: 0.1724  data: 0.0001  max mem: 15821
[17:04:34.967728] Test:  [200/345]  eta: 0:00:24  loss: 0.2802 (0.2665)  time: 0.1727  data: 0.0001  max mem: 15821
[17:04:36.699479] Test:  [210/345]  eta: 0:00:23  loss: 0.2855 (0.2663)  time: 0.1730  data: 0.0001  max mem: 15821
[17:04:38.435373] Test:  [220/345]  eta: 0:00:21  loss: 0.2467 (0.2656)  time: 0.1733  data: 0.0001  max mem: 15821
[17:04:40.174004] Test:  [230/345]  eta: 0:00:19  loss: 0.2444 (0.2647)  time: 0.1737  data: 0.0001  max mem: 15821
[17:04:41.916438] Test:  [240/345]  eta: 0:00:18  loss: 0.2355 (0.2638)  time: 0.1740  data: 0.0001  max mem: 15821
[17:04:43.664157] Test:  [250/345]  eta: 0:00:16  loss: 0.2393 (0.2642)  time: 0.1744  data: 0.0001  max mem: 15821
[17:04:45.412925] Test:  [260/345]  eta: 0:00:14  loss: 0.2631 (0.2650)  time: 0.1748  data: 0.0001  max mem: 15821
[17:04:47.166020] Test:  [270/345]  eta: 0:00:12  loss: 0.2631 (0.2648)  time: 0.1750  data: 0.0001  max mem: 15821
[17:04:48.921457] Test:  [280/345]  eta: 0:00:11  loss: 0.2404 (0.2642)  time: 0.1754  data: 0.0001  max mem: 15821
[17:04:50.680483] Test:  [290/345]  eta: 0:00:09  loss: 0.2389 (0.2638)  time: 0.1757  data: 0.0001  max mem: 15821
[17:04:52.444082] Test:  [300/345]  eta: 0:00:07  loss: 0.2329 (0.2632)  time: 0.1761  data: 0.0001  max mem: 15821
[17:04:54.211218] Test:  [310/345]  eta: 0:00:06  loss: 0.2471 (0.2629)  time: 0.1765  data: 0.0001  max mem: 15821
[17:04:55.980529] Test:  [320/345]  eta: 0:00:04  loss: 0.2475 (0.2628)  time: 0.1768  data: 0.0001  max mem: 15821
[17:04:57.753893] Test:  [330/345]  eta: 0:00:02  loss: 0.2601 (0.2629)  time: 0.1771  data: 0.0001  max mem: 15821
[17:04:59.530741] Test:  [340/345]  eta: 0:00:00  loss: 0.2573 (0.2626)  time: 0.1775  data: 0.0001  max mem: 15821
[17:05:00.242515] Test:  [344/345]  eta: 0:00:00  loss: 0.2479 (0.2623)  time: 0.1776  data: 0.0001  max mem: 15821
[17:05:00.309359] Test: Total time: 0:00:59 (0.1733 s / it)
[17:05:10.704273] Test:  [ 0/57]  eta: 0:00:27  loss: 0.3311 (0.3311)  time: 0.4910  data: 0.3287  max mem: 15821
[17:05:12.351377] Test:  [10/57]  eta: 0:00:09  loss: 0.3641 (0.3896)  time: 0.1943  data: 0.0300  max mem: 15821
[17:05:14.003263] Test:  [20/57]  eta: 0:00:06  loss: 0.3572 (0.3640)  time: 0.1649  data: 0.0001  max mem: 15821
[17:05:15.658911] Test:  [30/57]  eta: 0:00:04  loss: 0.2842 (0.3305)  time: 0.1653  data: 0.0001  max mem: 15821
[17:05:17.317573] Test:  [40/57]  eta: 0:00:02  loss: 0.2486 (0.3089)  time: 0.1656  data: 0.0001  max mem: 15821
[17:05:18.980017] Test:  [50/57]  eta: 0:00:01  loss: 0.2486 (0.3073)  time: 0.1660  data: 0.0001  max mem: 15821
[17:05:19.878099] Test:  [56/57]  eta: 0:00:00  loss: 0.2976 (0.3195)  time: 0.1612  data: 0.0001  max mem: 15821
[17:05:19.939915] Test: Total time: 0:00:09 (0.1707 s / it)
[17:05:21.667224] Dice score of the network on the train images: 0.734191, val images: 0.773147
[17:05:21.667459] saving best_dice_model_0 @ epoch 17
[17:05:23.044221] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[17:05:23.985187] Epoch: [18]  [  0/345]  eta: 0:05:24  lr: 0.000113  loss: 0.3297 (0.3297)  time: 0.9399  data: 0.3380  max mem: 15821
[17:05:35.957959] Epoch: [18]  [ 20/345]  eta: 0:03:19  lr: 0.000113  loss: 0.2794 (0.2914)  time: 0.5986  data: 0.0001  max mem: 15821
[17:05:47.971298] Epoch: [18]  [ 40/345]  eta: 0:03:05  lr: 0.000113  loss: 0.2625 (0.2808)  time: 0.6006  data: 0.0001  max mem: 15821
[17:06:00.006709] Epoch: [18]  [ 60/345]  eta: 0:02:52  lr: 0.000114  loss: 0.2591 (0.2777)  time: 0.6017  data: 0.0001  max mem: 15821
[17:06:12.060006] Epoch: [18]  [ 80/345]  eta: 0:02:40  lr: 0.000114  loss: 0.2684 (0.2769)  time: 0.6026  data: 0.0001  max mem: 15821
[17:06:24.127325] Epoch: [18]  [100/345]  eta: 0:02:28  lr: 0.000114  loss: 0.2694 (0.2750)  time: 0.6033  data: 0.0001  max mem: 15821
[17:06:36.203937] Epoch: [18]  [120/345]  eta: 0:02:16  lr: 0.000115  loss: 0.2322 (0.2690)  time: 0.6038  data: 0.0001  max mem: 15821
[17:06:48.285108] Epoch: [18]  [140/345]  eta: 0:02:03  lr: 0.000115  loss: 0.2412 (0.2665)  time: 0.6040  data: 0.0001  max mem: 15821
[17:07:00.367032] Epoch: [18]  [160/345]  eta: 0:01:51  lr: 0.000115  loss: 0.2353 (0.2632)  time: 0.6041  data: 0.0001  max mem: 15821
[17:07:12.452470] Epoch: [18]  [180/345]  eta: 0:01:39  lr: 0.000116  loss: 0.2549 (0.2641)  time: 0.6042  data: 0.0001  max mem: 15821

[17:07:24.527670] Epoch: [18]  [200/345]  eta: 0:01:27  lr: 0.000116  loss: 0.2427 (0.2623)  time: 0.6037  data: 0.0001  max mem: 15821
[17:07:36.603436] Epoch: [18]  [220/345]  eta: 0:01:15  lr: 0.000116  loss: 0.2496 (0.2612)  time: 0.6037  data: 0.0001  max mem: 15821
[17:07:48.685711] Epoch: [18]  [240/345]  eta: 0:01:03  lr: 0.000117  loss: 0.2623 (0.2616)  time: 0.6041  data: 0.0001  max mem: 15821
[17:08:00.745588] Epoch: [18]  [260/345]  eta: 0:00:51  lr: 0.000117  loss: 0.2411 (0.2612)  time: 0.6029  data: 0.0001  max mem: 15821
[17:08:12.815196] Epoch: [18]  [280/345]  eta: 0:00:39  lr: 0.000118  loss: 0.3010 (0.2637)  time: 0.6034  data: 0.0001  max mem: 15821
[17:08:24.860292] Epoch: [18]  [300/345]  eta: 0:00:27  lr: 0.000118  loss: 0.2654 (0.2633)  time: 0.6022  data: 0.0001  max mem: 15821
[17:08:36.900781] Epoch: [18]  [320/345]  eta: 0:00:15  lr: 0.000118  loss: 0.2371 (0.2619)  time: 0.6020  data: 0.0001  max mem: 15821
[17:08:48.944345] Epoch: [18]  [340/345]  eta: 0:00:03  lr: 0.000119  loss: 0.2551 (0.2611)  time: 0.6021  data: 0.0001  max mem: 15821
[17:08:51.352898] Epoch: [18]  [344/345]  eta: 0:00:00  lr: 0.000119  loss: 0.2575 (0.2609)  time: 0.6021  data: 0.0001  max mem: 15821
[17:08:51.425484] Epoch: [18] Total time: 0:03:28 (0.6040 s / it)
[17:08:51.425968] Averaged stats: lr: 0.000119  loss: 0.2575 (0.2609)
[17:08:51.979033] Test:  [  0/345]  eta: 0:03:08  loss: 0.2060 (0.2060)  time: 0.5475  data: 0.3839  max mem: 15821
[17:08:53.645241] Test:  [ 10/345]  eta: 0:01:07  loss: 0.2367 (0.2337)  time: 0.2011  data: 0.0350  max mem: 15821
[17:08:55.312734] Test:  [ 20/345]  eta: 0:01:00  loss: 0.2367 (0.2401)  time: 0.1666  data: 0.0001  max mem: 15821
[17:08:56.985982] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2216 (0.2362)  time: 0.1670  data: 0.0001  max mem: 15821
[17:08:58.659550] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2152 (0.2330)  time: 0.1673  data: 0.0001  max mem: 15821
[17:09:00.337698] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2128 (0.2304)  time: 0.1675  data: 0.0001  max mem: 15821
[17:09:02.018663] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2128 (0.2292)  time: 0.1679  data: 0.0001  max mem: 15821
[17:09:03.703279] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2271 (0.2302)  time: 0.1682  data: 0.0001  max mem: 15821
[17:09:05.390920] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2359 (0.2315)  time: 0.1686  data: 0.0001  max mem: 15821
[17:09:07.081593] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2412 (0.2332)  time: 0.1689  data: 0.0001  max mem: 15821
[17:09:08.775007] Test:  [100/345]  eta: 0:00:42  loss: 0.2336 (0.2326)  time: 0.1691  data: 0.0001  max mem: 15821
[17:09:10.473955] Test:  [110/345]  eta: 0:00:40  loss: 0.2189 (0.2316)  time: 0.1696  data: 0.0001  max mem: 15821
[17:09:12.175582] Test:  [120/345]  eta: 0:00:38  loss: 0.2189 (0.2300)  time: 0.1700  data: 0.0001  max mem: 15821
[17:09:13.880393] Test:  [130/345]  eta: 0:00:36  loss: 0.2252 (0.2300)  time: 0.1703  data: 0.0001  max mem: 15821
[17:09:15.588337] Test:  [140/345]  eta: 0:00:35  loss: 0.2379 (0.2299)  time: 0.1706  data: 0.0001  max mem: 15821
[17:09:17.300600] Test:  [150/345]  eta: 0:00:33  loss: 0.2385 (0.2309)  time: 0.1709  data: 0.0001  max mem: 15821
[17:09:19.017397] Test:  [160/345]  eta: 0:00:31  loss: 0.2386 (0.2319)  time: 0.1714  data: 0.0001  max mem: 15821
[17:09:20.735659] Test:  [170/345]  eta: 0:00:29  loss: 0.2326 (0.2329)  time: 0.1717  data: 0.0001  max mem: 15821
[17:09:22.456902] Test:  [180/345]  eta: 0:00:28  loss: 0.2485 (0.2337)  time: 0.1719  data: 0.0001  max mem: 15821
[17:09:24.182996] Test:  [190/345]  eta: 0:00:26  loss: 0.2163 (0.2328)  time: 0.1723  data: 0.0001  max mem: 15821
[17:09:25.912400] Test:  [200/345]  eta: 0:00:24  loss: 0.2103 (0.2322)  time: 0.1727  data: 0.0001  max mem: 15821
[17:09:27.644986] Test:  [210/345]  eta: 0:00:23  loss: 0.2280 (0.2331)  time: 0.1730  data: 0.0001  max mem: 15821
[17:09:29.380093] Test:  [220/345]  eta: 0:00:21  loss: 0.2396 (0.2334)  time: 0.1733  data: 0.0001  max mem: 15821
[17:09:31.118570] Test:  [230/345]  eta: 0:00:19  loss: 0.2210 (0.2329)  time: 0.1736  data: 0.0001  max mem: 15821
[17:09:32.860059] Test:  [240/345]  eta: 0:00:18  loss: 0.2386 (0.2340)  time: 0.1739  data: 0.0001  max mem: 15821
[17:09:34.604645] Test:  [250/345]  eta: 0:00:16  loss: 0.2750 (0.2358)  time: 0.1742  data: 0.0001  max mem: 15821
[17:09:36.354310] Test:  [260/345]  eta: 0:00:14  loss: 0.2531 (0.2364)  time: 0.1746  data: 0.0001  max mem: 15821
[17:09:38.106546] Test:  [270/345]  eta: 0:00:12  loss: 0.2199 (0.2350)  time: 0.1750  data: 0.0001  max mem: 15821
[17:09:39.862363] Test:  [280/345]  eta: 0:00:11  loss: 0.2066 (0.2340)  time: 0.1753  data: 0.0001  max mem: 15821
[17:09:41.621125] Test:  [290/345]  eta: 0:00:09  loss: 0.2207 (0.2345)  time: 0.1757  data: 0.0001  max mem: 15821
[17:09:43.383197] Test:  [300/345]  eta: 0:00:07  loss: 0.2589 (0.2356)  time: 0.1760  data: 0.0001  max mem: 15821
[17:09:45.149618] Test:  [310/345]  eta: 0:00:06  loss: 0.2366 (0.2347)  time: 0.1764  data: 0.0001  max mem: 15821
[17:09:46.920529] Test:  [320/345]  eta: 0:00:04  loss: 0.2188 (0.2346)  time: 0.1768  data: 0.0001  max mem: 15821
[17:09:48.694268] Test:  [330/345]  eta: 0:00:02  loss: 0.2243 (0.2348)  time: 0.1772  data: 0.0001  max mem: 15821
[17:09:50.472470] Test:  [340/345]  eta: 0:00:00  loss: 0.2243 (0.2351)  time: 0.1775  data: 0.0001  max mem: 15821
[17:09:51.183641] Test:  [344/345]  eta: 0:00:00  loss: 0.2236 (0.2354)  time: 0.1776  data: 0.0001  max mem: 15821
[17:09:51.243576] Test: Total time: 0:00:59 (0.1734 s / it)
[17:10:01.503793] Test:  [ 0/57]  eta: 0:00:28  loss: 0.3445 (0.3445)  time: 0.4942  data: 0.3317  max mem: 15821
[17:10:03.150073] Test:  [10/57]  eta: 0:00:09  loss: 0.3445 (0.3693)  time: 0.1945  data: 0.0302  max mem: 15821
[17:10:04.801527] Test:  [20/57]  eta: 0:00:06  loss: 0.3702 (0.3642)  time: 0.1648  data: 0.0001  max mem: 15821
[17:10:06.458362] Test:  [30/57]  eta: 0:00:04  loss: 0.2947 (0.3318)  time: 0.1653  data: 0.0001  max mem: 15821
[17:10:08.117655] Test:  [40/57]  eta: 0:00:02  loss: 0.2639 (0.3075)  time: 0.1657  data: 0.0001  max mem: 15821
[17:10:09.780598] Test:  [50/57]  eta: 0:00:01  loss: 0.2728 (0.3140)  time: 0.1661  data: 0.0001  max mem: 15821
[17:10:10.677906] Test:  [56/57]  eta: 0:00:00  loss: 0.3062 (0.3343)  time: 0.1612  data: 0.0001  max mem: 15821
[17:10:10.747176] Test: Total time: 0:00:09 (0.1708 s / it)
[17:10:12.514083] Dice score of the network on the train images: 0.778530, val images: 0.765472
[17:10:12.518723] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[17:10:13.440378] Epoch: [19]  [  0/345]  eta: 0:05:17  lr: 0.000119  loss: 0.2871 (0.2871)  time: 0.9205  data: 0.3189  max mem: 15821
[17:10:25.434258] Epoch: [19]  [ 20/345]  eta: 0:03:19  lr: 0.000119  loss: 0.2381 (0.2346)  time: 0.5996  data: 0.0001  max mem: 15821
[17:10:37.444000] Epoch: [19]  [ 40/345]  eta: 0:03:05  lr: 0.000119  loss: 0.2141 (0.2314)  time: 0.6004  data: 0.0001  max mem: 15821
[17:10:49.471395] Epoch: [19]  [ 60/345]  eta: 0:02:52  lr: 0.000120  loss: 0.2565 (0.2388)  time: 0.6013  data: 0.0001  max mem: 15821
[17:11:01.520639] Epoch: [19]  [ 80/345]  eta: 0:02:40  lr: 0.000120  loss: 0.2326 (0.2388)  time: 0.6024  data: 0.0001  max mem: 15821
[17:11:13.583229] Epoch: [19]  [100/345]  eta: 0:02:28  lr: 0.000121  loss: 0.2387 (0.2397)  time: 0.6031  data: 0.0001  max mem: 15821
[17:11:25.656557] Epoch: [19]  [120/345]  eta: 0:02:15  lr: 0.000121  loss: 0.2358 (0.2378)  time: 0.6036  data: 0.0001  max mem: 15821
[17:11:37.737874] Epoch: [19]  [140/345]  eta: 0:02:03  lr: 0.000121  loss: 0.2285 (0.2371)  time: 0.6040  data: 0.0001  max mem: 15821
[17:11:49.818741] Epoch: [19]  [160/345]  eta: 0:01:51  lr: 0.000122  loss: 0.2424 (0.2382)  time: 0.6040  data: 0.0001  max mem: 15821
[17:12:01.889142] Epoch: [19]  [180/345]  eta: 0:01:39  lr: 0.000122  loss: 0.2400 (0.2381)  time: 0.6035  data: 0.0001  max mem: 15821
[17:12:13.970288] Epoch: [19]  [200/345]  eta: 0:01:27  lr: 0.000122  loss: 0.2450 (0.2389)  time: 0.6040  data: 0.0001  max mem: 15821
[17:12:26.045127] Epoch: [19]  [220/345]  eta: 0:01:15  lr: 0.000123  loss: 0.2565 (0.2399)  time: 0.6037  data: 0.0001  max mem: 15821
[17:12:38.120306] Epoch: [19]  [240/345]  eta: 0:01:03  lr: 0.000123  loss: 0.2731 (0.2423)  time: 0.6037  data: 0.0001  max mem: 15821
[17:12:50.192147] Epoch: [19]  [260/345]  eta: 0:00:51  lr: 0.000123  loss: 0.2269 (0.2416)  time: 0.6035  data: 0.0001  max mem: 15821
[17:13:02.251239] Epoch: [19]  [280/345]  eta: 0:00:39  lr: 0.000124  loss: 0.2163 (0.2402)  time: 0.6029  data: 0.0001  max mem: 15821
[17:13:14.300456] Epoch: [19]  [300/345]  eta: 0:00:27  lr: 0.000124  loss: 0.2397 (0.2394)  time: 0.6024  data: 0.0001  max mem: 15821
[17:13:26.339835] Epoch: [19]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.2414 (0.2395)  time: 0.6019  data: 0.0001  max mem: 15821
[17:13:38.383965] Epoch: [19]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.2519 (0.2401)  time: 0.6022  data: 0.0001  max mem: 15821
[17:13:40.794815] Epoch: [19]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.2519 (0.2402)  time: 0.6023  data: 0.0001  max mem: 15821
[17:13:40.864581] Epoch: [19] Total time: 0:03:28 (0.6039 s / it)
[17:13:40.864846] Averaged stats: lr: 0.000125  loss: 0.2519 (0.2402)
[17:13:41.409446] Test:  [  0/345]  eta: 0:03:05  loss: 0.2519 (0.2519)  time: 0.5369  data: 0.3730  max mem: 15821
[17:13:43.073065] Test:  [ 10/345]  eta: 0:01:06  loss: 0.2650 (0.2605)  time: 0.2000  data: 0.0340  max mem: 15821
[17:13:44.740038] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2618 (0.2551)  time: 0.1664  data: 0.0001  max mem: 15821
[17:13:46.410050] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2294 (0.2482)  time: 0.1668  data: 0.0001  max mem: 15821
[17:13:48.083115] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2277 (0.2505)  time: 0.1671  data: 0.0001  max mem: 15821
[17:13:49.757910] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2317 (0.2473)  time: 0.1673  data: 0.0001  max mem: 15821
[17:13:51.438037] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2413 (0.2469)  time: 0.1677  data: 0.0001  max mem: 15821
[17:13:53.121360] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2478 (0.2480)  time: 0.1681  data: 0.0001  max mem: 15821
[17:13:54.808728] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2412 (0.2489)  time: 0.1685  data: 0.0001  max mem: 15821
[17:13:56.499361] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2412 (0.2494)  time: 0.1688  data: 0.0001  max mem: 15821
[17:13:58.193640] Test:  [100/345]  eta: 0:00:42  loss: 0.2580 (0.2497)  time: 0.1692  data: 0.0001  max mem: 15821
[17:13:59.891650] Test:  [110/345]  eta: 0:00:40  loss: 0.2344 (0.2475)  time: 0.1696  data: 0.0001  max mem: 15821
[17:14:01.591222] Test:  [120/345]  eta: 0:00:38  loss: 0.2335 (0.2471)  time: 0.1698  data: 0.0001  max mem: 15821
[17:14:03.296410] Test:  [130/345]  eta: 0:00:36  loss: 0.2326 (0.2466)  time: 0.1702  data: 0.0001  max mem: 15821
[17:14:05.003761] Test:  [140/345]  eta: 0:00:35  loss: 0.2253 (0.2459)  time: 0.1706  data: 0.0001  max mem: 15821
[17:14:06.714671] Test:  [150/345]  eta: 0:00:33  loss: 0.2445 (0.2472)  time: 0.1709  data: 0.0001  max mem: 15821
[17:14:08.429340] Test:  [160/345]  eta: 0:00:31  loss: 0.2502 (0.2475)  time: 0.1712  data: 0.0001  max mem: 15821
[17:14:10.148634] Test:  [170/345]  eta: 0:00:29  loss: 0.2495 (0.2464)  time: 0.1716  data: 0.0001  max mem: 15821
[17:14:11.871987] Test:  [180/345]  eta: 0:00:28  loss: 0.2223 (0.2455)  time: 0.1721  data: 0.0001  max mem: 15821
[17:14:13.597932] Test:  [190/345]  eta: 0:00:26  loss: 0.2375 (0.2462)  time: 0.1724  data: 0.0001  max mem: 15821
[17:14:15.326555] Test:  [200/345]  eta: 0:00:24  loss: 0.2522 (0.2467)  time: 0.1727  data: 0.0001  max mem: 15821
[17:14:17.058243] Test:  [210/345]  eta: 0:00:23  loss: 0.2592 (0.2472)  time: 0.1729  data: 0.0001  max mem: 15821
[17:14:18.792912] Test:  [220/345]  eta: 0:00:21  loss: 0.2532 (0.2474)  time: 0.1733  data: 0.0001  max mem: 15821
[17:14:20.532055] Test:  [230/345]  eta: 0:00:19  loss: 0.2532 (0.2473)  time: 0.1736  data: 0.0001  max mem: 15821
[17:14:22.274057] Test:  [240/345]  eta: 0:00:18  loss: 0.2583 (0.2474)  time: 0.1740  data: 0.0001  max mem: 15821
[17:14:24.019171] Test:  [250/345]  eta: 0:00:16  loss: 0.2563 (0.2475)  time: 0.1743  data: 0.0001  max mem: 15821
[17:14:25.768793] Test:  [260/345]  eta: 0:00:14  loss: 0.2456 (0.2477)  time: 0.1747  data: 0.0001  max mem: 15821
[17:14:27.520742] Test:  [270/345]  eta: 0:00:12  loss: 0.2485 (0.2481)  time: 0.1750  data: 0.0001  max mem: 15821
[17:14:29.276382] Test:  [280/345]  eta: 0:00:11  loss: 0.2485 (0.2481)  time: 0.1753  data: 0.0001  max mem: 15821
[17:14:31.035021] Test:  [290/345]  eta: 0:00:09  loss: 0.2417 (0.2478)  time: 0.1756  data: 0.0001  max mem: 15821
[17:14:32.797415] Test:  [300/345]  eta: 0:00:07  loss: 0.2380 (0.2474)  time: 0.1760  data: 0.0001  max mem: 15821
[17:14:34.564376] Test:  [310/345]  eta: 0:00:06  loss: 0.2380 (0.2471)  time: 0.1764  data: 0.0001  max mem: 15821
[17:14:36.335446] Test:  [320/345]  eta: 0:00:04  loss: 0.2420 (0.2471)  time: 0.1768  data: 0.0001  max mem: 15821
[17:14:38.109790] Test:  [330/345]  eta: 0:00:02  loss: 0.2259 (0.2467)  time: 0.1772  data: 0.0001  max mem: 15821
[17:14:39.886939] Test:  [340/345]  eta: 0:00:00  loss: 0.2263 (0.2467)  time: 0.1775  data: 0.0001  max mem: 15821
[17:14:40.599317] Test:  [344/345]  eta: 0:00:00  loss: 0.2339 (0.2471)  time: 0.1777  data: 0.0001  max mem: 15821
[17:14:40.665809] Test: Total time: 0:00:59 (0.1733 s / it)
[17:14:51.051371] Test:  [ 0/57]  eta: 0:00:27  loss: 0.4299 (0.4299)  time: 0.4857  data: 0.3233  max mem: 15821
[17:14:52.698001] Test:  [10/57]  eta: 0:00:09  loss: 0.4299 (0.4220)  time: 0.1937  data: 0.0295  max mem: 15821
[17:14:54.350407] Test:  [20/57]  eta: 0:00:06  loss: 0.4402 (0.4224)  time: 0.1649  data: 0.0001  max mem: 15821
[17:14:56.006449] Test:  [30/57]  eta: 0:00:04  loss: 0.3465 (0.3852)  time: 0.1654  data: 0.0001  max mem: 15821
[17:14:57.665472] Test:  [40/57]  eta: 0:00:02  loss: 0.3053 (0.3674)  time: 0.1657  data: 0.0001  max mem: 15821
[17:14:59.330497] Test:  [50/57]  eta: 0:00:01  loss: 0.3308 (0.3772)  time: 0.1661  data: 0.0001  max mem: 15821
[17:15:00.228307] Test:  [56/57]  eta: 0:00:00  loss: 0.3810 (0.4004)  time: 0.1613  data: 0.0000  max mem: 15821
[17:15:00.279576] Test: Total time: 0:00:09 (0.1704 s / it)
[17:15:01.998390] Dice score of the network on the train images: 0.796316, val images: 0.732214
[17:15:01.998622] saving best_prec_model_0 @ epoch 19
[17:15:03.140790] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[17:15:04.066127] Epoch: [20]  [  0/345]  eta: 0:05:18  lr: 0.000125  loss: 0.2360 (0.2360)  time: 0.9242  data: 0.3233  max mem: 15821
[17:15:16.044125] Epoch: [20]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.2612 (0.2598)  time: 0.5988  data: 0.0001  max mem: 15821
[17:15:28.052630] Epoch: [20]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.2233 (0.2437)  time: 0.6004  data: 0.0001  max mem: 15821
[17:15:40.064439] Epoch: [20]  [ 60/345]  eta: 0:02:52  lr: 0.000125  loss: 0.2214 (0.2375)  time: 0.6005  data: 0.0001  max mem: 15821
[17:15:52.113768] Epoch: [20]  [ 80/345]  eta: 0:02:40  lr: 0.000125  loss: 0.2321 (0.2372)  time: 0.6024  data: 0.0001  max mem: 15821
[17:16:04.177753] Epoch: [20]  [100/345]  eta: 0:02:28  lr: 0.000125  loss: 0.2305 (0.2374)  time: 0.6032  data: 0.0001  max mem: 15821
[17:16:16.255018] Epoch: [20]  [120/345]  eta: 0:02:15  lr: 0.000125  loss: 0.1978 (0.2330)  time: 0.6038  data: 0.0001  max mem: 15821
[17:16:28.341595] Epoch: [20]  [140/345]  eta: 0:02:03  lr: 0.000125  loss: 0.2276 (0.2332)  time: 0.6043  data: 0.0001  max mem: 15821
[17:16:40.433500] Epoch: [20]  [160/345]  eta: 0:01:51  lr: 0.000125  loss: 0.2330 (0.2339)  time: 0.6045  data: 0.0001  max mem: 15821
[17:16:52.531462] Epoch: [20]  [180/345]  eta: 0:01:39  lr: 0.000125  loss: 0.2716 (0.2380)  time: 0.6049  data: 0.0001  max mem: 15821
[17:17:04.622676] Epoch: [20]  [200/345]  eta: 0:01:27  lr: 0.000125  loss: 0.2826 (0.2421)  time: 0.6045  data: 0.0001  max mem: 15821
[17:17:16.710292] Epoch: [20]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.2749 (0.2458)  time: 0.6043  data: 0.0001  max mem: 15821
[17:17:28.776347] Epoch: [20]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.2986 (0.2505)  time: 0.6033  data: 0.0001  max mem: 15821
[17:17:40.840751] Epoch: [20]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.2623 (0.2521)  time: 0.6032  data: 0.0001  max mem: 15821
[17:17:52.913012] Epoch: [20]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.2440 (0.2524)  time: 0.6036  data: 0.0001  max mem: 15821
[17:18:04.973451] Epoch: [20]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.2521 (0.2527)  time: 0.6030  data: 0.0001  max mem: 15821
[17:18:17.044018] Epoch: [20]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.2322 (0.2516)  time: 0.6035  data: 0.0001  max mem: 15821
[17:18:29.089104] Epoch: [20]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.2324 (0.2506)  time: 0.6022  data: 0.0001  max mem: 15821
[17:18:31.495258] Epoch: [20]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.2361 (0.2505)  time: 0.6020  data: 0.0001  max mem: 15821
[17:18:31.564118] Epoch: [20] Total time: 0:03:28 (0.6041 s / it)
[17:18:31.564744] Averaged stats: lr: 0.000125  loss: 0.2361 (0.2505)
[17:18:32.120506] Test:  [  0/345]  eta: 0:03:09  loss: 0.2308 (0.2308)  time: 0.5490  data: 0.3851  max mem: 15821
[17:18:33.786032] Test:  [ 10/345]  eta: 0:01:07  loss: 0.2312 (0.2398)  time: 0.2013  data: 0.0351  max mem: 15821
[17:18:35.455043] Test:  [ 20/345]  eta: 0:01:00  loss: 0.2304 (0.2346)  time: 0.1666  data: 0.0001  max mem: 15821
[17:18:37.125701] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2160 (0.2279)  time: 0.1669  data: 0.0001  max mem: 15821
[17:18:38.799207] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1990 (0.2227)  time: 0.1671  data: 0.0001  max mem: 15821
[17:18:40.475955] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2139 (0.2258)  time: 0.1674  data: 0.0001  max mem: 15821
[17:18:42.156678] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2225 (0.2226)  time: 0.1678  data: 0.0001  max mem: 15821
[17:18:43.840548] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2225 (0.2254)  time: 0.1682  data: 0.0001  max mem: 15821
[17:18:45.529261] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2214 (0.2235)  time: 0.1686  data: 0.0001  max mem: 15821
[17:18:47.219281] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2169 (0.2250)  time: 0.1689  data: 0.0001  max mem: 15821
[17:18:48.914290] Test:  [100/345]  eta: 0:00:42  loss: 0.2226 (0.2255)  time: 0.1692  data: 0.0001  max mem: 15821
[17:18:50.611446] Test:  [110/345]  eta: 0:00:40  loss: 0.2226 (0.2269)  time: 0.1695  data: 0.0001  max mem: 15821
[17:18:52.312714] Test:  [120/345]  eta: 0:00:38  loss: 0.2106 (0.2259)  time: 0.1699  data: 0.0001  max mem: 15821
[17:18:54.016240] Test:  [130/345]  eta: 0:00:36  loss: 0.2205 (0.2266)  time: 0.1702  data: 0.0001  max mem: 15821
[17:18:55.725370] Test:  [140/345]  eta: 0:00:35  loss: 0.2107 (0.2243)  time: 0.1706  data: 0.0001  max mem: 15821
[17:18:57.435941] Test:  [150/345]  eta: 0:00:33  loss: 0.1993 (0.2241)  time: 0.1709  data: 0.0001  max mem: 15821
[17:18:59.150550] Test:  [160/345]  eta: 0:00:31  loss: 0.2319 (0.2243)  time: 0.1712  data: 0.0001  max mem: 15821
[17:19:00.869688] Test:  [170/345]  eta: 0:00:29  loss: 0.2332 (0.2247)  time: 0.1716  data: 0.0001  max mem: 15821
[17:19:02.591831] Test:  [180/345]  eta: 0:00:28  loss: 0.2320 (0.2252)  time: 0.1720  data: 0.0001  max mem: 15821
[17:19:04.317523] Test:  [190/345]  eta: 0:00:26  loss: 0.2217 (0.2250)  time: 0.1723  data: 0.0001  max mem: 15821
[17:19:06.047235] Test:  [200/345]  eta: 0:00:24  loss: 0.2286 (0.2247)  time: 0.1727  data: 0.0001  max mem: 15821
[17:19:07.779531] Test:  [210/345]  eta: 0:00:23  loss: 0.2267 (0.2252)  time: 0.1730  data: 0.0001  max mem: 15821
[17:19:09.515282] Test:  [220/345]  eta: 0:00:21  loss: 0.2190 (0.2252)  time: 0.1733  data: 0.0001  max mem: 15821
[17:19:11.254583] Test:  [230/345]  eta: 0:00:19  loss: 0.1996 (0.2245)  time: 0.1737  data: 0.0001  max mem: 15821
[17:19:12.998146] Test:  [240/345]  eta: 0:00:18  loss: 0.2052 (0.2247)  time: 0.1741  data: 0.0001  max mem: 15821
[17:19:14.744243] Test:  [250/345]  eta: 0:00:16  loss: 0.2170 (0.2251)  time: 0.1744  data: 0.0001  max mem: 15821
[17:19:16.493147] Test:  [260/345]  eta: 0:00:14  loss: 0.2170 (0.2249)  time: 0.1747  data: 0.0001  max mem: 15821
[17:19:18.246248] Test:  [270/345]  eta: 0:00:12  loss: 0.2149 (0.2242)  time: 0.1750  data: 0.0001  max mem: 15821
[17:19:20.001930] Test:  [280/345]  eta: 0:00:11  loss: 0.2222 (0.2244)  time: 0.1754  data: 0.0001  max mem: 15821
[17:19:21.761101] Test:  [290/345]  eta: 0:00:09  loss: 0.2217 (0.2237)  time: 0.1757  data: 0.0001  max mem: 15821
[17:19:23.525173] Test:  [300/345]  eta: 0:00:07  loss: 0.1835 (0.2232)  time: 0.1761  data: 0.0001  max mem: 15821
[17:19:25.292161] Test:  [310/345]  eta: 0:00:06  loss: 0.2177 (0.2234)  time: 0.1765  data: 0.0001  max mem: 15821
[17:19:27.063822] Test:  [320/345]  eta: 0:00:04  loss: 0.2177 (0.2234)  time: 0.1769  data: 0.0001  max mem: 15821
[17:19:28.837979] Test:  [330/345]  eta: 0:00:02  loss: 0.2167 (0.2234)  time: 0.1772  data: 0.0001  max mem: 15821
[17:19:30.615197] Test:  [340/345]  eta: 0:00:00  loss: 0.2181 (0.2234)  time: 0.1775  data: 0.0001  max mem: 15821
[17:19:31.327916] Test:  [344/345]  eta: 0:00:00  loss: 0.2167 (0.2232)  time: 0.1777  data: 0.0001  max mem: 15821
[17:19:31.396188] Test: Total time: 0:00:59 (0.1734 s / it)
[17:19:41.791320] Test:  [ 0/57]  eta: 0:00:30  loss: 0.3449 (0.3449)  time: 0.5341  data: 0.3720  max mem: 15821
[17:19:43.438046] Test:  [10/57]  eta: 0:00:09  loss: 0.3706 (0.3885)  time: 0.1982  data: 0.0339  max mem: 15821
[17:19:45.090041] Test:  [20/57]  eta: 0:00:06  loss: 0.3706 (0.3802)  time: 0.1649  data: 0.0001  max mem: 15821
[17:19:46.745886] Test:  [30/57]  eta: 0:00:04  loss: 0.3123 (0.3454)  time: 0.1653  data: 0.0001  max mem: 15821
[17:19:48.405836] Test:  [40/57]  eta: 0:00:02  loss: 0.2587 (0.3229)  time: 0.1657  data: 0.0001  max mem: 15821
[17:19:50.070249] Test:  [50/57]  eta: 0:00:01  loss: 0.2721 (0.3286)  time: 0.1662  data: 0.0001  max mem: 15821
[17:19:50.967059] Test:  [56/57]  eta: 0:00:00  loss: 0.3231 (0.3511)  time: 0.1612  data: 0.0001  max mem: 15821
[17:19:51.028508] Test: Total time: 0:00:09 (0.1714 s / it)
[17:19:52.750086] Dice score of the network on the train images: 0.778074, val images: 0.758184
[17:19:52.754216] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[17:19:53.699460] Epoch: [21]  [  0/345]  eta: 0:05:25  lr: 0.000125  loss: 0.2244 (0.2244)  time: 0.9443  data: 0.3428  max mem: 15821
[17:20:05.675751] Epoch: [21]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.2318 (0.2373)  time: 0.5987  data: 0.0001  max mem: 15821
[17:20:17.685520] Epoch: [21]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.2365 (0.2372)  time: 0.6004  data: 0.0001  max mem: 15821
[17:20:29.834961] Epoch: [21]  [ 60/345]  eta: 0:02:53  lr: 0.000125  loss: 0.2327 (0.2329)  time: 0.6074  data: 0.0001  max mem: 15821
[17:20:41.850972] Epoch: [21]  [ 80/345]  eta: 0:02:40  lr: 0.000124  loss: 0.2270 (0.2323)  time: 0.6008  data: 0.0001  max mem: 15821
[17:20:53.893136] Epoch: [21]  [100/345]  eta: 0:02:28  lr: 0.000124  loss: 0.2307 (0.2306)  time: 0.6021  data: 0.0001  max mem: 15821
[17:21:05.955198] Epoch: [21]  [120/345]  eta: 0:02:16  lr: 0.000124  loss: 0.2124 (0.2299)  time: 0.6031  data: 0.0001  max mem: 15821
[17:21:18.031646] Epoch: [21]  [140/345]  eta: 0:02:03  lr: 0.000124  loss: 0.2313 (0.2300)  time: 0.6038  data: 0.0001  max mem: 15821
[17:21:30.102653] Epoch: [21]  [160/345]  eta: 0:01:51  lr: 0.000124  loss: 0.2107 (0.2281)  time: 0.6035  data: 0.0001  max mem: 15821
[17:21:42.181135] Epoch: [21]  [180/345]  eta: 0:01:39  lr: 0.000124  loss: 0.2215 (0.2271)  time: 0.6039  data: 0.0001  max mem: 15821
[17:21:54.260857] Epoch: [21]  [200/345]  eta: 0:01:27  lr: 0.000124  loss: 0.2214 (0.2268)  time: 0.6039  data: 0.0001  max mem: 15821
[17:22:06.336497] Epoch: [21]  [220/345]  eta: 0:01:15  lr: 0.000124  loss: 0.2201 (0.2267)  time: 0.6037  data: 0.0001  max mem: 15821
[17:22:18.415532] Epoch: [21]  [240/345]  eta: 0:01:03  lr: 0.000124  loss: 0.2192 (0.2259)  time: 0.6039  data: 0.0001  max mem: 15821
[17:22:30.491429] Epoch: [21]  [260/345]  eta: 0:00:51  lr: 0.000124  loss: 0.2482 (0.2282)  time: 0.6037  data: 0.0001  max mem: 15821
[17:22:42.566325] Epoch: [21]  [280/345]  eta: 0:00:39  lr: 0.000124  loss: 0.2245 (0.2282)  time: 0.6037  data: 0.0001  max mem: 15821
[17:22:54.635890] Epoch: [21]  [300/345]  eta: 0:00:27  lr: 0.000124  loss: 0.2292 (0.2285)  time: 0.6034  data: 0.0001  max mem: 15821
[17:23:06.794665] Epoch: [21]  [320/345]  eta: 0:00:15  lr: 0.000124  loss: 0.2396 (0.2287)  time: 0.6079  data: 0.0001  max mem: 15821
[17:23:18.860129] Epoch: [21]  [340/345]  eta: 0:00:03  lr: 0.000124  loss: 0.2429 (0.2297)  time: 0.6032  data: 0.0001  max mem: 15821
[17:23:21.273602] Epoch: [21]  [344/345]  eta: 0:00:00  lr: 0.000124  loss: 0.2350 (0.2297)  time: 0.6032  data: 0.0001  max mem: 15821
[17:23:21.348634] Epoch: [21] Total time: 0:03:28 (0.6046 s / it)
[17:23:21.348834] Averaged stats: lr: 0.000124  loss: 0.2350 (0.2297)
[17:23:21.855786] Test:  [  0/345]  eta: 0:02:52  loss: 0.2286 (0.2286)  time: 0.5011  data: 0.3372  max mem: 15821
[17:23:23.521427] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2069 (0.2239)  time: 0.1968  data: 0.0307  max mem: 15821
[17:23:25.189919] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2069 (0.2192)  time: 0.1666  data: 0.0001  max mem: 15821
[17:23:26.862313] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2099 (0.2180)  time: 0.1670  data: 0.0001  max mem: 15821
[17:23:28.535417] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2057 (0.2188)  time: 0.1672  data: 0.0001  max mem: 15821
[17:23:30.212867] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2188 (0.2235)  time: 0.1675  data: 0.0001  max mem: 15821
[17:23:31.893479] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2277 (0.2264)  time: 0.1678  data: 0.0001  max mem: 15821
[17:23:33.576992] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2277 (0.2284)  time: 0.1681  data: 0.0001  max mem: 15821
[17:23:35.263815] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2204 (0.2276)  time: 0.1685  data: 0.0001  max mem: 15821
[17:23:36.956505] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2177 (0.2259)  time: 0.1689  data: 0.0001  max mem: 15821
[17:23:38.651330] Test:  [100/345]  eta: 0:00:41  loss: 0.2177 (0.2261)  time: 0.1693  data: 0.0001  max mem: 15821
[17:23:40.351402] Test:  [110/345]  eta: 0:00:40  loss: 0.2227 (0.2262)  time: 0.1697  data: 0.0001  max mem: 15821
[17:23:42.053020] Test:  [120/345]  eta: 0:00:38  loss: 0.2333 (0.2285)  time: 0.1700  data: 0.0001  max mem: 15821
[17:23:43.758085] Test:  [130/345]  eta: 0:00:36  loss: 0.2352 (0.2277)  time: 0.1703  data: 0.0001  max mem: 15821
[17:23:45.465848] Test:  [140/345]  eta: 0:00:35  loss: 0.2048 (0.2266)  time: 0.1706  data: 0.0001  max mem: 15821
[17:23:47.177078] Test:  [150/345]  eta: 0:00:33  loss: 0.2057 (0.2263)  time: 0.1709  data: 0.0001  max mem: 15821
[17:23:48.893458] Test:  [160/345]  eta: 0:00:31  loss: 0.2354 (0.2286)  time: 0.1713  data: 0.0001  max mem: 15821
[17:23:50.615004] Test:  [170/345]  eta: 0:00:29  loss: 0.2408 (0.2286)  time: 0.1718  data: 0.0001  max mem: 15821
[17:23:52.337808] Test:  [180/345]  eta: 0:00:28  loss: 0.2270 (0.2283)  time: 0.1722  data: 0.0001  max mem: 15821
[17:23:54.064630] Test:  [190/345]  eta: 0:00:26  loss: 0.2140 (0.2276)  time: 0.1724  data: 0.0001  max mem: 15821
[17:23:55.793289] Test:  [200/345]  eta: 0:00:24  loss: 0.2076 (0.2265)  time: 0.1727  data: 0.0001  max mem: 15821
[17:23:57.525756] Test:  [210/345]  eta: 0:00:23  loss: 0.1977 (0.2255)  time: 0.1730  data: 0.0001  max mem: 15821
[17:23:59.261989] Test:  [220/345]  eta: 0:00:21  loss: 0.2109 (0.2256)  time: 0.1734  data: 0.0001  max mem: 15821
[17:24:01.000421] Test:  [230/345]  eta: 0:00:19  loss: 0.2360 (0.2266)  time: 0.1737  data: 0.0001  max mem: 15821
[17:24:02.743213] Test:  [240/345]  eta: 0:00:18  loss: 0.2254 (0.2265)  time: 0.1740  data: 0.0001  max mem: 15821
[17:24:04.490191] Test:  [250/345]  eta: 0:00:16  loss: 0.2192 (0.2266)  time: 0.1744  data: 0.0001  max mem: 15821
[17:24:06.238615] Test:  [260/345]  eta: 0:00:14  loss: 0.2290 (0.2267)  time: 0.1747  data: 0.0001  max mem: 15821
[17:24:07.992626] Test:  [270/345]  eta: 0:00:12  loss: 0.2202 (0.2260)  time: 0.1751  data: 0.0001  max mem: 15821
[17:24:09.749220] Test:  [280/345]  eta: 0:00:11  loss: 0.2140 (0.2264)  time: 0.1755  data: 0.0001  max mem: 15821
[17:24:11.509506] Test:  [290/345]  eta: 0:00:09  loss: 0.2352 (0.2264)  time: 0.1758  data: 0.0001  max mem: 15821
[17:24:13.274235] Test:  [300/345]  eta: 0:00:07  loss: 0.2170 (0.2259)  time: 0.1762  data: 0.0001  max mem: 15821
[17:24:15.041992] Test:  [310/345]  eta: 0:00:06  loss: 0.2248 (0.2264)  time: 0.1766  data: 0.0001  max mem: 15821
[17:24:16.813697] Test:  [320/345]  eta: 0:00:04  loss: 0.2081 (0.2256)  time: 0.1769  data: 0.0001  max mem: 15821
[17:24:18.588360] Test:  [330/345]  eta: 0:00:02  loss: 0.1973 (0.2252)  time: 0.1773  data: 0.0001  max mem: 15821
[17:24:20.366609] Test:  [340/345]  eta: 0:00:00  loss: 0.2074 (0.2255)  time: 0.1776  data: 0.0001  max mem: 15821
[17:24:21.078340] Test:  [344/345]  eta: 0:00:00  loss: 0.2061 (0.2253)  time: 0.1777  data: 0.0001  max mem: 15821
[17:24:21.131435] Test: Total time: 0:00:59 (0.1733 s / it)
[17:24:31.519018] Test:  [ 0/57]  eta: 0:00:28  loss: 0.3923 (0.3923)  time: 0.4943  data: 0.3319  max mem: 15821
[17:24:33.165955] Test:  [10/57]  eta: 0:00:09  loss: 0.3916 (0.3802)  time: 0.1946  data: 0.0303  max mem: 15821
[17:24:34.819059] Test:  [20/57]  eta: 0:00:06  loss: 0.3402 (0.3674)  time: 0.1649  data: 0.0001  max mem: 15821
[17:24:36.475767] Test:  [30/57]  eta: 0:00:04  loss: 0.2881 (0.3330)  time: 0.1654  data: 0.0001  max mem: 15821
[17:24:38.136079] Test:  [40/57]  eta: 0:00:02  loss: 0.2523 (0.3091)  time: 0.1658  data: 0.0001  max mem: 15821
[17:24:39.798964] Test:  [50/57]  eta: 0:00:01  loss: 0.2569 (0.3128)  time: 0.1661  data: 0.0001  max mem: 15821
[17:24:40.696239] Test:  [56/57]  eta: 0:00:00  loss: 0.3124 (0.3365)  time: 0.1612  data: 0.0000  max mem: 15821
[17:24:40.759037] Test: Total time: 0:00:09 (0.1708 s / it)
[17:24:42.454668] Dice score of the network on the train images: 0.756195, val images: 0.767213
[17:24:42.458952] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[17:24:43.408240] Epoch: [22]  [  0/345]  eta: 0:05:27  lr: 0.000124  loss: 0.2442 (0.2442)  time: 0.9482  data: 0.3444  max mem: 15821
[17:24:55.391959] Epoch: [22]  [ 20/345]  eta: 0:03:20  lr: 0.000124  loss: 0.2168 (0.2304)  time: 0.5991  data: 0.0001  max mem: 15821
[17:25:07.412628] Epoch: [22]  [ 40/345]  eta: 0:03:05  lr: 0.000123  loss: 0.2112 (0.2274)  time: 0.6010  data: 0.0001  max mem: 15821
[17:25:19.441097] Epoch: [22]  [ 60/345]  eta: 0:02:52  lr: 0.000123  loss: 0.2278 (0.2278)  time: 0.6014  data: 0.0001  max mem: 15821
[17:25:31.490648] Epoch: [22]  [ 80/345]  eta: 0:02:40  lr: 0.000123  loss: 0.2233 (0.2233)  time: 0.6024  data: 0.0001  max mem: 15821
[17:25:43.557406] Epoch: [22]  [100/345]  eta: 0:02:28  lr: 0.000123  loss: 0.2154 (0.2234)  time: 0.6033  data: 0.0001  max mem: 15821
[17:25:55.641773] Epoch: [22]  [120/345]  eta: 0:02:16  lr: 0.000123  loss: 0.2249 (0.2249)  time: 0.6042  data: 0.0001  max mem: 15821
[17:26:07.733654] Epoch: [22]  [140/345]  eta: 0:02:03  lr: 0.000123  loss: 0.2338 (0.2255)  time: 0.6046  data: 0.0001  max mem: 15821
[17:26:19.827338] Epoch: [22]  [160/345]  eta: 0:01:51  lr: 0.000123  loss: 0.2164 (0.2244)  time: 0.6046  data: 0.0001  max mem: 15821
[17:26:31.917469] Epoch: [22]  [180/345]  eta: 0:01:39  lr: 0.000123  loss: 0.2178 (0.2233)  time: 0.6045  data: 0.0001  max mem: 15821
[17:26:44.005244] Epoch: [22]  [200/345]  eta: 0:01:27  lr: 0.000123  loss: 0.1865 (0.2216)  time: 0.6043  data: 0.0001  max mem: 15821
[17:26:56.094942] Epoch: [22]  [220/345]  eta: 0:01:15  lr: 0.000123  loss: 0.2702 (0.2271)  time: 0.6044  data: 0.0001  max mem: 15821
[17:27:08.185311] Epoch: [22]  [240/345]  eta: 0:01:03  lr: 0.000123  loss: 0.2407 (0.2280)  time: 0.6045  data: 0.0001  max mem: 15821
[17:27:20.270261] Epoch: [22]  [260/345]  eta: 0:00:51  lr: 0.000122  loss: 0.2447 (0.2297)  time: 0.6042  data: 0.0001  max mem: 15821
[17:27:32.350673] Epoch: [22]  [280/345]  eta: 0:00:39  lr: 0.000122  loss: 0.2399 (0.2316)  time: 0.6040  data: 0.0001  max mem: 15821
[17:27:44.430556] Epoch: [22]  [300/345]  eta: 0:00:27  lr: 0.000122  loss: 0.2483 (0.2332)  time: 0.6040  data: 0.0001  max mem: 15821
[17:27:56.502383] Epoch: [22]  [320/345]  eta: 0:00:15  lr: 0.000122  loss: 0.2299 (0.2327)  time: 0.6035  data: 0.0001  max mem: 15821
[17:28:08.570045] Epoch: [22]  [340/345]  eta: 0:00:03  lr: 0.000122  loss: 0.2112 (0.2320)  time: 0.6033  data: 0.0001  max mem: 15821
[17:28:10.982660] Epoch: [22]  [344/345]  eta: 0:00:00  lr: 0.000122  loss: 0.2115 (0.2319)  time: 0.6032  data: 0.0001  max mem: 15821
[17:28:11.045146] Epoch: [22] Total time: 0:03:28 (0.6046 s / it)
[17:28:11.045481] Averaged stats: lr: 0.000122  loss: 0.2115 (0.2319)
[17:28:11.569653] Test:  [  0/345]  eta: 0:02:58  loss: 0.2251 (0.2251)  time: 0.5183  data: 0.3546  max mem: 15821
[17:28:13.234588] Test:  [ 10/345]  eta: 0:01:06  loss: 0.2297 (0.2319)  time: 0.1984  data: 0.0323  max mem: 15821
[17:28:14.903301] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2290 (0.2294)  time: 0.1666  data: 0.0001  max mem: 15821
[17:28:16.575618] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2290 (0.2299)  time: 0.1670  data: 0.0001  max mem: 15821
[17:28:18.252182] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2362 (0.2298)  time: 0.1674  data: 0.0001  max mem: 15821
[17:28:19.931691] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2157 (0.2287)  time: 0.1677  data: 0.0001  max mem: 15821
[17:28:21.612955] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2011 (0.2243)  time: 0.1680  data: 0.0001  max mem: 15821
[17:28:23.298369] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1820 (0.2207)  time: 0.1682  data: 0.0001  max mem: 15821
[17:28:24.987028] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2111 (0.2217)  time: 0.1686  data: 0.0001  max mem: 15821
[17:28:26.678765] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2312 (0.2249)  time: 0.1690  data: 0.0001  max mem: 15821
[17:28:28.372975] Test:  [100/345]  eta: 0:00:41  loss: 0.2184 (0.2239)  time: 0.1692  data: 0.0001  max mem: 15821
[17:28:30.072037] Test:  [110/345]  eta: 0:00:40  loss: 0.2184 (0.2250)  time: 0.1696  data: 0.0001  max mem: 15821
[17:28:31.773931] Test:  [120/345]  eta: 0:00:38  loss: 0.2300 (0.2255)  time: 0.1700  data: 0.0001  max mem: 15821
[17:28:33.479251] Test:  [130/345]  eta: 0:00:36  loss: 0.2145 (0.2254)  time: 0.1703  data: 0.0001  max mem: 15821
[17:28:35.188225] Test:  [140/345]  eta: 0:00:35  loss: 0.2155 (0.2251)  time: 0.1706  data: 0.0001  max mem: 15821
[17:28:36.900443] Test:  [150/345]  eta: 0:00:33  loss: 0.2315 (0.2252)  time: 0.1710  data: 0.0001  max mem: 15821
[17:28:38.615667] Test:  [160/345]  eta: 0:00:31  loss: 0.2266 (0.2243)  time: 0.1713  data: 0.0001  max mem: 15821
[17:28:40.336687] Test:  [170/345]  eta: 0:00:29  loss: 0.2318 (0.2254)  time: 0.1717  data: 0.0001  max mem: 15821
[17:28:42.058659] Test:  [180/345]  eta: 0:00:28  loss: 0.2286 (0.2245)  time: 0.1721  data: 0.0001  max mem: 15821
[17:28:43.783735] Test:  [190/345]  eta: 0:00:26  loss: 0.2176 (0.2260)  time: 0.1723  data: 0.0001  max mem: 15821
[17:28:45.512159] Test:  [200/345]  eta: 0:00:24  loss: 0.2556 (0.2277)  time: 0.1726  data: 0.0001  max mem: 15821
[17:28:47.245140] Test:  [210/345]  eta: 0:00:23  loss: 0.2384 (0.2278)  time: 0.1730  data: 0.0001  max mem: 15821
[17:28:48.980271] Test:  [220/345]  eta: 0:00:21  loss: 0.2237 (0.2281)  time: 0.1733  data: 0.0001  max mem: 15821
[17:28:50.719150] Test:  [230/345]  eta: 0:00:19  loss: 0.2167 (0.2279)  time: 0.1736  data: 0.0001  max mem: 15821
[17:28:52.462292] Test:  [240/345]  eta: 0:00:18  loss: 0.2046 (0.2265)  time: 0.1740  data: 0.0001  max mem: 15821
[17:28:54.208337] Test:  [250/345]  eta: 0:00:16  loss: 0.2046 (0.2267)  time: 0.1744  data: 0.0001  max mem: 15821
[17:28:55.959203] Test:  [260/345]  eta: 0:00:14  loss: 0.2351 (0.2278)  time: 0.1748  data: 0.0001  max mem: 15821
[17:28:57.713199] Test:  [270/345]  eta: 0:00:12  loss: 0.2433 (0.2283)  time: 0.1752  data: 0.0001  max mem: 15821
[17:28:59.469801] Test:  [280/345]  eta: 0:00:11  loss: 0.2227 (0.2279)  time: 0.1755  data: 0.0001  max mem: 15821
[17:29:01.229985] Test:  [290/345]  eta: 0:00:09  loss: 0.2167 (0.2273)  time: 0.1758  data: 0.0001  max mem: 15821
[17:29:02.992446] Test:  [300/345]  eta: 0:00:07  loss: 0.2196 (0.2275)  time: 0.1761  data: 0.0001  max mem: 15821
[17:29:04.759734] Test:  [310/345]  eta: 0:00:06  loss: 0.2153 (0.2273)  time: 0.1764  data: 0.0001  max mem: 15821
[17:29:06.529312] Test:  [320/345]  eta: 0:00:04  loss: 0.2244 (0.2280)  time: 0.1768  data: 0.0001  max mem: 15821
[17:29:08.303933] Test:  [330/345]  eta: 0:00:02  loss: 0.2424 (0.2282)  time: 0.1771  data: 0.0002  max mem: 15821
[17:29:10.082813] Test:  [340/345]  eta: 0:00:00  loss: 0.2384 (0.2283)  time: 0.1776  data: 0.0001  max mem: 15821
[17:29:10.795864] Test:  [344/345]  eta: 0:00:00  loss: 0.2348 (0.2284)  time: 0.1778  data: 0.0001  max mem: 15821
[17:29:10.846901] Test: Total time: 0:00:59 (0.1733 s / it)
[17:29:21.251221] Test:  [ 0/57]  eta: 0:00:27  loss: 0.4584 (0.4584)  time: 0.4810  data: 0.3186  max mem: 15821
[17:29:22.898606] Test:  [10/57]  eta: 0:00:09  loss: 0.4396 (0.4029)  time: 0.1934  data: 0.0290  max mem: 15821
[17:29:24.551148] Test:  [20/57]  eta: 0:00:06  loss: 0.3931 (0.3950)  time: 0.1649  data: 0.0001  max mem: 15821
[17:29:26.206717] Test:  [30/57]  eta: 0:00:04  loss: 0.3153 (0.3534)  time: 0.1653  data: 0.0001  max mem: 15821
[17:29:27.866935] Test:  [40/57]  eta: 0:00:02  loss: 0.2643 (0.3302)  time: 0.1657  data: 0.0001  max mem: 15821
[17:29:29.531121] Test:  [50/57]  eta: 0:00:01  loss: 0.2843 (0.3426)  time: 0.1662  data: 0.0001  max mem: 15821
[17:29:30.428585] Test:  [56/57]  eta: 0:00:00  loss: 0.3192 (0.3673)  time: 0.1613  data: 0.0001  max mem: 15821
[17:29:30.496380] Test: Total time: 0:00:09 (0.1706 s / it)
[17:29:32.228207] Dice score of the network on the train images: 0.780495, val images: 0.745432
[17:29:32.233290] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[17:29:33.157727] Epoch: [23]  [  0/345]  eta: 0:05:18  lr: 0.000122  loss: 0.1708 (0.1708)  time: 0.9232  data: 0.3220  max mem: 15821
[17:29:45.153966] Epoch: [23]  [ 20/345]  eta: 0:03:19  lr: 0.000122  loss: 0.2328 (0.2351)  time: 0.5998  data: 0.0001  max mem: 15821
[17:29:57.281038] Epoch: [23]  [ 40/345]  eta: 0:03:06  lr: 0.000122  loss: 0.2078 (0.2296)  time: 0.6063  data: 0.0001  max mem: 15821
[17:30:09.317315] Epoch: [23]  [ 60/345]  eta: 0:02:53  lr: 0.000122  loss: 0.2311 (0.2315)  time: 0.6018  data: 0.0001  max mem: 15821
[17:30:21.355869] Epoch: [23]  [ 80/345]  eta: 0:02:40  lr: 0.000121  loss: 0.2222 (0.2294)  time: 0.6019  data: 0.0001  max mem: 15821
[17:30:33.405099] Epoch: [23]  [100/345]  eta: 0:02:28  lr: 0.000121  loss: 0.2070 (0.2263)  time: 0.6024  data: 0.0001  max mem: 15821
[17:30:45.475919] Epoch: [23]  [120/345]  eta: 0:02:16  lr: 0.000121  loss: 0.2164 (0.2247)  time: 0.6035  data: 0.0001  max mem: 15821
[17:30:57.553067] Epoch: [23]  [140/345]  eta: 0:02:04  lr: 0.000121  loss: 0.2148 (0.2260)  time: 0.6038  data: 0.0001  max mem: 15821
[17:31:09.645868] Epoch: [23]  [160/345]  eta: 0:01:51  lr: 0.000121  loss: 0.1977 (0.2224)  time: 0.6046  data: 0.0001  max mem: 15821
[17:31:21.731587] Epoch: [23]  [180/345]  eta: 0:01:39  lr: 0.000121  loss: 0.2355 (0.2242)  time: 0.6042  data: 0.0001  max mem: 15821
[17:31:33.817258] Epoch: [23]  [200/345]  eta: 0:01:27  lr: 0.000121  loss: 0.2186 (0.2242)  time: 0.6042  data: 0.0001  max mem: 15821
[17:31:45.901971] Epoch: [23]  [220/345]  eta: 0:01:15  lr: 0.000121  loss: 0.2216 (0.2262)  time: 0.6042  data: 0.0001  max mem: 15821
[17:31:57.971044] Epoch: [23]  [240/345]  eta: 0:01:03  lr: 0.000120  loss: 0.2187 (0.2256)  time: 0.6034  data: 0.0001  max mem: 15821
[17:32:10.046669] Epoch: [23]  [260/345]  eta: 0:00:51  lr: 0.000120  loss: 0.2007 (0.2245)  time: 0.6037  data: 0.0001  max mem: 15821
[17:32:22.118395] Epoch: [23]  [280/345]  eta: 0:00:39  lr: 0.000120  loss: 0.2174 (0.2245)  time: 0.6035  data: 0.0001  max mem: 15821
[17:32:34.184927] Epoch: [23]  [300/345]  eta: 0:00:27  lr: 0.000120  loss: 0.2143 (0.2250)  time: 0.6033  data: 0.0001  max mem: 15821
[17:32:46.254951] Epoch: [23]  [320/345]  eta: 0:00:15  lr: 0.000120  loss: 0.2115 (0.2245)  time: 0.6034  data: 0.0001  max mem: 15821
[17:32:58.324087] Epoch: [23]  [340/345]  eta: 0:00:03  lr: 0.000120  loss: 0.2086 (0.2237)  time: 0.6034  data: 0.0001  max mem: 15821
[17:33:00.736776] Epoch: [23]  [344/345]  eta: 0:00:00  lr: 0.000120  loss: 0.2007 (0.2240)  time: 0.6033  data: 0.0001  max mem: 15821
[17:33:00.801350] Epoch: [23] Total time: 0:03:28 (0.6045 s / it)
[17:33:00.801649] Averaged stats: lr: 0.000120  loss: 0.2007 (0.2240)
[17:33:01.311675] Test:  [  0/345]  eta: 0:02:53  loss: 0.1893 (0.1893)  time: 0.5039  data: 0.3395  max mem: 15821
[17:33:02.977450] Test:  [ 10/345]  eta: 0:01:06  loss: 0.2093 (0.2097)  time: 0.1971  data: 0.0310  max mem: 15821
[17:33:04.646683] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2038 (0.2009)  time: 0.1667  data: 0.0001  max mem: 15821
[17:33:06.318560] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2193 (0.2082)  time: 0.1670  data: 0.0001  max mem: 15821
[17:33:07.993634] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2211 (0.2081)  time: 0.1673  data: 0.0001  max mem: 15821
[17:33:09.672144] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2095 (0.2094)  time: 0.1676  data: 0.0001  max mem: 15821
[17:33:11.354671] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2151 (0.2094)  time: 0.1680  data: 0.0001  max mem: 15821
[17:33:13.039762] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2175 (0.2126)  time: 0.1683  data: 0.0001  max mem: 15821
[17:33:14.728634] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2269 (0.2131)  time: 0.1686  data: 0.0001  max mem: 15821
[17:33:16.419884] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2128 (0.2129)  time: 0.1689  data: 0.0001  max mem: 15821
[17:33:18.114929] Test:  [100/345]  eta: 0:00:41  loss: 0.2069 (0.2109)  time: 0.1692  data: 0.0001  max mem: 15821
[17:33:19.814159] Test:  [110/345]  eta: 0:00:40  loss: 0.1915 (0.2095)  time: 0.1696  data: 0.0001  max mem: 15821
[17:33:21.516695] Test:  [120/345]  eta: 0:00:38  loss: 0.1915 (0.2097)  time: 0.1700  data: 0.0001  max mem: 15821
[17:33:23.221758] Test:  [130/345]  eta: 0:00:36  loss: 0.1945 (0.2100)  time: 0.1703  data: 0.0001  max mem: 15821
[17:33:24.930719] Test:  [140/345]  eta: 0:00:35  loss: 0.2317 (0.2111)  time: 0.1706  data: 0.0001  max mem: 15821
[17:33:26.644486] Test:  [150/345]  eta: 0:00:33  loss: 0.2317 (0.2119)  time: 0.1711  data: 0.0001  max mem: 15821
[17:33:28.361107] Test:  [160/345]  eta: 0:00:31  loss: 0.2043 (0.2116)  time: 0.1714  data: 0.0001  max mem: 15821
[17:33:30.080318] Test:  [170/345]  eta: 0:00:29  loss: 0.2043 (0.2120)  time: 0.1717  data: 0.0001  max mem: 15821
[17:33:31.803232] Test:  [180/345]  eta: 0:00:28  loss: 0.2073 (0.2121)  time: 0.1720  data: 0.0001  max mem: 15821
[17:33:33.530477] Test:  [190/345]  eta: 0:00:26  loss: 0.2073 (0.2131)  time: 0.1724  data: 0.0001  max mem: 15821
[17:33:35.260783] Test:  [200/345]  eta: 0:00:24  loss: 0.2199 (0.2134)  time: 0.1728  data: 0.0001  max mem: 15821
[17:33:36.993962] Test:  [210/345]  eta: 0:00:23  loss: 0.2097 (0.2131)  time: 0.1731  data: 0.0001  max mem: 15821
[17:33:38.730946] Test:  [220/345]  eta: 0:00:21  loss: 0.2097 (0.2140)  time: 0.1734  data: 0.0001  max mem: 15821
[17:33:40.472428] Test:  [230/345]  eta: 0:00:19  loss: 0.2248 (0.2141)  time: 0.1738  data: 0.0001  max mem: 15821
[17:33:42.216648] Test:  [240/345]  eta: 0:00:18  loss: 0.2028 (0.2142)  time: 0.1742  data: 0.0001  max mem: 15821
[17:33:43.963736] Test:  [250/345]  eta: 0:00:16  loss: 0.2077 (0.2145)  time: 0.1745  data: 0.0001  max mem: 15821
[17:33:45.713734] Test:  [260/345]  eta: 0:00:14  loss: 0.2126 (0.2139)  time: 0.1748  data: 0.0001  max mem: 15821
[17:33:47.466617] Test:  [270/345]  eta: 0:00:12  loss: 0.2036 (0.2138)  time: 0.1751  data: 0.0001  max mem: 15821
[17:33:49.224882] Test:  [280/345]  eta: 0:00:11  loss: 0.2271 (0.2147)  time: 0.1755  data: 0.0001  max mem: 15821
[17:33:50.984647] Test:  [290/345]  eta: 0:00:09  loss: 0.2220 (0.2146)  time: 0.1758  data: 0.0001  max mem: 15821
[17:33:52.747685] Test:  [300/345]  eta: 0:00:07  loss: 0.2002 (0.2141)  time: 0.1761  data: 0.0001  max mem: 15821
[17:33:54.515362] Test:  [310/345]  eta: 0:00:06  loss: 0.2027 (0.2142)  time: 0.1765  data: 0.0001  max mem: 15821
[17:33:56.286687] Test:  [320/345]  eta: 0:00:04  loss: 0.1989 (0.2140)  time: 0.1769  data: 0.0001  max mem: 15821
[17:33:58.063127] Test:  [330/345]  eta: 0:00:02  loss: 0.2012 (0.2141)  time: 0.1773  data: 0.0001  max mem: 15821
[17:33:59.841416] Test:  [340/345]  eta: 0:00:00  loss: 0.2167 (0.2143)  time: 0.1777  data: 0.0001  max mem: 15821
[17:34:00.552076] Test:  [344/345]  eta: 0:00:00  loss: 0.2416 (0.2147)  time: 0.1777  data: 0.0001  max mem: 15821
[17:34:00.617169] Test: Total time: 0:00:59 (0.1734 s / it)
[17:34:11.065877] Test:  [ 0/57]  eta: 0:00:27  loss: 0.4021 (0.4021)  time: 0.4754  data: 0.3128  max mem: 15821
[17:34:12.712981] Test:  [10/57]  eta: 0:00:09  loss: 0.3673 (0.3737)  time: 0.1929  data: 0.0285  max mem: 15821
[17:34:14.366005] Test:  [20/57]  eta: 0:00:06  loss: 0.3561 (0.3586)  time: 0.1649  data: 0.0001  max mem: 15821
[17:34:16.021600] Test:  [30/57]  eta: 0:00:04  loss: 0.2893 (0.3298)  time: 0.1654  data: 0.0001  max mem: 15821
[17:34:17.682042] Test:  [40/57]  eta: 0:00:02  loss: 0.2560 (0.3102)  time: 0.1657  data: 0.0001  max mem: 15821
[17:34:19.346375] Test:  [50/57]  eta: 0:00:01  loss: 0.2616 (0.3177)  time: 0.1662  data: 0.0001  max mem: 15821
[17:34:20.244688] Test:  [56/57]  eta: 0:00:00  loss: 0.3167 (0.3279)  time: 0.1613  data: 0.0001  max mem: 15821
[17:34:20.315765] Test: Total time: 0:00:09 (0.1706 s / it)
[17:34:22.068574] Dice score of the network on the train images: 0.760528, val images: 0.765435
[17:34:22.072699] log_dir: /root/seg_framework/MS-Mamba/output_dir_new/mslesseg/train_ft
[17:34:23.051051] Epoch: [24]  [  0/345]  eta: 0:05:37  lr: 0.000120  loss: 0.2385 (0.2385)  time: 0.9774  data: 0.3770  max mem: 15821
[17:34:35.025542] Epoch: [24]  [ 20/345]  eta: 0:03:20  lr: 0.000119  loss: 0.2148 (0.2216)  time: 0.5987  data: 0.0001  max mem: 15821
[17:34:47.032816] Epoch: [24]  [ 40/345]  eta: 0:03:05  lr: 0.000119  loss: 0.2037 (0.2174)  time: 0.6003  data: 0.0001  max mem: 15821
[17:34:59.063501] Epoch: [24]  [ 60/345]  eta: 0:02:52  lr: 0.000119  loss: 0.2027 (0.2153)  time: 0.6015  data: 0.0001  max mem: 15821
[17:35:11.109431] Epoch: [24]  [ 80/345]  eta: 0:02:40  lr: 0.000119  loss: 0.2407 (0.2208)  time: 0.6023  data: 0.0001  max mem: 15821
[17:35:23.175940] Epoch: [24]  [100/345]  eta: 0:02:28  lr: 0.000119  loss: 0.2049 (0.2195)  time: 0.6033  data: 0.0001  max mem: 15821
[17:35:35.266548] Epoch: [24]  [120/345]  eta: 0:02:16  lr: 0.000119  loss: 0.2261 (0.2191)  time: 0.6045  data: 0.0001  max mem: 15821
[17:35:47.347745] Epoch: [24]  [140/345]  eta: 0:02:03  lr: 0.000118  loss: 0.1916 (0.2170)  time: 0.6040  data: 0.0001  max mem: 15821
[17:35:59.440783] Epoch: [24]  [160/345]  eta: 0:01:51  lr: 0.000118  loss: 0.2328 (0.2186)  time: 0.6046  data: 0.0001  max mem: 15821
[17:36:11.523146] Epoch: [24]  [180/345]  eta: 0:01:39  lr: 0.000118  loss: 0.2269 (0.2192)  time: 0.6041  data: 0.0001  max mem: 15821
[17:36:23.607719] Epoch: [24]  [200/345]  eta: 0:01:27  lr: 0.000118  loss: 0.1913 (0.2177)  time: 0.6042  data: 0.0001  max mem: 15821
[17:36:35.678546] Epoch: [24]  [220/345]  eta: 0:01:15  lr: 0.000118  loss: 0.2250 (0.2182)  time: 0.6035  data: 0.0001  max mem: 15821
[17:36:46.112448] Loss is nan, stopping training
Not using distributed mode
[10:16:56.949549] job dir: /root/seg_framework/MS-Mamba/run_scripts
[10:16:56.949693] Namespace(accum_iter=1,
model='SegFormer3D',
in_channels=1,
lr=None,
blr=0.001,
min_lr=0,
dist_on_itp=False,
warmup_epochs=20,
device='cuda:0',
seed=42,
layer_decay=0.75,
clip_grad=None,
num_workers=8,
pin_mem=True,
resume='',
world_size=1,
embed_dim=1,
local_rank=-1,
dist_url='env://',
nb_classes=2,
data_dir='/root/MSLesSeg24/data',
datalist=None,
modalities='T1,T2,FLAIR,T1ce',
preprocess=False,
distributed=False)
[10:16:56.949808] device  cuda:0
[10:16:57.771972] number of params: 28910337
[10:16:57.772228] model: Vivim(
  (encoder): mamba_block(
    (downsample_layers): MixVisionTransformer(
      (embeds): ModuleList(
        (0): PatchEmbedding(
          (patch_embeddings): Conv3d(2, 64, kernel_size=(7, 7, 7), stride=(4, 4, 4), padding=(3, 3, 3))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): PatchEmbedding(
          (patch_embeddings): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): PatchEmbedding(
          (patch_embeddings): Conv3d(128, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (3): PatchEmbedding(
          (patch_embeddings): Conv3d(320, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (blocks): ModuleList(
        (0): ModuleList(
          (0-1): 2 x TransformerBlock(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key_value): Linear(in_features=64, out_features=128, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=64, out_features=64, bias=True)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (sr): Conv3d(64, 64, kernel_size=(4, 4, 4), stride=(4, 4, 4))
              (sr_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): _MLP(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
                (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (act_fn): GELU(approximate='none')
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleList(
          (0-1): 2 x TransformerBlock(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SelfAttention(
              (query): Linear(in_features=128, out_features=128, bias=True)
              (key_value): Linear(in_features=128, out_features=256, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (sr): Conv3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
              (sr_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): _MLP(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
                (bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (act_fn): GELU(approximate='none')
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleList(
          (0-1): 2 x TransformerBlock(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SelfAttention(
              (query): Linear(in_features=320, out_features=320, bias=True)
              (key_value): Linear(in_features=320, out_features=640, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=320, out_features=320, bias=True)
              (proj_dropout): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): _MLP(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
                (bn): BatchNorm3d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (act_fn): GELU(approximate='none')
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleList(
          (0-1): 2 x TransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SelfAttention(
              (query): Linear(in_features=512, out_features=512, bias=True)
              (key_value): Linear(in_features=512, out_features=1024, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_dropout): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): _MLP(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
                (bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (act_fn): GELU(approximate='none')
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv4D(
                  (conv3d): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
                )
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv4D(
                  (conv3d): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
                )
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv4D(
                  (conv3d): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
                )
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv4D(
                  (conv3d): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
                )
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (2): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv4D(
                  (conv3d): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
                )
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv4D(
                  (conv3d): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
                )
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (3): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv4D(
                  (conv3d): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
                )
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv4D(
                  (conv3d): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
                )
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (decoder): SegFormerDecoderHead(
    (linear_c): ModuleList(
      (0): MLP_(
        (proj): Linear(in_features=512, out_features=256, bias=True)
        (bn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (1): MLP_(
        (proj): Linear(in_features=320, out_features=256, bias=True)
        (bn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (2): MLP_(
        (proj): Linear(in_features=128, out_features=256, bias=True)
        (bn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (3): MLP_(
        (proj): Linear(in_features=64, out_features=256, bias=True)
        (bn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (linear_fuse): Sequential(
      (0): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (linear_pred): Conv3d(256, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (upsample_volume): Upsample(scale_factor=4.0, mode='trilinear')
  )
)
[10:16:57.773858] base lr: 1.00e-03
[10:16:57.773923] actual lr: 3.91e-06
[10:16:57.773978] accumulate grad iterations: 1
[10:16:57.774029] effective batch size: 1
[10:16:57.774914] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 3.90625e-06
    maximize: False
    weight_decay: 0.01
)
[10:16:57.777064] Start training for 1000 epochs
[10:16:57.778164] log_dir: /root/seg_framework/MS-Mamba/output_dir_test/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir_test/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir_test/mslesseg/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
[10:17:18.275846] Epoch: [0]  [0/2]  eta: 0:00:40  lr: 0.000000  loss: 1.8479 (1.8479)  time: 20.4967  data: 2.3857  max mem: 33093
[10:17:19.630664] Epoch: [0]  [1/2]  eta: 0:00:10  lr: 0.000000  loss: 1.8479 (1.8891)  time: 10.9254  data: 1.1929  max mem: 33093
[10:17:19.661784] Epoch: [0] Total time: 0:00:21 (10.9418 s / it)
[10:17:19.662262] Averaged stats: lr: 0.000000  loss: 1.8479 (1.8891)
[10:17:22.363966] Test:  [0/2]  eta: 0:00:05  loss: 1.8011 (1.8011)  time: 2.6995  data: 1.9608  max mem: 33093
[10:17:23.104232] Test:  [1/2]  eta: 0:00:01  loss: 1.7694 (1.7852)  time: 1.7196  data: 0.9805  max mem: 33093
[10:17:23.149498] Test: Total time: 0:00:03 (1.7428 s / it)
[10:17:26.958392] Test:  [0/1]  eta: 0:00:02  loss: 1.6745 (1.6745)  time: 2.6533  data: 1.9177  max mem: 33093
[10:17:27.008003] Test: Total time: 0:00:02 (2.7035 s / it)
[10:17:27.604634] Dice score of the network on the train images: 0.013553, val images: 0.000556
[10:17:27.604883] saving best_prec_model @ epoch 0
[10:17:27.874866] saving best_rec_model @ epoch 0
[10:17:28.138291] saving best_dice_model @ epoch 0
[10:17:28.395232] log_dir: /root/seg_framework/MS-Mamba/output_dir_test/mslesseg/train_ft
[10:17:31.943787] Epoch: [1]  [0/2]  eta: 0:00:07  lr: 0.000000  loss: 1.9304 (1.9304)  time: 3.5475  data: 2.1439  max mem: 33093
[10:17:33.293890] Epoch: [1]  [1/2]  eta: 0:00:02  lr: 0.000000  loss: 1.8206 (1.8755)  time: 2.4483  data: 1.0720  max mem: 33093
[10:17:33.342047] Epoch: [1] Total time: 0:00:04 (2.4733 s / it)
[10:17:33.342228] Averaged stats: lr: 0.000000  loss: 1.8206 (1.8755)
[10:17:36.142155] Test:  [0/2]  eta: 0:00:05  loss: 1.6297 (1.6297)  time: 2.7974  data: 2.0606  max mem: 33093
[10:17:36.881117] Test:  [1/2]  eta: 0:00:01  loss: 1.6297 (1.6316)  time: 1.7679  data: 1.0303  max mem: 33093
[10:17:36.927055] Test: Total time: 0:00:03 (1.7914 s / it)
[10:17:40.971856] Test:  [0/1]  eta: 0:00:02  loss: 1.5252 (1.5252)  time: 2.7701  data: 2.0310  max mem: 33093
[10:17:41.021558] Test: Total time: 0:00:02 (2.8204 s / it)
[10:17:41.676096] Dice score of the network on the train images: 0.011279, val images: 0.000431
[10:17:41.679419] log_dir: /root/seg_framework/MS-Mamba/output_dir_test/mslesseg/train_ft
[10:17:45.071470] Epoch: [2]  [0/2]  eta: 0:00:06  lr: 0.000000  loss: 1.8454 (1.8454)  time: 3.3909  data: 2.0377  max mem: 33093
[10:17:46.425564] Epoch: [2]  [1/2]  eta: 0:00:02  lr: 0.000000  loss: 1.7299 (1.7876)  time: 2.3720  data: 1.0189  max mem: 33093
[10:17:46.481964] Epoch: [2] Total time: 0:00:04 (2.4012 s / it)
[10:17:46.482211] Averaged stats: lr: 0.000000  loss: 1.7299 (1.7876)
[10:17:49.281808] Test:  [0/2]  eta: 0:00:05  loss: 1.5208 (1.5208)  time: 2.7970  data: 2.0569  max mem: 33093
[10:17:50.024135] Test:  [1/2]  eta: 0:00:01  loss: 1.5208 (1.5358)  time: 1.7693  data: 1.0285  max mem: 33093
[10:17:50.074399] Test: Total time: 0:00:03 (1.7951 s / it)
[10:17:54.133697] Test:  [0/1]  eta: 0:00:02  loss: 1.4357 (1.4357)  time: 2.7815  data: 2.0418  max mem: 33093
[10:17:54.187902] Test: Total time: 0:00:02 (2.8363 s / it)
[10:17:54.842676] Dice score of the network on the train images: 0.009239, val images: 0.000234
[10:17:54.846388] log_dir: /root/seg_framework/MS-Mamba/output_dir_test/mslesseg/train_ft
[10:17:58.271227] Epoch: [3]  [0/2]  eta: 0:00:06  lr: 0.000001  loss: 1.7106 (1.7106)  time: 3.4237  data: 2.0674  max mem: 33093
[10:17:59.628081] Epoch: [3]  [1/2]  eta: 0:00:02  lr: 0.000001  loss: 1.6084 (1.6595)  time: 2.3898  data: 1.0337  max mem: 33093
[10:17:59.674816] Epoch: [3] Total time: 0:00:04 (2.4141 s / it)
[10:17:59.674983] Averaged stats: lr: 0.000001  loss: 1.6084 (1.6595)
[10:18:02.479537] Test:  [0/2]  eta: 0:00:05  loss: 1.4385 (1.4385)  time: 2.8019  data: 2.0598  max mem: 33093
[10:18:03.226943] Test:  [1/2]  eta: 0:00:01  loss: 1.4385 (1.4655)  time: 1.7743  data: 1.0299  max mem: 33093
[10:18:03.278813] Test: Total time: 0:00:03 (1.8009 s / it)
[10:18:07.313174] Test:  [0/1]  eta: 0:00:02  loss: 1.3719 (1.3719)  time: 2.7736  data: 2.0315  max mem: 33093
[10:18:07.367085] Test: Total time: 0:00:02 (2.8281 s / it)
[10:18:08.024096] Dice score of the network on the train images: 0.006437, val images: 0.000094
[10:18:08.027300] log_dir: /root/seg_framework/MS-Mamba/output_dir_test/mslesseg/train_ft
[10:18:11.527033] Epoch: [4]  [0/2]  eta: 0:00:06  lr: 0.000001  loss: 1.5437 (1.5437)  time: 3.4986  data: 2.1387  max mem: 33093
[10:18:12.885976] Epoch: [4]  [1/2]  eta: 0:00:02  lr: 0.000001  loss: 1.4772 (1.5105)  time: 2.4283  data: 1.0694  max mem: 33093
[10:18:12.936444] Epoch: [4] Total time: 0:00:04 (2.4545 s / it)
[10:18:12.936723] Averaged stats: lr: 0.000001  loss: 1.4772 (1.5105)
[10:18:15.756376] Test:  [0/2]  eta: 0:00:05  loss: 1.3747 (1.3747)  time: 2.8093  data: 2.0667  max mem: 33093
[10:18:16.505318] Test:  [1/2]  eta: 0:00:01  loss: 1.3747 (1.4111)  time: 1.7788  data: 1.0334  max mem: 33093
[10:18:16.556547] Test: Total time: 0:00:03 (1.8051 s / it)
[10:18:20.618673] Test:  [0/1]  eta: 0:00:02  loss: 1.3249 (1.3249)  time: 2.7785  data: 2.0343  max mem: 33093
[10:18:20.668414] Test: Total time: 0:00:02 (2.8288 s / it)
[10:18:21.319544] Dice score of the network on the train images: 0.003792, val images: 0.000077
[10:18:21.323285] log_dir: /root/seg_framework/MS-Mamba/output_dir_test/mslesseg/train_ft
[10:18:24.838738] Epoch: [5]  [0/2]  eta: 0:00:07  lr: 0.000001  loss: 1.4192 (1.4192)  time: 3.5144  data: 2.1491  max mem: 33093
[10:18:26.200006] Epoch: [5]  [1/2]  eta: 0:00:02  lr: 0.000001  loss: 1.3431 (1.3812)  time: 2.4374  data: 1.0746  max mem: 33093
[10:18:26.255670] Epoch: [5] Total time: 0:00:04 (2.4661 s / it)
[10:18:26.255857] Averaged stats: lr: 0.000001  loss: 1.3431 (1.3812)
[10:18:29.065683] Test:  [0/2]  eta: 0:00:05  loss: 1.4158 (1.4158)  time: 2.8072  data: 2.0623  max mem: 33093
[10:18:29.812678] Test:  [1/2]  eta: 0:00:01  loss: 1.3328 (1.3743)  time: 1.7768  data: 1.0312  max mem: 33093
[10:18:29.864717] Test: Total time: 0:00:03 (1.8034 s / it)
[10:18:33.913437] Test:  [0/1]  eta: 0:00:02  loss: 1.2938 (1.2938)  time: 2.7673  data: 2.0242  max mem: 33093
[10:18:33.964883] Test: Total time: 0:00:02 (2.8193 s / it)
[10:18:34.522397] [10:18:34.522617] [10:18:34.522695] [10:18:34.522761] [10:18:34.522824] [10:18:34.522883] [10:18:34.522940] [10:18:34.523005] [10:18:34.523072]
Traceback (most recent call last):
  File "/root/anaconda3/envs/vivim/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/anaconda3/envs/vivim/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/seg_framework/MS-Mamba/run_scripts/train_mslesseg.py", line 438, in <module>
    main(args)
  File "/root/seg_framework/MS-Mamba/run_scripts/train_mslesseg.py", line 343, in main
    val_stats = evaluate(dataloader_val, model, device)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/seg_framework/MS-Mamba/run_scripts/train_mslesseg.py", line 204, in evaluate
    prec, rec, dice = precision(outPRED, outGT), recall(outPRED, outGT), dice_score(outPRED, outGT)
  File "/root/seg_framework/MS-Mamba/utils/metric.py", line 47, in dice_score
    target = metric_utils.flatten(target).cpu().detach().float()
KeyboardInterrupt
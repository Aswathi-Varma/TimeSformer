Not using distributed mode
[20:06:26.125971] job dir: /root/seg_framework/MS-Mamba/run_scripts
[20:06:26.126116] Namespace(accum_iter=1,
model='SegFormer3D',
in_channels=1,
lr=None,
blr=0.001,
min_lr=0,
dist_on_itp=False,
warmup_epochs=20,
device='cuda:0',
seed=42,
layer_decay=0.75,
clip_grad=None,
num_workers=8,
pin_mem=True,
resume='',
mask_mode='concatenate to image',
world_size=1,
embed_dim=1,
local_rank=-1,
dist_url='env://',
nb_classes=2,
data_dir='/root/MSLesSeg24/data',
datalist=None,
preprocess=False,
dim=2,
distributed=False)
[20:06:26.126226] device  cuda:0
[20:06:26.127034] Starting for fold 0
[20:06:26.323384] Elements in data_dir_paths: 11052
[20:06:26.359053] Elements in data_dir_paths: 1803
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/maskenc/fold_0/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/maskenc/fold_0/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[20:06:28.312612] number of params: 61538391
[20:06:28.312859] model: Vivim2D(
  (encoder): mamba_block(
    (downsample_layers): SegformerEncoder(
      (patch_embeddings): ModuleList(
        (0): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(2, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (3): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (block): ModuleList(
        (0): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): Identity()
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.003703703870996833)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.007407407741993666)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.011111111380159855)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.014814815483987331)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.018518518656492233)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.02222222276031971)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.025925926864147186)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.029629630967974663)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03333333507180214)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03703703731298447)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04074074327945709)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04444444552063942)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.048148151487112045)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.051851850003004074)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0555555559694767)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.05925925821065903)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06296296417713165)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06666667014360428)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07037036865949631)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07407407462596893)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07777778059244156)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08148147910833359)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08518518507480621)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08888889104127884)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.09259259700775146)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0962962955236435)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.10000000149011612)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (layer_norm): ModuleList(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (2): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (3): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (decoder): SegformerDecodeHead(
    (linear_c): ModuleList(
      (0): SegformerMLP(
        (proj): Linear(in_features=64, out_features=768, bias=True)
      )
      (1): SegformerMLP(
        (proj): Linear(in_features=128, out_features=768, bias=True)
      )
      (2): SegformerMLP(
        (proj): Linear(in_features=320, out_features=768, bias=True)
      )
      (3): SegformerMLP(
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
    )
    (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Conv2d(768, 150, kernel_size=(1, 1), stride=(1, 1))
  )
  (out): Conv2d(768, 1, kernel_size=(1, 1), stride=(1, 1))
  (mask_encoder): mask_block(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
        (1): ReLU()
      )
      (1): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU()
      )
      (2): Sequential(
        (0): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU()
      )
      (3): Sequential(
        (0): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU()
      )
    )
  )
)
[20:06:28.316011] base lr: 1.00e-03
[20:06:28.316073] actual lr: 1.25e-04
[20:06:28.316128] accumulate grad iterations: 1
[20:06:28.316176] effective batch size: 32
[20:06:28.317785] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.000125
    maximize: False
    weight_decay: 0.01
)
[20:06:28.319871] Start training for 50 epochs
[20:06:28.319960] Number of samples in train dataloader:  345
[20:06:28.321787] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/maskenc/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
[20:06:38.047379] Epoch: [0]  [  0/345]  eta: 0:55:55  lr: 0.000000  loss: 1.6675 (1.6675)  time: 9.7250  data: 0.2752  max mem: 15850
[20:06:49.774065] Epoch: [0]  [ 20/345]  eta: 0:05:31  lr: 0.000000  loss: 1.6684 (1.6689)  time: 0.5863  data: 0.0001  max mem: 15850
[20:07:01.584443] Epoch: [0]  [ 40/345]  eta: 0:04:07  lr: 0.000001  loss: 1.6659 (1.6674)  time: 0.5905  data: 0.0001  max mem: 15850
[20:07:13.467011] Epoch: [0]  [ 60/345]  eta: 0:03:30  lr: 0.000001  loss: 1.6589 (1.6648)  time: 0.5941  data: 0.0001  max mem: 15850
[20:07:25.408896] Epoch: [0]  [ 80/345]  eta: 0:03:06  lr: 0.000001  loss: 1.6543 (1.6625)  time: 0.5970  data: 0.0001  max mem: 15850
[20:07:37.388749] Epoch: [0]  [100/345]  eta: 0:02:47  lr: 0.000002  loss: 1.6470 (1.6594)  time: 0.5989  data: 0.0001  max mem: 15850
[20:07:49.399983] Epoch: [0]  [120/345]  eta: 0:02:30  lr: 0.000002  loss: 1.6385 (1.6559)  time: 0.6005  data: 0.0001  max mem: 15850
[20:08:01.436159] Epoch: [0]  [140/345]  eta: 0:02:15  lr: 0.000003  loss: 1.6250 (1.6515)  time: 0.6018  data: 0.0001  max mem: 15850
[20:08:13.493163] Epoch: [0]  [160/345]  eta: 0:02:00  lr: 0.000003  loss: 1.6058 (1.6455)  time: 0.6028  data: 0.0001  max mem: 15850
[20:08:25.564558] Epoch: [0]  [180/345]  eta: 0:01:46  lr: 0.000003  loss: 1.5630 (1.6365)  time: 0.6035  data: 0.0001  max mem: 15850
[20:08:37.636860] Epoch: [0]  [200/345]  eta: 0:01:33  lr: 0.000004  loss: 1.5062 (1.6238)  time: 0.6036  data: 0.0001  max mem: 15850
[20:08:49.715578] Epoch: [0]  [220/345]  eta: 0:01:19  lr: 0.000004  loss: 1.4493 (1.6078)  time: 0.6039  data: 0.0001  max mem: 15850
[20:09:01.818056] Epoch: [0]  [240/345]  eta: 0:01:06  lr: 0.000004  loss: 1.3859 (1.5893)  time: 0.6051  data: 0.0001  max mem: 15850
[20:09:13.929605] Epoch: [0]  [260/345]  eta: 0:00:53  lr: 0.000005  loss: 1.3274 (1.5693)  time: 0.6055  data: 0.0001  max mem: 15850
[20:09:26.044322] Epoch: [0]  [280/345]  eta: 0:00:41  lr: 0.000005  loss: 1.2716 (1.5483)  time: 0.6057  data: 0.0001  max mem: 15850
[20:09:38.150055] Epoch: [0]  [300/345]  eta: 0:00:28  lr: 0.000005  loss: 1.2233 (1.5268)  time: 0.6052  data: 0.0001  max mem: 15850
[20:09:50.249070] Epoch: [0]  [320/345]  eta: 0:00:15  lr: 0.000006  loss: 1.1763 (1.5054)  time: 0.6049  data: 0.0001  max mem: 15850
[20:10:02.336822] Epoch: [0]  [340/345]  eta: 0:00:03  lr: 0.000006  loss: 1.1411 (1.4842)  time: 0.6043  data: 0.0001  max mem: 15850
[20:10:04.753513] Epoch: [0]  [344/345]  eta: 0:00:00  lr: 0.000006  loss: 1.1383 (1.4800)  time: 0.6044  data: 0.0001  max mem: 15850
[20:10:04.800001] Epoch: [0] Total time: 0:03:36 (0.6275 s / it)
[20:10:04.800302] Averaged stats: lr: 0.000006  loss: 1.1383 (1.4800)
[20:10:05.260282] Test:  [  0/345]  eta: 0:02:37  loss: 1.1419 (1.1419)  time: 0.4555  data: 0.2888  max mem: 15850
[20:10:06.937739] Test:  [ 10/345]  eta: 0:01:04  loss: 1.1419 (1.1411)  time: 0.1938  data: 0.0263  max mem: 15850
[20:10:08.618111] Test:  [ 20/345]  eta: 0:00:58  loss: 1.1402 (1.1407)  time: 0.1678  data: 0.0001  max mem: 15850
[20:10:10.303009] Test:  [ 30/345]  eta: 0:00:55  loss: 1.1414 (1.1413)  time: 0.1682  data: 0.0001  max mem: 15850
[20:10:11.991390] Test:  [ 40/345]  eta: 0:00:53  loss: 1.1421 (1.1414)  time: 0.1686  data: 0.0001  max mem: 15850
[20:10:13.682770] Test:  [ 50/345]  eta: 0:00:51  loss: 1.1416 (1.1413)  time: 0.1689  data: 0.0001  max mem: 15850
[20:10:15.377826] Test:  [ 60/345]  eta: 0:00:49  loss: 1.1416 (1.1413)  time: 0.1693  data: 0.0001  max mem: 15850
[20:10:17.076418] Test:  [ 70/345]  eta: 0:00:47  loss: 1.1417 (1.1413)  time: 0.1696  data: 0.0001  max mem: 15850
[20:10:18.778420] Test:  [ 80/345]  eta: 0:00:45  loss: 1.1414 (1.1412)  time: 0.1700  data: 0.0001  max mem: 15850
[20:10:20.485060] Test:  [ 90/345]  eta: 0:00:43  loss: 1.1409 (1.1412)  time: 0.1704  data: 0.0001  max mem: 15850
[20:10:22.195741] Test:  [100/345]  eta: 0:00:42  loss: 1.1409 (1.1412)  time: 0.1708  data: 0.0001  max mem: 15850
[20:10:23.909181] Test:  [110/345]  eta: 0:00:40  loss: 1.1408 (1.1412)  time: 0.1711  data: 0.0001  max mem: 15850
[20:10:25.625628] Test:  [120/345]  eta: 0:00:38  loss: 1.1408 (1.1411)  time: 0.1714  data: 0.0001  max mem: 15850
[20:10:27.347829] Test:  [130/345]  eta: 0:00:36  loss: 1.1407 (1.1411)  time: 0.1719  data: 0.0001  max mem: 15850
[20:10:29.071019] Test:  [140/345]  eta: 0:00:35  loss: 1.1409 (1.1411)  time: 0.1722  data: 0.0001  max mem: 15850
[20:10:30.798454] Test:  [150/345]  eta: 0:00:33  loss: 1.1410 (1.1411)  time: 0.1725  data: 0.0001  max mem: 15850
[20:10:32.530272] Test:  [160/345]  eta: 0:00:31  loss: 1.1400 (1.1410)  time: 0.1729  data: 0.0001  max mem: 15850
[20:10:34.265992] Test:  [170/345]  eta: 0:00:30  loss: 1.1400 (1.1409)  time: 0.1733  data: 0.0001  max mem: 15850
[20:10:36.003600] Test:  [180/345]  eta: 0:00:28  loss: 1.1409 (1.1409)  time: 0.1736  data: 0.0001  max mem: 15850
[20:10:37.746114] Test:  [190/345]  eta: 0:00:26  loss: 1.1409 (1.1409)  time: 0.1739  data: 0.0001  max mem: 15850
[20:10:39.490321] Test:  [200/345]  eta: 0:00:25  loss: 1.1405 (1.1409)  time: 0.1743  data: 0.0001  max mem: 15850
[20:10:41.632045] Test:  [210/345]  eta: 0:00:23  loss: 1.1406 (1.1410)  time: 0.1942  data: 0.0001  max mem: 15850
[20:10:43.396174] Test:  [220/345]  eta: 0:00:21  loss: 1.1411 (1.1410)  time: 0.1952  data: 0.0001  max mem: 15850
[20:10:45.284352] Test:  [230/345]  eta: 0:00:20  loss: 1.1406 (1.1409)  time: 0.1826  data: 0.0001  max mem: 15850
[20:10:47.046435] Test:  [240/345]  eta: 0:00:18  loss: 1.1406 (1.1409)  time: 0.1825  data: 0.0001  max mem: 15850
[20:10:49.055778] Test:  [250/345]  eta: 0:00:16  loss: 1.1411 (1.1410)  time: 0.1885  data: 0.0001  max mem: 15850
[20:10:51.061954] Test:  [260/345]  eta: 0:00:15  loss: 1.1411 (1.1410)  time: 0.2007  data: 0.0001  max mem: 15850
[20:10:52.990869] Test:  [270/345]  eta: 0:00:13  loss: 1.1402 (1.1410)  time: 0.1967  data: 0.0001  max mem: 15850
[20:10:54.789410] Test:  [280/345]  eta: 0:00:11  loss: 1.1401 (1.1409)  time: 0.1863  data: 0.0001  max mem: 15850
[20:10:56.754696] Test:  [290/345]  eta: 0:00:09  loss: 1.1392 (1.1409)  time: 0.1881  data: 0.0001  max mem: 15850
[20:10:58.790337] Test:  [300/345]  eta: 0:00:08  loss: 1.1407 (1.1409)  time: 0.2000  data: 0.0001  max mem: 15850
[20:11:00.795134] Test:  [310/345]  eta: 0:00:06  loss: 1.1406 (1.1409)  time: 0.2020  data: 0.0001  max mem: 15850
[20:11:02.823952] Test:  [320/345]  eta: 0:00:04  loss: 1.1398 (1.1409)  time: 0.2016  data: 0.0001  max mem: 15850
[20:11:04.858184] Test:  [330/345]  eta: 0:00:02  loss: 1.1406 (1.1409)  time: 0.2031  data: 0.0001  max mem: 15850
[20:11:06.685522] Test:  [340/345]  eta: 0:00:00  loss: 1.1394 (1.1408)  time: 0.1930  data: 0.0001  max mem: 15850
[20:11:07.726683] Test:  [344/345]  eta: 0:00:00  loss: 1.1401 (1.1408)  time: 0.2084  data: 0.0001  max mem: 15850
[20:11:07.795579] Test: Total time: 0:01:02 (0.1826 s / it)
[20:11:17.718399] Test:  [ 0/57]  eta: 0:00:32  loss: 1.1473 (1.1473)  time: 0.5675  data: 0.4042  max mem: 15850
[20:11:19.377424] Test:  [10/57]  eta: 0:00:09  loss: 1.1443 (1.1442)  time: 0.2023  data: 0.0369  max mem: 15850
[20:11:21.040362] Test:  [20/57]  eta: 0:00:06  loss: 1.1443 (1.1436)  time: 0.1660  data: 0.0001  max mem: 15850
[20:11:22.705880] Test:  [30/57]  eta: 0:00:04  loss: 1.1369 (1.1391)  time: 0.1663  data: 0.0001  max mem: 15850
[20:11:24.376442] Test:  [40/57]  eta: 0:00:02  loss: 1.1291 (1.1367)  time: 0.1667  data: 0.0001  max mem: 15850
[20:11:26.051507] Test:  [50/57]  eta: 0:00:01  loss: 1.1329 (1.1355)  time: 0.1672  data: 0.0001  max mem: 15850
[20:11:27.785049] Test:  [56/57]  eta: 0:00:00  loss: 1.1340 (1.1350)  time: 0.2038  data: 0.0001  max mem: 15850
[20:11:27.851391] Test: Total time: 0:00:10 (0.1877 s / it)
[20:11:29.498100] Dice score of the network on the train images: 0.000000, val images: 0.000000
[20:11:29.498337] saving best_dice_model_0 @ epoch 0
[20:11:30.424301] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[20:11:31.293120] Epoch: [1]  [  0/345]  eta: 0:04:59  lr: 0.000006  loss: 1.1235 (1.1235)  time: 0.8675  data: 0.2657  max mem: 15850
[20:11:43.259489] Epoch: [1]  [ 20/345]  eta: 0:03:18  lr: 0.000007  loss: 1.1112 (1.1122)  time: 0.5983  data: 0.0001  max mem: 15850
[20:11:55.253920] Epoch: [1]  [ 40/345]  eta: 0:03:04  lr: 0.000007  loss: 1.0930 (1.1029)  time: 0.5997  data: 0.0001  max mem: 15850
[20:12:07.263002] Epoch: [1]  [ 60/345]  eta: 0:02:52  lr: 0.000007  loss: 1.0785 (1.0950)  time: 0.6004  data: 0.0001  max mem: 15850
[20:12:19.289343] Epoch: [1]  [ 80/345]  eta: 0:02:39  lr: 0.000008  loss: 1.0683 (1.0885)  time: 0.6013  data: 0.0001  max mem: 15850
[20:12:31.337614] Epoch: [1]  [100/345]  eta: 0:02:27  lr: 0.000008  loss: 1.0615 (1.0832)  time: 0.6024  data: 0.0001  max mem: 15850
[20:12:43.402321] Epoch: [1]  [120/345]  eta: 0:02:15  lr: 0.000008  loss: 1.0548 (1.0785)  time: 0.6032  data: 0.0001  max mem: 15850
[20:12:55.480389] Epoch: [1]  [140/345]  eta: 0:02:03  lr: 0.000009  loss: 1.0511 (1.0746)  time: 0.6039  data: 0.0001  max mem: 15850
[20:13:07.574319] Epoch: [1]  [160/345]  eta: 0:01:51  lr: 0.000009  loss: 1.0461 (1.0710)  time: 0.6046  data: 0.0001  max mem: 15850
[20:13:19.668227] Epoch: [1]  [180/345]  eta: 0:01:39  lr: 0.000010  loss: 1.0404 (1.0677)  time: 0.6046  data: 0.0001  max mem: 15850
[20:13:31.761606] Epoch: [1]  [200/345]  eta: 0:01:27  lr: 0.000010  loss: 1.0360 (1.0646)  time: 0.6046  data: 0.0001  max mem: 15850
[20:13:43.842002] Epoch: [1]  [220/345]  eta: 0:01:15  lr: 0.000010  loss: 1.0327 (1.0617)  time: 0.6040  data: 0.0001  max mem: 15850
[20:13:55.918455] Epoch: [1]  [240/345]  eta: 0:01:03  lr: 0.000011  loss: 1.0281 (1.0590)  time: 0.6038  data: 0.0001  max mem: 15850
[20:14:07.992831] Epoch: [1]  [260/345]  eta: 0:00:51  lr: 0.000011  loss: 1.0247 (1.0563)  time: 0.6037  data: 0.0001  max mem: 15850
[20:14:20.057622] Epoch: [1]  [280/345]  eta: 0:00:39  lr: 0.000011  loss: 1.0221 (1.0537)  time: 0.6032  data: 0.0001  max mem: 15850
[20:14:32.124038] Epoch: [1]  [300/345]  eta: 0:00:27  lr: 0.000012  loss: 1.0113 (1.0509)  time: 0.6033  data: 0.0001  max mem: 15850
[20:14:44.179383] Epoch: [1]  [320/345]  eta: 0:00:15  lr: 0.000012  loss: 1.0065 (1.0481)  time: 0.6027  data: 0.0001  max mem: 15850
[20:14:56.235294] Epoch: [1]  [340/345]  eta: 0:00:03  lr: 0.000012  loss: 0.9949 (1.0451)  time: 0.6028  data: 0.0001  max mem: 15850
[20:14:58.645854] Epoch: [1]  [344/345]  eta: 0:00:00  lr: 0.000012  loss: 0.9940 (1.0445)  time: 0.6027  data: 0.0001  max mem: 15850
[20:14:58.723243] Epoch: [1] Total time: 0:03:28 (0.6038 s / it)
[20:14:58.723578] Averaged stats: lr: 0.000012  loss: 0.9940 (1.0445)
[20:14:59.230617] Test:  [  0/345]  eta: 0:02:52  loss: 0.9790 (0.9790)  time: 0.5011  data: 0.3362  max mem: 15850
[20:15:00.909468] Test:  [ 10/345]  eta: 0:01:06  loss: 0.9790 (0.9806)  time: 0.1981  data: 0.0306  max mem: 15850
[20:15:02.590794] Test:  [ 20/345]  eta: 0:00:59  loss: 0.9788 (0.9808)  time: 0.1679  data: 0.0001  max mem: 15850
[20:15:04.275145] Test:  [ 30/345]  eta: 0:00:56  loss: 0.9778 (0.9813)  time: 0.1682  data: 0.0001  max mem: 15850
[20:15:05.963002] Test:  [ 40/345]  eta: 0:00:53  loss: 0.9774 (0.9813)  time: 0.1685  data: 0.0001  max mem: 15850
[20:15:07.654546] Test:  [ 50/345]  eta: 0:00:51  loss: 0.9772 (0.9812)  time: 0.1689  data: 0.0001  max mem: 15850
[20:15:09.349952] Test:  [ 60/345]  eta: 0:00:49  loss: 0.9769 (0.9805)  time: 0.1693  data: 0.0001  max mem: 15850
[20:15:11.047727] Test:  [ 70/345]  eta: 0:00:47  loss: 0.9775 (0.9804)  time: 0.1696  data: 0.0001  max mem: 15850
[20:15:12.750232] Test:  [ 80/345]  eta: 0:00:45  loss: 0.9790 (0.9798)  time: 0.1700  data: 0.0001  max mem: 15850
[20:15:14.455508] Test:  [ 90/345]  eta: 0:00:44  loss: 0.9767 (0.9794)  time: 0.1703  data: 0.0001  max mem: 15850
[20:15:16.163759] Test:  [100/345]  eta: 0:00:42  loss: 0.9779 (0.9795)  time: 0.1706  data: 0.0001  max mem: 15850
[20:15:17.875544] Test:  [110/345]  eta: 0:00:40  loss: 0.9796 (0.9795)  time: 0.1709  data: 0.0001  max mem: 15850
[20:15:19.591566] Test:  [120/345]  eta: 0:00:38  loss: 0.9792 (0.9796)  time: 0.1713  data: 0.0001  max mem: 15850
[20:15:21.311069] Test:  [130/345]  eta: 0:00:37  loss: 0.9785 (0.9798)  time: 0.1717  data: 0.0001  max mem: 15850
[20:15:23.033123] Test:  [140/345]  eta: 0:00:35  loss: 0.9807 (0.9800)  time: 0.1720  data: 0.0001  max mem: 15850
[20:15:24.758938] Test:  [150/345]  eta: 0:00:33  loss: 0.9778 (0.9799)  time: 0.1723  data: 0.0001  max mem: 15850
[20:15:26.487831] Test:  [160/345]  eta: 0:00:31  loss: 0.9778 (0.9797)  time: 0.1727  data: 0.0001  max mem: 15850
[20:15:28.220431] Test:  [170/345]  eta: 0:00:30  loss: 0.9809 (0.9800)  time: 0.1730  data: 0.0001  max mem: 15850
[20:15:29.956372] Test:  [180/345]  eta: 0:00:28  loss: 0.9867 (0.9803)  time: 0.1734  data: 0.0001  max mem: 15850
[20:15:31.695939] Test:  [190/345]  eta: 0:00:26  loss: 0.9838 (0.9802)  time: 0.1737  data: 0.0001  max mem: 15850
[20:15:33.439001] Test:  [200/345]  eta: 0:00:25  loss: 0.9781 (0.9801)  time: 0.1741  data: 0.0001  max mem: 15850
[20:15:35.186801] Test:  [210/345]  eta: 0:00:23  loss: 0.9769 (0.9800)  time: 0.1745  data: 0.0001  max mem: 15850
[20:15:37.310081] Test:  [220/345]  eta: 0:00:21  loss: 0.9798 (0.9802)  time: 0.1935  data: 0.0001  max mem: 15850
[20:15:39.061453] Test:  [230/345]  eta: 0:00:20  loss: 0.9811 (0.9802)  time: 0.1937  data: 0.0001  max mem: 15850
[20:15:40.985082] Test:  [240/345]  eta: 0:00:18  loss: 0.9820 (0.9803)  time: 0.1837  data: 0.0001  max mem: 15850
[20:15:42.762545] Test:  [250/345]  eta: 0:00:16  loss: 0.9820 (0.9803)  time: 0.1850  data: 0.0001  max mem: 15850
[20:15:44.662528] Test:  [260/345]  eta: 0:00:14  loss: 0.9795 (0.9802)  time: 0.1838  data: 0.0001  max mem: 15850
[20:15:46.456594] Test:  [270/345]  eta: 0:00:13  loss: 0.9754 (0.9798)  time: 0.1846  data: 0.0001  max mem: 15850
[20:15:48.369990] Test:  [280/345]  eta: 0:00:11  loss: 0.9754 (0.9799)  time: 0.1853  data: 0.0001  max mem: 15850
[20:15:50.169970] Test:  [290/345]  eta: 0:00:09  loss: 0.9786 (0.9799)  time: 0.1856  data: 0.0001  max mem: 15850
[20:15:52.123663] Test:  [300/345]  eta: 0:00:07  loss: 0.9786 (0.9800)  time: 0.1876  data: 0.0001  max mem: 15850
[20:15:53.921366] Test:  [310/345]  eta: 0:00:06  loss: 0.9818 (0.9802)  time: 0.1875  data: 0.0001  max mem: 15850
[20:15:55.891029] Test:  [320/345]  eta: 0:00:04  loss: 0.9826 (0.9802)  time: 0.1883  data: 0.0001  max mem: 15850
[20:15:57.686577] Test:  [330/345]  eta: 0:00:02  loss: 0.9792 (0.9801)  time: 0.1882  data: 0.0001  max mem: 15850
[20:15:59.653537] Test:  [340/345]  eta: 0:00:00  loss: 0.9761 (0.9800)  time: 0.1881  data: 0.0001  max mem: 15850
[20:16:00.372371] Test:  [344/345]  eta: 0:00:00  loss: 0.9745 (0.9800)  time: 0.1879  data: 0.0001  max mem: 15850
[20:16:00.438908] Test: Total time: 0:01:01 (0.1789 s / it)
[20:16:10.236504] Test:  [ 0/57]  eta: 0:00:26  loss: 1.0102 (1.0102)  time: 0.4571  data: 0.2930  max mem: 15850
[20:16:11.896229] Test:  [10/57]  eta: 0:00:09  loss: 1.0036 (1.0038)  time: 0.1924  data: 0.0267  max mem: 15850
[20:16:13.561387] Test:  [20/57]  eta: 0:00:06  loss: 1.0017 (1.0003)  time: 0.1662  data: 0.0001  max mem: 15850
[20:16:15.229359] Test:  [30/57]  eta: 0:00:04  loss: 0.9603 (0.9779)  time: 0.1666  data: 0.0001  max mem: 15850
[20:16:16.901673] Test:  [40/57]  eta: 0:00:02  loss: 0.9427 (0.9667)  time: 0.1669  data: 0.0001  max mem: 15850
[20:16:18.578870] Test:  [50/57]  eta: 0:00:01  loss: 0.9427 (0.9609)  time: 0.1674  data: 0.0001  max mem: 15850
[20:16:19.484586] Test:  [56/57]  eta: 0:00:00  loss: 0.9434 (0.9565)  time: 0.1626  data: 0.0000  max mem: 15850
[20:16:19.555983] Test: Total time: 0:00:09 (0.1715 s / it)
[20:16:21.221057] Dice score of the network on the train images: 0.000000, val images: 0.000000
[20:16:21.225073] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[20:16:22.150567] Epoch: [2]  [  0/345]  eta: 0:05:18  lr: 0.000013  loss: 0.9875 (0.9875)  time: 0.9246  data: 0.3260  max mem: 15850
[20:16:34.097462] Epoch: [2]  [ 20/345]  eta: 0:03:19  lr: 0.000013  loss: 0.9821 (0.9813)  time: 0.5973  data: 0.0001  max mem: 15850
[20:16:46.067754] Epoch: [2]  [ 40/345]  eta: 0:03:04  lr: 0.000013  loss: 0.9652 (0.9747)  time: 0.5985  data: 0.0001  max mem: 15850
[20:16:58.062615] Epoch: [2]  [ 60/345]  eta: 0:02:52  lr: 0.000014  loss: 0.9466 (0.9668)  time: 0.5997  data: 0.0001  max mem: 15850
[20:17:10.073463] Epoch: [2]  [ 80/345]  eta: 0:02:39  lr: 0.000014  loss: 0.9325 (0.9581)  time: 0.6005  data: 0.0001  max mem: 15850
[20:17:22.101749] Epoch: [2]  [100/345]  eta: 0:02:27  lr: 0.000014  loss: 0.9194 (0.9510)  time: 0.6014  data: 0.0001  max mem: 15850
[20:17:34.138977] Epoch: [2]  [120/345]  eta: 0:02:15  lr: 0.000015  loss: 0.9148 (0.9441)  time: 0.6018  data: 0.0001  max mem: 15850
[20:17:46.195436] Epoch: [2]  [140/345]  eta: 0:02:03  lr: 0.000015  loss: 0.9078 (0.9384)  time: 0.6028  data: 0.0001  max mem: 15850
[20:17:58.259271] Epoch: [2]  [160/345]  eta: 0:01:51  lr: 0.000015  loss: 0.8913 (0.9326)  time: 0.6031  data: 0.0001  max mem: 15850
[20:18:10.325311] Epoch: [2]  [180/345]  eta: 0:01:39  lr: 0.000016  loss: 0.8733 (0.9267)  time: 0.6033  data: 0.0001  max mem: 15850
[20:18:22.392103] Epoch: [2]  [200/345]  eta: 0:01:27  lr: 0.000016  loss: 0.8624 (0.9207)  time: 0.6033  data: 0.0001  max mem: 15850
[20:18:34.458118] Epoch: [2]  [220/345]  eta: 0:01:15  lr: 0.000016  loss: 0.8628 (0.9157)  time: 0.6033  data: 0.0001  max mem: 15850
[20:18:46.519087] Epoch: [2]  [240/345]  eta: 0:01:03  lr: 0.000017  loss: 0.8551 (0.9107)  time: 0.6030  data: 0.0001  max mem: 15850
[20:18:58.578517] Epoch: [2]  [260/345]  eta: 0:00:51  lr: 0.000017  loss: 0.8342 (0.9049)  time: 0.6029  data: 0.0001  max mem: 15850
[20:19:10.627964] Epoch: [2]  [280/345]  eta: 0:00:39  lr: 0.000018  loss: 0.8299 (0.8994)  time: 0.6024  data: 0.0001  max mem: 15850
[20:19:22.680428] Epoch: [2]  [300/345]  eta: 0:00:27  lr: 0.000018  loss: 0.8272 (0.8943)  time: 0.6026  data: 0.0001  max mem: 15850
[20:19:34.728799] Epoch: [2]  [320/345]  eta: 0:00:15  lr: 0.000018  loss: 0.8187 (0.8895)  time: 0.6024  data: 0.0001  max mem: 15850
[20:19:46.768191] Epoch: [2]  [340/345]  eta: 0:00:03  lr: 0.000019  loss: 0.8028 (0.8847)  time: 0.6019  data: 0.0001  max mem: 15850
[20:19:49.176776] Epoch: [2]  [344/345]  eta: 0:00:00  lr: 0.000019  loss: 0.8039 (0.8837)  time: 0.6019  data: 0.0001  max mem: 15850
[20:19:49.246425] Epoch: [2] Total time: 0:03:28 (0.6030 s / it)
[20:19:49.246805] Averaged stats: lr: 0.000019  loss: 0.8039 (0.8837)
[20:19:49.731185] Test:  [  0/345]  eta: 0:02:45  loss: 0.7472 (0.7472)  time: 0.4789  data: 0.3136  max mem: 15850
[20:19:51.409851] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7472 (0.7623)  time: 0.1960  data: 0.0286  max mem: 15850
[20:19:53.092528] Test:  [ 20/345]  eta: 0:00:59  loss: 0.7405 (0.7528)  time: 0.1680  data: 0.0001  max mem: 15850
[20:19:54.777902] Test:  [ 30/345]  eta: 0:00:56  loss: 0.7600 (0.7633)  time: 0.1683  data: 0.0001  max mem: 15850
[20:19:56.466348] Test:  [ 40/345]  eta: 0:00:53  loss: 0.7761 (0.7640)  time: 0.1686  data: 0.0001  max mem: 15850
[20:19:58.157763] Test:  [ 50/345]  eta: 0:00:51  loss: 0.7574 (0.7645)  time: 0.1689  data: 0.0001  max mem: 15850
[20:19:59.853533] Test:  [ 60/345]  eta: 0:00:49  loss: 0.7664 (0.7620)  time: 0.1693  data: 0.0001  max mem: 15850
[20:20:01.552444] Test:  [ 70/345]  eta: 0:00:47  loss: 0.7664 (0.7623)  time: 0.1697  data: 0.0001  max mem: 15850
[20:20:03.253654] Test:  [ 80/345]  eta: 0:00:45  loss: 0.7676 (0.7621)  time: 0.1699  data: 0.0001  max mem: 15850
[20:20:04.959655] Test:  [ 90/345]  eta: 0:00:43  loss: 0.7599 (0.7606)  time: 0.1703  data: 0.0001  max mem: 15850
[20:20:06.668118] Test:  [100/345]  eta: 0:00:42  loss: 0.7555 (0.7597)  time: 0.1707  data: 0.0001  max mem: 15850
[20:20:08.380665] Test:  [110/345]  eta: 0:00:40  loss: 0.7595 (0.7586)  time: 0.1710  data: 0.0001  max mem: 15850
[20:20:10.098015] Test:  [120/345]  eta: 0:00:38  loss: 0.7537 (0.7586)  time: 0.1714  data: 0.0001  max mem: 15850
[20:20:11.817917] Test:  [130/345]  eta: 0:00:37  loss: 0.7478 (0.7588)  time: 0.1718  data: 0.0001  max mem: 15850
[20:20:13.540927] Test:  [140/345]  eta: 0:00:35  loss: 0.7478 (0.7574)  time: 0.1721  data: 0.0001  max mem: 15850
[20:20:15.268079] Test:  [150/345]  eta: 0:00:33  loss: 0.7398 (0.7570)  time: 0.1724  data: 0.0001  max mem: 15850
[20:20:16.999064] Test:  [160/345]  eta: 0:00:31  loss: 0.7348 (0.7558)  time: 0.1728  data: 0.0001  max mem: 15850
[20:20:18.733761] Test:  [170/345]  eta: 0:00:30  loss: 0.7377 (0.7567)  time: 0.1732  data: 0.0001  max mem: 15850
[20:20:20.470792] Test:  [180/345]  eta: 0:00:28  loss: 0.7648 (0.7564)  time: 0.1735  data: 0.0001  max mem: 15850
[20:20:22.211462] Test:  [190/345]  eta: 0:00:26  loss: 0.7617 (0.7569)  time: 0.1738  data: 0.0001  max mem: 15850
[20:20:23.956290] Test:  [200/345]  eta: 0:00:25  loss: 0.7560 (0.7569)  time: 0.1742  data: 0.0001  max mem: 15850
[20:20:25.703913] Test:  [210/345]  eta: 0:00:23  loss: 0.7560 (0.7572)  time: 0.1746  data: 0.0001  max mem: 15850
[20:20:27.455828] Test:  [220/345]  eta: 0:00:21  loss: 0.7528 (0.7569)  time: 0.1749  data: 0.0001  max mem: 15850
[20:20:29.211515] Test:  [230/345]  eta: 0:00:19  loss: 0.7514 (0.7566)  time: 0.1753  data: 0.0001  max mem: 15850
[20:20:30.968663] Test:  [240/345]  eta: 0:00:18  loss: 0.7509 (0.7561)  time: 0.1756  data: 0.0001  max mem: 15850
[20:20:32.730512] Test:  [250/345]  eta: 0:00:16  loss: 0.7523 (0.7567)  time: 0.1759  data: 0.0001  max mem: 15850
[20:20:34.494321] Test:  [260/345]  eta: 0:00:14  loss: 0.7764 (0.7567)  time: 0.1762  data: 0.0001  max mem: 15850
[20:20:36.262555] Test:  [270/345]  eta: 0:00:13  loss: 0.7728 (0.7570)  time: 0.1765  data: 0.0001  max mem: 15850
[20:20:38.035008] Test:  [280/345]  eta: 0:00:11  loss: 0.7740 (0.7573)  time: 0.1770  data: 0.0001  max mem: 15850
[20:20:39.809861] Test:  [290/345]  eta: 0:00:09  loss: 0.7341 (0.7564)  time: 0.1773  data: 0.0001  max mem: 15850
[20:20:41.588559] Test:  [300/345]  eta: 0:00:07  loss: 0.7229 (0.7557)  time: 0.1776  data: 0.0001  max mem: 15850
[20:20:43.371325] Test:  [310/345]  eta: 0:00:06  loss: 0.7364 (0.7558)  time: 0.1780  data: 0.0001  max mem: 15850
[20:20:45.156194] Test:  [320/345]  eta: 0:00:04  loss: 0.7570 (0.7551)  time: 0.1783  data: 0.0001  max mem: 15850
[20:20:46.943808] Test:  [330/345]  eta: 0:00:02  loss: 0.7549 (0.7550)  time: 0.1786  data: 0.0001  max mem: 15850
[20:20:48.737092] Test:  [340/345]  eta: 0:00:00  loss: 0.7559 (0.7552)  time: 0.1790  data: 0.0001  max mem: 15850
[20:20:49.456034] Test:  [344/345]  eta: 0:00:00  loss: 0.7624 (0.7551)  time: 0.1792  data: 0.0001  max mem: 15850
[20:20:49.531485] Test: Total time: 0:01:00 (0.1747 s / it)
[20:20:59.334917] Test:  [ 0/57]  eta: 0:00:25  loss: 0.8873 (0.8873)  time: 0.4508  data: 0.2870  max mem: 15850
[20:21:00.996635] Test:  [10/57]  eta: 0:00:09  loss: 0.8194 (0.8250)  time: 0.1920  data: 0.0262  max mem: 15850
[20:21:02.663622] Test:  [20/57]  eta: 0:00:06  loss: 0.8156 (0.8164)  time: 0.1664  data: 0.0001  max mem: 15850
[20:21:04.333863] Test:  [30/57]  eta: 0:00:04  loss: 0.7055 (0.7574)  time: 0.1668  data: 0.0001  max mem: 15850
[20:21:06.007557] Test:  [40/57]  eta: 0:00:02  loss: 0.6142 (0.7260)  time: 0.1671  data: 0.0001  max mem: 15850
[20:21:07.686338] Test:  [50/57]  eta: 0:00:01  loss: 0.6320 (0.7135)  time: 0.1676  data: 0.0001  max mem: 15850
[20:21:08.591434] Test:  [56/57]  eta: 0:00:00  loss: 0.6456 (0.7077)  time: 0.1627  data: 0.0000  max mem: 15850
[20:21:08.646243] Test: Total time: 0:00:09 (0.1713 s / it)
[20:21:10.287600] Dice score of the network on the train images: 0.523813, val images: 0.599760
[20:21:10.287829] saving best_prec_model_0 @ epoch 2
[20:21:11.209113] saving best_rec_model_0 @ epoch 2
[20:21:12.095466] saving best_dice_model_0 @ epoch 2
[20:21:13.353915] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[20:21:14.263998] Epoch: [3]  [  0/345]  eta: 0:05:13  lr: 0.000019  loss: 0.8123 (0.8123)  time: 0.9089  data: 0.3087  max mem: 15850
[20:21:26.219566] Epoch: [3]  [ 20/345]  eta: 0:03:19  lr: 0.000019  loss: 0.7848 (0.7876)  time: 0.5977  data: 0.0001  max mem: 15850
[20:21:38.197820] Epoch: [3]  [ 40/345]  eta: 0:03:04  lr: 0.000019  loss: 0.7623 (0.7771)  time: 0.5989  data: 0.0001  max mem: 15850
[20:21:50.204492] Epoch: [3]  [ 60/345]  eta: 0:02:52  lr: 0.000020  loss: 0.7469 (0.7680)  time: 0.6003  data: 0.0001  max mem: 15850
[20:22:02.235259] Epoch: [3]  [ 80/345]  eta: 0:02:39  lr: 0.000020  loss: 0.7333 (0.7615)  time: 0.6015  data: 0.0001  max mem: 15850
[20:22:14.285865] Epoch: [3]  [100/345]  eta: 0:02:27  lr: 0.000021  loss: 0.7245 (0.7548)  time: 0.6025  data: 0.0001  max mem: 15850
[20:22:26.353191] Epoch: [3]  [120/345]  eta: 0:02:15  lr: 0.000021  loss: 0.7159 (0.7491)  time: 0.6033  data: 0.0001  max mem: 15850
[20:22:38.448775] Epoch: [3]  [140/345]  eta: 0:02:03  lr: 0.000021  loss: 0.6926 (0.7411)  time: 0.6047  data: 0.0001  max mem: 15850
[20:22:50.551373] Epoch: [3]  [160/345]  eta: 0:01:51  lr: 0.000022  loss: 0.6703 (0.7329)  time: 0.6051  data: 0.0001  max mem: 15850
[20:23:02.662725] Epoch: [3]  [180/345]  eta: 0:01:39  lr: 0.000022  loss: 0.6618 (0.7253)  time: 0.6055  data: 0.0001  max mem: 15850
[20:23:14.770964] Epoch: [3]  [200/345]  eta: 0:01:27  lr: 0.000022  loss: 0.6521 (0.7187)  time: 0.6054  data: 0.0001  max mem: 15850
[20:23:26.868021] Epoch: [3]  [220/345]  eta: 0:01:15  lr: 0.000023  loss: 0.6349 (0.7110)  time: 0.6048  data: 0.0001  max mem: 15850
[20:23:38.958582] Epoch: [3]  [240/345]  eta: 0:01:03  lr: 0.000023  loss: 0.6214 (0.7043)  time: 0.6045  data: 0.0001  max mem: 15850
[20:23:51.044504] Epoch: [3]  [260/345]  eta: 0:00:51  lr: 0.000023  loss: 0.6101 (0.6972)  time: 0.6043  data: 0.0001  max mem: 15850
[20:24:03.118208] Epoch: [3]  [280/345]  eta: 0:00:39  lr: 0.000024  loss: 0.6034 (0.6903)  time: 0.6036  data: 0.0001  max mem: 15850
[20:24:15.191688] Epoch: [3]  [300/345]  eta: 0:00:27  lr: 0.000024  loss: 0.5966 (0.6838)  time: 0.6036  data: 0.0001  max mem: 15850

[20:24:27.265087] Epoch: [3]  [320/345]  eta: 0:00:15  lr: 0.000025  loss: 0.5729 (0.6772)  time: 0.6036  data: 0.0001  max mem: 15850
[20:24:39.325987] Epoch: [3]  [340/345]  eta: 0:00:03  lr: 0.000025  loss: 0.5715 (0.6705)  time: 0.6030  data: 0.0001  max mem: 15850
[20:24:41.738452] Epoch: [3]  [344/345]  eta: 0:00:00  lr: 0.000025  loss: 0.5732 (0.6694)  time: 0.6029  data: 0.0001  max mem: 15850
[20:24:41.806608] Epoch: [3] Total time: 0:03:28 (0.6042 s / it)
[20:24:41.806937] Averaged stats: lr: 0.000025  loss: 0.5732 (0.6694)
[20:24:42.273391] Test:  [  0/345]  eta: 0:02:39  loss: 0.5478 (0.5478)  time: 0.4610  data: 0.2958  max mem: 15850
[20:24:43.952532] Test:  [ 10/345]  eta: 0:01:05  loss: 0.5058 (0.5120)  time: 0.1945  data: 0.0270  max mem: 15850
[20:24:45.634135] Test:  [ 20/345]  eta: 0:00:59  loss: 0.5006 (0.5126)  time: 0.1680  data: 0.0001  max mem: 15850
[20:24:47.320221] Test:  [ 30/345]  eta: 0:00:55  loss: 0.5074 (0.5159)  time: 0.1683  data: 0.0001  max mem: 15850
[20:24:49.008109] Test:  [ 40/345]  eta: 0:00:53  loss: 0.5230 (0.5261)  time: 0.1686  data: 0.0001  max mem: 15850
[20:24:50.700453] Test:  [ 50/345]  eta: 0:00:51  loss: 0.5505 (0.5299)  time: 0.1689  data: 0.0001  max mem: 15850
[20:24:52.396272] Test:  [ 60/345]  eta: 0:00:49  loss: 0.5220 (0.5272)  time: 0.1693  data: 0.0001  max mem: 15850
[20:24:54.095279] Test:  [ 70/345]  eta: 0:00:47  loss: 0.5195 (0.5258)  time: 0.1697  data: 0.0001  max mem: 15850
[20:24:55.797377] Test:  [ 80/345]  eta: 0:00:45  loss: 0.5016 (0.5231)  time: 0.1700  data: 0.0001  max mem: 15850
[20:24:57.503327] Test:  [ 90/345]  eta: 0:00:43  loss: 0.5059 (0.5224)  time: 0.1703  data: 0.0001  max mem: 15850
[20:24:59.211810] Test:  [100/345]  eta: 0:00:42  loss: 0.5139 (0.5226)  time: 0.1707  data: 0.0001  max mem: 15850
[20:25:00.924620] Test:  [110/345]  eta: 0:00:40  loss: 0.5175 (0.5223)  time: 0.1710  data: 0.0001  max mem: 15850
[20:25:02.640888] Test:  [120/345]  eta: 0:00:38  loss: 0.5257 (0.5224)  time: 0.1714  data: 0.0001  max mem: 15850
[20:25:04.360679] Test:  [130/345]  eta: 0:00:36  loss: 0.5230 (0.5225)  time: 0.1717  data: 0.0001  max mem: 15850
[20:25:06.084605] Test:  [140/345]  eta: 0:00:35  loss: 0.5038 (0.5222)  time: 0.1721  data: 0.0001  max mem: 15850
[20:25:07.811764] Test:  [150/345]  eta: 0:00:33  loss: 0.5224 (0.5221)  time: 0.1725  data: 0.0001  max mem: 15850
[20:25:09.542229] Test:  [160/345]  eta: 0:00:31  loss: 0.5284 (0.5219)  time: 0.1728  data: 0.0001  max mem: 15850
[20:25:11.277938] Test:  [170/345]  eta: 0:00:30  loss: 0.5167 (0.5222)  time: 0.1732  data: 0.0001  max mem: 15850
[20:25:13.014232] Test:  [180/345]  eta: 0:00:28  loss: 0.5037 (0.5216)  time: 0.1735  data: 0.0001  max mem: 15850
[20:25:14.755412] Test:  [190/345]  eta: 0:00:26  loss: 0.4954 (0.5204)  time: 0.1738  data: 0.0001  max mem: 15850
[20:25:16.499525] Test:  [200/345]  eta: 0:00:25  loss: 0.5145 (0.5216)  time: 0.1742  data: 0.0001  max mem: 15850
[20:25:18.248692] Test:  [210/345]  eta: 0:00:23  loss: 0.5209 (0.5212)  time: 0.1746  data: 0.0001  max mem: 15850
[20:25:19.999245] Test:  [220/345]  eta: 0:00:21  loss: 0.5108 (0.5213)  time: 0.1749  data: 0.0001  max mem: 15850
[20:25:21.753073] Test:  [230/345]  eta: 0:00:19  loss: 0.5178 (0.5214)  time: 0.1752  data: 0.0001  max mem: 15850
[20:25:23.511353] Test:  [240/345]  eta: 0:00:18  loss: 0.5119 (0.5208)  time: 0.1755  data: 0.0001  max mem: 15850
[20:25:25.273587] Test:  [250/345]  eta: 0:00:16  loss: 0.5102 (0.5207)  time: 0.1760  data: 0.0001  max mem: 15850
[20:25:27.038699] Test:  [260/345]  eta: 0:00:14  loss: 0.5176 (0.5212)  time: 0.1763  data: 0.0001  max mem: 15850
[20:25:28.806827] Test:  [270/345]  eta: 0:00:13  loss: 0.5227 (0.5217)  time: 0.1766  data: 0.0001  max mem: 15850
[20:25:30.579725] Test:  [280/345]  eta: 0:00:11  loss: 0.5227 (0.5213)  time: 0.1770  data: 0.0001  max mem: 15850
[20:25:32.355403] Test:  [290/345]  eta: 0:00:09  loss: 0.5158 (0.5218)  time: 0.1774  data: 0.0001  max mem: 15850
[20:25:34.133770] Test:  [300/345]  eta: 0:00:07  loss: 0.5102 (0.5211)  time: 0.1776  data: 0.0001  max mem: 15850
[20:25:35.915338] Test:  [310/345]  eta: 0:00:06  loss: 0.5153 (0.5218)  time: 0.1779  data: 0.0001  max mem: 15850
[20:25:37.700316] Test:  [320/345]  eta: 0:00:04  loss: 0.5301 (0.5218)  time: 0.1783  data: 0.0001  max mem: 15850
[20:25:39.487864] Test:  [330/345]  eta: 0:00:02  loss: 0.5155 (0.5221)  time: 0.1786  data: 0.0001  max mem: 15850
[20:25:41.281096] Test:  [340/345]  eta: 0:00:00  loss: 0.5183 (0.5221)  time: 0.1790  data: 0.0001  max mem: 15850
[20:25:42.000225] Test:  [344/345]  eta: 0:00:00  loss: 0.5183 (0.5220)  time: 0.1792  data: 0.0001  max mem: 15850
[20:25:42.064522] Test: Total time: 0:01:00 (0.1746 s / it)
[20:25:51.954722] Test:  [ 0/57]  eta: 0:00:24  loss: 0.6599 (0.6599)  time: 0.4340  data: 0.2703  max mem: 15850
[20:25:53.616692] Test:  [10/57]  eta: 0:00:08  loss: 0.6094 (0.6009)  time: 0.1904  data: 0.0247  max mem: 15850
[20:25:55.282750] Test:  [20/57]  eta: 0:00:06  loss: 0.6082 (0.6028)  time: 0.1663  data: 0.0001  max mem: 15850
[20:25:56.951728] Test:  [30/57]  eta: 0:00:04  loss: 0.4518 (0.5341)  time: 0.1667  data: 0.0001  max mem: 15850
[20:25:58.626255] Test:  [40/57]  eta: 0:00:02  loss: 0.3697 (0.4982)  time: 0.1671  data: 0.0001  max mem: 15850
[20:26:00.304320] Test:  [50/57]  eta: 0:00:01  loss: 0.3774 (0.4871)  time: 0.1676  data: 0.0001  max mem: 15850
[20:26:01.210230] Test:  [56/57]  eta: 0:00:00  loss: 0.4359 (0.4869)  time: 0.1626  data: 0.0001  max mem: 15850
[20:26:01.284979] Test: Total time: 0:00:09 (0.1713 s / it)
[20:26:02.904603] Dice score of the network on the train images: 0.714134, val images: 0.765701
[20:26:02.904826] saving best_prec_model_0 @ epoch 3
[20:26:04.134385] saving best_rec_model_0 @ epoch 3
[20:26:05.311931] saving best_dice_model_0 @ epoch 3
[20:26:06.604393] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[20:26:07.483175] Epoch: [4]  [  0/345]  eta: 0:05:02  lr: 0.000025  loss: 0.6765 (0.6765)  time: 0.8777  data: 0.2785  max mem: 15850
[20:26:19.435305] Epoch: [4]  [ 20/345]  eta: 0:03:18  lr: 0.000025  loss: 0.5416 (0.5511)  time: 0.5975  data: 0.0001  max mem: 15850
[20:26:31.411710] Epoch: [4]  [ 40/345]  eta: 0:03:04  lr: 0.000026  loss: 0.5450 (0.5471)  time: 0.5988  data: 0.0001  max mem: 15850
[20:26:43.419593] Epoch: [4]  [ 60/345]  eta: 0:02:51  lr: 0.000026  loss: 0.5326 (0.5449)  time: 0.6004  data: 0.0001  max mem: 15850
[20:26:55.458714] Epoch: [4]  [ 80/345]  eta: 0:02:39  lr: 0.000026  loss: 0.5095 (0.5385)  time: 0.6019  data: 0.0001  max mem: 15850
[20:27:07.512670] Epoch: [4]  [100/345]  eta: 0:02:27  lr: 0.000027  loss: 0.4877 (0.5309)  time: 0.6027  data: 0.0001  max mem: 15850
[20:27:19.598746] Epoch: [4]  [120/345]  eta: 0:02:15  lr: 0.000027  loss: 0.5074 (0.5264)  time: 0.6043  data: 0.0001  max mem: 15850
[20:27:31.702506] Epoch: [4]  [140/345]  eta: 0:02:03  lr: 0.000028  loss: 0.5147 (0.5232)  time: 0.6051  data: 0.0001  max mem: 15850
[20:27:43.812841] Epoch: [4]  [160/345]  eta: 0:01:51  lr: 0.000028  loss: 0.4912 (0.5191)  time: 0.6055  data: 0.0001  max mem: 15850
[20:27:55.924482] Epoch: [4]  [180/345]  eta: 0:01:39  lr: 0.000028  loss: 0.4848 (0.5161)  time: 0.6055  data: 0.0001  max mem: 15850

[20:28:08.036346] Epoch: [4]  [200/345]  eta: 0:01:27  lr: 0.000029  loss: 0.4764 (0.5132)  time: 0.6055  data: 0.0001  max mem: 15850
[20:28:20.140862] Epoch: [4]  [220/345]  eta: 0:01:15  lr: 0.000029  loss: 0.4724 (0.5092)  time: 0.6052  data: 0.0001  max mem: 15850
[20:28:32.356352] Epoch: [4]  [240/345]  eta: 0:01:03  lr: 0.000029  loss: 0.4767 (0.5060)  time: 0.6107  data: 0.0001  max mem: 15850
[20:28:44.450081] Epoch: [4]  [260/345]  eta: 0:00:51  lr: 0.000030  loss: 0.4574 (0.5023)  time: 0.6046  data: 0.0001  max mem: 15850
[20:28:56.536656] Epoch: [4]  [280/345]  eta: 0:00:39  lr: 0.000030  loss: 0.4406 (0.4981)  time: 0.6043  data: 0.0001  max mem: 15850
[20:29:08.621723] Epoch: [4]  [300/345]  eta: 0:00:27  lr: 0.000030  loss: 0.4533 (0.4953)  time: 0.6042  data: 0.0001  max mem: 15850
[20:29:20.689309] Epoch: [4]  [320/345]  eta: 0:00:15  lr: 0.000031  loss: 0.4413 (0.4924)  time: 0.6033  data: 0.0001  max mem: 15850
[20:29:32.736533] Epoch: [4]  [340/345]  eta: 0:00:03  lr: 0.000031  loss: 0.4294 (0.4894)  time: 0.6023  data: 0.0001  max mem: 15850
[20:29:35.147911] Epoch: [4]  [344/345]  eta: 0:00:00  lr: 0.000031  loss: 0.4363 (0.4888)  time: 0.6024  data: 0.0001  max mem: 15850
[20:29:35.218187] Epoch: [4] Total time: 0:03:28 (0.6047 s / it)
[20:29:35.218534] Averaged stats: lr: 0.000031  loss: 0.4363 (0.4888)
[20:29:35.685219] Test:  [  0/345]  eta: 0:02:39  loss: 0.3831 (0.3831)  time: 0.4614  data: 0.2967  max mem: 15850
[20:29:37.364544] Test:  [ 10/345]  eta: 0:01:05  loss: 0.3745 (0.3918)  time: 0.1945  data: 0.0271  max mem: 15850
[20:29:39.045670] Test:  [ 20/345]  eta: 0:00:59  loss: 0.4150 (0.4096)  time: 0.1679  data: 0.0001  max mem: 15850
[20:29:40.730868] Test:  [ 30/345]  eta: 0:00:55  loss: 0.4021 (0.3970)  time: 0.1683  data: 0.0001  max mem: 15850
[20:29:42.420749] Test:  [ 40/345]  eta: 0:00:53  loss: 0.3745 (0.3982)  time: 0.1687  data: 0.0001  max mem: 15850
[20:29:44.111633] Test:  [ 50/345]  eta: 0:00:51  loss: 0.4177 (0.4041)  time: 0.1690  data: 0.0001  max mem: 15850
[20:29:45.806992] Test:  [ 60/345]  eta: 0:00:49  loss: 0.4330 (0.4090)  time: 0.1692  data: 0.0001  max mem: 15850
[20:29:47.507295] Test:  [ 70/345]  eta: 0:00:47  loss: 0.4106 (0.4080)  time: 0.1697  data: 0.0001  max mem: 15850
[20:29:49.208574] Test:  [ 80/345]  eta: 0:00:45  loss: 0.4106 (0.4101)  time: 0.1700  data: 0.0001  max mem: 15850
[20:29:50.914192] Test:  [ 90/345]  eta: 0:00:43  loss: 0.4087 (0.4090)  time: 0.1703  data: 0.0001  max mem: 15850
[20:29:52.622590] Test:  [100/345]  eta: 0:00:42  loss: 0.4044 (0.4097)  time: 0.1706  data: 0.0001  max mem: 15850
[20:29:54.334873] Test:  [110/345]  eta: 0:00:40  loss: 0.4134 (0.4117)  time: 0.1710  data: 0.0001  max mem: 15850
[20:29:56.050629] Test:  [120/345]  eta: 0:00:38  loss: 0.4190 (0.4120)  time: 0.1713  data: 0.0001  max mem: 15850
[20:29:57.770469] Test:  [130/345]  eta: 0:00:36  loss: 0.4266 (0.4132)  time: 0.1717  data: 0.0001  max mem: 15850
[20:29:59.494211] Test:  [140/345]  eta: 0:00:35  loss: 0.4039 (0.4128)  time: 0.1721  data: 0.0001  max mem: 15850
[20:30:01.221128] Test:  [150/345]  eta: 0:00:33  loss: 0.4242 (0.4143)  time: 0.1725  data: 0.0001  max mem: 15850
[20:30:02.950684] Test:  [160/345]  eta: 0:00:31  loss: 0.4242 (0.4152)  time: 0.1728  data: 0.0001  max mem: 15850
[20:30:04.683985] Test:  [170/345]  eta: 0:00:30  loss: 0.3960 (0.4147)  time: 0.1731  data: 0.0001  max mem: 15850
[20:30:06.420492] Test:  [180/345]  eta: 0:00:28  loss: 0.3960 (0.4141)  time: 0.1734  data: 0.0001  max mem: 15850
[20:30:08.160152] Test:  [190/345]  eta: 0:00:26  loss: 0.4172 (0.4155)  time: 0.1737  data: 0.0001  max mem: 15850
[20:30:09.902607] Test:  [200/345]  eta: 0:00:25  loss: 0.4184 (0.4158)  time: 0.1740  data: 0.0001  max mem: 15850
[20:30:11.649054] Test:  [210/345]  eta: 0:00:23  loss: 0.4146 (0.4153)  time: 0.1744  data: 0.0001  max mem: 15850
[20:30:13.399640] Test:  [220/345]  eta: 0:00:21  loss: 0.4120 (0.4150)  time: 0.1748  data: 0.0001  max mem: 15850
[20:30:15.153147] Test:  [230/345]  eta: 0:00:19  loss: 0.4140 (0.4152)  time: 0.1751  data: 0.0001  max mem: 15850
[20:30:16.910696] Test:  [240/345]  eta: 0:00:18  loss: 0.4261 (0.4155)  time: 0.1755  data: 0.0001  max mem: 15850
[20:30:18.672215] Test:  [250/345]  eta: 0:00:16  loss: 0.4014 (0.4148)  time: 0.1759  data: 0.0001  max mem: 15850
[20:30:20.437003] Test:  [260/345]  eta: 0:00:14  loss: 0.4000 (0.4145)  time: 0.1762  data: 0.0001  max mem: 15850
[20:30:22.204363] Test:  [270/345]  eta: 0:00:12  loss: 0.4136 (0.4146)  time: 0.1765  data: 0.0001  max mem: 15850
[20:30:23.975638] Test:  [280/345]  eta: 0:00:11  loss: 0.4159 (0.4147)  time: 0.1769  data: 0.0001  max mem: 15850
[20:30:25.748614] Test:  [290/345]  eta: 0:00:09  loss: 0.4241 (0.4152)  time: 0.1771  data: 0.0001  max mem: 15850
[20:30:27.526995] Test:  [300/345]  eta: 0:00:07  loss: 0.4159 (0.4149)  time: 0.1775  data: 0.0001  max mem: 15850
[20:30:29.308725] Test:  [310/345]  eta: 0:00:06  loss: 0.4189 (0.4156)  time: 0.1779  data: 0.0001  max mem: 15850
[20:30:31.094543] Test:  [320/345]  eta: 0:00:04  loss: 0.4255 (0.4156)  time: 0.1783  data: 0.0001  max mem: 15850
[20:30:32.882522] Test:  [330/345]  eta: 0:00:02  loss: 0.4024 (0.4151)  time: 0.1786  data: 0.0001  max mem: 15850
[20:30:34.672945] Test:  [340/345]  eta: 0:00:00  loss: 0.3829 (0.4141)  time: 0.1789  data: 0.0001  max mem: 15850
[20:30:35.389711] Test:  [344/345]  eta: 0:00:00  loss: 0.3829 (0.4139)  time: 0.1790  data: 0.0001  max mem: 15850
[20:30:35.454063] Test: Total time: 0:01:00 (0.1746 s / it)
[20:30:45.255413] Test:  [ 0/57]  eta: 0:00:24  loss: 0.5342 (0.5342)  time: 0.4331  data: 0.2696  max mem: 15850
[20:30:46.916158] Test:  [10/57]  eta: 0:00:08  loss: 0.5319 (0.5161)  time: 0.1903  data: 0.0246  max mem: 15850
[20:30:48.581351] Test:  [20/57]  eta: 0:00:06  loss: 0.5319 (0.5115)  time: 0.1662  data: 0.0001  max mem: 15850
[20:30:50.249195] Test:  [30/57]  eta: 0:00:04  loss: 0.3387 (0.4454)  time: 0.1666  data: 0.0001  max mem: 15850
[20:30:51.922159] Test:  [40/57]  eta: 0:00:02  loss: 0.2960 (0.4107)  time: 0.1670  data: 0.0001  max mem: 15850
[20:30:53.599581] Test:  [50/57]  eta: 0:00:01  loss: 0.3320 (0.4022)  time: 0.1675  data: 0.0001  max mem: 15850
[20:30:54.504783] Test:  [56/57]  eta: 0:00:00  loss: 0.3320 (0.4033)  time: 0.1625  data: 0.0000  max mem: 15850
[20:30:54.568688] Test: Total time: 0:00:09 (0.1710 s / it)
[20:30:56.215294] Dice score of the network on the train images: 0.726555, val images: 0.787475
[20:30:56.215497] saving best_rec_model_0 @ epoch 4
[20:30:57.499286] saving best_dice_model_0 @ epoch 4
[20:30:58.693226] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[20:30:59.586925] Epoch: [5]  [  0/345]  eta: 0:05:07  lr: 0.000031  loss: 0.4838 (0.4838)  time: 0.8924  data: 0.2918  max mem: 15850
[20:31:11.540285] Epoch: [5]  [ 20/345]  eta: 0:03:18  lr: 0.000032  loss: 0.4381 (0.4435)  time: 0.5975  data: 0.0001  max mem: 15850
[20:31:23.630722] Epoch: [5]  [ 40/345]  eta: 0:03:05  lr: 0.000032  loss: 0.4032 (0.4305)  time: 0.6045  data: 0.0001  max mem: 15850
[20:31:35.627907] Epoch: [5]  [ 60/345]  eta: 0:02:52  lr: 0.000032  loss: 0.4300 (0.4302)  time: 0.5998  data: 0.0001  max mem: 15850
[20:31:47.631957] Epoch: [5]  [ 80/345]  eta: 0:02:40  lr: 0.000033  loss: 0.4035 (0.4244)  time: 0.6002  data: 0.0001  max mem: 15850
[20:31:59.663289] Epoch: [5]  [100/345]  eta: 0:02:27  lr: 0.000033  loss: 0.4137 (0.4230)  time: 0.6015  data: 0.0001  max mem: 15850
[20:32:11.723105] Epoch: [5]  [120/345]  eta: 0:02:15  lr: 0.000033  loss: 0.4018 (0.4204)  time: 0.6029  data: 0.0001  max mem: 15850
[20:32:23.796410] Epoch: [5]  [140/345]  eta: 0:02:03  lr: 0.000034  loss: 0.3775 (0.4161)  time: 0.6036  data: 0.0001  max mem: 15850
[20:32:35.875979] Epoch: [5]  [160/345]  eta: 0:01:51  lr: 0.000034  loss: 0.3967 (0.4149)  time: 0.6039  data: 0.0001  max mem: 15850
[20:32:47.970847] Epoch: [5]  [180/345]  eta: 0:01:39  lr: 0.000035  loss: 0.4140 (0.4149)  time: 0.6047  data: 0.0001  max mem: 15850
[20:33:00.076742] Epoch: [5]  [200/345]  eta: 0:01:27  lr: 0.000035  loss: 0.3859 (0.4126)  time: 0.6052  data: 0.0001  max mem: 15850
[20:33:12.172104] Epoch: [5]  [220/345]  eta: 0:01:15  lr: 0.000035  loss: 0.3924 (0.4111)  time: 0.6047  data: 0.0001  max mem: 15850
[20:33:24.260817] Epoch: [5]  [240/345]  eta: 0:01:03  lr: 0.000036  loss: 0.4011 (0.4110)  time: 0.6044  data: 0.0001  max mem: 15850
[20:33:36.345140] Epoch: [5]  [260/345]  eta: 0:00:51  lr: 0.000036  loss: 0.4032 (0.4107)  time: 0.6042  data: 0.0001  max mem: 15850
[20:33:48.510378] Epoch: [5]  [280/345]  eta: 0:00:39  lr: 0.000036  loss: 0.3848 (0.4093)  time: 0.6082  data: 0.0001  max mem: 15850
[20:34:00.587880] Epoch: [5]  [300/345]  eta: 0:00:27  lr: 0.000037  loss: 0.3621 (0.4065)  time: 0.6038  data: 0.0001  max mem: 15850
[20:34:12.651913] Epoch: [5]  [320/345]  eta: 0:00:15  lr: 0.000037  loss: 0.3904 (0.4055)  time: 0.6032  data: 0.0001  max mem: 15850
[20:34:24.718363] Epoch: [5]  [340/345]  eta: 0:00:03  lr: 0.000037  loss: 0.3571 (0.4029)  time: 0.6033  data: 0.0001  max mem: 15850
[20:34:27.131625] Epoch: [5]  [344/345]  eta: 0:00:00  lr: 0.000037  loss: 0.3497 (0.4023)  time: 0.6033  data: 0.0001  max mem: 15850
[20:34:27.200415] Epoch: [5] Total time: 0:03:28 (0.6044 s / it)
[20:34:27.201062] Averaged stats: lr: 0.000037  loss: 0.3497 (0.4023)
[20:34:27.720892] Test:  [  0/345]  eta: 0:02:57  loss: 0.3442 (0.3442)  time: 0.5146  data: 0.3495  max mem: 15850
[20:34:29.398136] Test:  [ 10/345]  eta: 0:01:06  loss: 0.3442 (0.3400)  time: 0.1992  data: 0.0319  max mem: 15850
[20:34:31.077810] Test:  [ 20/345]  eta: 0:00:59  loss: 0.3501 (0.3510)  time: 0.1678  data: 0.0001  max mem: 15850
[20:34:32.761673] Test:  [ 30/345]  eta: 0:00:56  loss: 0.3541 (0.3546)  time: 0.1681  data: 0.0001  max mem: 15850
[20:34:34.449158] Test:  [ 40/345]  eta: 0:00:53  loss: 0.3595 (0.3560)  time: 0.1685  data: 0.0001  max mem: 15850
[20:34:36.139685] Test:  [ 50/345]  eta: 0:00:51  loss: 0.3505 (0.3572)  time: 0.1688  data: 0.0001  max mem: 15850
[20:34:37.834278] Test:  [ 60/345]  eta: 0:00:49  loss: 0.3505 (0.3596)  time: 0.1692  data: 0.0001  max mem: 15850
[20:34:39.531026] Test:  [ 70/345]  eta: 0:00:47  loss: 0.3487 (0.3585)  time: 0.1695  data: 0.0001  max mem: 15850
[20:34:41.232103] Test:  [ 80/345]  eta: 0:00:45  loss: 0.3467 (0.3571)  time: 0.1698  data: 0.0001  max mem: 15850
[20:34:42.936219] Test:  [ 90/345]  eta: 0:00:44  loss: 0.3467 (0.3567)  time: 0.1702  data: 0.0001  max mem: 15850
[20:34:44.644380] Test:  [100/345]  eta: 0:00:42  loss: 0.3568 (0.3565)  time: 0.1705  data: 0.0001  max mem: 15850
[20:34:46.354378] Test:  [110/345]  eta: 0:00:40  loss: 0.3568 (0.3557)  time: 0.1708  data: 0.0001  max mem: 15850
[20:34:48.068990] Test:  [120/345]  eta: 0:00:38  loss: 0.3328 (0.3548)  time: 0.1712  data: 0.0001  max mem: 15850
[20:34:49.787202] Test:  [130/345]  eta: 0:00:37  loss: 0.3339 (0.3551)  time: 0.1716  data: 0.0001  max mem: 15850
[20:34:51.508777] Test:  [140/345]  eta: 0:00:35  loss: 0.3448 (0.3544)  time: 0.1719  data: 0.0001  max mem: 15850
[20:34:53.233976] Test:  [150/345]  eta: 0:00:33  loss: 0.3283 (0.3521)  time: 0.1723  data: 0.0001  max mem: 15850
[20:34:54.963176] Test:  [160/345]  eta: 0:00:31  loss: 0.3463 (0.3534)  time: 0.1727  data: 0.0001  max mem: 15850
[20:34:56.696775] Test:  [170/345]  eta: 0:00:30  loss: 0.3656 (0.3541)  time: 0.1731  data: 0.0001  max mem: 15850
[20:34:58.433689] Test:  [180/345]  eta: 0:00:28  loss: 0.3467 (0.3532)  time: 0.1735  data: 0.0001  max mem: 15850
[20:35:00.171968] Test:  [190/345]  eta: 0:00:26  loss: 0.3364 (0.3525)  time: 0.1737  data: 0.0001  max mem: 15850
[20:35:01.913484] Test:  [200/345]  eta: 0:00:25  loss: 0.3403 (0.3528)  time: 0.1739  data: 0.0001  max mem: 15850
[20:35:03.660430] Test:  [210/345]  eta: 0:00:23  loss: 0.3444 (0.3529)  time: 0.1744  data: 0.0001  max mem: 15850
[20:35:05.408748] Test:  [220/345]  eta: 0:00:21  loss: 0.3712 (0.3551)  time: 0.1747  data: 0.0001  max mem: 15850
[20:35:07.160352] Test:  [230/345]  eta: 0:00:19  loss: 0.3491 (0.3548)  time: 0.1749  data: 0.0001  max mem: 15850
[20:35:08.916025] Test:  [240/345]  eta: 0:00:18  loss: 0.3409 (0.3543)  time: 0.1753  data: 0.0001  max mem: 15850
[20:35:10.676620] Test:  [250/345]  eta: 0:00:16  loss: 0.3510 (0.3545)  time: 0.1758  data: 0.0001  max mem: 15850
[20:35:12.440903] Test:  [260/345]  eta: 0:00:14  loss: 0.3305 (0.3535)  time: 0.1762  data: 0.0001  max mem: 15850
[20:35:14.207585] Test:  [270/345]  eta: 0:00:13  loss: 0.3305 (0.3533)  time: 0.1765  data: 0.0001  max mem: 15850
[20:35:15.977420] Test:  [280/345]  eta: 0:00:11  loss: 0.3381 (0.3531)  time: 0.1768  data: 0.0001  max mem: 15850
[20:35:17.751793] Test:  [290/345]  eta: 0:00:09  loss: 0.3489 (0.3530)  time: 0.1771  data: 0.0001  max mem: 15850
[20:35:19.527892] Test:  [300/345]  eta: 0:00:07  loss: 0.3472 (0.3530)  time: 0.1775  data: 0.0001  max mem: 15850
[20:35:21.308388] Test:  [310/345]  eta: 0:00:06  loss: 0.3490 (0.3528)  time: 0.1778  data: 0.0001  max mem: 15850
[20:35:23.092129] Test:  [320/345]  eta: 0:00:04  loss: 0.3493 (0.3525)  time: 0.1781  data: 0.0001  max mem: 15850
[20:35:24.880842] Test:  [330/345]  eta: 0:00:02  loss: 0.3493 (0.3524)  time: 0.1786  data: 0.0001  max mem: 15850
[20:35:26.671177] Test:  [340/345]  eta: 0:00:00  loss: 0.3603 (0.3528)  time: 0.1789  data: 0.0001  max mem: 15850
[20:35:27.388851] Test:  [344/345]  eta: 0:00:00  loss: 0.3634 (0.3528)  time: 0.1790  data: 0.0001  max mem: 15850
[20:35:27.465070] Test: Total time: 0:01:00 (0.1747 s / it)
[20:35:37.298856] Test:  [ 0/57]  eta: 0:00:25  loss: 0.5172 (0.5172)  time: 0.4448  data: 0.2812  max mem: 15850
[20:35:38.958119] Test:  [10/57]  eta: 0:00:08  loss: 0.5044 (0.4921)  time: 0.1912  data: 0.0256  max mem: 15850
[20:35:40.623060] Test:  [20/57]  eta: 0:00:06  loss: 0.4897 (0.4846)  time: 0.1661  data: 0.0001  max mem: 15850
[20:35:42.290909] Test:  [30/57]  eta: 0:00:04  loss: 0.3024 (0.4182)  time: 0.1666  data: 0.0001  max mem: 15850
[20:35:43.963110] Test:  [40/57]  eta: 0:00:02  loss: 0.2773 (0.3846)  time: 0.1669  data: 0.0001  max mem: 15850
[20:35:45.639161] Test:  [50/57]  eta: 0:00:01  loss: 0.2840 (0.3757)  time: 0.1673  data: 0.0001  max mem: 15850
[20:35:46.543590] Test:  [56/57]  eta: 0:00:00  loss: 0.3363 (0.3776)  time: 0.1624  data: 0.0001  max mem: 15850
[20:35:46.613378] Test: Total time: 0:00:09 (0.1712 s / it)
[20:35:48.252861] Dice score of the network on the train images: 0.746365, val images: 0.793446
[20:35:48.253091] saving best_dice_model_0 @ epoch 5
[20:35:49.492852] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[20:35:50.372461] Epoch: [6]  [  0/345]  eta: 0:05:03  lr: 0.000038  loss: 0.3567 (0.3567)  time: 0.8783  data: 0.2764  max mem: 15850
[20:36:02.334768] Epoch: [6]  [ 20/345]  eta: 0:03:18  lr: 0.000038  loss: 0.3752 (0.3748)  time: 0.5981  data: 0.0001  max mem: 15850
[20:36:14.319925] Epoch: [6]  [ 40/345]  eta: 0:03:04  lr: 0.000038  loss: 0.3406 (0.3594)  time: 0.5992  data: 0.0001  max mem: 15850
[20:36:26.312047] Epoch: [6]  [ 60/345]  eta: 0:02:52  lr: 0.000039  loss: 0.3633 (0.3625)  time: 0.5996  data: 0.0001  max mem: 15850
[20:36:38.318738] Epoch: [6]  [ 80/345]  eta: 0:02:39  lr: 0.000039  loss: 0.3597 (0.3628)  time: 0.6003  data: 0.0001  max mem: 15850
[20:36:50.346713] Epoch: [6]  [100/345]  eta: 0:02:27  lr: 0.000039  loss: 0.3564 (0.3623)  time: 0.6014  data: 0.0001  max mem: 15850
[20:37:02.391809] Epoch: [6]  [120/345]  eta: 0:02:15  lr: 0.000040  loss: 0.3471 (0.3601)  time: 0.6022  data: 0.0001  max mem: 15850
[20:37:14.447417] Epoch: [6]  [140/345]  eta: 0:02:03  lr: 0.000040  loss: 0.3351 (0.3581)  time: 0.6027  data: 0.0001  max mem: 15850
[20:37:26.520207] Epoch: [6]  [160/345]  eta: 0:01:51  lr: 0.000040  loss: 0.3644 (0.3592)  time: 0.6036  data: 0.0001  max mem: 15850
[20:37:38.592564] Epoch: [6]  [180/345]  eta: 0:01:39  lr: 0.000041  loss: 0.3397 (0.3585)  time: 0.6036  data: 0.0001  max mem: 15850
[20:37:50.664710] Epoch: [6]  [200/345]  eta: 0:01:27  lr: 0.000041  loss: 0.3673 (0.3591)  time: 0.6036  data: 0.0001  max mem: 15850
[20:38:02.730636] Epoch: [6]  [220/345]  eta: 0:01:15  lr: 0.000041  loss: 0.3306 (0.3565)  time: 0.6033  data: 0.0001  max mem: 15850

[20:38:14.797458] Epoch: [6]  [240/345]  eta: 0:01:03  lr: 0.000042  loss: 0.3209 (0.3547)  time: 0.6033  data: 0.0001  max mem: 15850
[20:38:26.867265] Epoch: [6]  [260/345]  eta: 0:00:51  lr: 0.000042  loss: 0.3348 (0.3538)  time: 0.6034  data: 0.0001  max mem: 15850
[20:38:38.929913] Epoch: [6]  [280/345]  eta: 0:00:39  lr: 0.000043  loss: 0.3380 (0.3529)  time: 0.6031  data: 0.0001  max mem: 15850
[20:38:50.986226] Epoch: [6]  [300/345]  eta: 0:00:27  lr: 0.000043  loss: 0.3439 (0.3525)  time: 0.6028  data: 0.0001  max mem: 15850
[20:39:03.038136] Epoch: [6]  [320/345]  eta: 0:00:15  lr: 0.000043  loss: 0.3321 (0.3514)  time: 0.6025  data: 0.0001  max mem: 15850
[20:39:15.095549] Epoch: [6]  [340/345]  eta: 0:00:03  lr: 0.000044  loss: 0.3424 (0.3513)  time: 0.6028  data: 0.0001  max mem: 15850
[20:39:17.504498] Epoch: [6]  [344/345]  eta: 0:00:00  lr: 0.000044  loss: 0.3424 (0.3510)  time: 0.6028  data: 0.0001  max mem: 15850
[20:39:17.574988] Epoch: [6] Total time: 0:03:28 (0.6031 s / it)
[20:39:17.575250] Averaged stats: lr: 0.000044  loss: 0.3424 (0.3510)
[20:39:18.048839] Test:  [  0/345]  eta: 0:02:41  loss: 0.3653 (0.3653)  time: 0.4678  data: 0.3029  max mem: 15850
[20:39:19.725401] Test:  [ 10/345]  eta: 0:01:05  loss: 0.3444 (0.3445)  time: 0.1949  data: 0.0276  max mem: 15850
[20:39:21.406164] Test:  [ 20/345]  eta: 0:00:59  loss: 0.3235 (0.3327)  time: 0.1678  data: 0.0001  max mem: 15850
[20:39:23.091679] Test:  [ 30/345]  eta: 0:00:55  loss: 0.3174 (0.3388)  time: 0.1682  data: 0.0001  max mem: 15850
[20:39:24.779457] Test:  [ 40/345]  eta: 0:00:53  loss: 0.3324 (0.3330)  time: 0.1685  data: 0.0001  max mem: 15850
[20:39:26.470382] Test:  [ 50/345]  eta: 0:00:51  loss: 0.3178 (0.3309)  time: 0.1689  data: 0.0001  max mem: 15850
[20:39:28.165344] Test:  [ 60/345]  eta: 0:00:49  loss: 0.3178 (0.3278)  time: 0.1692  data: 0.0001  max mem: 15850
[20:39:29.862998] Test:  [ 70/345]  eta: 0:00:47  loss: 0.3082 (0.3263)  time: 0.1696  data: 0.0001  max mem: 15850
[20:39:31.563725] Test:  [ 80/345]  eta: 0:00:45  loss: 0.3042 (0.3238)  time: 0.1699  data: 0.0001  max mem: 15850
[20:39:33.269652] Test:  [ 90/345]  eta: 0:00:43  loss: 0.3078 (0.3241)  time: 0.1703  data: 0.0001  max mem: 15850
[20:39:34.977575] Test:  [100/345]  eta: 0:00:42  loss: 0.3105 (0.3233)  time: 0.1706  data: 0.0001  max mem: 15850
[20:39:36.690037] Test:  [110/345]  eta: 0:00:40  loss: 0.3134 (0.3234)  time: 0.1710  data: 0.0001  max mem: 15850
[20:39:38.404792] Test:  [120/345]  eta: 0:00:38  loss: 0.3194 (0.3235)  time: 0.1713  data: 0.0001  max mem: 15850
[20:39:40.122192] Test:  [130/345]  eta: 0:00:36  loss: 0.3194 (0.3231)  time: 0.1715  data: 0.0001  max mem: 15850
[20:39:41.843285] Test:  [140/345]  eta: 0:00:35  loss: 0.3034 (0.3221)  time: 0.1719  data: 0.0001  max mem: 15850
[20:39:43.568516] Test:  [150/345]  eta: 0:00:33  loss: 0.3035 (0.3233)  time: 0.1723  data: 0.0001  max mem: 15850
[20:39:45.296288] Test:  [160/345]  eta: 0:00:31  loss: 0.3038 (0.3226)  time: 0.1726  data: 0.0001  max mem: 15850
[20:39:47.028367] Test:  [170/345]  eta: 0:00:30  loss: 0.3021 (0.3218)  time: 0.1729  data: 0.0001  max mem: 15850
[20:39:48.764023] Test:  [180/345]  eta: 0:00:28  loss: 0.3129 (0.3217)  time: 0.1733  data: 0.0001  max mem: 15850
[20:39:50.503092] Test:  [190/345]  eta: 0:00:26  loss: 0.3161 (0.3218)  time: 0.1737  data: 0.0001  max mem: 15850
[20:39:52.246046] Test:  [200/345]  eta: 0:00:24  loss: 0.3064 (0.3206)  time: 0.1740  data: 0.0001  max mem: 15850
[20:39:53.992236] Test:  [210/345]  eta: 0:00:23  loss: 0.3141 (0.3214)  time: 0.1744  data: 0.0001  max mem: 15850
[20:39:55.740845] Test:  [220/345]  eta: 0:00:21  loss: 0.3319 (0.3221)  time: 0.1747  data: 0.0001  max mem: 15850
[20:39:57.494340] Test:  [230/345]  eta: 0:00:19  loss: 0.3319 (0.3218)  time: 0.1750  data: 0.0001  max mem: 15850
[20:39:59.249812] Test:  [240/345]  eta: 0:00:18  loss: 0.3354 (0.3219)  time: 0.1754  data: 0.0001  max mem: 15850
[20:40:01.009780] Test:  [250/345]  eta: 0:00:16  loss: 0.3102 (0.3215)  time: 0.1757  data: 0.0001  max mem: 15850
[20:40:02.773581] Test:  [260/345]  eta: 0:00:14  loss: 0.3030 (0.3205)  time: 0.1761  data: 0.0001  max mem: 15850
[20:40:04.539567] Test:  [270/345]  eta: 0:00:12  loss: 0.3060 (0.3208)  time: 0.1764  data: 0.0001  max mem: 15850
[20:40:06.311445] Test:  [280/345]  eta: 0:00:11  loss: 0.3245 (0.3211)  time: 0.1768  data: 0.0001  max mem: 15850
[20:40:08.085117] Test:  [290/345]  eta: 0:00:09  loss: 0.3200 (0.3213)  time: 0.1772  data: 0.0001  max mem: 15850
[20:40:09.861387] Test:  [300/345]  eta: 0:00:07  loss: 0.3161 (0.3208)  time: 0.1774  data: 0.0001  max mem: 15850
[20:40:11.641407] Test:  [310/345]  eta: 0:00:06  loss: 0.3021 (0.3208)  time: 0.1778  data: 0.0001  max mem: 15850
[20:40:13.425437] Test:  [320/345]  eta: 0:00:04  loss: 0.3186 (0.3207)  time: 0.1781  data: 0.0001  max mem: 15850
[20:40:15.213596] Test:  [330/345]  eta: 0:00:02  loss: 0.3186 (0.3209)  time: 0.1786  data: 0.0001  max mem: 15850
[20:40:17.005406] Test:  [340/345]  eta: 0:00:00  loss: 0.3230 (0.3209)  time: 0.1789  data: 0.0001  max mem: 15850
[20:40:17.722026] Test:  [344/345]  eta: 0:00:00  loss: 0.3185 (0.3207)  time: 0.1791  data: 0.0001  max mem: 15850
[20:40:17.789891] Test: Total time: 0:01:00 (0.1745 s / it)
[20:40:27.719269] Test:  [ 0/57]  eta: 0:00:26  loss: 0.5167 (0.5167)  time: 0.4570  data: 0.2930  max mem: 15850
[20:40:29.379307] Test:  [10/57]  eta: 0:00:09  loss: 0.4781 (0.4762)  time: 0.1924  data: 0.0267  max mem: 15850
[20:40:31.044141] Test:  [20/57]  eta: 0:00:06  loss: 0.4781 (0.4747)  time: 0.1662  data: 0.0001  max mem: 15850
[20:40:32.711862] Test:  [30/57]  eta: 0:00:04  loss: 0.3006 (0.4065)  time: 0.1666  data: 0.0001  max mem: 15850
[20:40:34.385171] Test:  [40/57]  eta: 0:00:02  loss: 0.2678 (0.3741)  time: 0.1670  data: 0.0001  max mem: 15850
[20:40:36.061689] Test:  [50/57]  eta: 0:00:01  loss: 0.2763 (0.3660)  time: 0.1674  data: 0.0001  max mem: 15850
[20:40:36.966469] Test:  [56/57]  eta: 0:00:00  loss: 0.2814 (0.3659)  time: 0.1625  data: 0.0000  max mem: 15850
[20:40:37.046782] Test: Total time: 0:00:09 (0.1717 s / it)
[20:40:38.660556] Dice score of the network on the train images: 0.762432, val images: 0.804320
[20:40:38.660783] saving best_prec_model_0 @ epoch 6
[20:40:39.956277] saving best_rec_model_0 @ epoch 6
[20:40:41.149494] saving best_dice_model_0 @ epoch 6
[20:40:42.370474] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[20:40:43.271051] Epoch: [7]  [  0/345]  eta: 0:05:10  lr: 0.000044  loss: 0.3239 (0.3239)  time: 0.8996  data: 0.2993  max mem: 15850
[20:40:55.220873] Epoch: [7]  [ 20/345]  eta: 0:03:18  lr: 0.000044  loss: 0.3221 (0.3264)  time: 0.5974  data: 0.0001  max mem: 15850
[20:41:07.189662] Epoch: [7]  [ 40/345]  eta: 0:03:04  lr: 0.000044  loss: 0.3285 (0.3295)  time: 0.5984  data: 0.0001  max mem: 15850
[20:41:19.185985] Epoch: [7]  [ 60/345]  eta: 0:02:51  lr: 0.000045  loss: 0.3238 (0.3294)  time: 0.5998  data: 0.0001  max mem: 15850
[20:41:31.219756] Epoch: [7]  [ 80/345]  eta: 0:02:39  lr: 0.000045  loss: 0.3334 (0.3303)  time: 0.6016  data: 0.0001  max mem: 15850
[20:41:43.269557] Epoch: [7]  [100/345]  eta: 0:02:27  lr: 0.000046  loss: 0.3118 (0.3263)  time: 0.6024  data: 0.0001  max mem: 15850
[20:41:55.335529] Epoch: [7]  [120/345]  eta: 0:02:15  lr: 0.000046  loss: 0.3276 (0.3280)  time: 0.6033  data: 0.0001  max mem: 15850
[20:42:07.428515] Epoch: [7]  [140/345]  eta: 0:02:03  lr: 0.000046  loss: 0.3163 (0.3271)  time: 0.6046  data: 0.0001  max mem: 15850
[20:42:19.527497] Epoch: [7]  [160/345]  eta: 0:01:51  lr: 0.000047  loss: 0.3365 (0.3275)  time: 0.6049  data: 0.0001  max mem: 15850
[20:42:31.622062] Epoch: [7]  [180/345]  eta: 0:01:39  lr: 0.000047  loss: 0.3115 (0.3267)  time: 0.6047  data: 0.0001  max mem: 15850
[20:42:43.715771] Epoch: [7]  [200/345]  eta: 0:01:27  lr: 0.000047  loss: 0.3106 (0.3258)  time: 0.6046  data: 0.0001  max mem: 15850
[20:42:55.801657] Epoch: [7]  [220/345]  eta: 0:01:15  lr: 0.000048  loss: 0.3394 (0.3262)  time: 0.6042  data: 0.0001  max mem: 15850
[20:43:07.879598] Epoch: [7]  [240/345]  eta: 0:01:03  lr: 0.000048  loss: 0.3062 (0.3249)  time: 0.6039  data: 0.0001  max mem: 15850
[20:43:19.947440] Epoch: [7]  [260/345]  eta: 0:00:51  lr: 0.000048  loss: 0.3000 (0.3235)  time: 0.6033  data: 0.0001  max mem: 15850
[20:43:32.018293] Epoch: [7]  [280/345]  eta: 0:00:39  lr: 0.000049  loss: 0.3101 (0.3235)  time: 0.6035  data: 0.0001  max mem: 15850
[20:43:44.084138] Epoch: [7]  [300/345]  eta: 0:00:27  lr: 0.000049  loss: 0.3010 (0.3221)  time: 0.6032  data: 0.0001  max mem: 15850
[20:43:56.142470] Epoch: [7]  [320/345]  eta: 0:00:15  lr: 0.000050  loss: 0.3075 (0.3209)  time: 0.6029  data: 0.0001  max mem: 15850
[20:44:08.191194] Epoch: [7]  [340/345]  eta: 0:00:03  lr: 0.000050  loss: 0.3191 (0.3208)  time: 0.6024  data: 0.0001  max mem: 15850
[20:44:10.602268] Epoch: [7]  [344/345]  eta: 0:00:00  lr: 0.000050  loss: 0.3121 (0.3204)  time: 0.6025  data: 0.0001  max mem: 15850
[20:44:10.676978] Epoch: [7] Total time: 0:03:28 (0.6038 s / it)
[20:44:10.677297] Averaged stats: lr: 0.000050  loss: 0.3121 (0.3204)
[20:44:11.224473] Test:  [  0/345]  eta: 0:03:06  loss: 0.3410 (0.3410)  time: 0.5419  data: 0.3765  max mem: 15850
[20:44:12.902176] Test:  [ 10/345]  eta: 0:01:07  loss: 0.3041 (0.3153)  time: 0.2017  data: 0.0343  max mem: 15850
[20:44:14.583350] Test:  [ 20/345]  eta: 0:01:00  loss: 0.2978 (0.3069)  time: 0.1679  data: 0.0001  max mem: 15850
[20:44:16.267755] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2754 (0.3006)  time: 0.1682  data: 0.0001  max mem: 15850
[20:44:17.956816] Test:  [ 40/345]  eta: 0:00:54  loss: 0.2890 (0.3008)  time: 0.1686  data: 0.0001  max mem: 15850
[20:44:19.647613] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2912 (0.2978)  time: 0.1689  data: 0.0001  max mem: 15850
[20:44:21.342107] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2912 (0.2999)  time: 0.1692  data: 0.0001  max mem: 15850
[20:44:23.041079] Test:  [ 70/345]  eta: 0:00:47  loss: 0.3046 (0.2989)  time: 0.1696  data: 0.0001  max mem: 15850
[20:44:24.741706] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2884 (0.2979)  time: 0.1699  data: 0.0001  max mem: 15850
[20:44:26.446685] Test:  [ 90/345]  eta: 0:00:44  loss: 0.2817 (0.2966)  time: 0.1702  data: 0.0001  max mem: 15850
[20:44:28.155175] Test:  [100/345]  eta: 0:00:42  loss: 0.2935 (0.2966)  time: 0.1706  data: 0.0001  max mem: 15850
[20:44:29.868731] Test:  [110/345]  eta: 0:00:40  loss: 0.2946 (0.2963)  time: 0.1710  data: 0.0001  max mem: 15850
[20:44:31.584698] Test:  [120/345]  eta: 0:00:38  loss: 0.2910 (0.2948)  time: 0.1714  data: 0.0001  max mem: 15850
[20:44:33.303734] Test:  [130/345]  eta: 0:00:37  loss: 0.2707 (0.2942)  time: 0.1717  data: 0.0001  max mem: 15850
[20:44:35.025775] Test:  [140/345]  eta: 0:00:35  loss: 0.2904 (0.2943)  time: 0.1720  data: 0.0001  max mem: 15850
[20:44:36.752544] Test:  [150/345]  eta: 0:00:33  loss: 0.2913 (0.2950)  time: 0.1724  data: 0.0001  max mem: 15850
[20:44:38.481550] Test:  [160/345]  eta: 0:00:31  loss: 0.2952 (0.2947)  time: 0.1727  data: 0.0001  max mem: 15850
[20:44:40.214722] Test:  [170/345]  eta: 0:00:30  loss: 0.2884 (0.2946)  time: 0.1730  data: 0.0001  max mem: 15850
[20:44:41.952539] Test:  [180/345]  eta: 0:00:28  loss: 0.2879 (0.2944)  time: 0.1735  data: 0.0001  max mem: 15850
[20:44:43.693556] Test:  [190/345]  eta: 0:00:26  loss: 0.2974 (0.2944)  time: 0.1739  data: 0.0001  max mem: 15850
[20:44:45.436567] Test:  [200/345]  eta: 0:00:25  loss: 0.2973 (0.2947)  time: 0.1741  data: 0.0001  max mem: 15850
[20:44:47.182402] Test:  [210/345]  eta: 0:00:23  loss: 0.2972 (0.2948)  time: 0.1744  data: 0.0001  max mem: 15850
[20:44:48.933540] Test:  [220/345]  eta: 0:00:21  loss: 0.3044 (0.2952)  time: 0.1748  data: 0.0001  max mem: 15850
[20:44:50.688045] Test:  [230/345]  eta: 0:00:19  loss: 0.2896 (0.2951)  time: 0.1752  data: 0.0001  max mem: 15850
[20:44:52.444895] Test:  [240/345]  eta: 0:00:18  loss: 0.2846 (0.2946)  time: 0.1755  data: 0.0001  max mem: 15850
[20:44:54.206281] Test:  [250/345]  eta: 0:00:16  loss: 0.2835 (0.2946)  time: 0.1758  data: 0.0001  max mem: 15850
[20:44:55.971667] Test:  [260/345]  eta: 0:00:14  loss: 0.2923 (0.2946)  time: 0.1763  data: 0.0001  max mem: 15850
[20:44:57.738328] Test:  [270/345]  eta: 0:00:13  loss: 0.3002 (0.2947)  time: 0.1765  data: 0.0001  max mem: 15850
[20:44:59.509698] Test:  [280/345]  eta: 0:00:11  loss: 0.3021 (0.2949)  time: 0.1768  data: 0.0001  max mem: 15850
[20:45:01.283270] Test:  [290/345]  eta: 0:00:09  loss: 0.2924 (0.2945)  time: 0.1772  data: 0.0001  max mem: 15850
[20:45:03.060554] Test:  [300/345]  eta: 0:00:07  loss: 0.2750 (0.2937)  time: 0.1775  data: 0.0001  max mem: 15850
[20:45:04.842066] Test:  [310/345]  eta: 0:00:06  loss: 0.2896 (0.2943)  time: 0.1779  data: 0.0001  max mem: 15850
[20:45:06.628418] Test:  [320/345]  eta: 0:00:04  loss: 0.3013 (0.2942)  time: 0.1783  data: 0.0001  max mem: 15850
[20:45:08.417987] Test:  [330/345]  eta: 0:00:02  loss: 0.2814 (0.2937)  time: 0.1787  data: 0.0001  max mem: 15850
[20:45:10.210280] Test:  [340/345]  eta: 0:00:00  loss: 0.2836 (0.2942)  time: 0.1790  data: 0.0001  max mem: 15850
[20:45:10.927266] Test:  [344/345]  eta: 0:00:00  loss: 0.2851 (0.2940)  time: 0.1792  data: 0.0001  max mem: 15850
[20:45:10.992387] Test: Total time: 0:01:00 (0.1748 s / it)
[20:45:20.863738] Test:  [ 0/57]  eta: 0:00:24  loss: 0.4993 (0.4993)  time: 0.4334  data: 0.2697  max mem: 15850
[20:45:22.525353] Test:  [10/57]  eta: 0:00:08  loss: 0.4516 (0.4564)  time: 0.1904  data: 0.0246  max mem: 15850
[20:45:24.192178] Test:  [20/57]  eta: 0:00:06  loss: 0.4516 (0.4505)  time: 0.1663  data: 0.0001  max mem: 15850
[20:45:25.862347] Test:  [30/57]  eta: 0:00:04  loss: 0.2731 (0.3858)  time: 0.1668  data: 0.0001  max mem: 15850
[20:45:27.536164] Test:  [40/57]  eta: 0:00:02  loss: 0.2585 (0.3550)  time: 0.1671  data: 0.0001  max mem: 15850
[20:45:29.213159] Test:  [50/57]  eta: 0:00:01  loss: 0.2620 (0.3474)  time: 0.1675  data: 0.0001  max mem: 15850
[20:45:30.118275] Test:  [56/57]  eta: 0:00:00  loss: 0.2783 (0.3477)  time: 0.1625  data: 0.0000  max mem: 15850
[20:45:30.182758] Test: Total time: 0:00:09 (0.1711 s / it)
[20:45:31.824523] Dice score of the network on the train images: 0.771150, val images: 0.805699
[20:45:31.824758] saving best_rec_model_0 @ epoch 7
[20:45:33.191239] saving best_dice_model_0 @ epoch 7
[20:45:34.406467] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[20:45:35.314613] Epoch: [8]  [  0/345]  eta: 0:05:12  lr: 0.000050  loss: 0.2757 (0.2757)  time: 0.9069  data: 0.3063  max mem: 15850
[20:45:47.258487] Epoch: [8]  [ 20/345]  eta: 0:03:18  lr: 0.000050  loss: 0.3038 (0.3039)  time: 0.5971  data: 0.0001  max mem: 15850
[20:45:59.216562] Epoch: [8]  [ 40/345]  eta: 0:03:04  lr: 0.000051  loss: 0.2976 (0.3035)  time: 0.5979  data: 0.0001  max mem: 15850
[20:46:11.202285] Epoch: [8]  [ 60/345]  eta: 0:02:51  lr: 0.000051  loss: 0.2971 (0.3031)  time: 0.5992  data: 0.0001  max mem: 15850
[20:46:23.207297] Epoch: [8]  [ 80/345]  eta: 0:02:39  lr: 0.000051  loss: 0.2910 (0.3005)  time: 0.6002  data: 0.0001  max mem: 15850
[20:46:35.238001] Epoch: [8]  [100/345]  eta: 0:02:27  lr: 0.000052  loss: 0.2953 (0.3012)  time: 0.6015  data: 0.0001  max mem: 15850
[20:46:47.287956] Epoch: [8]  [120/345]  eta: 0:02:15  lr: 0.000052  loss: 0.3103 (0.3035)  time: 0.6025  data: 0.0001  max mem: 15850
[20:46:59.358700] Epoch: [8]  [140/345]  eta: 0:02:03  lr: 0.000053  loss: 0.2943 (0.3025)  time: 0.6035  data: 0.0001  max mem: 15850

[20:47:11.456894] Epoch: [8]  [160/345]  eta: 0:01:51  lr: 0.000053  loss: 0.2939 (0.3018)  time: 0.6049  data: 0.0001  max mem: 15850
[20:47:23.551570] Epoch: [8]  [180/345]  eta: 0:01:39  lr: 0.000053  loss: 0.3011 (0.3019)  time: 0.6047  data: 0.0001  max mem: 15850
[20:47:35.772629] Epoch: [8]  [200/345]  eta: 0:01:27  lr: 0.000054  loss: 0.2978 (0.3018)  time: 0.6110  data: 0.0001  max mem: 15850
[20:47:47.868407] Epoch: [8]  [220/345]  eta: 0:01:15  lr: 0.000054  loss: 0.2872 (0.3012)  time: 0.6047  data: 0.0001  max mem: 15850
[20:47:59.959532] Epoch: [8]  [240/345]  eta: 0:01:03  lr: 0.000054  loss: 0.2838 (0.2996)  time: 0.6045  data: 0.0001  max mem: 15850
[20:48:12.043657] Epoch: [8]  [260/345]  eta: 0:00:51  lr: 0.000055  loss: 0.3148 (0.3009)  time: 0.6042  data: 0.0001  max mem: 15850
[20:48:24.118461] Epoch: [8]  [280/345]  eta: 0:00:39  lr: 0.000055  loss: 0.2818 (0.3002)  time: 0.6037  data: 0.0001  max mem: 15850
[20:48:36.190165] Epoch: [8]  [300/345]  eta: 0:00:27  lr: 0.000055  loss: 0.3085 (0.3003)  time: 0.6035  data: 0.0001  max mem: 15850
[20:48:48.261824] Epoch: [8]  [320/345]  eta: 0:00:15  lr: 0.000056  loss: 0.2940 (0.3002)  time: 0.6035  data: 0.0001  max mem: 15850
[20:49:00.315888] Epoch: [8]  [340/345]  eta: 0:00:03  lr: 0.000056  loss: 0.2939 (0.3001)  time: 0.6027  data: 0.0001  max mem: 15850
[20:49:02.726867] Epoch: [8]  [344/345]  eta: 0:00:00  lr: 0.000056  loss: 0.2828 (0.2996)  time: 0.6024  data: 0.0001  max mem: 15850
[20:49:02.805102] Epoch: [8] Total time: 0:03:28 (0.6041 s / it)
[20:49:02.805455] Averaged stats: lr: 0.000056  loss: 0.2828 (0.2996)
[20:49:03.288738] Test:  [  0/345]  eta: 0:02:44  loss: 0.2635 (0.2635)  time: 0.4778  data: 0.3115  max mem: 15850
[20:49:04.966466] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2616 (0.2691)  time: 0.1959  data: 0.0284  max mem: 15850
[20:49:06.647605] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2770 (0.2795)  time: 0.1679  data: 0.0001  max mem: 15850
[20:49:08.331774] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2770 (0.2807)  time: 0.1682  data: 0.0001  max mem: 15850
[20:49:10.020117] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2810 (0.2845)  time: 0.1686  data: 0.0001  max mem: 15850
[20:49:11.711675] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2856 (0.2824)  time: 0.1689  data: 0.0001  max mem: 15850
[20:49:13.406392] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2657 (0.2798)  time: 0.1692  data: 0.0001  max mem: 15850
[20:49:15.104362] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2586 (0.2786)  time: 0.1696  data: 0.0001  max mem: 15850
[20:49:16.804720] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2567 (0.2768)  time: 0.1699  data: 0.0001  max mem: 15850
[20:49:18.509880] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2659 (0.2767)  time: 0.1702  data: 0.0001  max mem: 15850
[20:49:20.218206] Test:  [100/345]  eta: 0:00:42  loss: 0.2634 (0.2759)  time: 0.1706  data: 0.0001  max mem: 15850
[20:49:21.930298] Test:  [110/345]  eta: 0:00:40  loss: 0.2695 (0.2772)  time: 0.1710  data: 0.0001  max mem: 15850
[20:49:23.647143] Test:  [120/345]  eta: 0:00:38  loss: 0.2842 (0.2775)  time: 0.1714  data: 0.0001  max mem: 15850
[20:49:25.365905] Test:  [130/345]  eta: 0:00:37  loss: 0.2794 (0.2775)  time: 0.1717  data: 0.0001  max mem: 15850
[20:49:27.087627] Test:  [140/345]  eta: 0:00:35  loss: 0.2846 (0.2780)  time: 0.1720  data: 0.0001  max mem: 15850
[20:49:28.813353] Test:  [150/345]  eta: 0:00:33  loss: 0.2846 (0.2789)  time: 0.1723  data: 0.0001  max mem: 15850
[20:49:30.542729] Test:  [160/345]  eta: 0:00:31  loss: 0.2822 (0.2791)  time: 0.1727  data: 0.0001  max mem: 15850
[20:49:32.277077] Test:  [170/345]  eta: 0:00:30  loss: 0.2822 (0.2789)  time: 0.1731  data: 0.0001  max mem: 15850
[20:49:34.012383] Test:  [180/345]  eta: 0:00:28  loss: 0.2791 (0.2789)  time: 0.1734  data: 0.0001  max mem: 15850
[20:49:35.752843] Test:  [190/345]  eta: 0:00:26  loss: 0.2773 (0.2783)  time: 0.1737  data: 0.0001  max mem: 15850
[20:49:37.497164] Test:  [200/345]  eta: 0:00:25  loss: 0.2755 (0.2787)  time: 0.1742  data: 0.0001  max mem: 15850
[20:49:39.243283] Test:  [210/345]  eta: 0:00:23  loss: 0.2707 (0.2779)  time: 0.1745  data: 0.0001  max mem: 15850
[20:49:40.992204] Test:  [220/345]  eta: 0:00:21  loss: 0.2741 (0.2781)  time: 0.1747  data: 0.0001  max mem: 15850
[20:49:42.747021] Test:  [230/345]  eta: 0:00:19  loss: 0.2899 (0.2790)  time: 0.1751  data: 0.0001  max mem: 15850
[20:49:44.505369] Test:  [240/345]  eta: 0:00:18  loss: 0.2861 (0.2794)  time: 0.1756  data: 0.0001  max mem: 15850
[20:49:46.266745] Test:  [250/345]  eta: 0:00:16  loss: 0.2764 (0.2791)  time: 0.1759  data: 0.0001  max mem: 15850
[20:49:48.031593] Test:  [260/345]  eta: 0:00:14  loss: 0.2768 (0.2799)  time: 0.1762  data: 0.0001  max mem: 15850
[20:49:49.800198] Test:  [270/345]  eta: 0:00:12  loss: 0.2812 (0.2799)  time: 0.1766  data: 0.0001  max mem: 15850
[20:49:51.572591] Test:  [280/345]  eta: 0:00:11  loss: 0.2748 (0.2800)  time: 0.1770  data: 0.0001  max mem: 15850
[20:49:53.347126] Test:  [290/345]  eta: 0:00:09  loss: 0.2797 (0.2806)  time: 0.1773  data: 0.0001  max mem: 15850
[20:49:55.126237] Test:  [300/345]  eta: 0:00:07  loss: 0.2760 (0.2804)  time: 0.1776  data: 0.0001  max mem: 15850
[20:49:56.907986] Test:  [310/345]  eta: 0:00:06  loss: 0.2633 (0.2803)  time: 0.1780  data: 0.0001  max mem: 15850
[20:49:58.695189] Test:  [320/345]  eta: 0:00:04  loss: 0.2691 (0.2800)  time: 0.1784  data: 0.0001  max mem: 15850
[20:50:00.483604] Test:  [330/345]  eta: 0:00:02  loss: 0.2691 (0.2797)  time: 0.1787  data: 0.0001  max mem: 15850
[20:50:02.278195] Test:  [340/345]  eta: 0:00:00  loss: 0.2670 (0.2794)  time: 0.1791  data: 0.0001  max mem: 15850
[20:50:02.996616] Test:  [344/345]  eta: 0:00:00  loss: 0.2673 (0.2795)  time: 0.1793  data: 0.0001  max mem: 15850
[20:50:03.069768] Test: Total time: 0:01:00 (0.1747 s / it)
[20:50:12.842392] Test:  [ 0/57]  eta: 0:00:24  loss: 0.5284 (0.5284)  time: 0.4373  data: 0.2729  max mem: 15850
[20:50:14.503157] Test:  [10/57]  eta: 0:00:08  loss: 0.4775 (0.4661)  time: 0.1906  data: 0.0249  max mem: 15850
[20:50:16.170279] Test:  [20/57]  eta: 0:00:06  loss: 0.4702 (0.4544)  time: 0.1663  data: 0.0001  max mem: 15850
[20:50:17.840314] Test:  [30/57]  eta: 0:00:04  loss: 0.2685 (0.3894)  time: 0.1668  data: 0.0001  max mem: 15850
[20:50:19.513901] Test:  [40/57]  eta: 0:00:02  loss: 0.2527 (0.3595)  time: 0.1671  data: 0.0001  max mem: 15850
[20:50:21.191676] Test:  [50/57]  eta: 0:00:01  loss: 0.2654 (0.3513)  time: 0.1675  data: 0.0001  max mem: 15850
[20:50:22.096631] Test:  [56/57]  eta: 0:00:00  loss: 0.2909 (0.3544)  time: 0.1626  data: 0.0000  max mem: 15850
[20:50:22.167794] Test: Total time: 0:00:09 (0.1713 s / it)
[20:50:23.797204] Dice score of the network on the train images: 0.773819, val images: 0.806072
[20:50:23.797453] saving best_rec_model_0 @ epoch 8
[20:50:25.097329] saving best_dice_model_0 @ epoch 8
[20:50:26.421509] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[20:50:27.333473] Epoch: [9]  [  0/345]  eta: 0:05:14  lr: 0.000056  loss: 0.2958 (0.2958)  time: 0.9109  data: 0.3120  max mem: 15850
[20:50:39.269987] Epoch: [9]  [ 20/345]  eta: 0:03:18  lr: 0.000057  loss: 0.2641 (0.2720)  time: 0.5968  data: 0.0001  max mem: 15850
[20:50:51.230271] Epoch: [9]  [ 40/345]  eta: 0:03:04  lr: 0.000057  loss: 0.2791 (0.2753)  time: 0.5980  data: 0.0001  max mem: 15850
[20:51:03.225381] Epoch: [9]  [ 60/345]  eta: 0:02:51  lr: 0.000057  loss: 0.2715 (0.2768)  time: 0.5997  data: 0.0001  max mem: 15850
[20:51:15.237916] Epoch: [9]  [ 80/345]  eta: 0:02:39  lr: 0.000058  loss: 0.2731 (0.2757)  time: 0.6006  data: 0.0001  max mem: 15850
[20:51:27.268208] Epoch: [9]  [100/345]  eta: 0:02:27  lr: 0.000058  loss: 0.2951 (0.2782)  time: 0.6015  data: 0.0001  max mem: 15850
[20:51:39.317589] Epoch: [9]  [120/345]  eta: 0:02:15  lr: 0.000058  loss: 0.2857 (0.2802)  time: 0.6024  data: 0.0001  max mem: 15850
[20:51:51.388912] Epoch: [9]  [140/345]  eta: 0:02:03  lr: 0.000059  loss: 0.2657 (0.2788)  time: 0.6035  data: 0.0001  max mem: 15850
[20:52:03.464947] Epoch: [9]  [160/345]  eta: 0:01:51  lr: 0.000059  loss: 0.2745 (0.2797)  time: 0.6038  data: 0.0001  max mem: 15850
[20:52:15.547726] Epoch: [9]  [180/345]  eta: 0:01:39  lr: 0.000060  loss: 0.2696 (0.2788)  time: 0.6041  data: 0.0001  max mem: 15850
[20:52:27.631009] Epoch: [9]  [200/345]  eta: 0:01:27  lr: 0.000060  loss: 0.2706 (0.2775)  time: 0.6041  data: 0.0001  max mem: 15850
[20:52:39.711146] Epoch: [9]  [220/345]  eta: 0:01:15  lr: 0.000060  loss: 0.2751 (0.2790)  time: 0.6040  data: 0.0001  max mem: 15850
[20:52:51.785313] Epoch: [9]  [240/345]  eta: 0:01:03  lr: 0.000061  loss: 0.2954 (0.2798)  time: 0.6037  data: 0.0001  max mem: 15850

[20:53:03.858008] Epoch: [9]  [260/345]  eta: 0:00:51  lr: 0.000061  loss: 0.2718 (0.2798)  time: 0.6036  data: 0.0001  max mem: 15850
[20:53:15.922503] Epoch: [9]  [280/345]  eta: 0:00:39  lr: 0.000061  loss: 0.2667 (0.2789)  time: 0.6032  data: 0.0001  max mem: 15850
[20:53:27.983088] Epoch: [9]  [300/345]  eta: 0:00:27  lr: 0.000062  loss: 0.2730 (0.2790)  time: 0.6030  data: 0.0001  max mem: 15850
[20:53:40.040764] Epoch: [9]  [320/345]  eta: 0:00:15  lr: 0.000062  loss: 0.2914 (0.2799)  time: 0.6028  data: 0.0001  max mem: 15850
[20:53:52.089391] Epoch: [9]  [340/345]  eta: 0:00:03  lr: 0.000062  loss: 0.2730 (0.2800)  time: 0.6024  data: 0.0001  max mem: 15850
[20:53:54.498943] Epoch: [9]  [344/345]  eta: 0:00:00  lr: 0.000062  loss: 0.2730 (0.2800)  time: 0.6023  data: 0.0001  max mem: 15850
[20:53:54.568017] Epoch: [9] Total time: 0:03:28 (0.6033 s / it)
[20:53:54.568736] Averaged stats: lr: 0.000062  loss: 0.2730 (0.2800)
[20:53:55.034401] Test:  [  0/345]  eta: 0:02:39  loss: 0.2382 (0.2382)  time: 0.4616  data: 0.2961  max mem: 15850
[20:53:56.713612] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2583 (0.2641)  time: 0.1945  data: 0.0270  max mem: 15850
[20:53:58.395636] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2544 (0.2614)  time: 0.1680  data: 0.0001  max mem: 15850
[20:54:00.079212] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2544 (0.2594)  time: 0.1682  data: 0.0001  max mem: 15850
[20:54:01.768951] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2589 (0.2585)  time: 0.1686  data: 0.0001  max mem: 15850
[20:54:03.460321] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2604 (0.2611)  time: 0.1690  data: 0.0001  max mem: 15850
[20:54:05.155126] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2652 (0.2620)  time: 0.1692  data: 0.0001  max mem: 15850
[20:54:06.853384] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2607 (0.2614)  time: 0.1696  data: 0.0001  max mem: 15850
[20:54:08.554164] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2511 (0.2603)  time: 0.1699  data: 0.0001  max mem: 15850
[20:54:10.259981] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2588 (0.2616)  time: 0.1703  data: 0.0001  max mem: 15850
[20:54:11.968918] Test:  [100/345]  eta: 0:00:42  loss: 0.2736 (0.2628)  time: 0.1707  data: 0.0001  max mem: 15850
[20:54:13.681473] Test:  [110/345]  eta: 0:00:40  loss: 0.2738 (0.2637)  time: 0.1710  data: 0.0001  max mem: 15850
[20:54:15.396403] Test:  [120/345]  eta: 0:00:38  loss: 0.2673 (0.2641)  time: 0.1713  data: 0.0001  max mem: 15850
[20:54:17.115879] Test:  [130/345]  eta: 0:00:36  loss: 0.2567 (0.2637)  time: 0.1717  data: 0.0001  max mem: 15850
[20:54:18.839724] Test:  [140/345]  eta: 0:00:35  loss: 0.2535 (0.2633)  time: 0.1721  data: 0.0001  max mem: 15850
[20:54:20.566538] Test:  [150/345]  eta: 0:00:33  loss: 0.2568 (0.2633)  time: 0.1725  data: 0.0001  max mem: 15850
[20:54:22.296307] Test:  [160/345]  eta: 0:00:31  loss: 0.2645 (0.2627)  time: 0.1728  data: 0.0001  max mem: 15850
[20:54:24.030181] Test:  [170/345]  eta: 0:00:30  loss: 0.2672 (0.2631)  time: 0.1731  data: 0.0001  max mem: 15850
[20:54:25.767691] Test:  [180/345]  eta: 0:00:28  loss: 0.2674 (0.2627)  time: 0.1735  data: 0.0001  max mem: 15850
[20:54:27.508388] Test:  [190/345]  eta: 0:00:26  loss: 0.2677 (0.2638)  time: 0.1738  data: 0.0001  max mem: 15850
[20:54:29.252712] Test:  [200/345]  eta: 0:00:25  loss: 0.2526 (0.2633)  time: 0.1742  data: 0.0001  max mem: 15850
[20:54:31.000737] Test:  [210/345]  eta: 0:00:23  loss: 0.2497 (0.2628)  time: 0.1746  data: 0.0001  max mem: 15850
[20:54:32.751204] Test:  [220/345]  eta: 0:00:21  loss: 0.2536 (0.2627)  time: 0.1749  data: 0.0001  max mem: 15850
[20:54:34.506061] Test:  [230/345]  eta: 0:00:19  loss: 0.2441 (0.2621)  time: 0.1752  data: 0.0001  max mem: 15850
[20:54:36.265226] Test:  [240/345]  eta: 0:00:18  loss: 0.2412 (0.2612)  time: 0.1756  data: 0.0001  max mem: 15850
[20:54:38.026905] Test:  [250/345]  eta: 0:00:16  loss: 0.2442 (0.2616)  time: 0.1760  data: 0.0001  max mem: 15850
[20:54:39.791524] Test:  [260/345]  eta: 0:00:14  loss: 0.2524 (0.2611)  time: 0.1763  data: 0.0001  max mem: 15850
[20:54:41.561560] Test:  [270/345]  eta: 0:00:12  loss: 0.2524 (0.2612)  time: 0.1767  data: 0.0001  max mem: 15850
[20:54:43.332009] Test:  [280/345]  eta: 0:00:11  loss: 0.2682 (0.2614)  time: 0.1770  data: 0.0001  max mem: 15850
[20:54:45.106508] Test:  [290/345]  eta: 0:00:09  loss: 0.2460 (0.2603)  time: 0.1772  data: 0.0001  max mem: 15850
[20:54:46.886527] Test:  [300/345]  eta: 0:00:07  loss: 0.2478 (0.2610)  time: 0.1777  data: 0.0001  max mem: 15850
[20:54:48.668504] Test:  [310/345]  eta: 0:00:06  loss: 0.2500 (0.2603)  time: 0.1780  data: 0.0001  max mem: 15850
[20:54:50.453812] Test:  [320/345]  eta: 0:00:04  loss: 0.2433 (0.2602)  time: 0.1783  data: 0.0001  max mem: 15850
[20:54:52.244724] Test:  [330/345]  eta: 0:00:02  loss: 0.2494 (0.2602)  time: 0.1788  data: 0.0001  max mem: 15850
[20:54:54.037115] Test:  [340/345]  eta: 0:00:00  loss: 0.2667 (0.2606)  time: 0.1791  data: 0.0001  max mem: 15850
[20:54:54.754304] Test:  [344/345]  eta: 0:00:00  loss: 0.2691 (0.2607)  time: 0.1792  data: 0.0001  max mem: 15850
[20:54:54.834282] Test: Total time: 0:01:00 (0.1747 s / it)
[20:55:04.863243] Test:  [ 0/57]  eta: 0:00:28  loss: 0.5542 (0.5542)  time: 0.4941  data: 0.3304  max mem: 15850
[20:55:06.523325] Test:  [10/57]  eta: 0:00:09  loss: 0.4513 (0.4669)  time: 0.1958  data: 0.0301  max mem: 15850
[20:55:08.189291] Test:  [20/57]  eta: 0:00:06  loss: 0.4638 (0.4636)  time: 0.1662  data: 0.0001  max mem: 15850
[20:55:09.858061] Test:  [30/57]  eta: 0:00:04  loss: 0.2797 (0.3951)  time: 0.1667  data: 0.0001  max mem: 15850
[20:55:11.531504] Test:  [40/57]  eta: 0:00:02  loss: 0.2572 (0.3638)  time: 0.1670  data: 0.0001  max mem: 15850
[20:55:13.209951] Test:  [50/57]  eta: 0:00:01  loss: 0.2680 (0.3554)  time: 0.1675  data: 0.0001  max mem: 15850
[20:55:14.115526] Test:  [56/57]  eta: 0:00:00  loss: 0.2775 (0.3536)  time: 0.1626  data: 0.0001  max mem: 15850
[20:55:14.185290] Test: Total time: 0:00:09 (0.1722 s / it)
[20:55:15.829509] Dice score of the network on the train images: 0.800151, val images: 0.815619
[20:55:15.829738] saving best_prec_model_0 @ epoch 9
[20:55:17.046903] saving best_dice_model_0 @ epoch 9
[20:55:18.251712] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[20:55:19.132458] Epoch: [10]  [  0/345]  eta: 0:05:03  lr: 0.000063  loss: 0.2307 (0.2307)  time: 0.8796  data: 0.2802  max mem: 15850
[20:55:31.075568] Epoch: [10]  [ 20/345]  eta: 0:03:18  lr: 0.000063  loss: 0.2601 (0.2652)  time: 0.5971  data: 0.0001  max mem: 15850
[20:55:43.039071] Epoch: [10]  [ 40/345]  eta: 0:03:04  lr: 0.000063  loss: 0.2642 (0.2655)  time: 0.5981  data: 0.0001  max mem: 15850
[20:55:55.029096] Epoch: [10]  [ 60/345]  eta: 0:02:51  lr: 0.000064  loss: 0.2637 (0.2655)  time: 0.5995  data: 0.0001  max mem: 15850
[20:56:07.039420] Epoch: [10]  [ 80/345]  eta: 0:02:39  lr: 0.000064  loss: 0.2533 (0.2625)  time: 0.6005  data: 0.0001  max mem: 15850
[20:56:19.081838] Epoch: [10]  [100/345]  eta: 0:02:27  lr: 0.000064  loss: 0.2548 (0.2637)  time: 0.6021  data: 0.0001  max mem: 15850
[20:56:31.249073] Epoch: [10]  [120/345]  eta: 0:02:15  lr: 0.000065  loss: 0.2641 (0.2632)  time: 0.6083  data: 0.0001  max mem: 15850
[20:56:43.318280] Epoch: [10]  [140/345]  eta: 0:02:03  lr: 0.000065  loss: 0.2677 (0.2636)  time: 0.6034  data: 0.0001  max mem: 15850
[20:56:55.413346] Epoch: [10]  [160/345]  eta: 0:01:51  lr: 0.000065  loss: 0.2663 (0.2648)  time: 0.6047  data: 0.0001  max mem: 15850
[20:57:07.514617] Epoch: [10]  [180/345]  eta: 0:01:39  lr: 0.000066  loss: 0.2751 (0.2662)  time: 0.6050  data: 0.0001  max mem: 15850
[20:57:19.615898] Epoch: [10]  [200/345]  eta: 0:01:27  lr: 0.000066  loss: 0.2588 (0.2659)  time: 0.6050  data: 0.0001  max mem: 15850
[20:57:31.714597] Epoch: [10]  [220/345]  eta: 0:01:15  lr: 0.000066  loss: 0.2637 (0.2656)  time: 0.6049  data: 0.0001  max mem: 15850
[20:57:43.805932] Epoch: [10]  [240/345]  eta: 0:01:03  lr: 0.000067  loss: 0.2573 (0.2654)  time: 0.6045  data: 0.0001  max mem: 15850
[20:57:55.890093] Epoch: [10]  [260/345]  eta: 0:00:51  lr: 0.000067  loss: 0.2546 (0.2653)  time: 0.6042  data: 0.0001  max mem: 15850
[20:58:07.969028] Epoch: [10]  [280/345]  eta: 0:00:39  lr: 0.000068  loss: 0.2790 (0.2660)  time: 0.6039  data: 0.0001  max mem: 15850
[20:58:20.028075] Epoch: [10]  [300/345]  eta: 0:00:27  lr: 0.000068  loss: 0.2616 (0.2660)  time: 0.6029  data: 0.0001  max mem: 15850
[20:58:32.099449] Epoch: [10]  [320/345]  eta: 0:00:15  lr: 0.000068  loss: 0.2621 (0.2658)  time: 0.6035  data: 0.0001  max mem: 15850
[20:58:44.163921] Epoch: [10]  [340/345]  eta: 0:00:03  lr: 0.000069  loss: 0.2551 (0.2652)  time: 0.6032  data: 0.0001  max mem: 15850
[20:58:46.576529] Epoch: [10]  [344/345]  eta: 0:00:00  lr: 0.000069  loss: 0.2620 (0.2653)  time: 0.6030  data: 0.0001  max mem: 15850
[20:58:46.650086] Epoch: [10] Total time: 0:03:28 (0.6041 s / it)
[20:58:46.650316] Averaged stats: lr: 0.000069  loss: 0.2620 (0.2653)
[20:58:47.181685] Test:  [  0/345]  eta: 0:03:01  loss: 0.2969 (0.2969)  time: 0.5261  data: 0.3605  max mem: 15850
[20:58:48.859399] Test:  [ 10/345]  eta: 0:01:07  loss: 0.2398 (0.2494)  time: 0.2003  data: 0.0328  max mem: 15850
[20:58:50.541292] Test:  [ 20/345]  eta: 0:01:00  loss: 0.2366 (0.2466)  time: 0.1679  data: 0.0001  max mem: 15850
[20:58:52.226324] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2297 (0.2388)  time: 0.1683  data: 0.0001  max mem: 15850
[20:58:53.913375] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2332 (0.2390)  time: 0.1685  data: 0.0001  max mem: 15850
[20:58:55.605562] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2380 (0.2403)  time: 0.1689  data: 0.0001  max mem: 15850
[20:58:57.300526] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2194 (0.2375)  time: 0.1693  data: 0.0001  max mem: 15850
[20:58:58.998034] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2222 (0.2379)  time: 0.1696  data: 0.0001  max mem: 15850
[20:59:00.699833] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2373 (0.2379)  time: 0.1699  data: 0.0001  max mem: 15850
[20:59:02.404871] Test:  [ 90/345]  eta: 0:00:44  loss: 0.2303 (0.2372)  time: 0.1703  data: 0.0001  max mem: 15850
[20:59:04.112863] Test:  [100/345]  eta: 0:00:42  loss: 0.2345 (0.2373)  time: 0.1706  data: 0.0001  max mem: 15850
[20:59:05.826182] Test:  [110/345]  eta: 0:00:40  loss: 0.2416 (0.2387)  time: 0.1710  data: 0.0001  max mem: 15850
[20:59:07.541165] Test:  [120/345]  eta: 0:00:38  loss: 0.2449 (0.2381)  time: 0.1714  data: 0.0001  max mem: 15850
[20:59:09.258973] Test:  [130/345]  eta: 0:00:37  loss: 0.2245 (0.2374)  time: 0.1716  data: 0.0001  max mem: 15850
[20:59:10.981850] Test:  [140/345]  eta: 0:00:35  loss: 0.2305 (0.2379)  time: 0.1720  data: 0.0001  max mem: 15850
[20:59:12.708881] Test:  [150/345]  eta: 0:00:33  loss: 0.2332 (0.2378)  time: 0.1724  data: 0.0001  max mem: 15850
[20:59:14.437872] Test:  [160/345]  eta: 0:00:31  loss: 0.2356 (0.2382)  time: 0.1727  data: 0.0001  max mem: 15850
[20:59:16.172069] Test:  [170/345]  eta: 0:00:30  loss: 0.2383 (0.2386)  time: 0.1731  data: 0.0001  max mem: 15850
[20:59:17.909699] Test:  [180/345]  eta: 0:00:28  loss: 0.2259 (0.2384)  time: 0.1735  data: 0.0001  max mem: 15850
[20:59:19.650486] Test:  [190/345]  eta: 0:00:26  loss: 0.2315 (0.2385)  time: 0.1739  data: 0.0001  max mem: 15850
[20:59:21.394562] Test:  [200/345]  eta: 0:00:25  loss: 0.2447 (0.2387)  time: 0.1742  data: 0.0001  max mem: 15850
[20:59:23.141517] Test:  [210/345]  eta: 0:00:23  loss: 0.2389 (0.2385)  time: 0.1745  data: 0.0001  max mem: 15850
[20:59:24.891506] Test:  [220/345]  eta: 0:00:21  loss: 0.2416 (0.2389)  time: 0.1748  data: 0.0001  max mem: 15850
[20:59:26.645032] Test:  [230/345]  eta: 0:00:19  loss: 0.2451 (0.2393)  time: 0.1751  data: 0.0001  max mem: 15850
[20:59:28.402260] Test:  [240/345]  eta: 0:00:18  loss: 0.2435 (0.2396)  time: 0.1755  data: 0.0001  max mem: 15850
[20:59:30.163894] Test:  [250/345]  eta: 0:00:16  loss: 0.2550 (0.2406)  time: 0.1759  data: 0.0001  max mem: 15850
[20:59:31.927984] Test:  [260/345]  eta: 0:00:14  loss: 0.2533 (0.2405)  time: 0.1762  data: 0.0001  max mem: 15850
[20:59:33.695262] Test:  [270/345]  eta: 0:00:13  loss: 0.2291 (0.2405)  time: 0.1765  data: 0.0001  max mem: 15850
[20:59:35.468348] Test:  [280/345]  eta: 0:00:11  loss: 0.2373 (0.2407)  time: 0.1770  data: 0.0001  max mem: 15850
[20:59:37.243461] Test:  [290/345]  eta: 0:00:09  loss: 0.2413 (0.2415)  time: 0.1773  data: 0.0001  max mem: 15850
[20:59:39.021155] Test:  [300/345]  eta: 0:00:07  loss: 0.2472 (0.2422)  time: 0.1776  data: 0.0001  max mem: 15850
[20:59:40.802911] Test:  [310/345]  eta: 0:00:06  loss: 0.2446 (0.2421)  time: 0.1779  data: 0.0001  max mem: 15850
[20:59:42.587787] Test:  [320/345]  eta: 0:00:04  loss: 0.2459 (0.2425)  time: 0.1783  data: 0.0001  max mem: 15850
[20:59:44.375837] Test:  [330/345]  eta: 0:00:02  loss: 0.2507 (0.2424)  time: 0.1786  data: 0.0001  max mem: 15850
[20:59:46.170127] Test:  [340/345]  eta: 0:00:00  loss: 0.2358 (0.2420)  time: 0.1791  data: 0.0001  max mem: 15850
[20:59:46.887770] Test:  [344/345]  eta: 0:00:00  loss: 0.2358 (0.2422)  time: 0.1792  data: 0.0001  max mem: 15850
[20:59:46.951491] Test: Total time: 0:01:00 (0.1748 s / it)
[20:59:56.865181] Test:  [ 0/57]  eta: 0:00:24  loss: 0.5146 (0.5146)  time: 0.4369  data: 0.2731  max mem: 15850
[20:59:58.527427] Test:  [10/57]  eta: 0:00:08  loss: 0.4349 (0.4544)  time: 0.1908  data: 0.0249  max mem: 15850
[21:00:00.191761] Test:  [20/57]  eta: 0:00:06  loss: 0.4349 (0.4472)  time: 0.1663  data: 0.0001  max mem: 15850
[21:00:01.860936] Test:  [30/57]  eta: 0:00:04  loss: 0.2823 (0.3820)  time: 0.1666  data: 0.0001  max mem: 15850
[21:00:03.534592] Test:  [40/57]  eta: 0:00:02  loss: 0.2546 (0.3515)  time: 0.1671  data: 0.0001  max mem: 15850
[21:00:05.212159] Test:  [50/57]  eta: 0:00:01  loss: 0.2730 (0.3443)  time: 0.1675  data: 0.0001  max mem: 15850
[21:00:06.117393] Test:  [56/57]  eta: 0:00:00  loss: 0.3070 (0.3501)  time: 0.1626  data: 0.0000  max mem: 15850
[21:00:06.194962] Test: Total time: 0:00:09 (0.1714 s / it)
[21:00:07.833285] Dice score of the network on the train images: 0.798200, val images: 0.797861
[21:00:07.838570] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:00:08.715132] Epoch: [11]  [  0/345]  eta: 0:05:02  lr: 0.000069  loss: 0.2882 (0.2882)  time: 0.8754  data: 0.2733  max mem: 15850
[21:00:20.696855] Epoch: [11]  [ 20/345]  eta: 0:03:18  lr: 0.000069  loss: 0.2719 (0.2712)  time: 0.5990  data: 0.0001  max mem: 15850
[21:00:32.683426] Epoch: [11]  [ 40/345]  eta: 0:03:04  lr: 0.000069  loss: 0.2553 (0.2626)  time: 0.5993  data: 0.0001  max mem: 15850
[21:00:44.687200] Epoch: [11]  [ 60/345]  eta: 0:02:52  lr: 0.000070  loss: 0.2450 (0.2579)  time: 0.6001  data: 0.0001  max mem: 15850
[21:00:56.721623] Epoch: [11]  [ 80/345]  eta: 0:02:39  lr: 0.000070  loss: 0.2476 (0.2565)  time: 0.6017  data: 0.0001  max mem: 15850
[21:01:08.768409] Epoch: [11]  [100/345]  eta: 0:02:27  lr: 0.000071  loss: 0.2422 (0.2565)  time: 0.6023  data: 0.0001  max mem: 15850
[21:01:20.828677] Epoch: [11]  [120/345]  eta: 0:02:15  lr: 0.000071  loss: 0.2633 (0.2578)  time: 0.6030  data: 0.0001  max mem: 15850
[21:01:32.901843] Epoch: [11]  [140/345]  eta: 0:02:03  lr: 0.000071  loss: 0.2704 (0.2596)  time: 0.6036  data: 0.0001  max mem: 15850
[21:01:44.978896] Epoch: [11]  [160/345]  eta: 0:01:51  lr: 0.000072  loss: 0.2471 (0.2591)  time: 0.6038  data: 0.0001  max mem: 15850
[21:01:57.061082] Epoch: [11]  [180/345]  eta: 0:01:39  lr: 0.000072  loss: 0.2591 (0.2591)  time: 0.6041  data: 0.0001  max mem: 15850
[21:02:09.144821] Epoch: [11]  [200/345]  eta: 0:01:27  lr: 0.000072  loss: 0.2500 (0.2589)  time: 0.6041  data: 0.0001  max mem: 15850
[21:02:21.231066] Epoch: [11]  [220/345]  eta: 0:01:15  lr: 0.000073  loss: 0.2376 (0.2570)  time: 0.6043  data: 0.0001  max mem: 15850
[21:02:33.320466] Epoch: [11]  [240/345]  eta: 0:01:03  lr: 0.000073  loss: 0.2359 (0.2555)  time: 0.6044  data: 0.0001  max mem: 15850
[21:02:45.487840] Epoch: [11]  [260/345]  eta: 0:00:51  lr: 0.000073  loss: 0.2516 (0.2554)  time: 0.6083  data: 0.0001  max mem: 15850
[21:02:57.571069] Epoch: [11]  [280/345]  eta: 0:00:39  lr: 0.000074  loss: 0.2500 (0.2555)  time: 0.6041  data: 0.0001  max mem: 15850
[21:03:09.651753] Epoch: [11]  [300/345]  eta: 0:00:27  lr: 0.000074  loss: 0.2547 (0.2550)  time: 0.6040  data: 0.0001  max mem: 15850
[21:03:21.731797] Epoch: [11]  [320/345]  eta: 0:00:15  lr: 0.000075  loss: 0.2367 (0.2541)  time: 0.6040  data: 0.0001  max mem: 15850
[21:03:33.782787] Epoch: [11]  [340/345]  eta: 0:00:03  lr: 0.000075  loss: 0.2567 (0.2540)  time: 0.6025  data: 0.0001  max mem: 15850
[21:03:36.196961] Epoch: [11]  [344/345]  eta: 0:00:00  lr: 0.000075  loss: 0.2575 (0.2540)  time: 0.6024  data: 0.0001  max mem: 15850
[21:03:36.265283] Epoch: [11] Total time: 0:03:28 (0.6041 s / it)
[21:03:36.265723] Averaged stats: lr: 0.000075  loss: 0.2575 (0.2540)
[21:03:36.749173] Test:  [  0/345]  eta: 0:02:44  loss: 0.2105 (0.2105)  time: 0.4776  data: 0.3117  max mem: 15850
[21:03:38.427659] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2191 (0.2250)  time: 0.1959  data: 0.0284  max mem: 15850
[21:03:40.109360] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2283 (0.2282)  time: 0.1679  data: 0.0001  max mem: 15850
[21:03:41.794040] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2283 (0.2294)  time: 0.1683  data: 0.0001  max mem: 15850
[21:03:43.482769] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2253 (0.2309)  time: 0.1686  data: 0.0001  max mem: 15850
[21:03:45.174393] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2270 (0.2308)  time: 0.1690  data: 0.0001  max mem: 15850
[21:03:46.869256] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2392 (0.2317)  time: 0.1693  data: 0.0001  max mem: 15850
[21:03:48.567339] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2400 (0.2325)  time: 0.1696  data: 0.0001  max mem: 15850
[21:03:50.269036] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2401 (0.2330)  time: 0.1699  data: 0.0001  max mem: 15850
[21:03:51.974368] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2271 (0.2328)  time: 0.1703  data: 0.0001  max mem: 15850
[21:03:53.682539] Test:  [100/345]  eta: 0:00:42  loss: 0.2381 (0.2341)  time: 0.1706  data: 0.0001  max mem: 15850
[21:03:55.394735] Test:  [110/345]  eta: 0:00:40  loss: 0.2406 (0.2343)  time: 0.1709  data: 0.0001  max mem: 15850
[21:03:57.110443] Test:  [120/345]  eta: 0:00:38  loss: 0.2267 (0.2342)  time: 0.1713  data: 0.0001  max mem: 15850
[21:03:58.830658] Test:  [130/345]  eta: 0:00:37  loss: 0.2402 (0.2344)  time: 0.1717  data: 0.0001  max mem: 15850
[21:04:00.553695] Test:  [140/345]  eta: 0:00:35  loss: 0.2302 (0.2342)  time: 0.1721  data: 0.0001  max mem: 15850
[21:04:02.279893] Test:  [150/345]  eta: 0:00:33  loss: 0.2302 (0.2346)  time: 0.1724  data: 0.0001  max mem: 15850
[21:04:04.010236] Test:  [160/345]  eta: 0:00:31  loss: 0.2457 (0.2358)  time: 0.1728  data: 0.0001  max mem: 15850
[21:04:05.744992] Test:  [170/345]  eta: 0:00:30  loss: 0.2508 (0.2370)  time: 0.1732  data: 0.0001  max mem: 15850
[21:04:07.482009] Test:  [180/345]  eta: 0:00:28  loss: 0.2444 (0.2364)  time: 0.1735  data: 0.0001  max mem: 15850
[21:04:09.222939] Test:  [190/345]  eta: 0:00:26  loss: 0.2246 (0.2362)  time: 0.1738  data: 0.0001  max mem: 15850
[21:04:10.966077] Test:  [200/345]  eta: 0:00:25  loss: 0.2372 (0.2366)  time: 0.1741  data: 0.0001  max mem: 15850
[21:04:12.714162] Test:  [210/345]  eta: 0:00:23  loss: 0.2403 (0.2373)  time: 0.1745  data: 0.0001  max mem: 15850
[21:04:14.463961] Test:  [220/345]  eta: 0:00:21  loss: 0.2351 (0.2376)  time: 0.1748  data: 0.0001  max mem: 15850
[21:04:16.218357] Test:  [230/345]  eta: 0:00:19  loss: 0.2351 (0.2375)  time: 0.1751  data: 0.0001  max mem: 15850
[21:04:17.975958] Test:  [240/345]  eta: 0:00:18  loss: 0.2259 (0.2374)  time: 0.1755  data: 0.0001  max mem: 15850
[21:04:19.737195] Test:  [250/345]  eta: 0:00:16  loss: 0.2224 (0.2369)  time: 0.1758  data: 0.0001  max mem: 15850
[21:04:21.501675] Test:  [260/345]  eta: 0:00:14  loss: 0.2255 (0.2367)  time: 0.1762  data: 0.0001  max mem: 15850
[21:04:23.270296] Test:  [270/345]  eta: 0:00:13  loss: 0.2343 (0.2371)  time: 0.1766  data: 0.0001  max mem: 15850
[21:04:25.040516] Test:  [280/345]  eta: 0:00:11  loss: 0.2401 (0.2377)  time: 0.1769  data: 0.0001  max mem: 15850
[21:04:26.815128] Test:  [290/345]  eta: 0:00:09  loss: 0.2360 (0.2376)  time: 0.1772  data: 0.0001  max mem: 15850
[21:04:28.593121] Test:  [300/345]  eta: 0:00:07  loss: 0.2280 (0.2377)  time: 0.1776  data: 0.0001  max mem: 15850
[21:04:30.374732] Test:  [310/345]  eta: 0:00:06  loss: 0.2310 (0.2376)  time: 0.1779  data: 0.0001  max mem: 15850
[21:04:32.160071] Test:  [320/345]  eta: 0:00:04  loss: 0.2353 (0.2376)  time: 0.1783  data: 0.0001  max mem: 15850
[21:04:33.949892] Test:  [330/345]  eta: 0:00:02  loss: 0.2428 (0.2380)  time: 0.1787  data: 0.0001  max mem: 15850
[21:04:35.740951] Test:  [340/345]  eta: 0:00:00  loss: 0.2429 (0.2378)  time: 0.1790  data: 0.0001  max mem: 15850
[21:04:36.458887] Test:  [344/345]  eta: 0:00:00  loss: 0.2429 (0.2379)  time: 0.1791  data: 0.0001  max mem: 15850
[21:04:36.533811] Test: Total time: 0:01:00 (0.1747 s / it)
[21:04:46.302668] Test:  [ 0/57]  eta: 0:00:24  loss: 0.4688 (0.4688)  time: 0.4308  data: 0.2669  max mem: 15850
[21:04:47.963271] Test:  [10/57]  eta: 0:00:08  loss: 0.4391 (0.4397)  time: 0.1900  data: 0.0244  max mem: 15850
[21:04:49.630048] Test:  [20/57]  eta: 0:00:06  loss: 0.4606 (0.4456)  time: 0.1663  data: 0.0001  max mem: 15850
[21:04:51.299389] Test:  [30/57]  eta: 0:00:04  loss: 0.2822 (0.3787)  time: 0.1667  data: 0.0001  max mem: 15850
[21:04:52.972880] Test:  [40/57]  eta: 0:00:02  loss: 0.2305 (0.3477)  time: 0.1671  data: 0.0001  max mem: 15850
[21:04:54.650116] Test:  [50/57]  eta: 0:00:01  loss: 0.2654 (0.3396)  time: 0.1675  data: 0.0001  max mem: 15850
[21:04:55.555298] Test:  [56/57]  eta: 0:00:00  loss: 0.2754 (0.3459)  time: 0.1625  data: 0.0000  max mem: 15850
[21:04:55.621843] Test: Total time: 0:00:09 (0.1711 s / it)
[21:04:57.267965] Dice score of the network on the train images: 0.802034, val images: 0.812571
[21:04:57.272035] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:04:58.188274] Epoch: [12]  [  0/345]  eta: 0:05:15  lr: 0.000075  loss: 0.2093 (0.2093)  time: 0.9151  data: 0.3134  max mem: 15850
[21:05:10.165305] Epoch: [12]  [ 20/345]  eta: 0:03:19  lr: 0.000075  loss: 0.2459 (0.2476)  time: 0.5988  data: 0.0001  max mem: 15850
[21:05:22.162847] Epoch: [12]  [ 40/345]  eta: 0:03:05  lr: 0.000076  loss: 0.2496 (0.2480)  time: 0.5998  data: 0.0001  max mem: 15850
[21:05:34.184070] Epoch: [12]  [ 60/345]  eta: 0:02:52  lr: 0.000076  loss: 0.2347 (0.2474)  time: 0.6010  data: 0.0001  max mem: 15850
[21:05:46.200425] Epoch: [12]  [ 80/345]  eta: 0:02:40  lr: 0.000076  loss: 0.2434 (0.2464)  time: 0.6008  data: 0.0001  max mem: 15850
[21:05:58.230011] Epoch: [12]  [100/345]  eta: 0:02:27  lr: 0.000077  loss: 0.2419 (0.2455)  time: 0.6014  data: 0.0001  max mem: 15850
[21:06:10.270507] Epoch: [12]  [120/345]  eta: 0:02:15  lr: 0.000077  loss: 0.2481 (0.2460)  time: 0.6020  data: 0.0001  max mem: 15850
[21:06:22.324352] Epoch: [12]  [140/345]  eta: 0:02:03  lr: 0.000078  loss: 0.2508 (0.2470)  time: 0.6026  data: 0.0001  max mem: 15850
[21:06:34.376990] Epoch: [12]  [160/345]  eta: 0:01:51  lr: 0.000078  loss: 0.2326 (0.2461)  time: 0.6026  data: 0.0001  max mem: 15850
[21:06:46.440977] Epoch: [12]  [180/345]  eta: 0:01:39  lr: 0.000078  loss: 0.2471 (0.2466)  time: 0.6031  data: 0.0001  max mem: 15850
[21:06:58.504722] Epoch: [12]  [200/345]  eta: 0:01:27  lr: 0.000079  loss: 0.2459 (0.2467)  time: 0.6031  data: 0.0001  max mem: 15850
[21:07:10.565610] Epoch: [12]  [220/345]  eta: 0:01:15  lr: 0.000079  loss: 0.2301 (0.2455)  time: 0.6030  data: 0.0001  max mem: 15850
[21:07:22.624393] Epoch: [12]  [240/345]  eta: 0:01:03  lr: 0.000079  loss: 0.2363 (0.2455)  time: 0.6029  data: 0.0001  max mem: 15850
[21:07:34.679931] Epoch: [12]  [260/345]  eta: 0:00:51  lr: 0.000080  loss: 0.2416 (0.2456)  time: 0.6027  data: 0.0001  max mem: 15850
[21:07:46.728032] Epoch: [12]  [280/345]  eta: 0:00:39  lr: 0.000080  loss: 0.2538 (0.2459)  time: 0.6024  data: 0.0001  max mem: 15850
[21:07:58.782142] Epoch: [12]  [300/345]  eta: 0:00:27  lr: 0.000080  loss: 0.2424 (0.2455)  time: 0.6027  data: 0.0001  max mem: 15850
[21:08:10.829879] Epoch: [12]  [320/345]  eta: 0:00:15  lr: 0.000081  loss: 0.2367 (0.2455)  time: 0.6023  data: 0.0001  max mem: 15850
[21:08:22.870613] Epoch: [12]  [340/345]  eta: 0:00:03  lr: 0.000081  loss: 0.2240 (0.2450)  time: 0.6020  data: 0.0001  max mem: 15850
[21:08:25.278110] Epoch: [12]  [344/345]  eta: 0:00:00  lr: 0.000081  loss: 0.2268 (0.2451)  time: 0.6019  data: 0.0001  max mem: 15850
[21:08:25.352012] Epoch: [12] Total time: 0:03:28 (0.6031 s / it)
[21:08:25.352466] Averaged stats: lr: 0.000081  loss: 0.2268 (0.2451)
[21:08:25.856453] Test:  [  0/345]  eta: 0:02:51  loss: 0.2216 (0.2216)  time: 0.4985  data: 0.3327  max mem: 15850
[21:08:27.533310] Test:  [ 10/345]  eta: 0:01:06  loss: 0.2216 (0.2265)  time: 0.1977  data: 0.0303  max mem: 15850
[21:08:29.214697] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2190 (0.2234)  time: 0.1678  data: 0.0001  max mem: 15850
[21:08:30.899215] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2251 (0.2281)  time: 0.1682  data: 0.0001  max mem: 15850
[21:08:32.588079] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2251 (0.2267)  time: 0.1686  data: 0.0001  max mem: 15850
[21:08:34.278596] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2215 (0.2275)  time: 0.1689  data: 0.0001  max mem: 15850
[21:08:35.973020] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2159 (0.2248)  time: 0.1692  data: 0.0001  max mem: 15850
[21:08:37.671343] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2158 (0.2259)  time: 0.1696  data: 0.0001  max mem: 15850
[21:08:39.371685] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2279 (0.2274)  time: 0.1699  data: 0.0001  max mem: 15850
[21:08:41.077293] Test:  [ 90/345]  eta: 0:00:44  loss: 0.2279 (0.2265)  time: 0.1702  data: 0.0001  max mem: 15850
[21:08:42.785180] Test:  [100/345]  eta: 0:00:42  loss: 0.2235 (0.2256)  time: 0.1706  data: 0.0001  max mem: 15850
[21:08:44.497473] Test:  [110/345]  eta: 0:00:40  loss: 0.2247 (0.2265)  time: 0.1709  data: 0.0001  max mem: 15850
[21:08:46.212145] Test:  [120/345]  eta: 0:00:38  loss: 0.2214 (0.2266)  time: 0.1713  data: 0.0001  max mem: 15850
[21:08:47.930604] Test:  [130/345]  eta: 0:00:37  loss: 0.2191 (0.2268)  time: 0.1716  data: 0.0001  max mem: 15850
[21:08:49.652774] Test:  [140/345]  eta: 0:00:35  loss: 0.2042 (0.2259)  time: 0.1720  data: 0.0001  max mem: 15850
[21:08:51.379848] Test:  [150/345]  eta: 0:00:33  loss: 0.2128 (0.2260)  time: 0.1724  data: 0.0001  max mem: 15850
[21:08:53.110188] Test:  [160/345]  eta: 0:00:31  loss: 0.2271 (0.2270)  time: 0.1728  data: 0.0001  max mem: 15850
[21:08:54.844787] Test:  [170/345]  eta: 0:00:30  loss: 0.2303 (0.2271)  time: 0.1732  data: 0.0001  max mem: 15850
[21:08:56.582360] Test:  [180/345]  eta: 0:00:28  loss: 0.2345 (0.2283)  time: 0.1736  data: 0.0001  max mem: 15850
[21:08:58.323488] Test:  [190/345]  eta: 0:00:26  loss: 0.2240 (0.2279)  time: 0.1739  data: 0.0001  max mem: 15850
[21:09:00.067640] Test:  [200/345]  eta: 0:00:25  loss: 0.2119 (0.2273)  time: 0.1742  data: 0.0001  max mem: 15850
[21:09:01.814778] Test:  [210/345]  eta: 0:00:23  loss: 0.2297 (0.2282)  time: 0.1745  data: 0.0001  max mem: 15850
[21:09:03.565580] Test:  [220/345]  eta: 0:00:21  loss: 0.2313 (0.2282)  time: 0.1748  data: 0.0001  max mem: 15850
[21:09:05.321121] Test:  [230/345]  eta: 0:00:19  loss: 0.2250 (0.2278)  time: 0.1753  data: 0.0001  max mem: 15850
[21:09:07.078739] Test:  [240/345]  eta: 0:00:18  loss: 0.2161 (0.2277)  time: 0.1756  data: 0.0001  max mem: 15850
[21:09:08.840522] Test:  [250/345]  eta: 0:00:16  loss: 0.2219 (0.2279)  time: 0.1759  data: 0.0001  max mem: 15850
[21:09:10.603912] Test:  [260/345]  eta: 0:00:14  loss: 0.2235 (0.2277)  time: 0.1762  data: 0.0001  max mem: 15850
[21:09:12.371952] Test:  [270/345]  eta: 0:00:13  loss: 0.2228 (0.2275)  time: 0.1765  data: 0.0001  max mem: 15850
[21:09:14.144201] Test:  [280/345]  eta: 0:00:11  loss: 0.2266 (0.2273)  time: 0.1770  data: 0.0001  max mem: 15850
[21:09:15.920381] Test:  [290/345]  eta: 0:00:09  loss: 0.2267 (0.2273)  time: 0.1774  data: 0.0001  max mem: 15850
[21:09:17.699660] Test:  [300/345]  eta: 0:00:07  loss: 0.2283 (0.2273)  time: 0.1777  data: 0.0001  max mem: 15850
[21:09:19.482341] Test:  [310/345]  eta: 0:00:06  loss: 0.2310 (0.2277)  time: 0.1780  data: 0.0001  max mem: 15850
[21:09:21.268068] Test:  [320/345]  eta: 0:00:04  loss: 0.2411 (0.2280)  time: 0.1784  data: 0.0001  max mem: 15850
[21:09:23.057729] Test:  [330/345]  eta: 0:00:02  loss: 0.2357 (0.2281)  time: 0.1787  data: 0.0001  max mem: 15850
[21:09:24.847626] Test:  [340/345]  eta: 0:00:00  loss: 0.2129 (0.2275)  time: 0.1789  data: 0.0001  max mem: 15850
[21:09:25.566191] Test:  [344/345]  eta: 0:00:00  loss: 0.2109 (0.2272)  time: 0.1791  data: 0.0001  max mem: 15850
[21:09:25.642381] Test: Total time: 0:01:00 (0.1747 s / it)
[21:09:35.462052] Test:  [ 0/57]  eta: 0:00:28  loss: 0.4854 (0.4854)  time: 0.4931  data: 0.3292  max mem: 15850
[21:09:37.122996] Test:  [10/57]  eta: 0:00:09  loss: 0.4448 (0.4521)  time: 0.1957  data: 0.0300  max mem: 15850
[21:09:38.788968] Test:  [20/57]  eta: 0:00:06  loss: 0.4410 (0.4397)  time: 0.1663  data: 0.0001  max mem: 15850
[21:09:40.458361] Test:  [30/57]  eta: 0:00:04  loss: 0.2634 (0.3705)  time: 0.1667  data: 0.0001  max mem: 15850
[21:09:42.131555] Test:  [40/57]  eta: 0:00:02  loss: 0.2404 (0.3404)  time: 0.1671  data: 0.0001  max mem: 15850
[21:09:43.809825] Test:  [50/57]  eta: 0:00:01  loss: 0.2421 (0.3330)  time: 0.1675  data: 0.0001  max mem: 15850
[21:09:44.715292] Test:  [56/57]  eta: 0:00:00  loss: 0.2750 (0.3352)  time: 0.1626  data: 0.0001  max mem: 15850
[21:09:44.784533] Test: Total time: 0:00:09 (0.1722 s / it)
[21:09:46.423597] Dice score of the network on the train images: 0.800644, val images: 0.817032
[21:09:46.423877] saving best_rec_model_0 @ epoch 12
[21:09:47.718396] saving best_dice_model_0 @ epoch 12
[21:09:49.027425] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:09:49.906946] Epoch: [13]  [  0/345]  eta: 0:05:03  lr: 0.000081  loss: 0.1994 (0.1994)  time: 0.8787  data: 0.2783  max mem: 15850
[21:10:01.862779] Epoch: [13]  [ 20/345]  eta: 0:03:18  lr: 0.000082  loss: 0.2340 (0.2295)  time: 0.5977  data: 0.0001  max mem: 15850
[21:10:13.840775] Epoch: [13]  [ 40/345]  eta: 0:03:04  lr: 0.000082  loss: 0.2220 (0.2310)  time: 0.5989  data: 0.0001  max mem: 15850
[21:10:25.853153] Epoch: [13]  [ 60/345]  eta: 0:02:52  lr: 0.000082  loss: 0.2271 (0.2283)  time: 0.6006  data: 0.0001  max mem: 15850
[21:10:37.879500] Epoch: [13]  [ 80/345]  eta: 0:02:39  lr: 0.000083  loss: 0.2202 (0.2284)  time: 0.6013  data: 0.0001  max mem: 15850
[21:10:49.931701] Epoch: [13]  [100/345]  eta: 0:02:27  lr: 0.000083  loss: 0.2285 (0.2286)  time: 0.6026  data: 0.0001  max mem: 15850
[21:11:02.000635] Epoch: [13]  [120/345]  eta: 0:02:15  lr: 0.000083  loss: 0.2452 (0.2317)  time: 0.6034  data: 0.0001  max mem: 15850
[21:11:14.085573] Epoch: [13]  [140/345]  eta: 0:02:03  lr: 0.000084  loss: 0.2248 (0.2325)  time: 0.6042  data: 0.0001  max mem: 15850
[21:11:26.185766] Epoch: [13]  [160/345]  eta: 0:01:51  lr: 0.000084  loss: 0.2226 (0.2321)  time: 0.6050  data: 0.0001  max mem: 15850
[21:11:38.281589] Epoch: [13]  [180/345]  eta: 0:01:39  lr: 0.000085  loss: 0.2276 (0.2319)  time: 0.6048  data: 0.0001  max mem: 15850
[21:11:50.381234] Epoch: [13]  [200/345]  eta: 0:01:27  lr: 0.000085  loss: 0.2323 (0.2323)  time: 0.6049  data: 0.0001  max mem: 15850
[21:12:02.474915] Epoch: [13]  [220/345]  eta: 0:01:15  lr: 0.000085  loss: 0.2317 (0.2330)  time: 0.6046  data: 0.0001  max mem: 15850
[21:12:14.564777] Epoch: [13]  [240/345]  eta: 0:01:03  lr: 0.000086  loss: 0.2337 (0.2333)  time: 0.6045  data: 0.0001  max mem: 15850
[21:12:26.643844] Epoch: [13]  [260/345]  eta: 0:00:51  lr: 0.000086  loss: 0.2288 (0.2337)  time: 0.6039  data: 0.0001  max mem: 15850
[21:12:38.720796] Epoch: [13]  [280/345]  eta: 0:00:39  lr: 0.000086  loss: 0.2279 (0.2334)  time: 0.6038  data: 0.0001  max mem: 15850
[21:12:50.793544] Epoch: [13]  [300/345]  eta: 0:00:27  lr: 0.000087  loss: 0.2431 (0.2347)  time: 0.6036  data: 0.0001  max mem: 15850
[21:13:02.863815] Epoch: [13]  [320/345]  eta: 0:00:15  lr: 0.000087  loss: 0.2518 (0.2357)  time: 0.6035  data: 0.0001  max mem: 15850
[21:13:14.925966] Epoch: [13]  [340/345]  eta: 0:00:03  lr: 0.000087  loss: 0.2252 (0.2352)  time: 0.6031  data: 0.0001  max mem: 15850
[21:13:17.337005] Epoch: [13]  [344/345]  eta: 0:00:00  lr: 0.000087  loss: 0.2238 (0.2349)  time: 0.6029  data: 0.0001  max mem: 15850
[21:13:17.410265] Epoch: [13] Total time: 0:03:28 (0.6040 s / it)
[21:13:17.410590] Averaged stats: lr: 0.000087  loss: 0.2238 (0.2349)
[21:13:17.909070] Test:  [  0/345]  eta: 0:02:49  loss: 0.2557 (0.2557)  time: 0.4926  data: 0.3278  max mem: 15850
[21:13:19.587361] Test:  [ 10/345]  eta: 0:01:06  loss: 0.2195 (0.2251)  time: 0.1973  data: 0.0299  max mem: 15850
[21:13:21.268679] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2195 (0.2229)  time: 0.1679  data: 0.0001  max mem: 15850
[21:13:22.952560] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2187 (0.2219)  time: 0.1682  data: 0.0001  max mem: 15850
[21:13:24.641401] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2173 (0.2218)  time: 0.1686  data: 0.0001  max mem: 15850
[21:13:26.332840] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2208 (0.2220)  time: 0.1689  data: 0.0001  max mem: 15850
[21:13:28.028138] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2298 (0.2237)  time: 0.1693  data: 0.0001  max mem: 15850
[21:13:29.725709] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2231 (0.2228)  time: 0.1696  data: 0.0001  max mem: 15850
[21:13:31.427019] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2248 (0.2248)  time: 0.1699  data: 0.0001  max mem: 15850
[21:13:33.132123] Test:  [ 90/345]  eta: 0:00:44  loss: 0.2357 (0.2254)  time: 0.1703  data: 0.0001  max mem: 15850
[21:13:34.840408] Test:  [100/345]  eta: 0:00:42  loss: 0.2232 (0.2253)  time: 0.1706  data: 0.0001  max mem: 15850
[21:13:36.552836] Test:  [110/345]  eta: 0:00:40  loss: 0.2210 (0.2244)  time: 0.1710  data: 0.0001  max mem: 15850
[21:13:38.268270] Test:  [120/345]  eta: 0:00:38  loss: 0.2222 (0.2246)  time: 0.1713  data: 0.0001  max mem: 15850
[21:13:39.987904] Test:  [130/345]  eta: 0:00:37  loss: 0.2253 (0.2244)  time: 0.1717  data: 0.0001  max mem: 15850
[21:13:41.709056] Test:  [140/345]  eta: 0:00:35  loss: 0.2254 (0.2249)  time: 0.1720  data: 0.0001  max mem: 15850
[21:13:43.434503] Test:  [150/345]  eta: 0:00:33  loss: 0.2213 (0.2251)  time: 0.1723  data: 0.0001  max mem: 15850
[21:13:45.164475] Test:  [160/345]  eta: 0:00:31  loss: 0.2282 (0.2253)  time: 0.1727  data: 0.0001  max mem: 15850
[21:13:46.897181] Test:  [170/345]  eta: 0:00:30  loss: 0.2141 (0.2244)  time: 0.1730  data: 0.0001  max mem: 15850
[21:13:48.633914] Test:  [180/345]  eta: 0:00:28  loss: 0.2106 (0.2238)  time: 0.1734  data: 0.0001  max mem: 15850
[21:13:50.373766] Test:  [190/345]  eta: 0:00:26  loss: 0.2123 (0.2236)  time: 0.1738  data: 0.0001  max mem: 15850
[21:13:52.115887] Test:  [200/345]  eta: 0:00:25  loss: 0.2178 (0.2237)  time: 0.1740  data: 0.0001  max mem: 15850
[21:13:53.862963] Test:  [210/345]  eta: 0:00:23  loss: 0.2258 (0.2238)  time: 0.1744  data: 0.0001  max mem: 15850
[21:13:55.613956] Test:  [220/345]  eta: 0:00:21  loss: 0.2243 (0.2238)  time: 0.1748  data: 0.0001  max mem: 15850
[21:13:57.368608] Test:  [230/345]  eta: 0:00:19  loss: 0.2164 (0.2237)  time: 0.1752  data: 0.0001  max mem: 15850
[21:13:59.125065] Test:  [240/345]  eta: 0:00:18  loss: 0.2116 (0.2232)  time: 0.1755  data: 0.0001  max mem: 15850
[21:14:00.884709] Test:  [250/345]  eta: 0:00:16  loss: 0.2196 (0.2245)  time: 0.1757  data: 0.0001  max mem: 15850
[21:14:02.648072] Test:  [260/345]  eta: 0:00:14  loss: 0.2137 (0.2238)  time: 0.1761  data: 0.0001  max mem: 15850
[21:14:04.414803] Test:  [270/345]  eta: 0:00:13  loss: 0.2100 (0.2238)  time: 0.1764  data: 0.0001  max mem: 15850
[21:14:06.185797] Test:  [280/345]  eta: 0:00:11  loss: 0.2264 (0.2240)  time: 0.1768  data: 0.0001  max mem: 15850
[21:14:07.959685] Test:  [290/345]  eta: 0:00:09  loss: 0.2264 (0.2242)  time: 0.1772  data: 0.0001  max mem: 15850
[21:14:09.737184] Test:  [300/345]  eta: 0:00:07  loss: 0.2270 (0.2242)  time: 0.1775  data: 0.0001  max mem: 15850
[21:14:11.519348] Test:  [310/345]  eta: 0:00:06  loss: 0.2238 (0.2245)  time: 0.1779  data: 0.0001  max mem: 15850
[21:14:13.304576] Test:  [320/345]  eta: 0:00:04  loss: 0.2230 (0.2245)  time: 0.1783  data: 0.0001  max mem: 15850
[21:14:15.092575] Test:  [330/345]  eta: 0:00:02  loss: 0.2239 (0.2247)  time: 0.1786  data: 0.0001  max mem: 15850
[21:14:16.885525] Test:  [340/345]  eta: 0:00:00  loss: 0.2288 (0.2246)  time: 0.1790  data: 0.0001  max mem: 15850
[21:14:17.602512] Test:  [344/345]  eta: 0:00:00  loss: 0.2182 (0.2244)  time: 0.1791  data: 0.0001  max mem: 15850
[21:14:17.680429] Test: Total time: 0:01:00 (0.1747 s / it)
[21:14:27.529661] Test:  [ 0/57]  eta: 0:00:24  loss: 0.4688 (0.4688)  time: 0.4355  data: 0.2718  max mem: 15850
[21:14:29.188097] Test:  [10/57]  eta: 0:00:08  loss: 0.4075 (0.4422)  time: 0.1903  data: 0.0248  max mem: 15850
[21:14:30.852684] Test:  [20/57]  eta: 0:00:06  loss: 0.4279 (0.4487)  time: 0.1661  data: 0.0001  max mem: 15850
[21:14:32.520773] Test:  [30/57]  eta: 0:00:04  loss: 0.2783 (0.3829)  time: 0.1666  data: 0.0001  max mem: 15850
[21:14:34.194181] Test:  [40/57]  eta: 0:00:02  loss: 0.2430 (0.3555)  time: 0.1670  data: 0.0001  max mem: 15850
[21:14:35.871794] Test:  [50/57]  eta: 0:00:01  loss: 0.2822 (0.3471)  time: 0.1675  data: 0.0001  max mem: 15850
[21:14:36.778987] Test:  [56/57]  eta: 0:00:00  loss: 0.3211 (0.3588)  time: 0.1627  data: 0.0000  max mem: 15850
[21:14:36.846474] Test: Total time: 0:00:09 (0.1711 s / it)
[21:14:38.503369] Dice score of the network on the train images: 0.820836, val images: 0.801113
[21:14:38.503600] saving best_prec_model_0 @ epoch 13
[21:14:39.890988] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:14:40.774893] Epoch: [14]  [  0/345]  eta: 0:05:04  lr: 0.000087  loss: 0.2936 (0.2936)  time: 0.8829  data: 0.2827  max mem: 15850
[21:14:52.736520] Epoch: [14]  [ 20/345]  eta: 0:03:18  lr: 0.000088  loss: 0.2253 (0.2394)  time: 0.5980  data: 0.0001  max mem: 15850
[21:15:04.714898] Epoch: [14]  [ 40/345]  eta: 0:03:04  lr: 0.000088  loss: 0.2236 (0.2315)  time: 0.5989  data: 0.0001  max mem: 15850
[21:15:16.720599] Epoch: [14]  [ 60/345]  eta: 0:02:52  lr: 0.000089  loss: 0.2249 (0.2301)  time: 0.6002  data: 0.0001  max mem: 15850
[21:15:28.744762] Epoch: [14]  [ 80/345]  eta: 0:02:39  lr: 0.000089  loss: 0.2125 (0.2269)  time: 0.6012  data: 0.0001  max mem: 15850
[21:15:40.786340] Epoch: [14]  [100/345]  eta: 0:02:27  lr: 0.000089  loss: 0.2303 (0.2292)  time: 0.6020  data: 0.0001  max mem: 15850
[21:15:52.843403] Epoch: [14]  [120/345]  eta: 0:02:15  lr: 0.000090  loss: 0.2252 (0.2291)  time: 0.6028  data: 0.0001  max mem: 15850
[21:16:04.916642] Epoch: [14]  [140/345]  eta: 0:02:03  lr: 0.000090  loss: 0.2210 (0.2287)  time: 0.6036  data: 0.0001  max mem: 15850
[21:16:16.993316] Epoch: [14]  [160/345]  eta: 0:01:51  lr: 0.000090  loss: 0.2278 (0.2282)  time: 0.6038  data: 0.0001  max mem: 15850
[21:16:29.075284] Epoch: [14]  [180/345]  eta: 0:01:39  lr: 0.000091  loss: 0.2249 (0.2283)  time: 0.6041  data: 0.0001  max mem: 15850
[21:16:41.162380] Epoch: [14]  [200/345]  eta: 0:01:27  lr: 0.000091  loss: 0.2184 (0.2278)  time: 0.6043  data: 0.0001  max mem: 15850
[21:16:53.242206] Epoch: [14]  [220/345]  eta: 0:01:15  lr: 0.000091  loss: 0.2151 (0.2269)  time: 0.6040  data: 0.0001  max mem: 15850
[21:17:05.318117] Epoch: [14]  [240/345]  eta: 0:01:03  lr: 0.000092  loss: 0.2191 (0.2267)  time: 0.6038  data: 0.0001  max mem: 15850
[21:17:17.383824] Epoch: [14]  [260/345]  eta: 0:00:51  lr: 0.000092  loss: 0.2246 (0.2271)  time: 0.6032  data: 0.0001  max mem: 15850
[21:17:29.452953] Epoch: [14]  [280/345]  eta: 0:00:39  lr: 0.000093  loss: 0.2186 (0.2271)  time: 0.6034  data: 0.0001  max mem: 15850
[21:17:41.518072] Epoch: [14]  [300/345]  eta: 0:00:27  lr: 0.000093  loss: 0.2384 (0.2279)  time: 0.6032  data: 0.0001  max mem: 15850
[21:17:53.576810] Epoch: [14]  [320/345]  eta: 0:00:15  lr: 0.000093  loss: 0.2160 (0.2275)  time: 0.6029  data: 0.0001  max mem: 15850
[21:18:05.629886] Epoch: [14]  [340/345]  eta: 0:00:03  lr: 0.000094  loss: 0.2221 (0.2275)  time: 0.6026  data: 0.0001  max mem: 15850
[21:18:08.041786] Epoch: [14]  [344/345]  eta: 0:00:00  lr: 0.000094  loss: 0.2276 (0.2276)  time: 0.6026  data: 0.0001  max mem: 15850
[21:18:08.098247] Epoch: [14] Total time: 0:03:28 (0.6035 s / it)
[21:18:08.098470] Averaged stats: lr: 0.000094  loss: 0.2276 (0.2276)
[21:18:08.630230] Test:  [  0/345]  eta: 0:03:01  loss: 0.2215 (0.2215)  time: 0.5265  data: 0.3604  max mem: 15850
[21:18:10.308486] Test:  [ 10/345]  eta: 0:01:07  loss: 0.2124 (0.2135)  time: 0.2003  data: 0.0328  max mem: 15850
[21:18:11.990047] Test:  [ 20/345]  eta: 0:01:00  loss: 0.2112 (0.2170)  time: 0.1679  data: 0.0001  max mem: 15850
[21:18:13.674572] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2167 (0.2184)  time: 0.1682  data: 0.0001  max mem: 15850
[21:18:15.361894] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2141 (0.2167)  time: 0.1685  data: 0.0001  max mem: 15850
[21:18:17.053340] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2129 (0.2164)  time: 0.1689  data: 0.0001  max mem: 15850
[21:18:18.748447] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1980 (0.2144)  time: 0.1693  data: 0.0001  max mem: 15850
[21:18:20.446045] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2078 (0.2151)  time: 0.1696  data: 0.0001  max mem: 15850
[21:18:22.147296] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2140 (0.2140)  time: 0.1699  data: 0.0001  max mem: 15850
[21:18:23.851478] Test:  [ 90/345]  eta: 0:00:44  loss: 0.2076 (0.2136)  time: 0.1702  data: 0.0001  max mem: 15850
[21:18:25.559677] Test:  [100/345]  eta: 0:00:42  loss: 0.2186 (0.2151)  time: 0.1706  data: 0.0001  max mem: 15850
[21:18:27.272298] Test:  [110/345]  eta: 0:00:40  loss: 0.2246 (0.2164)  time: 0.1710  data: 0.0001  max mem: 15850
[21:18:28.988453] Test:  [120/345]  eta: 0:00:38  loss: 0.2246 (0.2168)  time: 0.1714  data: 0.0001  max mem: 15850
[21:18:30.706982] Test:  [130/345]  eta: 0:00:37  loss: 0.2183 (0.2168)  time: 0.1717  data: 0.0001  max mem: 15850
[21:18:32.429297] Test:  [140/345]  eta: 0:00:35  loss: 0.2138 (0.2171)  time: 0.1720  data: 0.0001  max mem: 15850
[21:18:34.154882] Test:  [150/345]  eta: 0:00:33  loss: 0.2098 (0.2171)  time: 0.1723  data: 0.0001  max mem: 15850
[21:18:35.883472] Test:  [160/345]  eta: 0:00:31  loss: 0.2098 (0.2168)  time: 0.1726  data: 0.0001  max mem: 15850
[21:18:37.617196] Test:  [170/345]  eta: 0:00:30  loss: 0.2128 (0.2167)  time: 0.1731  data: 0.0001  max mem: 15850
[21:18:39.353325] Test:  [180/345]  eta: 0:00:28  loss: 0.2162 (0.2172)  time: 0.1734  data: 0.0001  max mem: 15850
[21:18:41.093690] Test:  [190/345]  eta: 0:00:26  loss: 0.2322 (0.2182)  time: 0.1738  data: 0.0001  max mem: 15850
[21:18:42.835558] Test:  [200/345]  eta: 0:00:25  loss: 0.2268 (0.2178)  time: 0.1740  data: 0.0001  max mem: 15850
[21:18:44.580544] Test:  [210/345]  eta: 0:00:23  loss: 0.2205 (0.2185)  time: 0.1743  data: 0.0001  max mem: 15850
[21:18:46.330456] Test:  [220/345]  eta: 0:00:21  loss: 0.2243 (0.2184)  time: 0.1747  data: 0.0001  max mem: 15850
[21:18:48.083467] Test:  [230/345]  eta: 0:00:19  loss: 0.2119 (0.2178)  time: 0.1751  data: 0.0001  max mem: 15850
[21:18:49.839985] Test:  [240/345]  eta: 0:00:18  loss: 0.2119 (0.2180)  time: 0.1754  data: 0.0001  max mem: 15850
[21:18:51.600162] Test:  [250/345]  eta: 0:00:16  loss: 0.2166 (0.2185)  time: 0.1758  data: 0.0001  max mem: 15850
[21:18:53.363572] Test:  [260/345]  eta: 0:00:14  loss: 0.2260 (0.2187)  time: 0.1761  data: 0.0001  max mem: 15850
[21:18:55.131792] Test:  [270/345]  eta: 0:00:13  loss: 0.2082 (0.2185)  time: 0.1765  data: 0.0001  max mem: 15850
[21:18:56.901057] Test:  [280/345]  eta: 0:00:11  loss: 0.2036 (0.2180)  time: 0.1768  data: 0.0001  max mem: 15850
[21:18:58.675522] Test:  [290/345]  eta: 0:00:09  loss: 0.2146 (0.2185)  time: 0.1771  data: 0.0001  max mem: 15850
[21:19:00.453938] Test:  [300/345]  eta: 0:00:07  loss: 0.2107 (0.2176)  time: 0.1776  data: 0.0001  max mem: 15850
[21:19:02.235420] Test:  [310/345]  eta: 0:00:06  loss: 0.1966 (0.2172)  time: 0.1779  data: 0.0001  max mem: 15850
[21:19:04.020033] Test:  [320/345]  eta: 0:00:04  loss: 0.2042 (0.2173)  time: 0.1782  data: 0.0001  max mem: 15850
[21:19:05.809656] Test:  [330/345]  eta: 0:00:02  loss: 0.2267 (0.2177)  time: 0.1787  data: 0.0001  max mem: 15850
[21:19:07.599546] Test:  [340/345]  eta: 0:00:00  loss: 0.2257 (0.2180)  time: 0.1789  data: 0.0001  max mem: 15850
[21:19:08.319362] Test:  [344/345]  eta: 0:00:00  loss: 0.2222 (0.2181)  time: 0.1791  data: 0.0001  max mem: 15850
[21:19:08.391367] Test: Total time: 0:01:00 (0.1747 s / it)
[21:19:18.405529] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4989 (0.4989)  time: 0.4483  data: 0.2849  max mem: 15850
[21:19:20.066131] Test:  [10/57]  eta: 0:00:09  loss: 0.4329 (0.4431)  time: 0.1916  data: 0.0260  max mem: 15850
[21:19:21.731749] Test:  [20/57]  eta: 0:00:06  loss: 0.4396 (0.4519)  time: 0.1662  data: 0.0001  max mem: 15850
[21:19:23.400219] Test:  [30/57]  eta: 0:00:04  loss: 0.2746 (0.3801)  time: 0.1666  data: 0.0001  max mem: 15850
[21:19:25.073661] Test:  [40/57]  eta: 0:00:02  loss: 0.2410 (0.3488)  time: 0.1670  data: 0.0001  max mem: 15850
[21:19:26.750041] Test:  [50/57]  eta: 0:00:01  loss: 0.2614 (0.3412)  time: 0.1674  data: 0.0001  max mem: 15850
[21:19:27.654832] Test:  [56/57]  eta: 0:00:00  loss: 0.2660 (0.3446)  time: 0.1625  data: 0.0001  max mem: 15850
[21:19:27.720431] Test: Total time: 0:00:09 (0.1713 s / it)
[21:19:29.350160] Dice score of the network on the train images: 0.821403, val images: 0.822471
[21:19:29.350389] saving best_prec_model_0 @ epoch 14
[21:19:30.541121] saving best_dice_model_0 @ epoch 14
[21:19:31.848175] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:19:32.730101] Epoch: [15]  [  0/345]  eta: 0:05:03  lr: 0.000094  loss: 0.2469 (0.2469)  time: 0.8809  data: 0.2814  max mem: 15850
[21:19:44.682674] Epoch: [15]  [ 20/345]  eta: 0:03:18  lr: 0.000094  loss: 0.2120 (0.2175)  time: 0.5976  data: 0.0001  max mem: 15850
[21:19:56.658458] Epoch: [15]  [ 40/345]  eta: 0:03:04  lr: 0.000094  loss: 0.2245 (0.2199)  time: 0.5987  data: 0.0001  max mem: 15850
[21:20:08.665099] Epoch: [15]  [ 60/345]  eta: 0:02:51  lr: 0.000095  loss: 0.2274 (0.2208)  time: 0.6003  data: 0.0001  max mem: 15850
[21:20:20.692749] Epoch: [15]  [ 80/345]  eta: 0:02:39  lr: 0.000095  loss: 0.2170 (0.2206)  time: 0.6013  data: 0.0001  max mem: 15850
[21:20:32.744210] Epoch: [15]  [100/345]  eta: 0:02:27  lr: 0.000096  loss: 0.2114 (0.2210)  time: 0.6025  data: 0.0001  max mem: 15850
[21:20:44.817285] Epoch: [15]  [120/345]  eta: 0:02:15  lr: 0.000096  loss: 0.2186 (0.2215)  time: 0.6036  data: 0.0001  max mem: 15850
[21:20:56.911687] Epoch: [15]  [140/345]  eta: 0:02:03  lr: 0.000096  loss: 0.2136 (0.2210)  time: 0.6047  data: 0.0001  max mem: 15850
[21:21:09.009248] Epoch: [15]  [160/345]  eta: 0:01:51  lr: 0.000097  loss: 0.2194 (0.2215)  time: 0.6048  data: 0.0001  max mem: 15850
[21:21:21.116826] Epoch: [15]  [180/345]  eta: 0:01:39  lr: 0.000097  loss: 0.2092 (0.2204)  time: 0.6053  data: 0.0001  max mem: 15850
[21:21:33.218064] Epoch: [15]  [200/345]  eta: 0:01:27  lr: 0.000097  loss: 0.2244 (0.2209)  time: 0.6050  data: 0.0001  max mem: 15850
[21:21:45.312315] Epoch: [15]  [220/345]  eta: 0:01:15  lr: 0.000098  loss: 0.2247 (0.2213)  time: 0.6047  data: 0.0001  max mem: 15850
[21:21:57.407010] Epoch: [15]  [240/345]  eta: 0:01:03  lr: 0.000098  loss: 0.2033 (0.2210)  time: 0.6047  data: 0.0001  max mem: 15850
[21:22:09.492945] Epoch: [15]  [260/345]  eta: 0:00:51  lr: 0.000098  loss: 0.2138 (0.2204)  time: 0.6043  data: 0.0001  max mem: 15850
[21:22:21.571267] Epoch: [15]  [280/345]  eta: 0:00:39  lr: 0.000099  loss: 0.2209 (0.2209)  time: 0.6039  data: 0.0001  max mem: 15850
[21:22:33.642544] Epoch: [15]  [300/345]  eta: 0:00:27  lr: 0.000099  loss: 0.2351 (0.2217)  time: 0.6035  data: 0.0001  max mem: 15850
[21:22:45.714319] Epoch: [15]  [320/345]  eta: 0:00:15  lr: 0.000100  loss: 0.2178 (0.2215)  time: 0.6035  data: 0.0001  max mem: 15850
[21:22:57.779598] Epoch: [15]  [340/345]  eta: 0:00:03  lr: 0.000100  loss: 0.2163 (0.2211)  time: 0.6032  data: 0.0001  max mem: 15850
[21:23:00.191648] Epoch: [15]  [344/345]  eta: 0:00:00  lr: 0.000100  loss: 0.2214 (0.2212)  time: 0.6031  data: 0.0001  max mem: 15850
[21:23:00.256463] Epoch: [15] Total time: 0:03:28 (0.6041 s / it)
[21:23:00.256936] Averaged stats: lr: 0.000100  loss: 0.2214 (0.2212)
[21:23:00.762179] Test:  [  0/345]  eta: 0:02:52  loss: 0.2207 (0.2207)  time: 0.4998  data: 0.3350  max mem: 15850
[21:23:02.439744] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1946 (0.2015)  time: 0.1979  data: 0.0305  max mem: 15850
[21:23:04.121012] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2015 (0.2072)  time: 0.1679  data: 0.0001  max mem: 15850
[21:23:05.804642] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2182 (0.2119)  time: 0.1682  data: 0.0001  max mem: 15850
[21:23:07.493051] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2028 (0.2087)  time: 0.1685  data: 0.0001  max mem: 15850
[21:23:09.184111] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2159 (0.2133)  time: 0.1689  data: 0.0001  max mem: 15850
[21:23:10.880813] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2202 (0.2127)  time: 0.1693  data: 0.0001  max mem: 15850
[21:23:12.579013] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2012 (0.2107)  time: 0.1697  data: 0.0001  max mem: 15850
[21:23:14.279781] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2012 (0.2104)  time: 0.1699  data: 0.0001  max mem: 15850
[21:23:15.985701] Test:  [ 90/345]  eta: 0:00:44  loss: 0.2093 (0.2109)  time: 0.1703  data: 0.0001  max mem: 15850
[21:23:17.694650] Test:  [100/345]  eta: 0:00:42  loss: 0.2093 (0.2113)  time: 0.1707  data: 0.0001  max mem: 15850
[21:23:19.405795] Test:  [110/345]  eta: 0:00:40  loss: 0.2053 (0.2107)  time: 0.1709  data: 0.0001  max mem: 15850
[21:23:21.121472] Test:  [120/345]  eta: 0:00:38  loss: 0.1995 (0.2103)  time: 0.1713  data: 0.0001  max mem: 15850
[21:23:22.839708] Test:  [130/345]  eta: 0:00:37  loss: 0.1956 (0.2096)  time: 0.1716  data: 0.0001  max mem: 15850
[21:23:24.562085] Test:  [140/345]  eta: 0:00:35  loss: 0.2045 (0.2104)  time: 0.1720  data: 0.0001  max mem: 15850
[21:23:26.288384] Test:  [150/345]  eta: 0:00:33  loss: 0.2045 (0.2098)  time: 0.1724  data: 0.0001  max mem: 15850
[21:23:28.018050] Test:  [160/345]  eta: 0:00:31  loss: 0.2012 (0.2098)  time: 0.1727  data: 0.0001  max mem: 15850
[21:23:29.750574] Test:  [170/345]  eta: 0:00:30  loss: 0.2042 (0.2096)  time: 0.1731  data: 0.0001  max mem: 15850
[21:23:31.487113] Test:  [180/345]  eta: 0:00:28  loss: 0.2042 (0.2095)  time: 0.1734  data: 0.0001  max mem: 15850
[21:23:33.227775] Test:  [190/345]  eta: 0:00:26  loss: 0.2204 (0.2103)  time: 0.1738  data: 0.0001  max mem: 15850
[21:23:34.970944] Test:  [200/345]  eta: 0:00:25  loss: 0.2048 (0.2094)  time: 0.1741  data: 0.0001  max mem: 15850
[21:23:36.718887] Test:  [210/345]  eta: 0:00:23  loss: 0.1933 (0.2091)  time: 0.1745  data: 0.0001  max mem: 15850
[21:23:38.468102] Test:  [220/345]  eta: 0:00:21  loss: 0.2048 (0.2094)  time: 0.1748  data: 0.0001  max mem: 15850
[21:23:40.221678] Test:  [230/345]  eta: 0:00:19  loss: 0.2100 (0.2092)  time: 0.1751  data: 0.0001  max mem: 15850
[21:23:41.980399] Test:  [240/345]  eta: 0:00:18  loss: 0.1997 (0.2088)  time: 0.1756  data: 0.0001  max mem: 15850
[21:23:43.741699] Test:  [250/345]  eta: 0:00:16  loss: 0.1935 (0.2084)  time: 0.1759  data: 0.0001  max mem: 15850
[21:23:45.504723] Test:  [260/345]  eta: 0:00:14  loss: 0.2096 (0.2089)  time: 0.1762  data: 0.0001  max mem: 15850
[21:23:47.273261] Test:  [270/345]  eta: 0:00:13  loss: 0.2150 (0.2094)  time: 0.1765  data: 0.0001  max mem: 15850
[21:23:49.044539] Test:  [280/345]  eta: 0:00:11  loss: 0.2042 (0.2089)  time: 0.1769  data: 0.0001  max mem: 15850
[21:23:50.819865] Test:  [290/345]  eta: 0:00:09  loss: 0.2033 (0.2090)  time: 0.1773  data: 0.0001  max mem: 15850
[21:23:52.599082] Test:  [300/345]  eta: 0:00:07  loss: 0.2131 (0.2093)  time: 0.1777  data: 0.0001  max mem: 15850
[21:23:54.381289] Test:  [310/345]  eta: 0:00:06  loss: 0.2055 (0.2091)  time: 0.1780  data: 0.0001  max mem: 15850
[21:23:56.167070] Test:  [320/345]  eta: 0:00:04  loss: 0.2078 (0.2093)  time: 0.1783  data: 0.0001  max mem: 15850
[21:23:57.958209] Test:  [330/345]  eta: 0:00:02  loss: 0.2078 (0.2095)  time: 0.1788  data: 0.0001  max mem: 15850
[21:23:59.749519] Test:  [340/345]  eta: 0:00:00  loss: 0.2062 (0.2097)  time: 0.1791  data: 0.0001  max mem: 15850
[21:24:00.466173] Test:  [344/345]  eta: 0:00:00  loss: 0.2194 (0.2098)  time: 0.1791  data: 0.0001  max mem: 15850
[21:24:00.535907] Test: Total time: 0:01:00 (0.1747 s / it)
[21:24:10.523425] Test:  [ 0/57]  eta: 0:00:24  loss: 0.4388 (0.4388)  time: 0.4315  data: 0.2677  max mem: 15850
[21:24:12.183966] Test:  [10/57]  eta: 0:00:08  loss: 0.4334 (0.4278)  time: 0.1901  data: 0.0244  max mem: 15850
[21:24:13.849129] Test:  [20/57]  eta: 0:00:06  loss: 0.4562 (0.4396)  time: 0.1662  data: 0.0001  max mem: 15850
[21:24:15.518956] Test:  [30/57]  eta: 0:00:04  loss: 0.2808 (0.3715)  time: 0.1667  data: 0.0001  max mem: 15850
[21:24:17.193884] Test:  [40/57]  eta: 0:00:02  loss: 0.2350 (0.3418)  time: 0.1672  data: 0.0001  max mem: 15850
[21:24:18.871769] Test:  [50/57]  eta: 0:00:01  loss: 0.2510 (0.3323)  time: 0.1676  data: 0.0001  max mem: 15850
[21:24:19.777952] Test:  [56/57]  eta: 0:00:00  loss: 0.2562 (0.3329)  time: 0.1627  data: 0.0000  max mem: 15850
[21:24:19.853589] Test: Total time: 0:00:09 (0.1713 s / it)
[21:24:21.481580] Dice score of the network on the train images: 0.816032, val images: 0.821910
[21:24:21.485698] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:24:22.381722] Epoch: [16]  [  0/345]  eta: 0:05:08  lr: 0.000100  loss: 0.2184 (0.2184)  time: 0.8951  data: 0.2939  max mem: 15850
[21:24:34.349883] Epoch: [16]  [ 20/345]  eta: 0:03:19  lr: 0.000100  loss: 0.2084 (0.2135)  time: 0.5984  data: 0.0001  max mem: 15850
[21:24:46.470567] Epoch: [16]  [ 40/345]  eta: 0:03:05  lr: 0.000101  loss: 0.2194 (0.2182)  time: 0.6060  data: 0.0001  max mem: 15850
[21:24:58.477901] Epoch: [16]  [ 60/345]  eta: 0:02:52  lr: 0.000101  loss: 0.2153 (0.2170)  time: 0.6003  data: 0.0001  max mem: 15850
[21:25:10.505074] Epoch: [16]  [ 80/345]  eta: 0:02:40  lr: 0.000101  loss: 0.2168 (0.2180)  time: 0.6013  data: 0.0001  max mem: 15850
[21:25:22.539720] Epoch: [16]  [100/345]  eta: 0:02:28  lr: 0.000102  loss: 0.2144 (0.2182)  time: 0.6017  data: 0.0001  max mem: 15850
[21:25:34.595075] Epoch: [16]  [120/345]  eta: 0:02:15  lr: 0.000102  loss: 0.2130 (0.2167)  time: 0.6027  data: 0.0001  max mem: 15850
[21:25:46.657359] Epoch: [16]  [140/345]  eta: 0:02:03  lr: 0.000103  loss: 0.2054 (0.2156)  time: 0.6031  data: 0.0001  max mem: 15850
[21:25:58.733097] Epoch: [16]  [160/345]  eta: 0:01:51  lr: 0.000103  loss: 0.2144 (0.2160)  time: 0.6037  data: 0.0001  max mem: 15850
[21:26:10.809747] Epoch: [16]  [180/345]  eta: 0:01:39  lr: 0.000103  loss: 0.2029 (0.2154)  time: 0.6038  data: 0.0001  max mem: 15850
[21:26:22.884503] Epoch: [16]  [200/345]  eta: 0:01:27  lr: 0.000104  loss: 0.2081 (0.2152)  time: 0.6037  data: 0.0001  max mem: 15850
[21:26:34.967078] Epoch: [16]  [220/345]  eta: 0:01:15  lr: 0.000104  loss: 0.2224 (0.2163)  time: 0.6041  data: 0.0001  max mem: 15850
[21:26:47.052974] Epoch: [16]  [240/345]  eta: 0:01:03  lr: 0.000104  loss: 0.2284 (0.2179)  time: 0.6042  data: 0.0001  max mem: 15850
[21:26:59.124781] Epoch: [16]  [260/345]  eta: 0:00:51  lr: 0.000105  loss: 0.2077 (0.2179)  time: 0.6035  data: 0.0001  max mem: 15850
[21:27:11.190728] Epoch: [16]  [280/345]  eta: 0:00:39  lr: 0.000105  loss: 0.2117 (0.2174)  time: 0.6033  data: 0.0001  max mem: 15850
[21:27:23.336714] Epoch: [16]  [300/345]  eta: 0:00:27  lr: 0.000105  loss: 0.2210 (0.2183)  time: 0.6073  data: 0.0001  max mem: 15850
[21:27:35.394288] Epoch: [16]  [320/345]  eta: 0:00:15  lr: 0.000106  loss: 0.2103 (0.2178)  time: 0.6028  data: 0.0001  max mem: 15850
[21:27:47.450089] Epoch: [16]  [340/345]  eta: 0:00:03  lr: 0.000106  loss: 0.2110 (0.2180)  time: 0.6028  data: 0.0001  max mem: 15850
[21:27:49.858815] Epoch: [16]  [344/345]  eta: 0:00:00  lr: 0.000106  loss: 0.2254 (0.2181)  time: 0.6026  data: 0.0001  max mem: 15850
[21:27:49.929874] Epoch: [16] Total time: 0:03:28 (0.6042 s / it)
[21:27:49.930103] Averaged stats: lr: 0.000106  loss: 0.2254 (0.2181)
[21:27:50.392808] Test:  [  0/345]  eta: 0:02:37  loss: 0.1701 (0.1701)  time: 0.4573  data: 0.2919  max mem: 15850
[21:27:52.072103] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1972 (0.1983)  time: 0.1942  data: 0.0266  max mem: 15850
[21:27:53.752689] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2103 (0.2040)  time: 0.1679  data: 0.0001  max mem: 15850
[21:27:55.437580] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2012 (0.2023)  time: 0.1682  data: 0.0001  max mem: 15850
[21:27:57.125789] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2012 (0.2034)  time: 0.1686  data: 0.0001  max mem: 15850
[21:27:58.816414] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2128 (0.2055)  time: 0.1689  data: 0.0001  max mem: 15850
[21:28:00.511817] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2140 (0.2069)  time: 0.1692  data: 0.0001  max mem: 15850
[21:28:02.209421] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2140 (0.2076)  time: 0.1696  data: 0.0001  max mem: 15850
[21:28:03.909759] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2067 (0.2090)  time: 0.1698  data: 0.0001  max mem: 15850
[21:28:05.615876] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1958 (0.2086)  time: 0.1703  data: 0.0001  max mem: 15850
[21:28:07.325563] Test:  [100/345]  eta: 0:00:42  loss: 0.2043 (0.2087)  time: 0.1707  data: 0.0001  max mem: 15850
[21:28:09.039731] Test:  [110/345]  eta: 0:00:40  loss: 0.2072 (0.2090)  time: 0.1711  data: 0.0001  max mem: 15850
[21:28:10.755300] Test:  [120/345]  eta: 0:00:38  loss: 0.2040 (0.2084)  time: 0.1714  data: 0.0001  max mem: 15850
[21:28:12.475211] Test:  [130/345]  eta: 0:00:36  loss: 0.1937 (0.2075)  time: 0.1717  data: 0.0001  max mem: 15850
[21:28:14.199159] Test:  [140/345]  eta: 0:00:35  loss: 0.1937 (0.2067)  time: 0.1721  data: 0.0001  max mem: 15850
[21:28:15.925284] Test:  [150/345]  eta: 0:00:33  loss: 0.2033 (0.2080)  time: 0.1724  data: 0.0001  max mem: 15850
[21:28:17.654525] Test:  [160/345]  eta: 0:00:31  loss: 0.2173 (0.2086)  time: 0.1727  data: 0.0001  max mem: 15850
[21:28:19.388901] Test:  [170/345]  eta: 0:00:30  loss: 0.2059 (0.2081)  time: 0.1731  data: 0.0001  max mem: 15850
[21:28:21.126409] Test:  [180/345]  eta: 0:00:28  loss: 0.2010 (0.2082)  time: 0.1735  data: 0.0001  max mem: 15850
[21:28:22.866377] Test:  [190/345]  eta: 0:00:26  loss: 0.2031 (0.2080)  time: 0.1738  data: 0.0001  max mem: 15850
[21:28:24.610503] Test:  [200/345]  eta: 0:00:25  loss: 0.1987 (0.2080)  time: 0.1741  data: 0.0001  max mem: 15850
[21:28:26.357587] Test:  [210/345]  eta: 0:00:23  loss: 0.2106 (0.2080)  time: 0.1745  data: 0.0001  max mem: 15850
[21:28:28.107581] Test:  [220/345]  eta: 0:00:21  loss: 0.2104 (0.2079)  time: 0.1748  data: 0.0001  max mem: 15850
[21:28:29.861705] Test:  [230/345]  eta: 0:00:19  loss: 0.2097 (0.2078)  time: 0.1751  data: 0.0001  max mem: 15850
[21:28:31.618330] Test:  [240/345]  eta: 0:00:18  loss: 0.2043 (0.2077)  time: 0.1755  data: 0.0001  max mem: 15850
[21:28:33.379594] Test:  [250/345]  eta: 0:00:16  loss: 0.2032 (0.2074)  time: 0.1758  data: 0.0001  max mem: 15850
[21:28:35.145198] Test:  [260/345]  eta: 0:00:14  loss: 0.1947 (0.2070)  time: 0.1763  data: 0.0001  max mem: 15850
[21:28:36.913261] Test:  [270/345]  eta: 0:00:12  loss: 0.2118 (0.2072)  time: 0.1766  data: 0.0001  max mem: 15850
[21:28:38.686078] Test:  [280/345]  eta: 0:00:11  loss: 0.2082 (0.2070)  time: 0.1770  data: 0.0001  max mem: 15850
[21:28:40.459939] Test:  [290/345]  eta: 0:00:09  loss: 0.2084 (0.2078)  time: 0.1773  data: 0.0001  max mem: 15850
[21:28:42.238546] Test:  [300/345]  eta: 0:00:07  loss: 0.2129 (0.2081)  time: 0.1776  data: 0.0001  max mem: 15850
[21:28:44.021982] Test:  [310/345]  eta: 0:00:06  loss: 0.2052 (0.2083)  time: 0.1780  data: 0.0001  max mem: 15850
[21:28:45.807026] Test:  [320/345]  eta: 0:00:04  loss: 0.2052 (0.2084)  time: 0.1784  data: 0.0001  max mem: 15850
[21:28:47.596512] Test:  [330/345]  eta: 0:00:02  loss: 0.2135 (0.2088)  time: 0.1787  data: 0.0001  max mem: 15850
[21:28:49.388399] Test:  [340/345]  eta: 0:00:00  loss: 0.1992 (0.2082)  time: 0.1790  data: 0.0001  max mem: 15850
[21:28:50.106859] Test:  [344/345]  eta: 0:00:00  loss: 0.1992 (0.2081)  time: 0.1791  data: 0.0001  max mem: 15850
[21:28:50.180100] Test: Total time: 0:01:00 (0.1746 s / it)
[21:29:00.034409] Test:  [ 0/57]  eta: 0:00:24  loss: 0.4658 (0.4658)  time: 0.4318  data: 0.2680  max mem: 15850
[21:29:01.696715] Test:  [10/57]  eta: 0:00:08  loss: 0.4444 (0.4506)  time: 0.1903  data: 0.0244  max mem: 15850
[21:29:03.363216] Test:  [20/57]  eta: 0:00:06  loss: 0.4444 (0.4467)  time: 0.1664  data: 0.0001  max mem: 15850
[21:29:05.031478] Test:  [30/57]  eta: 0:00:04  loss: 0.2490 (0.3754)  time: 0.1667  data: 0.0001  max mem: 15850
[21:29:06.705181] Test:  [40/57]  eta: 0:00:02  loss: 0.2305 (0.3409)  time: 0.1670  data: 0.0001  max mem: 15850
[21:29:08.381557] Test:  [50/57]  eta: 0:00:01  loss: 0.2466 (0.3331)  time: 0.1674  data: 0.0001  max mem: 15850
[21:29:09.287013] Test:  [56/57]  eta: 0:00:00  loss: 0.3134 (0.3437)  time: 0.1625  data: 0.0000  max mem: 15850
[21:29:09.354497] Test: Total time: 0:00:09 (0.1711 s / it)
[21:29:10.981935] Dice score of the network on the train images: 0.798576, val images: 0.808319
[21:29:10.986098] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:29:11.866446] Epoch: [17]  [  0/345]  eta: 0:05:03  lr: 0.000106  loss: 0.2069 (0.2069)  time: 0.8794  data: 0.2795  max mem: 15850
[21:29:23.831649] Epoch: [17]  [ 20/345]  eta: 0:03:18  lr: 0.000107  loss: 0.2087 (0.2157)  time: 0.5982  data: 0.0001  max mem: 15850
[21:29:35.827832] Epoch: [17]  [ 40/345]  eta: 0:03:04  lr: 0.000107  loss: 0.2021 (0.2134)  time: 0.5998  data: 0.0001  max mem: 15850
[21:29:47.837152] Epoch: [17]  [ 60/345]  eta: 0:02:52  lr: 0.000107  loss: 0.2064 (0.2104)  time: 0.6004  data: 0.0001  max mem: 15850
[21:29:59.869000] Epoch: [17]  [ 80/345]  eta: 0:02:39  lr: 0.000108  loss: 0.2146 (0.2124)  time: 0.6015  data: 0.0001  max mem: 15850
[21:30:11.911844] Epoch: [17]  [100/345]  eta: 0:02:27  lr: 0.000108  loss: 0.2091 (0.2116)  time: 0.6021  data: 0.0001  max mem: 15850
[21:30:23.966680] Epoch: [17]  [120/345]  eta: 0:02:15  lr: 0.000108  loss: 0.2195 (0.2119)  time: 0.6027  data: 0.0001  max mem: 15850
[21:30:36.035777] Epoch: [17]  [140/345]  eta: 0:02:03  lr: 0.000109  loss: 0.1893 (0.2101)  time: 0.6034  data: 0.0001  max mem: 15850
[21:30:48.108448] Epoch: [17]  [160/345]  eta: 0:01:51  lr: 0.000109  loss: 0.2355 (0.2131)  time: 0.6036  data: 0.0001  max mem: 15850
[21:31:00.181644] Epoch: [17]  [180/345]  eta: 0:01:39  lr: 0.000110  loss: 0.2203 (0.2142)  time: 0.6036  data: 0.0001  max mem: 15850
[21:31:12.257659] Epoch: [17]  [200/345]  eta: 0:01:27  lr: 0.000110  loss: 0.2090 (0.2134)  time: 0.6038  data: 0.0001  max mem: 15850
[21:31:24.332693] Epoch: [17]  [220/345]  eta: 0:01:15  lr: 0.000110  loss: 0.2081 (0.2137)  time: 0.6037  data: 0.0001  max mem: 15850
[21:31:36.394695] Epoch: [17]  [240/345]  eta: 0:01:03  lr: 0.000111  loss: 0.2093 (0.2136)  time: 0.6031  data: 0.0001  max mem: 15850
[21:31:48.438918] Epoch: [17]  [260/345]  eta: 0:00:51  lr: 0.000111  loss: 0.2087 (0.2137)  time: 0.6022  data: 0.0001  max mem: 15850
[21:32:00.482817] Epoch: [17]  [280/345]  eta: 0:00:39  lr: 0.000111  loss: 0.2208 (0.2141)  time: 0.6021  data: 0.0001  max mem: 15850
[21:32:12.522064] Epoch: [17]  [300/345]  eta: 0:00:27  lr: 0.000112  loss: 0.2004 (0.2135)  time: 0.6019  data: 0.0001  max mem: 15850
[21:32:24.556457] Epoch: [17]  [320/345]  eta: 0:00:15  lr: 0.000112  loss: 0.2078 (0.2135)  time: 0.6017  data: 0.0001  max mem: 15850
[21:32:36.585628] Epoch: [17]  [340/345]  eta: 0:00:03  lr: 0.000112  loss: 0.2072 (0.2131)  time: 0.6014  data: 0.0001  max mem: 15850
[21:32:38.994808] Epoch: [17]  [344/345]  eta: 0:00:00  lr: 0.000112  loss: 0.2090 (0.2131)  time: 0.6015  data: 0.0001  max mem: 15850
[21:32:39.068685] Epoch: [17] Total time: 0:03:28 (0.6031 s / it)
[21:32:39.069174] Averaged stats: lr: 0.000112  loss: 0.2090 (0.2131)
[21:32:39.570618] Test:  [  0/345]  eta: 0:02:51  loss: 0.1702 (0.1702)  time: 0.4959  data: 0.3315  max mem: 15850
[21:32:41.249332] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1851 (0.1879)  time: 0.1976  data: 0.0302  max mem: 15850
[21:32:42.930092] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1851 (0.1913)  time: 0.1679  data: 0.0001  max mem: 15850
[21:32:44.613598] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1815 (0.1900)  time: 0.1681  data: 0.0001  max mem: 15850
[21:32:46.301096] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1991 (0.1953)  time: 0.1685  data: 0.0001  max mem: 15850
[21:32:47.992289] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1991 (0.1943)  time: 0.1689  data: 0.0001  max mem: 15850
[21:32:49.686656] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1909 (0.1959)  time: 0.1692  data: 0.0001  max mem: 15850
[21:32:51.384780] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1909 (0.1963)  time: 0.1696  data: 0.0001  max mem: 15850
[21:32:53.085366] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1915 (0.1967)  time: 0.1699  data: 0.0001  max mem: 15850
[21:32:54.790550] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1923 (0.1973)  time: 0.1702  data: 0.0001  max mem: 15850
[21:32:56.499312] Test:  [100/345]  eta: 0:00:42  loss: 0.1943 (0.1975)  time: 0.1706  data: 0.0001  max mem: 15850
[21:32:58.210983] Test:  [110/345]  eta: 0:00:40  loss: 0.2005 (0.1985)  time: 0.1710  data: 0.0001  max mem: 15850
[21:32:59.926092] Test:  [120/345]  eta: 0:00:38  loss: 0.1916 (0.1973)  time: 0.1713  data: 0.0001  max mem: 15850
[21:33:01.645042] Test:  [130/345]  eta: 0:00:37  loss: 0.1940 (0.1977)  time: 0.1716  data: 0.0001  max mem: 15850
[21:33:03.368092] Test:  [140/345]  eta: 0:00:35  loss: 0.1929 (0.1965)  time: 0.1720  data: 0.0001  max mem: 15850
[21:33:05.093270] Test:  [150/345]  eta: 0:00:33  loss: 0.1847 (0.1972)  time: 0.1724  data: 0.0001  max mem: 15850
[21:33:06.821287] Test:  [160/345]  eta: 0:00:31  loss: 0.1912 (0.1972)  time: 0.1726  data: 0.0001  max mem: 15850
[21:33:08.555177] Test:  [170/345]  eta: 0:00:30  loss: 0.1869 (0.1967)  time: 0.1730  data: 0.0001  max mem: 15850
[21:33:10.290715] Test:  [180/345]  eta: 0:00:28  loss: 0.1858 (0.1964)  time: 0.1734  data: 0.0001  max mem: 15850
[21:33:12.029470] Test:  [190/345]  eta: 0:00:26  loss: 0.1880 (0.1966)  time: 0.1737  data: 0.0001  max mem: 15850
[21:33:13.771707] Test:  [200/345]  eta: 0:00:25  loss: 0.1840 (0.1962)  time: 0.1740  data: 0.0001  max mem: 15850
[21:33:15.517425] Test:  [210/345]  eta: 0:00:23  loss: 0.1995 (0.1968)  time: 0.1743  data: 0.0001  max mem: 15850
[21:33:17.267517] Test:  [220/345]  eta: 0:00:21  loss: 0.2011 (0.1972)  time: 0.1747  data: 0.0001  max mem: 15850
[21:33:19.020299] Test:  [230/345]  eta: 0:00:19  loss: 0.1994 (0.1973)  time: 0.1751  data: 0.0001  max mem: 15850
[21:33:20.777204] Test:  [240/345]  eta: 0:00:18  loss: 0.2071 (0.1980)  time: 0.1754  data: 0.0001  max mem: 15850
[21:33:22.537968] Test:  [250/345]  eta: 0:00:16  loss: 0.2088 (0.1980)  time: 0.1758  data: 0.0001  max mem: 15850
[21:33:24.303370] Test:  [260/345]  eta: 0:00:14  loss: 0.1978 (0.1983)  time: 0.1762  data: 0.0001  max mem: 15850
[21:33:26.071475] Test:  [270/345]  eta: 0:00:13  loss: 0.1957 (0.1989)  time: 0.1766  data: 0.0001  max mem: 15850
[21:33:27.842369] Test:  [280/345]  eta: 0:00:11  loss: 0.1922 (0.1985)  time: 0.1768  data: 0.0001  max mem: 15850
[21:33:29.617552] Test:  [290/345]  eta: 0:00:09  loss: 0.1930 (0.1986)  time: 0.1772  data: 0.0001  max mem: 15850
[21:33:31.395461] Test:  [300/345]  eta: 0:00:07  loss: 0.1941 (0.1984)  time: 0.1776  data: 0.0001  max mem: 15850
[21:33:33.175981] Test:  [310/345]  eta: 0:00:06  loss: 0.1941 (0.1985)  time: 0.1779  data: 0.0001  max mem: 15850
[21:33:34.959371] Test:  [320/345]  eta: 0:00:04  loss: 0.1902 (0.1981)  time: 0.1781  data: 0.0001  max mem: 15850
[21:33:36.749181] Test:  [330/345]  eta: 0:00:02  loss: 0.1902 (0.1980)  time: 0.1786  data: 0.0001  max mem: 15850
[21:33:38.538407] Test:  [340/345]  eta: 0:00:00  loss: 0.1879 (0.1980)  time: 0.1789  data: 0.0001  max mem: 15850
[21:33:39.256450] Test:  [344/345]  eta: 0:00:00  loss: 0.2057 (0.1983)  time: 0.1790  data: 0.0001  max mem: 15850
[21:33:39.322451] Test: Total time: 0:01:00 (0.1746 s / it)
[21:33:49.150197] Test:  [ 0/57]  eta: 0:00:24  loss: 0.4995 (0.4995)  time: 0.4334  data: 0.2697  max mem: 15850
[21:33:50.809752] Test:  [10/57]  eta: 0:00:08  loss: 0.4306 (0.4509)  time: 0.1902  data: 0.0246  max mem: 15850
[21:33:52.474743] Test:  [20/57]  eta: 0:00:06  loss: 0.4339 (0.4499)  time: 0.1661  data: 0.0001  max mem: 15850
[21:33:54.143380] Test:  [30/57]  eta: 0:00:04  loss: 0.2738 (0.3804)  time: 0.1666  data: 0.0001  max mem: 15850
[21:33:55.816270] Test:  [40/57]  eta: 0:00:02  loss: 0.2326 (0.3497)  time: 0.1670  data: 0.0001  max mem: 15850
[21:33:57.493637] Test:  [50/57]  eta: 0:00:01  loss: 0.2585 (0.3399)  time: 0.1675  data: 0.0001  max mem: 15850
[21:33:58.399559] Test:  [56/57]  eta: 0:00:00  loss: 0.2829 (0.3436)  time: 0.1626  data: 0.0001  max mem: 15850
[21:33:58.474978] Test: Total time: 0:00:09 (0.1712 s / it)
[21:34:00.129430] Dice score of the network on the train images: 0.821511, val images: 0.812512
[21:34:00.133450] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:34:01.027140] Epoch: [18]  [  0/345]  eta: 0:05:07  lr: 0.000113  loss: 0.1804 (0.1804)  time: 0.8925  data: 0.2920  max mem: 15850
[21:34:12.996518] Epoch: [18]  [ 20/345]  eta: 0:03:19  lr: 0.000113  loss: 0.2106 (0.2116)  time: 0.5984  data: 0.0001  max mem: 15850
[21:34:24.986360] Epoch: [18]  [ 40/345]  eta: 0:03:04  lr: 0.000113  loss: 0.1998 (0.2074)  time: 0.5994  data: 0.0001  max mem: 15850
[21:34:37.001458] Epoch: [18]  [ 60/345]  eta: 0:02:52  lr: 0.000114  loss: 0.2212 (0.2106)  time: 0.6007  data: 0.0001  max mem: 15850
[21:34:49.035535] Epoch: [18]  [ 80/345]  eta: 0:02:39  lr: 0.000114  loss: 0.2022 (0.2095)  time: 0.6017  data: 0.0001  max mem: 15850
[21:35:01.076689] Epoch: [18]  [100/345]  eta: 0:02:27  lr: 0.000114  loss: 0.2156 (0.2115)  time: 0.6020  data: 0.0001  max mem: 15850
[21:35:13.136072] Epoch: [18]  [120/345]  eta: 0:02:15  lr: 0.000115  loss: 0.2276 (0.2137)  time: 0.6029  data: 0.0001  max mem: 15850
[21:35:25.202919] Epoch: [18]  [140/345]  eta: 0:02:03  lr: 0.000115  loss: 0.2097 (0.2127)  time: 0.6033  data: 0.0001  max mem: 15850
[21:35:37.277369] Epoch: [18]  [160/345]  eta: 0:01:51  lr: 0.000115  loss: 0.2045 (0.2116)  time: 0.6037  data: 0.0001  max mem: 15850
[21:35:49.353349] Epoch: [18]  [180/345]  eta: 0:01:39  lr: 0.000116  loss: 0.2082 (0.2114)  time: 0.6038  data: 0.0001  max mem: 15850
[21:36:01.429010] Epoch: [18]  [200/345]  eta: 0:01:27  lr: 0.000116  loss: 0.1947 (0.2103)  time: 0.6037  data: 0.0001  max mem: 15850
[21:36:13.504396] Epoch: [18]  [220/345]  eta: 0:01:15  lr: 0.000116  loss: 0.1943 (0.2096)  time: 0.6037  data: 0.0001  max mem: 15850
[21:36:25.575113] Epoch: [18]  [240/345]  eta: 0:01:03  lr: 0.000117  loss: 0.2029 (0.2088)  time: 0.6035  data: 0.0001  max mem: 15850
[21:36:37.646661] Epoch: [18]  [260/345]  eta: 0:00:51  lr: 0.000117  loss: 0.1981 (0.2082)  time: 0.6035  data: 0.0001  max mem: 15850
[21:36:49.713943] Epoch: [18]  [280/345]  eta: 0:00:39  lr: 0.000118  loss: 0.2020 (0.2078)  time: 0.6033  data: 0.0001  max mem: 15850
[21:37:01.775042] Epoch: [18]  [300/345]  eta: 0:00:27  lr: 0.000118  loss: 0.1956 (0.2074)  time: 0.6030  data: 0.0001  max mem: 15850
[21:37:13.921668] Epoch: [18]  [320/345]  eta: 0:00:15  lr: 0.000118  loss: 0.1993 (0.2069)  time: 0.6073  data: 0.0001  max mem: 15850
[21:37:25.969650] Epoch: [18]  [340/345]  eta: 0:00:03  lr: 0.000119  loss: 0.2058 (0.2071)  time: 0.6024  data: 0.0001  max mem: 15850
[21:37:28.381592] Epoch: [18]  [344/345]  eta: 0:00:00  lr: 0.000119  loss: 0.1907 (0.2070)  time: 0.6023  data: 0.0001  max mem: 15850
[21:37:28.446640] Epoch: [18] Total time: 0:03:28 (0.6038 s / it)
[21:37:28.446955] Averaged stats: lr: 0.000119  loss: 0.1907 (0.2070)
[21:37:28.914043] Test:  [  0/345]  eta: 0:02:39  loss: 0.2003 (0.2003)  time: 0.4615  data: 0.2965  max mem: 15850
[21:37:30.592558] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2003 (0.1980)  time: 0.1944  data: 0.0270  max mem: 15850
[21:37:32.272558] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2063 (0.2037)  time: 0.1678  data: 0.0001  max mem: 15850
[21:37:33.956809] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2055 (0.2026)  time: 0.1682  data: 0.0001  max mem: 15850
[21:37:35.643917] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1889 (0.1983)  time: 0.1685  data: 0.0001  max mem: 15850
[21:37:37.335390] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1863 (0.1968)  time: 0.1689  data: 0.0001  max mem: 15850
[21:37:39.030666] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1814 (0.1943)  time: 0.1693  data: 0.0001  max mem: 15850
[21:37:40.729322] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1789 (0.1924)  time: 0.1696  data: 0.0001  max mem: 15850
[21:37:42.431124] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1789 (0.1921)  time: 0.1700  data: 0.0001  max mem: 15850
[21:37:44.136896] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1917 (0.1930)  time: 0.1703  data: 0.0001  max mem: 15850
[21:37:45.844927] Test:  [100/345]  eta: 0:00:42  loss: 0.1922 (0.1935)  time: 0.1706  data: 0.0001  max mem: 15850
[21:37:47.558221] Test:  [110/345]  eta: 0:00:40  loss: 0.1958 (0.1937)  time: 0.1710  data: 0.0001  max mem: 15850
[21:37:49.274067] Test:  [120/345]  eta: 0:00:38  loss: 0.1996 (0.1944)  time: 0.1714  data: 0.0001  max mem: 15850
[21:37:50.992515] Test:  [130/345]  eta: 0:00:36  loss: 0.1977 (0.1943)  time: 0.1716  data: 0.0001  max mem: 15850
[21:37:52.714814] Test:  [140/345]  eta: 0:00:35  loss: 0.2005 (0.1959)  time: 0.1720  data: 0.0001  max mem: 15850
[21:37:54.440865] Test:  [150/345]  eta: 0:00:33  loss: 0.1990 (0.1960)  time: 0.1724  data: 0.0001  max mem: 15850
[21:37:56.170967] Test:  [160/345]  eta: 0:00:31  loss: 0.1978 (0.1960)  time: 0.1727  data: 0.0001  max mem: 15850
[21:37:57.903278] Test:  [170/345]  eta: 0:00:30  loss: 0.2029 (0.1969)  time: 0.1731  data: 0.0001  max mem: 15850
[21:37:59.639657] Test:  [180/345]  eta: 0:00:28  loss: 0.2046 (0.1970)  time: 0.1734  data: 0.0001  max mem: 15850
[21:38:01.380267] Test:  [190/345]  eta: 0:00:26  loss: 0.1966 (0.1968)  time: 0.1738  data: 0.0001  max mem: 15850
[21:38:03.123625] Test:  [200/345]  eta: 0:00:25  loss: 0.1838 (0.1965)  time: 0.1741  data: 0.0001  max mem: 15850
[21:38:04.869865] Test:  [210/345]  eta: 0:00:23  loss: 0.1904 (0.1966)  time: 0.1744  data: 0.0001  max mem: 15850
[21:38:06.619484] Test:  [220/345]  eta: 0:00:21  loss: 0.1954 (0.1969)  time: 0.1747  data: 0.0001  max mem: 15850
[21:38:08.373938] Test:  [230/345]  eta: 0:00:19  loss: 0.2072 (0.1975)  time: 0.1751  data: 0.0001  max mem: 15850
[21:38:10.129963] Test:  [240/345]  eta: 0:00:18  loss: 0.1913 (0.1980)  time: 0.1755  data: 0.0001  max mem: 15850
[21:38:11.891206] Test:  [250/345]  eta: 0:00:16  loss: 0.1799 (0.1978)  time: 0.1758  data: 0.0001  max mem: 15850
[21:38:13.656251] Test:  [260/345]  eta: 0:00:14  loss: 0.1799 (0.1973)  time: 0.1763  data: 0.0001  max mem: 15850
[21:38:15.423676] Test:  [270/345]  eta: 0:00:12  loss: 0.1924 (0.1970)  time: 0.1766  data: 0.0001  max mem: 15850
[21:38:17.195396] Test:  [280/345]  eta: 0:00:11  loss: 0.1952 (0.1971)  time: 0.1769  data: 0.0001  max mem: 15850
[21:38:18.968708] Test:  [290/345]  eta: 0:00:09  loss: 0.1985 (0.1975)  time: 0.1772  data: 0.0001  max mem: 15850
[21:38:20.746414] Test:  [300/345]  eta: 0:00:07  loss: 0.1972 (0.1974)  time: 0.1775  data: 0.0001  max mem: 15850
[21:38:22.528497] Test:  [310/345]  eta: 0:00:06  loss: 0.1972 (0.1975)  time: 0.1779  data: 0.0001  max mem: 15850
[21:38:24.313995] Test:  [320/345]  eta: 0:00:04  loss: 0.2016 (0.1977)  time: 0.1783  data: 0.0001  max mem: 15850
[21:38:26.101815] Test:  [330/345]  eta: 0:00:02  loss: 0.1936 (0.1975)  time: 0.1786  data: 0.0001  max mem: 15850
[21:38:27.894632] Test:  [340/345]  eta: 0:00:00  loss: 0.1938 (0.1976)  time: 0.1790  data: 0.0001  max mem: 15850
[21:38:28.612861] Test:  [344/345]  eta: 0:00:00  loss: 0.1930 (0.1974)  time: 0.1792  data: 0.0001  max mem: 15850
[21:38:28.683307] Test: Total time: 0:01:00 (0.1746 s / it)
[21:38:38.550555] Test:  [ 0/57]  eta: 0:00:24  loss: 0.4723 (0.4723)  time: 0.4346  data: 0.2713  max mem: 15850
[21:38:40.212071] Test:  [10/57]  eta: 0:00:08  loss: 0.4104 (0.4214)  time: 0.1905  data: 0.0247  max mem: 15850
[21:38:41.878440] Test:  [20/57]  eta: 0:00:06  loss: 0.4312 (0.4186)  time: 0.1663  data: 0.0001  max mem: 15850
[21:38:43.548444] Test:  [30/57]  eta: 0:00:04  loss: 0.2507 (0.3539)  time: 0.1668  data: 0.0001  max mem: 15850
[21:38:45.221800] Test:  [40/57]  eta: 0:00:02  loss: 0.2195 (0.3231)  time: 0.1671  data: 0.0001  max mem: 15850
[21:38:46.899012] Test:  [50/57]  eta: 0:00:01  loss: 0.2287 (0.3144)  time: 0.1675  data: 0.0001  max mem: 15850
[21:38:47.805465] Test:  [56/57]  eta: 0:00:00  loss: 0.2590 (0.3162)  time: 0.1626  data: 0.0001  max mem: 15850
[21:38:47.878513] Test: Total time: 0:00:09 (0.1713 s / it)
[21:38:49.516085] Dice score of the network on the train images: 0.811016, val images: 0.822823
[21:38:49.516319] saving best_rec_model_0 @ epoch 18
[21:38:50.723286] saving best_dice_model_0 @ epoch 18
[21:38:51.911999] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:38:52.831706] Epoch: [19]  [  0/345]  eta: 0:05:16  lr: 0.000119  loss: 0.1894 (0.1894)  time: 0.9186  data: 0.3196  max mem: 15850
[21:39:04.767050] Epoch: [19]  [ 20/345]  eta: 0:03:18  lr: 0.000119  loss: 0.1998 (0.1998)  time: 0.5967  data: 0.0001  max mem: 15850
[21:39:16.727092] Epoch: [19]  [ 40/345]  eta: 0:03:04  lr: 0.000119  loss: 0.1936 (0.1966)  time: 0.5980  data: 0.0001  max mem: 15850
[21:39:28.712763] Epoch: [19]  [ 60/345]  eta: 0:02:51  lr: 0.000120  loss: 0.1868 (0.1953)  time: 0.5992  data: 0.0001  max mem: 15850
[21:39:40.721342] Epoch: [19]  [ 80/345]  eta: 0:02:39  lr: 0.000120  loss: 0.1833 (0.1935)  time: 0.6004  data: 0.0001  max mem: 15850
[21:39:52.860834] Epoch: [19]  [100/345]  eta: 0:02:27  lr: 0.000121  loss: 0.2067 (0.1965)  time: 0.6069  data: 0.0001  max mem: 15850
[21:40:04.897242] Epoch: [19]  [120/345]  eta: 0:02:15  lr: 0.000121  loss: 0.2005 (0.1972)  time: 0.6018  data: 0.0001  max mem: 15850
[21:40:16.973866] Epoch: [19]  [140/345]  eta: 0:02:03  lr: 0.000121  loss: 0.2070 (0.1990)  time: 0.6038  data: 0.0001  max mem: 15850
[21:40:29.062409] Epoch: [19]  [160/345]  eta: 0:01:51  lr: 0.000122  loss: 0.1952 (0.1989)  time: 0.6044  data: 0.0001  max mem: 15850
[21:40:41.144042] Epoch: [19]  [180/345]  eta: 0:01:39  lr: 0.000122  loss: 0.1881 (0.1986)  time: 0.6040  data: 0.0001  max mem: 15850
[21:40:53.231559] Epoch: [19]  [200/345]  eta: 0:01:27  lr: 0.000122  loss: 0.1966 (0.1986)  time: 0.6043  data: 0.0001  max mem: 15850
[21:41:05.319908] Epoch: [19]  [220/345]  eta: 0:01:15  lr: 0.000123  loss: 0.1945 (0.1988)  time: 0.6044  data: 0.0001  max mem: 15850
[21:41:17.403676] Epoch: [19]  [240/345]  eta: 0:01:03  lr: 0.000123  loss: 0.2041 (0.2002)  time: 0.6041  data: 0.0001  max mem: 15850

[21:41:29.463649] Epoch: [19]  [260/345]  eta: 0:00:51  lr: 0.000123  loss: 0.1976 (0.2001)  time: 0.6030  data: 0.0001  max mem: 15850
[21:41:41.507177] Epoch: [19]  [280/345]  eta: 0:00:39  lr: 0.000124  loss: 0.2027 (0.2008)  time: 0.6021  data: 0.0001  max mem: 15850
[21:41:53.554464] Epoch: [19]  [300/345]  eta: 0:00:27  lr: 0.000124  loss: 0.1919 (0.2005)  time: 0.6023  data: 0.0001  max mem: 15850
[21:42:05.599144] Epoch: [19]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.1889 (0.1999)  time: 0.6022  data: 0.0001  max mem: 15850
[21:42:17.633615] Epoch: [19]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.2091 (0.2005)  time: 0.6017  data: 0.0001  max mem: 15850
[21:42:20.042568] Epoch: [19]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.2120 (0.2006)  time: 0.6016  data: 0.0001  max mem: 15850
[21:42:20.113026] Epoch: [19] Total time: 0:03:28 (0.6035 s / it)
[21:42:20.113729] Averaged stats: lr: 0.000125  loss: 0.2120 (0.2006)
[21:42:20.571720] Test:  [  0/345]  eta: 0:02:36  loss: 0.1974 (0.1974)  time: 0.4524  data: 0.2872  max mem: 15850
[21:42:22.250934] Test:  [ 10/345]  eta: 0:01:04  loss: 0.1858 (0.1842)  time: 0.1937  data: 0.0262  max mem: 15850
[21:42:23.932210] Test:  [ 20/345]  eta: 0:00:58  loss: 0.1770 (0.1810)  time: 0.1680  data: 0.0001  max mem: 15850
[21:42:25.617355] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1819 (0.1874)  time: 0.1683  data: 0.0001  max mem: 15850
[21:42:27.306323] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1926 (0.1885)  time: 0.1686  data: 0.0001  max mem: 15850
[21:42:28.997686] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1899 (0.1884)  time: 0.1690  data: 0.0001  max mem: 15850
[21:42:30.693077] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1899 (0.1893)  time: 0.1693  data: 0.0001  max mem: 15850
[21:42:32.391434] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1933 (0.1894)  time: 0.1696  data: 0.0001  max mem: 15850
[21:42:34.093884] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1933 (0.1901)  time: 0.1700  data: 0.0001  max mem: 15850
[21:42:35.798974] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1894 (0.1898)  time: 0.1703  data: 0.0001  max mem: 15850
[21:42:37.506671] Test:  [100/345]  eta: 0:00:42  loss: 0.1905 (0.1898)  time: 0.1706  data: 0.0001  max mem: 15850
[21:42:39.219353] Test:  [110/345]  eta: 0:00:40  loss: 0.1825 (0.1893)  time: 0.1710  data: 0.0001  max mem: 15850
[21:42:40.935664] Test:  [120/345]  eta: 0:00:38  loss: 0.1861 (0.1895)  time: 0.1714  data: 0.0001  max mem: 15850
[21:42:42.654636] Test:  [130/345]  eta: 0:00:36  loss: 0.1987 (0.1907)  time: 0.1717  data: 0.0001  max mem: 15850
[21:42:44.378900] Test:  [140/345]  eta: 0:00:35  loss: 0.2025 (0.1913)  time: 0.1721  data: 0.0001  max mem: 15850
[21:42:46.103905] Test:  [150/345]  eta: 0:00:33  loss: 0.1963 (0.1916)  time: 0.1724  data: 0.0001  max mem: 15850
[21:42:47.832894] Test:  [160/345]  eta: 0:00:31  loss: 0.1845 (0.1904)  time: 0.1726  data: 0.0001  max mem: 15850
[21:42:49.565899] Test:  [170/345]  eta: 0:00:30  loss: 0.1827 (0.1903)  time: 0.1730  data: 0.0001  max mem: 15850
[21:42:51.301467] Test:  [180/345]  eta: 0:00:28  loss: 0.1873 (0.1903)  time: 0.1734  data: 0.0001  max mem: 15850
[21:42:53.041111] Test:  [190/345]  eta: 0:00:26  loss: 0.1781 (0.1892)  time: 0.1737  data: 0.0001  max mem: 15850
[21:42:54.783527] Test:  [200/345]  eta: 0:00:24  loss: 0.1822 (0.1897)  time: 0.1740  data: 0.0001  max mem: 15850
[21:42:56.530266] Test:  [210/345]  eta: 0:00:23  loss: 0.1865 (0.1891)  time: 0.1744  data: 0.0001  max mem: 15850
[21:42:58.280686] Test:  [220/345]  eta: 0:00:21  loss: 0.1829 (0.1892)  time: 0.1748  data: 0.0001  max mem: 15850
[21:43:00.034419] Test:  [230/345]  eta: 0:00:19  loss: 0.1813 (0.1886)  time: 0.1751  data: 0.0001  max mem: 15850
[21:43:01.790388] Test:  [240/345]  eta: 0:00:18  loss: 0.1735 (0.1889)  time: 0.1754  data: 0.0001  max mem: 15850
[21:43:03.551869] Test:  [250/345]  eta: 0:00:16  loss: 0.1860 (0.1890)  time: 0.1758  data: 0.0001  max mem: 15850
[21:43:05.315572] Test:  [260/345]  eta: 0:00:14  loss: 0.1879 (0.1888)  time: 0.1762  data: 0.0001  max mem: 15850
[21:43:07.082614] Test:  [270/345]  eta: 0:00:12  loss: 0.1879 (0.1890)  time: 0.1765  data: 0.0001  max mem: 15850
[21:43:08.852517] Test:  [280/345]  eta: 0:00:11  loss: 0.2000 (0.1896)  time: 0.1768  data: 0.0001  max mem: 15850
[21:43:10.626369] Test:  [290/345]  eta: 0:00:09  loss: 0.1796 (0.1893)  time: 0.1771  data: 0.0001  max mem: 15850
[21:43:12.404671] Test:  [300/345]  eta: 0:00:07  loss: 0.1790 (0.1897)  time: 0.1775  data: 0.0001  max mem: 15850
[21:43:14.185347] Test:  [310/345]  eta: 0:00:06  loss: 0.2018 (0.1900)  time: 0.1779  data: 0.0001  max mem: 15850
[21:43:15.969568] Test:  [320/345]  eta: 0:00:04  loss: 0.1915 (0.1900)  time: 0.1782  data: 0.0001  max mem: 15850
[21:43:17.760770] Test:  [330/345]  eta: 0:00:02  loss: 0.1851 (0.1898)  time: 0.1787  data: 0.0001  max mem: 15850
[21:43:19.550846] Test:  [340/345]  eta: 0:00:00  loss: 0.1827 (0.1898)  time: 0.1790  data: 0.0001  max mem: 15850
[21:43:20.268226] Test:  [344/345]  eta: 0:00:00  loss: 0.1892 (0.1899)  time: 0.1790  data: 0.0001  max mem: 15850
[21:43:20.335689] Test: Total time: 0:01:00 (0.1745 s / it)
[21:43:30.229915] Test:  [ 0/57]  eta: 0:00:24  loss: 0.4978 (0.4978)  time: 0.4335  data: 0.2696  max mem: 15850
[21:43:31.890915] Test:  [10/57]  eta: 0:00:08  loss: 0.4241 (0.4563)  time: 0.1903  data: 0.0246  max mem: 15850
[21:43:33.555527] Test:  [20/57]  eta: 0:00:06  loss: 0.4241 (0.4516)  time: 0.1662  data: 0.0001  max mem: 15850
[21:43:35.223511] Test:  [30/57]  eta: 0:00:04  loss: 0.2670 (0.3780)  time: 0.1666  data: 0.0001  max mem: 15850
[21:43:36.896332] Test:  [40/57]  eta: 0:00:02  loss: 0.2280 (0.3437)  time: 0.1670  data: 0.0001  max mem: 15850
[21:43:38.572245] Test:  [50/57]  eta: 0:00:01  loss: 0.2373 (0.3347)  time: 0.1674  data: 0.0001  max mem: 15850
[21:43:39.477524] Test:  [56/57]  eta: 0:00:00  loss: 0.3108 (0.3485)  time: 0.1625  data: 0.0001  max mem: 15850
[21:43:39.544664] Test: Total time: 0:00:09 (0.1710 s / it)
[21:43:41.208889] Dice score of the network on the train images: 0.824453, val images: 0.806236
[21:43:41.213006] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:43:42.153493] Epoch: [20]  [  0/345]  eta: 0:05:24  lr: 0.000125  loss: 0.1714 (0.1714)  time: 0.9393  data: 0.3382  max mem: 15850
[21:43:54.127365] Epoch: [20]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.1975 (0.1943)  time: 0.5986  data: 0.0001  max mem: 15850
[21:44:06.108584] Epoch: [20]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.1993 (0.1987)  time: 0.5990  data: 0.0001  max mem: 15850
[21:44:18.114219] Epoch: [20]  [ 60/345]  eta: 0:02:52  lr: 0.000125  loss: 0.1837 (0.1956)  time: 0.6002  data: 0.0001  max mem: 15850
[21:44:30.141254] Epoch: [20]  [ 80/345]  eta: 0:02:40  lr: 0.000125  loss: 0.2047 (0.1978)  time: 0.6013  data: 0.0001  max mem: 15850
[21:44:42.182831] Epoch: [20]  [100/345]  eta: 0:02:27  lr: 0.000125  loss: 0.1979 (0.1977)  time: 0.6020  data: 0.0001  max mem: 15850
[21:44:54.237273] Epoch: [20]  [120/345]  eta: 0:02:15  lr: 0.000125  loss: 0.1905 (0.1967)  time: 0.6027  data: 0.0001  max mem: 15850
[21:45:06.299247] Epoch: [20]  [140/345]  eta: 0:02:03  lr: 0.000125  loss: 0.1998 (0.1981)  time: 0.6030  data: 0.0001  max mem: 15850
[21:45:18.365476] Epoch: [20]  [160/345]  eta: 0:01:51  lr: 0.000125  loss: 0.1921 (0.1979)  time: 0.6033  data: 0.0001  max mem: 15850
[21:45:30.435562] Epoch: [20]  [180/345]  eta: 0:01:39  lr: 0.000125  loss: 0.1952 (0.1976)  time: 0.6035  data: 0.0001  max mem: 15850
[21:45:42.516295] Epoch: [20]  [200/345]  eta: 0:01:27  lr: 0.000125  loss: 0.1914 (0.1972)  time: 0.6040  data: 0.0001  max mem: 15850
[21:45:54.585401] Epoch: [20]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.1885 (0.1964)  time: 0.6034  data: 0.0001  max mem: 15850
[21:46:06.655028] Epoch: [20]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.1823 (0.1948)  time: 0.6034  data: 0.0001  max mem: 15850
[21:46:18.714350] Epoch: [20]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.2012 (0.1958)  time: 0.6029  data: 0.0001  max mem: 15850
[21:46:30.777720] Epoch: [20]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.1996 (0.1966)  time: 0.6031  data: 0.0001  max mem: 15850
[21:46:42.841797] Epoch: [20]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.1921 (0.1969)  time: 0.6032  data: 0.0001  max mem: 15850
[21:46:54.899235] Epoch: [20]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.1953 (0.1969)  time: 0.6028  data: 0.0001  max mem: 15850
[21:47:06.948843] Epoch: [20]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.1924 (0.1967)  time: 0.6024  data: 0.0001  max mem: 15850
[21:47:09.356180] Epoch: [20]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.1948 (0.1968)  time: 0.6024  data: 0.0001  max mem: 15850
[21:47:09.435274] Epoch: [20] Total time: 0:03:28 (0.6035 s / it)
[21:47:09.435410] Averaged stats: lr: 0.000125  loss: 0.1948 (0.1968)
[21:47:09.961136] Test:  [  0/345]  eta: 0:02:59  loss: 0.1843 (0.1843)  time: 0.5213  data: 0.3558  max mem: 15850
[21:47:11.639775] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1843 (0.1892)  time: 0.1999  data: 0.0324  max mem: 15850
[21:47:13.321125] Test:  [ 20/345]  eta: 0:01:00  loss: 0.1774 (0.1859)  time: 0.1679  data: 0.0001  max mem: 15850
[21:47:15.006417] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1756 (0.1833)  time: 0.1683  data: 0.0001  max mem: 15850
[21:47:16.693739] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1858 (0.1857)  time: 0.1686  data: 0.0001  max mem: 15850
[21:47:18.384941] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1834 (0.1840)  time: 0.1689  data: 0.0001  max mem: 15850
[21:47:20.078593] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1762 (0.1847)  time: 0.1692  data: 0.0001  max mem: 15850
[21:47:21.776754] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1789 (0.1851)  time: 0.1695  data: 0.0001  max mem: 15850
[21:47:23.478772] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1804 (0.1839)  time: 0.1699  data: 0.0001  max mem: 15850
[21:47:25.183928] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1804 (0.1861)  time: 0.1703  data: 0.0001  max mem: 15850
[21:47:26.892544] Test:  [100/345]  eta: 0:00:42  loss: 0.1989 (0.1887)  time: 0.1706  data: 0.0001  max mem: 15850
[21:47:28.605293] Test:  [110/345]  eta: 0:00:40  loss: 0.1878 (0.1878)  time: 0.1710  data: 0.0001  max mem: 15850
[21:47:30.320824] Test:  [120/345]  eta: 0:00:38  loss: 0.1738 (0.1873)  time: 0.1714  data: 0.0001  max mem: 15850
[21:47:32.039523] Test:  [130/345]  eta: 0:00:37  loss: 0.1737 (0.1868)  time: 0.1716  data: 0.0001  max mem: 15850
[21:47:33.761925] Test:  [140/345]  eta: 0:00:35  loss: 0.1808 (0.1871)  time: 0.1720  data: 0.0001  max mem: 15850
[21:47:35.488418] Test:  [150/345]  eta: 0:00:33  loss: 0.1834 (0.1873)  time: 0.1724  data: 0.0001  max mem: 15850
[21:47:37.216604] Test:  [160/345]  eta: 0:00:31  loss: 0.1854 (0.1877)  time: 0.1727  data: 0.0001  max mem: 15850
[21:47:38.950500] Test:  [170/345]  eta: 0:00:30  loss: 0.1799 (0.1869)  time: 0.1730  data: 0.0001  max mem: 15850
[21:47:40.687285] Test:  [180/345]  eta: 0:00:28  loss: 0.1663 (0.1863)  time: 0.1735  data: 0.0001  max mem: 15850
[21:47:42.426878] Test:  [190/345]  eta: 0:00:26  loss: 0.1736 (0.1864)  time: 0.1738  data: 0.0001  max mem: 15850
[21:47:44.171066] Test:  [200/345]  eta: 0:00:25  loss: 0.1867 (0.1862)  time: 0.1741  data: 0.0001  max mem: 15850
[21:47:45.916961] Test:  [210/345]  eta: 0:00:23  loss: 0.1746 (0.1860)  time: 0.1744  data: 0.0001  max mem: 15850
[21:47:47.666917] Test:  [220/345]  eta: 0:00:21  loss: 0.1734 (0.1857)  time: 0.1747  data: 0.0001  max mem: 15850
[21:47:49.420139] Test:  [230/345]  eta: 0:00:19  loss: 0.1822 (0.1857)  time: 0.1751  data: 0.0001  max mem: 15850
[21:47:51.177295] Test:  [240/345]  eta: 0:00:18  loss: 0.1858 (0.1857)  time: 0.1755  data: 0.0001  max mem: 15850
[21:47:52.937625] Test:  [250/345]  eta: 0:00:16  loss: 0.1838 (0.1852)  time: 0.1758  data: 0.0001  max mem: 15850
[21:47:54.701849] Test:  [260/345]  eta: 0:00:14  loss: 0.1771 (0.1850)  time: 0.1762  data: 0.0001  max mem: 15850
[21:47:56.468840] Test:  [270/345]  eta: 0:00:13  loss: 0.1771 (0.1846)  time: 0.1765  data: 0.0001  max mem: 15850
[21:47:58.238711] Test:  [280/345]  eta: 0:00:11  loss: 0.1735 (0.1849)  time: 0.1768  data: 0.0001  max mem: 15850
[21:48:00.012795] Test:  [290/345]  eta: 0:00:09  loss: 0.1826 (0.1848)  time: 0.1771  data: 0.0001  max mem: 15850
[21:48:01.790526] Test:  [300/345]  eta: 0:00:07  loss: 0.1832 (0.1853)  time: 0.1775  data: 0.0001  max mem: 15850
[21:48:03.572264] Test:  [310/345]  eta: 0:00:06  loss: 0.1929 (0.1856)  time: 0.1779  data: 0.0001  max mem: 15850
[21:48:05.356228] Test:  [320/345]  eta: 0:00:04  loss: 0.1790 (0.1853)  time: 0.1782  data: 0.0001  max mem: 15850
[21:48:07.147402] Test:  [330/345]  eta: 0:00:02  loss: 0.1739 (0.1851)  time: 0.1787  data: 0.0001  max mem: 15850
[21:48:08.937730] Test:  [340/345]  eta: 0:00:00  loss: 0.1729 (0.1849)  time: 0.1790  data: 0.0001  max mem: 15850
[21:48:09.655075] Test:  [344/345]  eta: 0:00:00  loss: 0.1772 (0.1848)  time: 0.1791  data: 0.0001  max mem: 15850
[21:48:09.727997] Test: Total time: 0:01:00 (0.1748 s / it)
[21:48:19.592041] Test:  [ 0/57]  eta: 0:00:28  loss: 0.5332 (0.5332)  time: 0.4963  data: 0.3321  max mem: 15850
[21:48:21.251071] Test:  [10/57]  eta: 0:00:09  loss: 0.4195 (0.4631)  time: 0.1959  data: 0.0303  max mem: 15850
[21:48:22.915503] Test:  [20/57]  eta: 0:00:06  loss: 0.4686 (0.4589)  time: 0.1661  data: 0.0001  max mem: 15850
[21:48:24.584977] Test:  [30/57]  eta: 0:00:04  loss: 0.2738 (0.3818)  time: 0.1666  data: 0.0001  max mem: 15850
[21:48:26.258078] Test:  [40/57]  eta: 0:00:02  loss: 0.2251 (0.3463)  time: 0.1671  data: 0.0001  max mem: 15850
[21:48:27.934747] Test:  [50/57]  eta: 0:00:01  loss: 0.2445 (0.3384)  time: 0.1674  data: 0.0001  max mem: 15850
[21:48:28.840857] Test:  [56/57]  eta: 0:00:00  loss: 0.2786 (0.3450)  time: 0.1626  data: 0.0000  max mem: 15850
[21:48:28.899718] Test: Total time: 0:00:09 (0.1720 s / it)
[21:48:30.541774] Dice score of the network on the train images: 0.837836, val images: 0.819929
[21:48:30.542005] saving best_prec_model_0 @ epoch 20
[21:48:31.779527] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:48:32.754103] Epoch: [21]  [  0/345]  eta: 0:05:35  lr: 0.000125  loss: 0.1709 (0.1709)  time: 0.9735  data: 0.3734  max mem: 15850
[21:48:44.709256] Epoch: [21]  [ 20/345]  eta: 0:03:20  lr: 0.000125  loss: 0.1916 (0.1851)  time: 0.5977  data: 0.0001  max mem: 15850
[21:48:56.796781] Epoch: [21]  [ 40/345]  eta: 0:03:06  lr: 0.000125  loss: 0.1924 (0.1933)  time: 0.6043  data: 0.0001  max mem: 15850
[21:49:08.794968] Epoch: [21]  [ 60/345]  eta: 0:02:52  lr: 0.000125  loss: 0.1845 (0.1896)  time: 0.5999  data: 0.0001  max mem: 15850
[21:49:20.818515] Epoch: [21]  [ 80/345]  eta: 0:02:40  lr: 0.000124  loss: 0.1769 (0.1881)  time: 0.6011  data: 0.0001  max mem: 15850
[21:49:32.851843] Epoch: [21]  [100/345]  eta: 0:02:28  lr: 0.000124  loss: 0.1878 (0.1888)  time: 0.6016  data: 0.0001  max mem: 15850
[21:49:44.902052] Epoch: [21]  [120/345]  eta: 0:02:15  lr: 0.000124  loss: 0.1867 (0.1887)  time: 0.6025  data: 0.0001  max mem: 15850
[21:49:56.961210] Epoch: [21]  [140/345]  eta: 0:02:03  lr: 0.000124  loss: 0.1872 (0.1897)  time: 0.6029  data: 0.0001  max mem: 15850
[21:50:09.019917] Epoch: [21]  [160/345]  eta: 0:01:51  lr: 0.000124  loss: 0.1933 (0.1901)  time: 0.6029  data: 0.0001  max mem: 15850
[21:50:21.081921] Epoch: [21]  [180/345]  eta: 0:01:39  lr: 0.000124  loss: 0.1815 (0.1894)  time: 0.6031  data: 0.0001  max mem: 15850
[21:50:33.140575] Epoch: [21]  [200/345]  eta: 0:01:27  lr: 0.000124  loss: 0.1784 (0.1884)  time: 0.6029  data: 0.0001  max mem: 15850
[21:50:45.200405] Epoch: [21]  [220/345]  eta: 0:01:15  lr: 0.000124  loss: 0.1710 (0.1884)  time: 0.6029  data: 0.0001  max mem: 15850
[21:50:57.273001] Epoch: [21]  [240/345]  eta: 0:01:03  lr: 0.000124  loss: 0.1870 (0.1884)  time: 0.6036  data: 0.0001  max mem: 15850
[21:51:09.326238] Epoch: [21]  [260/345]  eta: 0:00:51  lr: 0.000124  loss: 0.1932 (0.1889)  time: 0.6026  data: 0.0001  max mem: 15850
[21:51:21.372050] Epoch: [21]  [280/345]  eta: 0:00:39  lr: 0.000124  loss: 0.1856 (0.1888)  time: 0.6022  data: 0.0001  max mem: 15850
[21:51:33.413771] Epoch: [21]  [300/345]  eta: 0:00:27  lr: 0.000124  loss: 0.1940 (0.1895)  time: 0.6020  data: 0.0001  max mem: 15850
[21:51:45.455345] Epoch: [21]  [320/345]  eta: 0:00:15  lr: 0.000124  loss: 0.1852 (0.1893)  time: 0.6020  data: 0.0001  max mem: 15850
[21:51:57.493476] Epoch: [21]  [340/345]  eta: 0:00:03  lr: 0.000124  loss: 0.1759 (0.1886)  time: 0.6019  data: 0.0001  max mem: 15850
[21:51:59.900739] Epoch: [21]  [344/345]  eta: 0:00:00  lr: 0.000124  loss: 0.1759 (0.1886)  time: 0.6018  data: 0.0001  max mem: 15850
[21:51:59.973725] Epoch: [21] Total time: 0:03:28 (0.6035 s / it)
[21:51:59.973962] Averaged stats: lr: 0.000124  loss: 0.1759 (0.1886)
[21:52:00.471165] Test:  [  0/345]  eta: 0:02:49  loss: 0.1679 (0.1679)  time: 0.4918  data: 0.3270  max mem: 15850
[21:52:02.149996] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1792 (0.1848)  time: 0.1972  data: 0.0298  max mem: 15850
[21:52:03.830882] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1787 (0.1806)  time: 0.1679  data: 0.0001  max mem: 15850
[21:52:05.516364] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1749 (0.1832)  time: 0.1683  data: 0.0001  max mem: 15850
[21:52:07.207562] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1784 (0.1812)  time: 0.1688  data: 0.0001  max mem: 15850
[21:52:08.899555] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1784 (0.1831)  time: 0.1691  data: 0.0001  max mem: 15850
[21:52:10.594685] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1821 (0.1830)  time: 0.1693  data: 0.0001  max mem: 15850
[21:52:12.294060] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1674 (0.1822)  time: 0.1697  data: 0.0001  max mem: 15850
[21:52:13.996799] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1694 (0.1812)  time: 0.1700  data: 0.0001  max mem: 15850
[21:52:15.702127] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1745 (0.1811)  time: 0.1703  data: 0.0001  max mem: 15850
[21:52:17.411155] Test:  [100/345]  eta: 0:00:42  loss: 0.1723 (0.1805)  time: 0.1707  data: 0.0001  max mem: 15850
[21:52:19.123009] Test:  [110/345]  eta: 0:00:40  loss: 0.1748 (0.1806)  time: 0.1710  data: 0.0001  max mem: 15850
[21:52:20.838374] Test:  [120/345]  eta: 0:00:38  loss: 0.1769 (0.1799)  time: 0.1713  data: 0.0001  max mem: 15850
[21:52:22.558287] Test:  [130/345]  eta: 0:00:37  loss: 0.1769 (0.1804)  time: 0.1717  data: 0.0001  max mem: 15850
[21:52:24.280704] Test:  [140/345]  eta: 0:00:35  loss: 0.1766 (0.1801)  time: 0.1721  data: 0.0001  max mem: 15850
[21:52:26.006377] Test:  [150/345]  eta: 0:00:33  loss: 0.1727 (0.1806)  time: 0.1723  data: 0.0001  max mem: 15850
[21:52:27.737150] Test:  [160/345]  eta: 0:00:31  loss: 0.1661 (0.1798)  time: 0.1728  data: 0.0001  max mem: 15850
[21:52:29.471448] Test:  [170/345]  eta: 0:00:30  loss: 0.1668 (0.1797)  time: 0.1732  data: 0.0001  max mem: 15850
[21:52:31.208388] Test:  [180/345]  eta: 0:00:28  loss: 0.1799 (0.1801)  time: 0.1735  data: 0.0001  max mem: 15850
[21:52:32.950421] Test:  [190/345]  eta: 0:00:26  loss: 0.1665 (0.1796)  time: 0.1739  data: 0.0001  max mem: 15850
[21:52:34.694667] Test:  [200/345]  eta: 0:00:25  loss: 0.1693 (0.1794)  time: 0.1742  data: 0.0001  max mem: 15850
[21:52:36.441600] Test:  [210/345]  eta: 0:00:23  loss: 0.1719 (0.1789)  time: 0.1745  data: 0.0001  max mem: 15850
[21:52:38.191635] Test:  [220/345]  eta: 0:00:21  loss: 0.1644 (0.1789)  time: 0.1748  data: 0.0001  max mem: 15850
[21:52:39.946376] Test:  [230/345]  eta: 0:00:19  loss: 0.1783 (0.1790)  time: 0.1752  data: 0.0001  max mem: 15850
[21:52:41.703396] Test:  [240/345]  eta: 0:00:18  loss: 0.1844 (0.1790)  time: 0.1755  data: 0.0001  max mem: 15850
[21:52:43.466499] Test:  [250/345]  eta: 0:00:16  loss: 0.1839 (0.1791)  time: 0.1759  data: 0.0001  max mem: 15850
[21:52:45.231194] Test:  [260/345]  eta: 0:00:14  loss: 0.1701 (0.1790)  time: 0.1763  data: 0.0001  max mem: 15850
[21:52:46.998473] Test:  [270/345]  eta: 0:00:13  loss: 0.1732 (0.1789)  time: 0.1765  data: 0.0001  max mem: 15850
[21:52:48.770745] Test:  [280/345]  eta: 0:00:11  loss: 0.1768 (0.1790)  time: 0.1769  data: 0.0001  max mem: 15850
[21:52:50.544503] Test:  [290/345]  eta: 0:00:09  loss: 0.1759 (0.1791)  time: 0.1772  data: 0.0001  max mem: 15850
[21:52:52.323556] Test:  [300/345]  eta: 0:00:07  loss: 0.1725 (0.1787)  time: 0.1776  data: 0.0001  max mem: 15850
[21:52:54.105116] Test:  [310/345]  eta: 0:00:06  loss: 0.1726 (0.1786)  time: 0.1780  data: 0.0001  max mem: 15850
[21:52:55.889455] Test:  [320/345]  eta: 0:00:04  loss: 0.1769 (0.1787)  time: 0.1782  data: 0.0001  max mem: 15850
[21:52:57.678324] Test:  [330/345]  eta: 0:00:02  loss: 0.1797 (0.1792)  time: 0.1786  data: 0.0001  max mem: 15850
[21:52:59.471916] Test:  [340/345]  eta: 0:00:00  loss: 0.1966 (0.1796)  time: 0.1791  data: 0.0001  max mem: 15850
[21:53:00.190295] Test:  [344/345]  eta: 0:00:00  loss: 0.1797 (0.1796)  time: 0.1793  data: 0.0001  max mem: 15850
[21:53:00.253701] Test: Total time: 0:01:00 (0.1747 s / it)
[21:53:10.079288] Test:  [ 0/57]  eta: 0:00:24  loss: 0.4817 (0.4817)  time: 0.4353  data: 0.2715  max mem: 15850
[21:53:11.740216] Test:  [10/57]  eta: 0:00:08  loss: 0.3826 (0.4302)  time: 0.1905  data: 0.0248  max mem: 15850
[21:53:13.406172] Test:  [20/57]  eta: 0:00:06  loss: 0.4330 (0.4344)  time: 0.1663  data: 0.0001  max mem: 15850
[21:53:15.075040] Test:  [30/57]  eta: 0:00:04  loss: 0.2474 (0.3644)  time: 0.1667  data: 0.0001  max mem: 15850
[21:53:16.748655] Test:  [40/57]  eta: 0:00:02  loss: 0.2057 (0.3290)  time: 0.1671  data: 0.0001  max mem: 15850
[21:53:18.425675] Test:  [50/57]  eta: 0:00:01  loss: 0.2161 (0.3218)  time: 0.1675  data: 0.0001  max mem: 15850
[21:53:19.330889] Test:  [56/57]  eta: 0:00:00  loss: 0.2672 (0.3283)  time: 0.1626  data: 0.0000  max mem: 15850
[21:53:19.396566] Test: Total time: 0:00:09 (0.1711 s / it)
[21:53:21.048511] Dice score of the network on the train images: 0.825333, val images: 0.820003
[21:53:21.053318] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:53:21.930880] Epoch: [22]  [  0/345]  eta: 0:05:02  lr: 0.000124  loss: 0.1475 (0.1475)  time: 0.8764  data: 0.2763  max mem: 15850
[21:53:33.885079] Epoch: [22]  [ 20/345]  eta: 0:03:18  lr: 0.000124  loss: 0.1825 (0.1778)  time: 0.5977  data: 0.0001  max mem: 15850
[21:53:45.859902] Epoch: [22]  [ 40/345]  eta: 0:03:04  lr: 0.000123  loss: 0.1780 (0.1771)  time: 0.5987  data: 0.0001  max mem: 15850
[21:53:57.862102] Epoch: [22]  [ 60/345]  eta: 0:02:51  lr: 0.000123  loss: 0.1851 (0.1799)  time: 0.6001  data: 0.0001  max mem: 15850
[21:54:09.860643] Epoch: [22]  [ 80/345]  eta: 0:02:39  lr: 0.000123  loss: 0.1737 (0.1800)  time: 0.5999  data: 0.0001  max mem: 15850
[21:54:21.888682] Epoch: [22]  [100/345]  eta: 0:02:27  lr: 0.000123  loss: 0.1840 (0.1810)  time: 0.6014  data: 0.0001  max mem: 15850
[21:54:33.930579] Epoch: [22]  [120/345]  eta: 0:02:15  lr: 0.000123  loss: 0.1786 (0.1816)  time: 0.6021  data: 0.0001  max mem: 15850
[21:54:45.980371] Epoch: [22]  [140/345]  eta: 0:02:03  lr: 0.000123  loss: 0.1750 (0.1809)  time: 0.6024  data: 0.0001  max mem: 15850
[21:54:58.031903] Epoch: [22]  [160/345]  eta: 0:01:51  lr: 0.000123  loss: 0.1781 (0.1814)  time: 0.6025  data: 0.0001  max mem: 15850
[21:55:10.095559] Epoch: [22]  [180/345]  eta: 0:01:39  lr: 0.000123  loss: 0.1834 (0.1815)  time: 0.6031  data: 0.0001  max mem: 15850
[21:55:22.157052] Epoch: [22]  [200/345]  eta: 0:01:27  lr: 0.000123  loss: 0.1765 (0.1812)  time: 0.6030  data: 0.0001  max mem: 15850
[21:55:34.215631] Epoch: [22]  [220/345]  eta: 0:01:15  lr: 0.000123  loss: 0.1940 (0.1827)  time: 0.6029  data: 0.0001  max mem: 15850
[21:55:46.273861] Epoch: [22]  [240/345]  eta: 0:01:03  lr: 0.000123  loss: 0.1887 (0.1835)  time: 0.6029  data: 0.0001  max mem: 15850
[21:55:58.326445] Epoch: [22]  [260/345]  eta: 0:00:51  lr: 0.000122  loss: 0.1652 (0.1830)  time: 0.6026  data: 0.0001  max mem: 15850
[21:56:10.462274] Epoch: [22]  [280/345]  eta: 0:00:39  lr: 0.000122  loss: 0.1915 (0.1836)  time: 0.6067  data: 0.0001  max mem: 15850
[21:56:22.508062] Epoch: [22]  [300/345]  eta: 0:00:27  lr: 0.000122  loss: 0.1722 (0.1832)  time: 0.6022  data: 0.0001  max mem: 15850
[21:56:34.550016] Epoch: [22]  [320/345]  eta: 0:00:15  lr: 0.000122  loss: 0.1834 (0.1830)  time: 0.6021  data: 0.0001  max mem: 15850

[21:56:46.584946] Epoch: [22]  [340/345]  eta: 0:00:03  lr: 0.000122  loss: 0.1858 (0.1831)  time: 0.6017  data: 0.0001  max mem: 15850
[21:56:48.990210] Epoch: [22]  [344/345]  eta: 0:00:00  lr: 0.000122  loss: 0.1873 (0.1831)  time: 0.6016  data: 0.0001  max mem: 15850
[21:56:49.062139] Epoch: [22] Total time: 0:03:28 (0.6029 s / it)
[21:56:49.062575] Averaged stats: lr: 0.000122  loss: 0.1873 (0.1831)
[21:56:49.573141] Test:  [  0/345]  eta: 0:02:54  loss: 0.1879 (0.1879)  time: 0.5051  data: 0.3399  max mem: 15850
[21:56:51.251814] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1681 (0.1716)  time: 0.1985  data: 0.0310  max mem: 15850
[21:56:52.933579] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1706 (0.1771)  time: 0.1679  data: 0.0001  max mem: 15850
[21:56:54.618693] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1722 (0.1745)  time: 0.1683  data: 0.0001  max mem: 15850
[21:56:56.307011] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1705 (0.1757)  time: 0.1686  data: 0.0001  max mem: 15850
[21:56:57.997322] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1733 (0.1756)  time: 0.1689  data: 0.0001  max mem: 15850
[21:56:59.691999] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1786 (0.1786)  time: 0.1692  data: 0.0001  max mem: 15850
[21:57:01.390388] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1795 (0.1777)  time: 0.1696  data: 0.0001  max mem: 15850
[21:57:03.092109] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1751 (0.1776)  time: 0.1699  data: 0.0001  max mem: 15850
[21:57:04.797550] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1751 (0.1777)  time: 0.1703  data: 0.0001  max mem: 15850
[21:57:06.505662] Test:  [100/345]  eta: 0:00:42  loss: 0.1718 (0.1767)  time: 0.1706  data: 0.0001  max mem: 15850
[21:57:08.218260] Test:  [110/345]  eta: 0:00:40  loss: 0.1700 (0.1764)  time: 0.1710  data: 0.0001  max mem: 15850
[21:57:09.933749] Test:  [120/345]  eta: 0:00:38  loss: 0.1733 (0.1767)  time: 0.1713  data: 0.0001  max mem: 15850
[21:57:11.652471] Test:  [130/345]  eta: 0:00:37  loss: 0.1803 (0.1768)  time: 0.1716  data: 0.0001  max mem: 15850
[21:57:13.375478] Test:  [140/345]  eta: 0:00:35  loss: 0.1603 (0.1753)  time: 0.1720  data: 0.0001  max mem: 15850
[21:57:15.102099] Test:  [150/345]  eta: 0:00:33  loss: 0.1603 (0.1754)  time: 0.1724  data: 0.0001  max mem: 15850
[21:57:16.829996] Test:  [160/345]  eta: 0:00:31  loss: 0.1714 (0.1755)  time: 0.1727  data: 0.0001  max mem: 15850
[21:57:18.562384] Test:  [170/345]  eta: 0:00:30  loss: 0.1795 (0.1761)  time: 0.1729  data: 0.0001  max mem: 15850
[21:57:20.298076] Test:  [180/345]  eta: 0:00:28  loss: 0.1739 (0.1757)  time: 0.1733  data: 0.0001  max mem: 15850
[21:57:22.037780] Test:  [190/345]  eta: 0:00:26  loss: 0.1602 (0.1755)  time: 0.1737  data: 0.0001  max mem: 15850
[21:57:23.780025] Test:  [200/345]  eta: 0:00:25  loss: 0.1646 (0.1754)  time: 0.1740  data: 0.0001  max mem: 15850
[21:57:25.526286] Test:  [210/345]  eta: 0:00:23  loss: 0.1710 (0.1755)  time: 0.1744  data: 0.0001  max mem: 15850
[21:57:27.276341] Test:  [220/345]  eta: 0:00:21  loss: 0.1645 (0.1750)  time: 0.1748  data: 0.0001  max mem: 15850
[21:57:29.028632] Test:  [230/345]  eta: 0:00:19  loss: 0.1637 (0.1746)  time: 0.1751  data: 0.0001  max mem: 15850
[21:57:30.784698] Test:  [240/345]  eta: 0:00:18  loss: 0.1671 (0.1745)  time: 0.1754  data: 0.0001  max mem: 15850
[21:57:32.544516] Test:  [250/345]  eta: 0:00:16  loss: 0.1691 (0.1743)  time: 0.1757  data: 0.0001  max mem: 15850
[21:57:34.308336] Test:  [260/345]  eta: 0:00:14  loss: 0.1723 (0.1745)  time: 0.1761  data: 0.0001  max mem: 15850
[21:57:36.075152] Test:  [270/345]  eta: 0:00:13  loss: 0.1738 (0.1745)  time: 0.1765  data: 0.0001  max mem: 15850
[21:57:37.846473] Test:  [280/345]  eta: 0:00:11  loss: 0.1805 (0.1748)  time: 0.1768  data: 0.0001  max mem: 15850
[21:57:39.620531] Test:  [290/345]  eta: 0:00:09  loss: 0.1787 (0.1749)  time: 0.1772  data: 0.0001  max mem: 15850
[21:57:41.396792] Test:  [300/345]  eta: 0:00:07  loss: 0.1772 (0.1753)  time: 0.1775  data: 0.0001  max mem: 15850
[21:57:43.177326] Test:  [310/345]  eta: 0:00:06  loss: 0.1752 (0.1752)  time: 0.1778  data: 0.0001  max mem: 15850
[21:57:44.960569] Test:  [320/345]  eta: 0:00:04  loss: 0.1763 (0.1756)  time: 0.1781  data: 0.0001  max mem: 15850
[21:57:46.746957] Test:  [330/345]  eta: 0:00:02  loss: 0.1856 (0.1759)  time: 0.1784  data: 0.0001  max mem: 15850
[21:57:48.539799] Test:  [340/345]  eta: 0:00:00  loss: 0.1743 (0.1757)  time: 0.1789  data: 0.0001  max mem: 15850
[21:57:49.258926] Test:  [344/345]  eta: 0:00:00  loss: 0.1748 (0.1759)  time: 0.1791  data: 0.0001  max mem: 15850
[21:57:49.327217] Test: Total time: 0:01:00 (0.1747 s / it)
[21:57:59.177664] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4605 (0.4605)  time: 0.4434  data: 0.2801  max mem: 15850
[21:58:00.836845] Test:  [10/57]  eta: 0:00:08  loss: 0.3826 (0.4196)  time: 0.1911  data: 0.0255  max mem: 15850
[21:58:02.499476] Test:  [20/57]  eta: 0:00:06  loss: 0.4301 (0.4221)  time: 0.1660  data: 0.0001  max mem: 15850
[21:58:04.166269] Test:  [30/57]  eta: 0:00:04  loss: 0.2452 (0.3549)  time: 0.1664  data: 0.0001  max mem: 15850
[21:58:05.838419] Test:  [40/57]  eta: 0:00:02  loss: 0.2237 (0.3238)  time: 0.1669  data: 0.0001  max mem: 15850
[21:58:07.513979] Test:  [50/57]  eta: 0:00:01  loss: 0.2419 (0.3167)  time: 0.1673  data: 0.0001  max mem: 15850
[21:58:08.418017] Test:  [56/57]  eta: 0:00:00  loss: 0.2713 (0.3243)  time: 0.1624  data: 0.0000  max mem: 15850
[21:58:08.488798] Test: Total time: 0:00:09 (0.1711 s / it)
[21:58:10.143736] Dice score of the network on the train images: 0.834140, val images: 0.818116
[21:58:10.148080] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[21:58:11.027171] Epoch: [23]  [  0/345]  eta: 0:05:02  lr: 0.000122  loss: 0.1771 (0.1771)  time: 0.8779  data: 0.2774  max mem: 15850
[21:58:22.998803] Epoch: [23]  [ 20/345]  eta: 0:03:18  lr: 0.000122  loss: 0.1690 (0.1741)  time: 0.5985  data: 0.0001  max mem: 15850
[21:58:34.991764] Epoch: [23]  [ 40/345]  eta: 0:03:04  lr: 0.000122  loss: 0.1836 (0.1817)  time: 0.5996  data: 0.0001  max mem: 15850
[21:58:47.011345] Epoch: [23]  [ 60/345]  eta: 0:02:52  lr: 0.000122  loss: 0.1712 (0.1806)  time: 0.6009  data: 0.0001  max mem: 15850
[21:58:59.043396] Epoch: [23]  [ 80/345]  eta: 0:02:39  lr: 0.000121  loss: 0.1746 (0.1805)  time: 0.6016  data: 0.0001  max mem: 15850
[21:59:11.087357] Epoch: [23]  [100/345]  eta: 0:02:27  lr: 0.000121  loss: 0.1687 (0.1794)  time: 0.6021  data: 0.0001  max mem: 15850
[21:59:23.142215] Epoch: [23]  [120/345]  eta: 0:02:15  lr: 0.000121  loss: 0.1819 (0.1800)  time: 0.6027  data: 0.0001  max mem: 15850
[21:59:35.218746] Epoch: [23]  [140/345]  eta: 0:02:03  lr: 0.000121  loss: 0.1757 (0.1795)  time: 0.6038  data: 0.0001  max mem: 15850
[21:59:47.299352] Epoch: [23]  [160/345]  eta: 0:01:51  lr: 0.000121  loss: 0.1793 (0.1795)  time: 0.6040  data: 0.0001  max mem: 15850
[21:59:59.379268] Epoch: [23]  [180/345]  eta: 0:01:39  lr: 0.000121  loss: 0.1821 (0.1800)  time: 0.6039  data: 0.0001  max mem: 15850
[22:00:11.459219] Epoch: [23]  [200/345]  eta: 0:01:27  lr: 0.000121  loss: 0.1826 (0.1804)  time: 0.6039  data: 0.0001  max mem: 15850
[22:00:23.539905] Epoch: [23]  [220/345]  eta: 0:01:15  lr: 0.000121  loss: 0.1720 (0.1798)  time: 0.6040  data: 0.0001  max mem: 15850
[22:00:35.603095] Epoch: [23]  [240/345]  eta: 0:01:03  lr: 0.000120  loss: 0.1773 (0.1799)  time: 0.6031  data: 0.0001  max mem: 15850
[22:00:47.658359] Epoch: [23]  [260/345]  eta: 0:00:51  lr: 0.000120  loss: 0.1694 (0.1799)  time: 0.6027  data: 0.0001  max mem: 15850
[22:00:59.705590] Epoch: [23]  [280/345]  eta: 0:00:39  lr: 0.000120  loss: 0.1697 (0.1794)  time: 0.6023  data: 0.0001  max mem: 15850
[22:01:11.756502] Epoch: [23]  [300/345]  eta: 0:00:27  lr: 0.000120  loss: 0.1764 (0.1794)  time: 0.6025  data: 0.0001  max mem: 15850
[22:01:23.801296] Epoch: [23]  [320/345]  eta: 0:00:15  lr: 0.000120  loss: 0.1783 (0.1795)  time: 0.6022  data: 0.0001  max mem: 15850
[22:01:35.838317] Epoch: [23]  [340/345]  eta: 0:00:03  lr: 0.000120  loss: 0.1814 (0.1796)  time: 0.6018  data: 0.0001  max mem: 15850
[22:01:38.247903] Epoch: [23]  [344/345]  eta: 0:00:00  lr: 0.000120  loss: 0.1769 (0.1795)  time: 0.6017  data: 0.0001  max mem: 15850
[22:01:38.321027] Epoch: [23] Total time: 0:03:28 (0.6034 s / it)
[22:01:38.321347] Averaged stats: lr: 0.000120  loss: 0.1769 (0.1795)
[22:01:38.789720] Test:  [  0/345]  eta: 0:02:39  loss: 0.1755 (0.1755)  time: 0.4633  data: 0.2983  max mem: 15850
[22:01:40.467825] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1676 (0.1773)  time: 0.1946  data: 0.0272  max mem: 15850
[22:01:42.148288] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1657 (0.1730)  time: 0.1679  data: 0.0001  max mem: 15850
[22:01:43.832675] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1705 (0.1750)  time: 0.1682  data: 0.0001  max mem: 15850
[22:01:45.520118] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1752 (0.1746)  time: 0.1685  data: 0.0001  max mem: 15850
[22:01:47.210912] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1706 (0.1737)  time: 0.1688  data: 0.0001  max mem: 15850
[22:01:48.905601] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1661 (0.1724)  time: 0.1692  data: 0.0001  max mem: 15850
[22:01:50.603236] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1661 (0.1734)  time: 0.1695  data: 0.0001  max mem: 15850
[22:01:52.303891] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1649 (0.1730)  time: 0.1698  data: 0.0001  max mem: 15850
[22:01:54.008461] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1649 (0.1719)  time: 0.1702  data: 0.0001  max mem: 15850
[22:01:55.715492] Test:  [100/345]  eta: 0:00:42  loss: 0.1715 (0.1725)  time: 0.1705  data: 0.0001  max mem: 15850
[22:01:57.427208] Test:  [110/345]  eta: 0:00:40  loss: 0.1684 (0.1720)  time: 0.1709  data: 0.0001  max mem: 15850
[22:01:59.141720] Test:  [120/345]  eta: 0:00:38  loss: 0.1691 (0.1724)  time: 0.1712  data: 0.0001  max mem: 15850
[22:02:00.858946] Test:  [130/345]  eta: 0:00:36  loss: 0.1722 (0.1721)  time: 0.1715  data: 0.0001  max mem: 15850
[22:02:02.581453] Test:  [140/345]  eta: 0:00:35  loss: 0.1662 (0.1724)  time: 0.1719  data: 0.0001  max mem: 15850
[22:02:04.306722] Test:  [150/345]  eta: 0:00:33  loss: 0.1747 (0.1725)  time: 0.1723  data: 0.0001  max mem: 15850
[22:02:06.035662] Test:  [160/345]  eta: 0:00:31  loss: 0.1747 (0.1728)  time: 0.1726  data: 0.0001  max mem: 15850
[22:02:07.769028] Test:  [170/345]  eta: 0:00:30  loss: 0.1747 (0.1734)  time: 0.1730  data: 0.0001  max mem: 15850
[22:02:09.505393] Test:  [180/345]  eta: 0:00:28  loss: 0.1745 (0.1735)  time: 0.1734  data: 0.0001  max mem: 15850
[22:02:11.245122] Test:  [190/345]  eta: 0:00:26  loss: 0.1702 (0.1735)  time: 0.1737  data: 0.0001  max mem: 15850
[22:02:12.988735] Test:  [200/345]  eta: 0:00:24  loss: 0.1757 (0.1735)  time: 0.1741  data: 0.0001  max mem: 15850
[22:02:14.733904] Test:  [210/345]  eta: 0:00:23  loss: 0.1761 (0.1739)  time: 0.1744  data: 0.0001  max mem: 15850
[22:02:16.483301] Test:  [220/345]  eta: 0:00:21  loss: 0.1794 (0.1743)  time: 0.1747  data: 0.0001  max mem: 15850
[22:02:18.236107] Test:  [230/345]  eta: 0:00:19  loss: 0.1735 (0.1742)  time: 0.1750  data: 0.0001  max mem: 15850
[22:02:19.993302] Test:  [240/345]  eta: 0:00:18  loss: 0.1591 (0.1739)  time: 0.1754  data: 0.0001  max mem: 15850
[22:02:21.753639] Test:  [250/345]  eta: 0:00:16  loss: 0.1627 (0.1743)  time: 0.1758  data: 0.0001  max mem: 15850
[22:02:23.518825] Test:  [260/345]  eta: 0:00:14  loss: 0.1675 (0.1742)  time: 0.1762  data: 0.0001  max mem: 15850
[22:02:25.286524] Test:  [270/345]  eta: 0:00:12  loss: 0.1667 (0.1738)  time: 0.1766  data: 0.0001  max mem: 15850
[22:02:27.056350] Test:  [280/345]  eta: 0:00:11  loss: 0.1607 (0.1737)  time: 0.1768  data: 0.0001  max mem: 15850
[22:02:28.829871] Test:  [290/345]  eta: 0:00:09  loss: 0.1606 (0.1737)  time: 0.1771  data: 0.0001  max mem: 15850
[22:02:30.607832] Test:  [300/345]  eta: 0:00:07  loss: 0.1727 (0.1739)  time: 0.1775  data: 0.0001  max mem: 15850
[22:02:32.389424] Test:  [310/345]  eta: 0:00:06  loss: 0.1689 (0.1739)  time: 0.1779  data: 0.0001  max mem: 15850
[22:02:34.173767] Test:  [320/345]  eta: 0:00:04  loss: 0.1660 (0.1739)  time: 0.1782  data: 0.0001  max mem: 15850
[22:02:35.964779] Test:  [330/345]  eta: 0:00:02  loss: 0.1736 (0.1741)  time: 0.1787  data: 0.0001  max mem: 15850
[22:02:37.755871] Test:  [340/345]  eta: 0:00:00  loss: 0.1755 (0.1741)  time: 0.1790  data: 0.0001  max mem: 15850
[22:02:38.473295] Test:  [344/345]  eta: 0:00:00  loss: 0.1755 (0.1741)  time: 0.1791  data: 0.0001  max mem: 15850
[22:02:38.546696] Test: Total time: 0:01:00 (0.1746 s / it)
[22:02:48.458428] Test:  [ 0/57]  eta: 0:00:25  loss: 0.5037 (0.5037)  time: 0.4465  data: 0.2828  max mem: 15850
[22:02:50.119013] Test:  [10/57]  eta: 0:00:08  loss: 0.4346 (0.4437)  time: 0.1915  data: 0.0258  max mem: 15850
[22:02:51.782964] Test:  [20/57]  eta: 0:00:06  loss: 0.4380 (0.4428)  time: 0.1661  data: 0.0001  max mem: 15850
[22:02:53.451777] Test:  [30/57]  eta: 0:00:04  loss: 0.2620 (0.3696)  time: 0.1666  data: 0.0001  max mem: 15850
[22:02:55.125126] Test:  [40/57]  eta: 0:00:02  loss: 0.2190 (0.3340)  time: 0.1670  data: 0.0001  max mem: 15850
[22:02:56.801904] Test:  [50/57]  eta: 0:00:01  loss: 0.2274 (0.3250)  time: 0.1674  data: 0.0001  max mem: 15850
[22:02:57.707450] Test:  [56/57]  eta: 0:00:00  loss: 0.2818 (0.3317)  time: 0.1625  data: 0.0000  max mem: 15850
[22:02:57.775718] Test: Total time: 0:00:09 (0.1713 s / it)
[22:02:59.445783] Dice score of the network on the train images: 0.829933, val images: 0.818302
[22:02:59.449963] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:03:00.354813] Epoch: [24]  [  0/345]  eta: 0:05:11  lr: 0.000120  loss: 0.1456 (0.1456)  time: 0.9039  data: 0.3033  max mem: 15850
[22:03:12.327354] Epoch: [24]  [ 20/345]  eta: 0:03:19  lr: 0.000119  loss: 0.1660 (0.1692)  time: 0.5986  data: 0.0001  max mem: 15850
[22:03:24.313058] Epoch: [24]  [ 40/345]  eta: 0:03:04  lr: 0.000119  loss: 0.1724 (0.1717)  time: 0.5992  data: 0.0001  max mem: 15850
[22:03:36.334006] Epoch: [24]  [ 60/345]  eta: 0:02:52  lr: 0.000119  loss: 0.1736 (0.1751)  time: 0.6010  data: 0.0001  max mem: 15850
[22:03:48.365153] Epoch: [24]  [ 80/345]  eta: 0:02:40  lr: 0.000119  loss: 0.1709 (0.1752)  time: 0.6015  data: 0.0001  max mem: 15850
[22:04:00.404726] Epoch: [24]  [100/345]  eta: 0:02:27  lr: 0.000119  loss: 0.1683 (0.1738)  time: 0.6019  data: 0.0001  max mem: 15850
[22:04:12.466386] Epoch: [24]  [120/345]  eta: 0:02:15  lr: 0.000119  loss: 0.1698 (0.1737)  time: 0.6030  data: 0.0001  max mem: 15850
[22:04:24.538608] Epoch: [24]  [140/345]  eta: 0:02:03  lr: 0.000118  loss: 0.1720 (0.1742)  time: 0.6036  data: 0.0001  max mem: 15850
[22:04:36.614471] Epoch: [24]  [160/345]  eta: 0:01:51  lr: 0.000118  loss: 0.1815 (0.1748)  time: 0.6037  data: 0.0001  max mem: 15850
[22:04:48.698063] Epoch: [24]  [180/345]  eta: 0:01:39  lr: 0.000118  loss: 0.1840 (0.1762)  time: 0.6041  data: 0.0001  max mem: 15850
[22:05:00.776913] Epoch: [24]  [200/345]  eta: 0:01:27  lr: 0.000118  loss: 0.1712 (0.1758)  time: 0.6039  data: 0.0001  max mem: 15850
[22:05:12.857216] Epoch: [24]  [220/345]  eta: 0:01:15  lr: 0.000118  loss: 0.1706 (0.1755)  time: 0.6040  data: 0.0001  max mem: 15850
[22:05:24.933878] Epoch: [24]  [240/345]  eta: 0:01:03  lr: 0.000118  loss: 0.1702 (0.1753)  time: 0.6038  data: 0.0001  max mem: 15850
[22:05:37.000863] Epoch: [24]  [260/345]  eta: 0:00:51  lr: 0.000117  loss: 0.1845 (0.1758)  time: 0.6033  data: 0.0001  max mem: 15850
[22:05:49.051333] Epoch: [24]  [280/345]  eta: 0:00:39  lr: 0.000117  loss: 0.1751 (0.1762)  time: 0.6025  data: 0.0001  max mem: 15850
[22:06:01.098182] Epoch: [24]  [300/345]  eta: 0:00:27  lr: 0.000117  loss: 0.1808 (0.1766)  time: 0.6023  data: 0.0001  max mem: 15850
[22:06:13.138872] Epoch: [24]  [320/345]  eta: 0:00:15  lr: 0.000117  loss: 0.1641 (0.1762)  time: 0.6020  data: 0.0001  max mem: 15850
[22:06:25.191063] Epoch: [24]  [340/345]  eta: 0:00:03  lr: 0.000117  loss: 0.1776 (0.1762)  time: 0.6026  data: 0.0001  max mem: 15850
[22:06:27.599515] Epoch: [24]  [344/345]  eta: 0:00:00  lr: 0.000117  loss: 0.1603 (0.1758)  time: 0.6025  data: 0.0001  max mem: 15850
[22:06:27.676324] Epoch: [24] Total time: 0:03:28 (0.6036 s / it)
[22:06:27.676796] Averaged stats: lr: 0.000117  loss: 0.1603 (0.1758)
[22:06:28.162612] Test:  [  0/345]  eta: 0:02:45  loss: 0.2005 (0.2005)  time: 0.4802  data: 0.3150  max mem: 15850
[22:06:29.840853] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1497 (0.1635)  time: 0.1961  data: 0.0287  max mem: 15850
[22:06:31.521004] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1510 (0.1643)  time: 0.1678  data: 0.0001  max mem: 15850
[22:06:33.204802] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1626 (0.1658)  time: 0.1681  data: 0.0001  max mem: 15850
[22:06:34.893236] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1628 (0.1656)  time: 0.1685  data: 0.0001  max mem: 15850
[22:06:36.583642] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1620 (0.1654)  time: 0.1689  data: 0.0001  max mem: 15850
[22:06:38.278408] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1616 (0.1658)  time: 0.1692  data: 0.0001  max mem: 15850
[22:06:39.975751] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1624 (0.1659)  time: 0.1695  data: 0.0001  max mem: 15850
[22:06:41.675837] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1633 (0.1661)  time: 0.1698  data: 0.0001  max mem: 15850
[22:06:43.378732] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1627 (0.1654)  time: 0.1701  data: 0.0001  max mem: 15850
[22:06:45.087189] Test:  [100/345]  eta: 0:00:42  loss: 0.1549 (0.1650)  time: 0.1705  data: 0.0001  max mem: 15850
[22:06:46.799430] Test:  [110/345]  eta: 0:00:40  loss: 0.1623 (0.1651)  time: 0.1709  data: 0.0001  max mem: 15850
[22:06:48.514451] Test:  [120/345]  eta: 0:00:38  loss: 0.1599 (0.1650)  time: 0.1713  data: 0.0001  max mem: 15850
[22:06:50.233094] Test:  [130/345]  eta: 0:00:36  loss: 0.1582 (0.1637)  time: 0.1716  data: 0.0001  max mem: 15850
[22:06:51.954798] Test:  [140/345]  eta: 0:00:35  loss: 0.1601 (0.1640)  time: 0.1719  data: 0.0001  max mem: 15850
[22:06:53.679165] Test:  [150/345]  eta: 0:00:33  loss: 0.1654 (0.1642)  time: 0.1722  data: 0.0001  max mem: 15850
[22:06:55.407472] Test:  [160/345]  eta: 0:00:31  loss: 0.1603 (0.1634)  time: 0.1726  data: 0.0001  max mem: 15850
[22:06:57.140823] Test:  [170/345]  eta: 0:00:30  loss: 0.1642 (0.1636)  time: 0.1730  data: 0.0001  max mem: 15850
[22:06:58.876261] Test:  [180/345]  eta: 0:00:28  loss: 0.1619 (0.1634)  time: 0.1734  data: 0.0001  max mem: 15850
[22:07:00.616670] Test:  [190/345]  eta: 0:00:26  loss: 0.1591 (0.1633)  time: 0.1737  data: 0.0001  max mem: 15850
[22:07:02.359068] Test:  [200/345]  eta: 0:00:25  loss: 0.1626 (0.1637)  time: 0.1741  data: 0.0001  max mem: 15850
[22:07:04.105648] Test:  [210/345]  eta: 0:00:23  loss: 0.1714 (0.1636)  time: 0.1744  data: 0.0001  max mem: 15850
[22:07:05.854278] Test:  [220/345]  eta: 0:00:21  loss: 0.1571 (0.1630)  time: 0.1747  data: 0.0001  max mem: 15850
[22:07:07.608026] Test:  [230/345]  eta: 0:00:19  loss: 0.1605 (0.1631)  time: 0.1751  data: 0.0001  max mem: 15850
[22:07:09.363455] Test:  [240/345]  eta: 0:00:18  loss: 0.1583 (0.1627)  time: 0.1754  data: 0.0001  max mem: 15850
[22:07:11.124360] Test:  [250/345]  eta: 0:00:16  loss: 0.1565 (0.1627)  time: 0.1758  data: 0.0001  max mem: 15850
[22:07:12.888482] Test:  [260/345]  eta: 0:00:14  loss: 0.1617 (0.1626)  time: 0.1762  data: 0.0001  max mem: 15850
[22:07:14.655265] Test:  [270/345]  eta: 0:00:12  loss: 0.1617 (0.1626)  time: 0.1765  data: 0.0001  max mem: 15850
[22:07:16.426704] Test:  [280/345]  eta: 0:00:11  loss: 0.1566 (0.1624)  time: 0.1768  data: 0.0001  max mem: 15850
[22:07:18.199892] Test:  [290/345]  eta: 0:00:09  loss: 0.1501 (0.1621)  time: 0.1772  data: 0.0001  max mem: 15850
[22:07:19.977508] Test:  [300/345]  eta: 0:00:07  loss: 0.1501 (0.1620)  time: 0.1775  data: 0.0001  max mem: 15850
[22:07:21.759367] Test:  [310/345]  eta: 0:00:06  loss: 0.1613 (0.1619)  time: 0.1779  data: 0.0001  max mem: 15850
[22:07:23.545185] Test:  [320/345]  eta: 0:00:04  loss: 0.1586 (0.1620)  time: 0.1783  data: 0.0001  max mem: 15850
[22:07:25.333771] Test:  [330/345]  eta: 0:00:02  loss: 0.1567 (0.1620)  time: 0.1787  data: 0.0001  max mem: 15850
[22:07:27.125862] Test:  [340/345]  eta: 0:00:00  loss: 0.1604 (0.1621)  time: 0.1790  data: 0.0001  max mem: 15850
[22:07:27.842712] Test:  [344/345]  eta: 0:00:00  loss: 0.1608 (0.1620)  time: 0.1791  data: 0.0001  max mem: 15850
[22:07:27.912403] Test: Total time: 0:01:00 (0.1746 s / it)
[22:07:37.703341] Test:  [ 0/57]  eta: 0:00:26  loss: 0.5087 (0.5087)  time: 0.4683  data: 0.3046  max mem: 15850
[22:07:39.363404] Test:  [10/57]  eta: 0:00:09  loss: 0.4035 (0.4411)  time: 0.1934  data: 0.0278  max mem: 15850
[22:07:41.029814] Test:  [20/57]  eta: 0:00:06  loss: 0.4304 (0.4378)  time: 0.1663  data: 0.0001  max mem: 15850
[22:07:42.698752] Test:  [30/57]  eta: 0:00:04  loss: 0.2557 (0.3653)  time: 0.1667  data: 0.0001  max mem: 15850
[22:07:44.372422] Test:  [40/57]  eta: 0:00:02  loss: 0.2120 (0.3306)  time: 0.1671  data: 0.0001  max mem: 15850
[22:07:46.049217] Test:  [50/57]  eta: 0:00:01  loss: 0.2318 (0.3215)  time: 0.1675  data: 0.0001  max mem: 15850
[22:07:46.954562] Test:  [56/57]  eta: 0:00:00  loss: 0.2685 (0.3276)  time: 0.1625  data: 0.0001  max mem: 15850
[22:07:47.006919] Test: Total time: 0:00:09 (0.1714 s / it)
[22:07:48.665829] Dice score of the network on the train images: 0.842871, val images: 0.828838
[22:07:48.666058] saving best_dice_model_0 @ epoch 24
[22:07:49.956288] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:07:50.853785] Epoch: [25]  [  0/345]  eta: 0:05:09  lr: 0.000117  loss: 0.1726 (0.1726)  time: 0.8963  data: 0.2959  max mem: 15850
[22:08:02.820306] Epoch: [25]  [ 20/345]  eta: 0:03:19  lr: 0.000116  loss: 0.1721 (0.1720)  time: 0.5983  data: 0.0001  max mem: 15850
[22:08:14.797787] Epoch: [25]  [ 40/345]  eta: 0:03:04  lr: 0.000116  loss: 0.1571 (0.1657)  time: 0.5988  data: 0.0001  max mem: 15850
[22:08:26.783961] Epoch: [25]  [ 60/345]  eta: 0:02:52  lr: 0.000116  loss: 0.1530 (0.1635)  time: 0.5993  data: 0.0001  max mem: 15850
[22:08:38.802381] Epoch: [25]  [ 80/345]  eta: 0:02:39  lr: 0.000116  loss: 0.1716 (0.1663)  time: 0.6009  data: 0.0001  max mem: 15850
[22:08:50.833564] Epoch: [25]  [100/345]  eta: 0:02:27  lr: 0.000116  loss: 0.1576 (0.1653)  time: 0.6015  data: 0.0001  max mem: 15850
[22:09:02.879377] Epoch: [25]  [120/345]  eta: 0:02:15  lr: 0.000115  loss: 0.1668 (0.1664)  time: 0.6022  data: 0.0001  max mem: 15850
[22:09:14.940818] Epoch: [25]  [140/345]  eta: 0:02:03  lr: 0.000115  loss: 0.1706 (0.1669)  time: 0.6030  data: 0.0001  max mem: 15850
[22:09:27.013916] Epoch: [25]  [160/345]  eta: 0:01:51  lr: 0.000115  loss: 0.1619 (0.1667)  time: 0.6036  data: 0.0001  max mem: 15850
[22:09:39.091807] Epoch: [25]  [180/345]  eta: 0:01:39  lr: 0.000115  loss: 0.1762 (0.1675)  time: 0.6039  data: 0.0001  max mem: 15850
[22:09:51.152299] Epoch: [25]  [200/345]  eta: 0:01:27  lr: 0.000115  loss: 0.1638 (0.1681)  time: 0.6030  data: 0.0001  max mem: 15850
[22:10:03.207761] Epoch: [25]  [220/345]  eta: 0:01:15  lr: 0.000114  loss: 0.1689 (0.1681)  time: 0.6027  data: 0.0001  max mem: 15850
[22:10:15.267077] Epoch: [25]  [240/345]  eta: 0:01:03  lr: 0.000114  loss: 0.1661 (0.1682)  time: 0.6029  data: 0.0001  max mem: 15850
[22:10:27.329612] Epoch: [25]  [260/345]  eta: 0:00:51  lr: 0.000114  loss: 0.1651 (0.1679)  time: 0.6031  data: 0.0001  max mem: 15850
[22:10:39.385313] Epoch: [25]  [280/345]  eta: 0:00:39  lr: 0.000114  loss: 0.1693 (0.1677)  time: 0.6027  data: 0.0001  max mem: 15850
[22:10:51.420907] Epoch: [25]  [300/345]  eta: 0:00:27  lr: 0.000114  loss: 0.1632 (0.1678)  time: 0.6017  data: 0.0001  max mem: 15850
[22:11:03.455014] Epoch: [25]  [320/345]  eta: 0:00:15  lr: 0.000113  loss: 0.1702 (0.1681)  time: 0.6017  data: 0.0001  max mem: 15850
[22:11:15.484443] Epoch: [25]  [340/345]  eta: 0:00:03  lr: 0.000113  loss: 0.1733 (0.1685)  time: 0.6014  data: 0.0001  max mem: 15850
[22:11:17.887705] Epoch: [25]  [344/345]  eta: 0:00:00  lr: 0.000113  loss: 0.1695 (0.1686)  time: 0.6012  data: 0.0001  max mem: 15850
[22:11:17.956856] Epoch: [25] Total time: 0:03:28 (0.6029 s / it)
[22:11:17.957468] Averaged stats: lr: 0.000113  loss: 0.1695 (0.1686)
[22:11:18.441306] Test:  [  0/345]  eta: 0:02:45  loss: 0.1558 (0.1558)  time: 0.4784  data: 0.3134  max mem: 15850
[22:11:20.119311] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1558 (0.1551)  time: 0.1959  data: 0.0286  max mem: 15850
[22:11:21.800837] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1580 (0.1567)  time: 0.1679  data: 0.0001  max mem: 15850
[22:11:23.485539] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1529 (0.1553)  time: 0.1682  data: 0.0001  max mem: 15850
[22:11:25.173765] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1526 (0.1568)  time: 0.1686  data: 0.0001  max mem: 15850
[22:11:26.864658] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1637 (0.1587)  time: 0.1689  data: 0.0001  max mem: 15850
[22:11:28.559638] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1626 (0.1602)  time: 0.1692  data: 0.0001  max mem: 15850
[22:11:30.258520] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1606 (0.1601)  time: 0.1696  data: 0.0001  max mem: 15850
[22:11:31.959319] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1537 (0.1604)  time: 0.1699  data: 0.0001  max mem: 15850
[22:11:33.664112] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1580 (0.1611)  time: 0.1702  data: 0.0001  max mem: 15850
[22:11:35.372688] Test:  [100/345]  eta: 0:00:42  loss: 0.1596 (0.1611)  time: 0.1706  data: 0.0001  max mem: 15850
[22:11:37.084238] Test:  [110/345]  eta: 0:00:40  loss: 0.1571 (0.1612)  time: 0.1709  data: 0.0001  max mem: 15850
[22:11:38.799140] Test:  [120/345]  eta: 0:00:38  loss: 0.1580 (0.1610)  time: 0.1713  data: 0.0001  max mem: 15850
[22:11:40.518784] Test:  [130/345]  eta: 0:00:37  loss: 0.1583 (0.1606)  time: 0.1717  data: 0.0001  max mem: 15850
[22:11:42.240380] Test:  [140/345]  eta: 0:00:35  loss: 0.1583 (0.1611)  time: 0.1720  data: 0.0001  max mem: 15850
[22:11:43.966551] Test:  [150/345]  eta: 0:00:33  loss: 0.1545 (0.1609)  time: 0.1723  data: 0.0001  max mem: 15850
[22:11:45.696973] Test:  [160/345]  eta: 0:00:31  loss: 0.1475 (0.1600)  time: 0.1728  data: 0.0001  max mem: 15850
[22:11:47.429528] Test:  [170/345]  eta: 0:00:30  loss: 0.1508 (0.1599)  time: 0.1731  data: 0.0001  max mem: 15850
[22:11:49.166245] Test:  [180/345]  eta: 0:00:28  loss: 0.1539 (0.1601)  time: 0.1734  data: 0.0001  max mem: 15850
[22:11:50.906124] Test:  [190/345]  eta: 0:00:26  loss: 0.1565 (0.1600)  time: 0.1738  data: 0.0001  max mem: 15850
[22:11:52.648826] Test:  [200/345]  eta: 0:00:25  loss: 0.1533 (0.1595)  time: 0.1741  data: 0.0001  max mem: 15850
[22:11:54.395220] Test:  [210/345]  eta: 0:00:23  loss: 0.1537 (0.1597)  time: 0.1744  data: 0.0001  max mem: 15850
[22:11:56.144186] Test:  [220/345]  eta: 0:00:21  loss: 0.1613 (0.1599)  time: 0.1747  data: 0.0001  max mem: 15850
[22:11:57.898521] Test:  [230/345]  eta: 0:00:19  loss: 0.1608 (0.1602)  time: 0.1751  data: 0.0001  max mem: 15850
[22:11:59.654975] Test:  [240/345]  eta: 0:00:18  loss: 0.1599 (0.1603)  time: 0.1755  data: 0.0001  max mem: 15850
[22:12:01.414745] Test:  [250/345]  eta: 0:00:16  loss: 0.1547 (0.1599)  time: 0.1758  data: 0.0001  max mem: 15850
[22:12:03.178086] Test:  [260/345]  eta: 0:00:14  loss: 0.1568 (0.1605)  time: 0.1761  data: 0.0001  max mem: 15850
[22:12:04.944485] Test:  [270/345]  eta: 0:00:12  loss: 0.1782 (0.1605)  time: 0.1764  data: 0.0001  max mem: 15850
[22:12:06.716051] Test:  [280/345]  eta: 0:00:11  loss: 0.1482 (0.1603)  time: 0.1768  data: 0.0001  max mem: 15850
[22:12:08.490515] Test:  [290/345]  eta: 0:00:09  loss: 0.1628 (0.1606)  time: 0.1772  data: 0.0001  max mem: 15850
[22:12:10.267838] Test:  [300/345]  eta: 0:00:07  loss: 0.1628 (0.1604)  time: 0.1775  data: 0.0001  max mem: 15850
[22:12:12.050801] Test:  [310/345]  eta: 0:00:06  loss: 0.1500 (0.1601)  time: 0.1780  data: 0.0001  max mem: 15850
[22:12:13.834526] Test:  [320/345]  eta: 0:00:04  loss: 0.1500 (0.1602)  time: 0.1783  data: 0.0001  max mem: 15850
[22:12:15.621173] Test:  [330/345]  eta: 0:00:02  loss: 0.1659 (0.1603)  time: 0.1785  data: 0.0001  max mem: 15850
[22:12:17.413724] Test:  [340/345]  eta: 0:00:00  loss: 0.1614 (0.1601)  time: 0.1789  data: 0.0001  max mem: 15850
[22:12:18.130861] Test:  [344/345]  eta: 0:00:00  loss: 0.1519 (0.1601)  time: 0.1791  data: 0.0001  max mem: 15850
[22:12:18.203374] Test: Total time: 0:01:00 (0.1746 s / it)
[22:12:28.053663] Test:  [ 0/57]  eta: 0:00:25  loss: 0.5052 (0.5052)  time: 0.4394  data: 0.2756  max mem: 15850
[22:12:29.713198] Test:  [10/57]  eta: 0:00:08  loss: 0.3940 (0.4287)  time: 0.1907  data: 0.0251  max mem: 15850
[22:12:31.377042] Test:  [20/57]  eta: 0:00:06  loss: 0.4094 (0.4280)  time: 0.1661  data: 0.0001  max mem: 15850
[22:12:33.045072] Test:  [30/57]  eta: 0:00:04  loss: 0.2481 (0.3565)  time: 0.1665  data: 0.0001  max mem: 15850
[22:12:34.718375] Test:  [40/57]  eta: 0:00:02  loss: 0.2198 (0.3241)  time: 0.1670  data: 0.0001  max mem: 15850
[22:12:36.395133] Test:  [50/57]  eta: 0:00:01  loss: 0.2298 (0.3169)  time: 0.1674  data: 0.0001  max mem: 15850
[22:12:37.299816] Test:  [56/57]  eta: 0:00:00  loss: 0.2481 (0.3193)  time: 0.1625  data: 0.0000  max mem: 15850
[22:12:37.376917] Test: Total time: 0:00:09 (0.1713 s / it)
[22:12:39.019972] Dice score of the network on the train images: 0.843323, val images: 0.832184
[22:12:39.020189] saving best_dice_model_0 @ epoch 25
[22:12:40.421717] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:12:41.306746] Epoch: [26]  [  0/345]  eta: 0:05:04  lr: 0.000113  loss: 0.1703 (0.1703)  time: 0.8830  data: 0.2820  max mem: 15850
[22:12:53.259881] Epoch: [26]  [ 20/345]  eta: 0:03:18  lr: 0.000113  loss: 0.1750 (0.1740)  time: 0.5976  data: 0.0001  max mem: 15850
[22:13:05.233566] Epoch: [26]  [ 40/345]  eta: 0:03:04  lr: 0.000113  loss: 0.1693 (0.1715)  time: 0.5986  data: 0.0001  max mem: 15850
[22:13:17.224653] Epoch: [26]  [ 60/345]  eta: 0:02:51  lr: 0.000112  loss: 0.1648 (0.1712)  time: 0.5995  data: 0.0001  max mem: 15850
[22:13:29.244941] Epoch: [26]  [ 80/345]  eta: 0:02:39  lr: 0.000112  loss: 0.1661 (0.1706)  time: 0.6010  data: 0.0001  max mem: 15850
[22:13:41.283188] Epoch: [26]  [100/345]  eta: 0:02:27  lr: 0.000112  loss: 0.1501 (0.1678)  time: 0.6019  data: 0.0001  max mem: 15850
[22:13:53.333473] Epoch: [26]  [120/345]  eta: 0:02:15  lr: 0.000112  loss: 0.1641 (0.1686)  time: 0.6025  data: 0.0001  max mem: 15850
[22:14:05.396677] Epoch: [26]  [140/345]  eta: 0:02:03  lr: 0.000111  loss: 0.1625 (0.1680)  time: 0.6031  data: 0.0001  max mem: 15850
[22:14:17.468630] Epoch: [26]  [160/345]  eta: 0:01:51  lr: 0.000111  loss: 0.1778 (0.1686)  time: 0.6036  data: 0.0001  max mem: 15850
[22:14:29.542210] Epoch: [26]  [180/345]  eta: 0:01:39  lr: 0.000111  loss: 0.1593 (0.1684)  time: 0.6036  data: 0.0001  max mem: 15850
[22:14:41.618771] Epoch: [26]  [200/345]  eta: 0:01:27  lr: 0.000111  loss: 0.1482 (0.1672)  time: 0.6038  data: 0.0001  max mem: 15850
[22:14:53.695289] Epoch: [26]  [220/345]  eta: 0:01:15  lr: 0.000110  loss: 0.1641 (0.1673)  time: 0.6038  data: 0.0001  max mem: 15850
[22:15:05.883968] Epoch: [26]  [240/345]  eta: 0:01:03  lr: 0.000110  loss: 0.1603 (0.1668)  time: 0.6094  data: 0.0001  max mem: 15850
[22:15:17.946191] Epoch: [26]  [260/345]  eta: 0:00:51  lr: 0.000110  loss: 0.1549 (0.1667)  time: 0.6030  data: 0.0001  max mem: 15850
[22:15:30.005296] Epoch: [26]  [280/345]  eta: 0:00:39  lr: 0.000110  loss: 0.1572 (0.1663)  time: 0.6029  data: 0.0001  max mem: 15850
[22:15:42.061458] Epoch: [26]  [300/345]  eta: 0:00:27  lr: 0.000110  loss: 0.1623 (0.1662)  time: 0.6028  data: 0.0001  max mem: 15850
[22:15:54.115137] Epoch: [26]  [320/345]  eta: 0:00:15  lr: 0.000109  loss: 0.1599 (0.1658)  time: 0.6026  data: 0.0001  max mem: 15850
[22:16:06.152299] Epoch: [26]  [340/345]  eta: 0:00:03  lr: 0.000109  loss: 0.1589 (0.1655)  time: 0.6018  data: 0.0001  max mem: 15850
[22:16:08.558222] Epoch: [26]  [344/345]  eta: 0:00:00  lr: 0.000109  loss: 0.1648 (0.1655)  time: 0.6014  data: 0.0001  max mem: 15850
[22:16:08.631450] Epoch: [26] Total time: 0:03:28 (0.6035 s / it)
[22:16:08.631668] Averaged stats: lr: 0.000109  loss: 0.1648 (0.1655)
[22:16:09.114077] Test:  [  0/345]  eta: 0:02:44  loss: 0.1496 (0.1496)  time: 0.4771  data: 0.3115  max mem: 15850
[22:16:10.791218] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1571 (0.1558)  time: 0.1958  data: 0.0284  max mem: 15850
[22:16:12.471614] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1571 (0.1570)  time: 0.1678  data: 0.0001  max mem: 15850
[22:16:14.155233] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1516 (0.1570)  time: 0.1681  data: 0.0001  max mem: 15850
[22:16:15.842690] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1517 (0.1572)  time: 0.1685  data: 0.0001  max mem: 15850
[22:16:17.532730] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1646 (0.1591)  time: 0.1688  data: 0.0001  max mem: 15850
[22:16:19.227720] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1591 (0.1591)  time: 0.1692  data: 0.0001  max mem: 15850
[22:16:20.925685] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1521 (0.1590)  time: 0.1696  data: 0.0001  max mem: 15850
[22:16:22.626526] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1564 (0.1592)  time: 0.1699  data: 0.0001  max mem: 15850
[22:16:24.331082] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1579 (0.1593)  time: 0.1702  data: 0.0001  max mem: 15850
[22:16:26.039319] Test:  [100/345]  eta: 0:00:42  loss: 0.1627 (0.1603)  time: 0.1706  data: 0.0001  max mem: 15850
[22:16:27.751074] Test:  [110/345]  eta: 0:00:40  loss: 0.1603 (0.1590)  time: 0.1709  data: 0.0001  max mem: 15850
[22:16:29.466171] Test:  [120/345]  eta: 0:00:38  loss: 0.1496 (0.1593)  time: 0.1713  data: 0.0001  max mem: 15850
[22:16:31.184281] Test:  [130/345]  eta: 0:00:36  loss: 0.1616 (0.1596)  time: 0.1716  data: 0.0001  max mem: 15850
[22:16:32.906420] Test:  [140/345]  eta: 0:00:35  loss: 0.1625 (0.1597)  time: 0.1720  data: 0.0001  max mem: 15850
[22:16:34.631774] Test:  [150/345]  eta: 0:00:33  loss: 0.1625 (0.1599)  time: 0.1723  data: 0.0001  max mem: 15850
[22:16:36.361807] Test:  [160/345]  eta: 0:00:31  loss: 0.1603 (0.1600)  time: 0.1727  data: 0.0001  max mem: 15850
[22:16:38.093456] Test:  [170/345]  eta: 0:00:30  loss: 0.1634 (0.1603)  time: 0.1730  data: 0.0001  max mem: 15850
[22:16:39.828836] Test:  [180/345]  eta: 0:00:28  loss: 0.1589 (0.1604)  time: 0.1733  data: 0.0001  max mem: 15850
[22:16:41.568186] Test:  [190/345]  eta: 0:00:26  loss: 0.1549 (0.1598)  time: 0.1737  data: 0.0001  max mem: 15850
[22:16:43.311484] Test:  [200/345]  eta: 0:00:25  loss: 0.1560 (0.1600)  time: 0.1741  data: 0.0001  max mem: 15850
[22:16:45.059188] Test:  [210/345]  eta: 0:00:23  loss: 0.1613 (0.1604)  time: 0.1745  data: 0.0001  max mem: 15850
[22:16:46.808221] Test:  [220/345]  eta: 0:00:21  loss: 0.1622 (0.1605)  time: 0.1748  data: 0.0001  max mem: 15850
[22:16:48.562508] Test:  [230/345]  eta: 0:00:19  loss: 0.1587 (0.1602)  time: 0.1751  data: 0.0001  max mem: 15850
[22:16:50.319771] Test:  [240/345]  eta: 0:00:18  loss: 0.1587 (0.1606)  time: 0.1755  data: 0.0001  max mem: 15850
[22:16:52.080822] Test:  [250/345]  eta: 0:00:16  loss: 0.1584 (0.1607)  time: 0.1759  data: 0.0001  max mem: 15850
[22:16:53.844334] Test:  [260/345]  eta: 0:00:14  loss: 0.1545 (0.1605)  time: 0.1762  data: 0.0001  max mem: 15850
[22:16:55.612160] Test:  [270/345]  eta: 0:00:12  loss: 0.1536 (0.1602)  time: 0.1765  data: 0.0001  max mem: 15850
[22:16:57.383635] Test:  [280/345]  eta: 0:00:11  loss: 0.1512 (0.1599)  time: 0.1769  data: 0.0001  max mem: 15850
[22:16:59.158481] Test:  [290/345]  eta: 0:00:09  loss: 0.1591 (0.1599)  time: 0.1773  data: 0.0001  max mem: 15850
[22:17:00.936273] Test:  [300/345]  eta: 0:00:07  loss: 0.1511 (0.1596)  time: 0.1776  data: 0.0001  max mem: 15850
[22:17:02.717220] Test:  [310/345]  eta: 0:00:06  loss: 0.1480 (0.1595)  time: 0.1779  data: 0.0001  max mem: 15850
[22:17:04.501783] Test:  [320/345]  eta: 0:00:04  loss: 0.1558 (0.1596)  time: 0.1782  data: 0.0001  max mem: 15850
[22:17:06.288590] Test:  [330/345]  eta: 0:00:02  loss: 0.1612 (0.1601)  time: 0.1785  data: 0.0001  max mem: 15850
[22:17:08.082112] Test:  [340/345]  eta: 0:00:00  loss: 0.1684 (0.1603)  time: 0.1790  data: 0.0001  max mem: 15850
[22:17:08.800778] Test:  [344/345]  eta: 0:00:00  loss: 0.1684 (0.1603)  time: 0.1791  data: 0.0001  max mem: 15850
[22:17:08.873020] Test: Total time: 0:01:00 (0.1746 s / it)
[22:17:18.735275] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4972 (0.4972)  time: 0.4453  data: 0.2820  max mem: 15850
[22:17:20.394534] Test:  [10/57]  eta: 0:00:08  loss: 0.3970 (0.4250)  time: 0.1912  data: 0.0257  max mem: 15850
[22:17:22.059585] Test:  [20/57]  eta: 0:00:06  loss: 0.4153 (0.4271)  time: 0.1661  data: 0.0001  max mem: 15850
[22:17:23.727656] Test:  [30/57]  eta: 0:00:04  loss: 0.2600 (0.3611)  time: 0.1666  data: 0.0001  max mem: 15850
[22:17:25.400487] Test:  [40/57]  eta: 0:00:02  loss: 0.2190 (0.3306)  time: 0.1670  data: 0.0001  max mem: 15850
[22:17:27.077204] Test:  [50/57]  eta: 0:00:01  loss: 0.2460 (0.3220)  time: 0.1674  data: 0.0001  max mem: 15850
[22:17:27.981552] Test:  [56/57]  eta: 0:00:00  loss: 0.2700 (0.3279)  time: 0.1625  data: 0.0001  max mem: 15850
[22:17:28.054277] Test: Total time: 0:00:09 (0.1713 s / it)
[22:17:29.683947] Dice score of the network on the train images: 0.847319, val images: 0.825004
[22:17:29.688339] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:17:30.581975] Epoch: [27]  [  0/345]  eta: 0:05:07  lr: 0.000109  loss: 0.1549 (0.1549)  time: 0.8926  data: 0.2926  max mem: 15850
[22:17:42.543449] Epoch: [27]  [ 20/345]  eta: 0:03:18  lr: 0.000109  loss: 0.1657 (0.1680)  time: 0.5980  data: 0.0001  max mem: 15850
[22:17:54.525341] Epoch: [27]  [ 40/345]  eta: 0:03:04  lr: 0.000108  loss: 0.1580 (0.1658)  time: 0.5991  data: 0.0001  max mem: 15850
[22:18:06.528770] Epoch: [27]  [ 60/345]  eta: 0:02:52  lr: 0.000108  loss: 0.1522 (0.1638)  time: 0.6001  data: 0.0001  max mem: 15850
[22:18:18.550332] Epoch: [27]  [ 80/345]  eta: 0:02:39  lr: 0.000108  loss: 0.1598 (0.1634)  time: 0.6010  data: 0.0001  max mem: 15850
[22:18:30.583651] Epoch: [27]  [100/345]  eta: 0:02:27  lr: 0.000108  loss: 0.1654 (0.1646)  time: 0.6016  data: 0.0001  max mem: 15850
[22:18:42.627987] Epoch: [27]  [120/345]  eta: 0:02:15  lr: 0.000107  loss: 0.1560 (0.1635)  time: 0.6022  data: 0.0001  max mem: 15850
[22:18:54.668417] Epoch: [27]  [140/345]  eta: 0:02:03  lr: 0.000107  loss: 0.1544 (0.1625)  time: 0.6020  data: 0.0001  max mem: 15850

[22:19:06.712626] Epoch: [27]  [160/345]  eta: 0:01:51  lr: 0.000107  loss: 0.1546 (0.1622)  time: 0.6022  data: 0.0001  max mem: 15850
[22:19:18.883663] Epoch: [27]  [180/345]  eta: 0:01:39  lr: 0.000107  loss: 0.1541 (0.1613)  time: 0.6085  data: 0.0001  max mem: 15850
[22:19:30.924692] Epoch: [27]  [200/345]  eta: 0:01:27  lr: 0.000106  loss: 0.1645 (0.1618)  time: 0.6020  data: 0.0001  max mem: 15850
[22:19:42.964250] Epoch: [27]  [220/345]  eta: 0:01:15  lr: 0.000106  loss: 0.1594 (0.1618)  time: 0.6019  data: 0.0001  max mem: 15850
[22:19:55.006784] Epoch: [27]  [240/345]  eta: 0:01:03  lr: 0.000106  loss: 0.1534 (0.1617)  time: 0.6021  data: 0.0001  max mem: 15850
[22:20:07.041980] Epoch: [27]  [260/345]  eta: 0:00:51  lr: 0.000106  loss: 0.1575 (0.1622)  time: 0.6017  data: 0.0001  max mem: 15850
[22:20:19.077960] Epoch: [27]  [280/345]  eta: 0:00:39  lr: 0.000105  loss: 0.1519 (0.1619)  time: 0.6018  data: 0.0001  max mem: 15850
[22:20:31.116932] Epoch: [27]  [300/345]  eta: 0:00:27  lr: 0.000105  loss: 0.1611 (0.1623)  time: 0.6019  data: 0.0001  max mem: 15850
[22:20:43.143720] Epoch: [27]  [320/345]  eta: 0:00:15  lr: 0.000105  loss: 0.1702 (0.1627)  time: 0.6013  data: 0.0001  max mem: 15850
[22:20:55.167803] Epoch: [27]  [340/345]  eta: 0:00:03  lr: 0.000104  loss: 0.1561 (0.1624)  time: 0.6012  data: 0.0001  max mem: 15850
[22:20:57.571650] Epoch: [27]  [344/345]  eta: 0:00:00  lr: 0.000104  loss: 0.1561 (0.1623)  time: 0.6010  data: 0.0001  max mem: 15850
[22:20:57.638960] Epoch: [27] Total time: 0:03:27 (0.6028 s / it)
[22:20:57.639345] Averaged stats: lr: 0.000104  loss: 0.1561 (0.1623)
[22:20:58.109302] Test:  [  0/345]  eta: 0:02:40  loss: 0.1522 (0.1522)  time: 0.4644  data: 0.2997  max mem: 15850
[22:20:59.787035] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1438 (0.1471)  time: 0.1947  data: 0.0273  max mem: 15850
[22:21:01.466756] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1475 (0.1555)  time: 0.1678  data: 0.0001  max mem: 15850
[22:21:03.151064] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1553 (0.1578)  time: 0.1681  data: 0.0001  max mem: 15850
[22:21:04.837951] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1525 (0.1573)  time: 0.1685  data: 0.0001  max mem: 15850
[22:21:06.529189] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1525 (0.1577)  time: 0.1688  data: 0.0001  max mem: 15850
[22:21:08.222750] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1460 (0.1549)  time: 0.1692  data: 0.0001  max mem: 15850
[22:21:09.919749] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1505 (0.1555)  time: 0.1695  data: 0.0001  max mem: 15850
[22:21:11.620324] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1514 (0.1551)  time: 0.1698  data: 0.0001  max mem: 15850
[22:21:13.324658] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1541 (0.1555)  time: 0.1702  data: 0.0001  max mem: 15850
[22:21:15.031591] Test:  [100/345]  eta: 0:00:42  loss: 0.1599 (0.1556)  time: 0.1705  data: 0.0001  max mem: 15850
[22:21:16.743448] Test:  [110/345]  eta: 0:00:40  loss: 0.1463 (0.1547)  time: 0.1709  data: 0.0001  max mem: 15850
[22:21:18.456940] Test:  [120/345]  eta: 0:00:38  loss: 0.1463 (0.1551)  time: 0.1712  data: 0.0001  max mem: 15850
[22:21:20.175443] Test:  [130/345]  eta: 0:00:36  loss: 0.1558 (0.1555)  time: 0.1715  data: 0.0001  max mem: 15850
[22:21:21.898130] Test:  [140/345]  eta: 0:00:35  loss: 0.1564 (0.1560)  time: 0.1720  data: 0.0001  max mem: 15850
[22:21:23.625039] Test:  [150/345]  eta: 0:00:33  loss: 0.1566 (0.1558)  time: 0.1724  data: 0.0001  max mem: 15850
[22:21:25.355273] Test:  [160/345]  eta: 0:00:31  loss: 0.1511 (0.1557)  time: 0.1728  data: 0.0001  max mem: 15850
[22:21:27.089107] Test:  [170/345]  eta: 0:00:30  loss: 0.1449 (0.1551)  time: 0.1731  data: 0.0001  max mem: 15850
[22:21:28.825405] Test:  [180/345]  eta: 0:00:28  loss: 0.1446 (0.1546)  time: 0.1734  data: 0.0001  max mem: 15850
[22:21:30.566328] Test:  [190/345]  eta: 0:00:26  loss: 0.1457 (0.1546)  time: 0.1738  data: 0.0001  max mem: 15850
[22:21:32.310253] Test:  [200/345]  eta: 0:00:24  loss: 0.1513 (0.1546)  time: 0.1742  data: 0.0001  max mem: 15850
[22:21:34.056241] Test:  [210/345]  eta: 0:00:23  loss: 0.1483 (0.1545)  time: 0.1744  data: 0.0001  max mem: 15850
[22:21:35.806423] Test:  [220/345]  eta: 0:00:21  loss: 0.1482 (0.1542)  time: 0.1747  data: 0.0001  max mem: 15850
[22:21:37.559939] Test:  [230/345]  eta: 0:00:19  loss: 0.1418 (0.1539)  time: 0.1751  data: 0.0001  max mem: 15850
[22:21:39.315861] Test:  [240/345]  eta: 0:00:18  loss: 0.1381 (0.1536)  time: 0.1754  data: 0.0001  max mem: 15850
[22:21:41.076434] Test:  [250/345]  eta: 0:00:16  loss: 0.1381 (0.1533)  time: 0.1758  data: 0.0001  max mem: 15850
[22:21:42.840879] Test:  [260/345]  eta: 0:00:14  loss: 0.1553 (0.1535)  time: 0.1762  data: 0.0001  max mem: 15850
[22:21:44.608070] Test:  [270/345]  eta: 0:00:12  loss: 0.1567 (0.1536)  time: 0.1765  data: 0.0001  max mem: 15850
[22:21:46.379209] Test:  [280/345]  eta: 0:00:11  loss: 0.1497 (0.1534)  time: 0.1769  data: 0.0001  max mem: 15850
[22:21:48.152008] Test:  [290/345]  eta: 0:00:09  loss: 0.1476 (0.1536)  time: 0.1771  data: 0.0001  max mem: 15850
[22:21:49.929986] Test:  [300/345]  eta: 0:00:07  loss: 0.1572 (0.1535)  time: 0.1775  data: 0.0001  max mem: 15850
[22:21:51.711204] Test:  [310/345]  eta: 0:00:06  loss: 0.1469 (0.1534)  time: 0.1779  data: 0.0001  max mem: 15850
[22:21:53.496257] Test:  [320/345]  eta: 0:00:04  loss: 0.1441 (0.1530)  time: 0.1783  data: 0.0001  max mem: 15850
[22:21:55.286103] Test:  [330/345]  eta: 0:00:02  loss: 0.1407 (0.1528)  time: 0.1787  data: 0.0001  max mem: 15850
[22:21:57.075546] Test:  [340/345]  eta: 0:00:00  loss: 0.1457 (0.1528)  time: 0.1789  data: 0.0001  max mem: 15850
[22:21:57.793141] Test:  [344/345]  eta: 0:00:00  loss: 0.1524 (0.1530)  time: 0.1790  data: 0.0001  max mem: 15850
[22:21:57.849157] Test: Total time: 0:01:00 (0.1745 s / it)
[22:22:07.697634] Test:  [ 0/57]  eta: 0:00:24  loss: 0.5146 (0.5146)  time: 0.4341  data: 0.2702  max mem: 15850
[22:22:09.357748] Test:  [10/57]  eta: 0:00:08  loss: 0.4087 (0.4561)  time: 0.1903  data: 0.0247  max mem: 15850
[22:22:11.022039] Test:  [20/57]  eta: 0:00:06  loss: 0.4385 (0.4539)  time: 0.1661  data: 0.0001  max mem: 15850
[22:22:12.691061] Test:  [30/57]  eta: 0:00:04  loss: 0.2604 (0.3774)  time: 0.1666  data: 0.0001  max mem: 15850
[22:22:14.364640] Test:  [40/57]  eta: 0:00:02  loss: 0.2115 (0.3404)  time: 0.1671  data: 0.0001  max mem: 15850
[22:22:16.041126] Test:  [50/57]  eta: 0:00:01  loss: 0.2417 (0.3312)  time: 0.1674  data: 0.0001  max mem: 15850
[22:22:16.945192] Test:  [56/57]  eta: 0:00:00  loss: 0.2713 (0.3346)  time: 0.1625  data: 0.0000  max mem: 15850
[22:22:17.009179] Test: Total time: 0:00:09 (0.1710 s / it)
[22:22:18.666428] Dice score of the network on the train images: 0.853684, val images: 0.830214
[22:22:18.671424] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:22:19.548440] Epoch: [28]  [  0/345]  eta: 0:05:02  lr: 0.000104  loss: 0.1478 (0.1478)  time: 0.8759  data: 0.2761  max mem: 15850
[22:22:31.507189] Epoch: [28]  [ 20/345]  eta: 0:03:18  lr: 0.000104  loss: 0.1527 (0.1514)  time: 0.5979  data: 0.0001  max mem: 15850
[22:22:43.486645] Epoch: [28]  [ 40/345]  eta: 0:03:04  lr: 0.000104  loss: 0.1465 (0.1517)  time: 0.5989  data: 0.0001  max mem: 15850
[22:22:55.495737] Epoch: [28]  [ 60/345]  eta: 0:02:52  lr: 0.000103  loss: 0.1440 (0.1503)  time: 0.6004  data: 0.0001  max mem: 15850
[22:23:07.521311] Epoch: [28]  [ 80/345]  eta: 0:02:39  lr: 0.000103  loss: 0.1588 (0.1524)  time: 0.6012  data: 0.0001  max mem: 15850
[22:23:19.562502] Epoch: [28]  [100/345]  eta: 0:02:27  lr: 0.000103  loss: 0.1586 (0.1535)  time: 0.6020  data: 0.0001  max mem: 15850
[22:23:31.610130] Epoch: [28]  [120/345]  eta: 0:02:15  lr: 0.000103  loss: 0.1493 (0.1533)  time: 0.6023  data: 0.0001  max mem: 15850
[22:23:43.661156] Epoch: [28]  [140/345]  eta: 0:02:03  lr: 0.000102  loss: 0.1491 (0.1527)  time: 0.6025  data: 0.0001  max mem: 15850
[22:23:55.702597] Epoch: [28]  [160/345]  eta: 0:01:51  lr: 0.000102  loss: 0.1468 (0.1525)  time: 0.6020  data: 0.0001  max mem: 15850
[22:24:07.745319] Epoch: [28]  [180/345]  eta: 0:01:39  lr: 0.000102  loss: 0.1483 (0.1528)  time: 0.6021  data: 0.0001  max mem: 15850
[22:24:19.793592] Epoch: [28]  [200/345]  eta: 0:01:27  lr: 0.000101  loss: 0.1624 (0.1539)  time: 0.6024  data: 0.0001  max mem: 15850
[22:24:31.842305] Epoch: [28]  [220/345]  eta: 0:01:15  lr: 0.000101  loss: 0.1533 (0.1542)  time: 0.6024  data: 0.0001  max mem: 15850
[22:24:43.887644] Epoch: [28]  [240/345]  eta: 0:01:03  lr: 0.000101  loss: 0.1535 (0.1547)  time: 0.6022  data: 0.0001  max mem: 15850
[22:24:55.921432] Epoch: [28]  [260/345]  eta: 0:00:51  lr: 0.000101  loss: 0.1541 (0.1548)  time: 0.6016  data: 0.0001  max mem: 15850
[22:25:07.969577] Epoch: [28]  [280/345]  eta: 0:00:39  lr: 0.000100  loss: 0.1476 (0.1546)  time: 0.6024  data: 0.0001  max mem: 15850
[22:25:20.018156] Epoch: [28]  [300/345]  eta: 0:00:27  lr: 0.000100  loss: 0.1492 (0.1548)  time: 0.6024  data: 0.0001  max mem: 15850
[22:25:32.065779] Epoch: [28]  [320/345]  eta: 0:00:15  lr: 0.000100  loss: 0.1629 (0.1557)  time: 0.6023  data: 0.0001  max mem: 15850
[22:25:44.110571] Epoch: [28]  [340/345]  eta: 0:00:03  lr: 0.000099  loss: 0.1472 (0.1558)  time: 0.6022  data: 0.0001  max mem: 15850
[22:25:46.518534] Epoch: [28]  [344/345]  eta: 0:00:00  lr: 0.000099  loss: 0.1396 (0.1555)  time: 0.6020  data: 0.0001  max mem: 15850
[22:25:46.594105] Epoch: [28] Total time: 0:03:27 (0.6027 s / it)
[22:25:46.594564] Averaged stats: lr: 0.000099  loss: 0.1396 (0.1555)
[22:25:47.106408] Test:  [  0/345]  eta: 0:02:54  loss: 0.1412 (0.1412)  time: 0.5067  data: 0.3413  max mem: 15850
[22:25:48.784931] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1421 (0.1416)  time: 0.1986  data: 0.0311  max mem: 15850
[22:25:50.466653] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1423 (0.1415)  time: 0.1679  data: 0.0001  max mem: 15850
[22:25:52.150482] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1470 (0.1464)  time: 0.1682  data: 0.0001  max mem: 15850
[22:25:53.837134] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1516 (0.1473)  time: 0.1685  data: 0.0001  max mem: 15850
[22:25:55.529624] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1486 (0.1469)  time: 0.1689  data: 0.0001  max mem: 15850
[22:25:57.225280] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1396 (0.1461)  time: 0.1693  data: 0.0001  max mem: 15850
[22:25:58.923985] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1410 (0.1478)  time: 0.1697  data: 0.0001  max mem: 15850
[22:26:00.625805] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1438 (0.1477)  time: 0.1700  data: 0.0001  max mem: 15850
[22:26:02.330590] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1423 (0.1465)  time: 0.1703  data: 0.0001  max mem: 15850
[22:26:04.038684] Test:  [100/345]  eta: 0:00:42  loss: 0.1432 (0.1475)  time: 0.1706  data: 0.0001  max mem: 15850
[22:26:05.750908] Test:  [110/345]  eta: 0:00:40  loss: 0.1454 (0.1483)  time: 0.1710  data: 0.0001  max mem: 15850
[22:26:07.466076] Test:  [120/345]  eta: 0:00:38  loss: 0.1441 (0.1477)  time: 0.1713  data: 0.0001  max mem: 15850
[22:26:09.185463] Test:  [130/345]  eta: 0:00:37  loss: 0.1441 (0.1475)  time: 0.1716  data: 0.0001  max mem: 15850
[22:26:10.908416] Test:  [140/345]  eta: 0:00:35  loss: 0.1441 (0.1472)  time: 0.1720  data: 0.0001  max mem: 15850
[22:26:12.634216] Test:  [150/345]  eta: 0:00:33  loss: 0.1474 (0.1476)  time: 0.1724  data: 0.0001  max mem: 15850
[22:26:14.362956] Test:  [160/345]  eta: 0:00:31  loss: 0.1452 (0.1473)  time: 0.1727  data: 0.0001  max mem: 15850
[22:26:16.095332] Test:  [170/345]  eta: 0:00:30  loss: 0.1381 (0.1471)  time: 0.1730  data: 0.0001  max mem: 15850
[22:26:17.831995] Test:  [180/345]  eta: 0:00:28  loss: 0.1412 (0.1472)  time: 0.1734  data: 0.0001  max mem: 15850
[22:26:19.571505] Test:  [190/345]  eta: 0:00:26  loss: 0.1463 (0.1474)  time: 0.1737  data: 0.0001  max mem: 15850
[22:26:21.314758] Test:  [200/345]  eta: 0:00:25  loss: 0.1483 (0.1477)  time: 0.1741  data: 0.0001  max mem: 15850
[22:26:23.060529] Test:  [210/345]  eta: 0:00:23  loss: 0.1450 (0.1476)  time: 0.1744  data: 0.0001  max mem: 15850
[22:26:24.811814] Test:  [220/345]  eta: 0:00:21  loss: 0.1449 (0.1474)  time: 0.1748  data: 0.0001  max mem: 15850
[22:26:26.565220] Test:  [230/345]  eta: 0:00:19  loss: 0.1331 (0.1470)  time: 0.1752  data: 0.0001  max mem: 15850
[22:26:28.322276] Test:  [240/345]  eta: 0:00:18  loss: 0.1382 (0.1473)  time: 0.1755  data: 0.0001  max mem: 15850
[22:26:30.082076] Test:  [250/345]  eta: 0:00:16  loss: 0.1478 (0.1471)  time: 0.1758  data: 0.0001  max mem: 15850
[22:26:31.846584] Test:  [260/345]  eta: 0:00:14  loss: 0.1418 (0.1470)  time: 0.1761  data: 0.0001  max mem: 15850
[22:26:33.613107] Test:  [270/345]  eta: 0:00:13  loss: 0.1536 (0.1477)  time: 0.1765  data: 0.0001  max mem: 15850
[22:26:35.382784] Test:  [280/345]  eta: 0:00:11  loss: 0.1446 (0.1474)  time: 0.1767  data: 0.0001  max mem: 15850
[22:26:37.156461] Test:  [290/345]  eta: 0:00:09  loss: 0.1426 (0.1473)  time: 0.1771  data: 0.0001  max mem: 15850
[22:26:38.933995] Test:  [300/345]  eta: 0:00:07  loss: 0.1506 (0.1475)  time: 0.1775  data: 0.0001  max mem: 15850
[22:26:40.714895] Test:  [310/345]  eta: 0:00:06  loss: 0.1513 (0.1475)  time: 0.1779  data: 0.0001  max mem: 15850
[22:26:42.498503] Test:  [320/345]  eta: 0:00:04  loss: 0.1433 (0.1476)  time: 0.1782  data: 0.0001  max mem: 15850
[22:26:44.288272] Test:  [330/345]  eta: 0:00:02  loss: 0.1433 (0.1476)  time: 0.1786  data: 0.0001  max mem: 15850
[22:26:46.078428] Test:  [340/345]  eta: 0:00:00  loss: 0.1464 (0.1477)  time: 0.1789  data: 0.0001  max mem: 15850
[22:26:46.796362] Test:  [344/345]  eta: 0:00:00  loss: 0.1464 (0.1477)  time: 0.1791  data: 0.0001  max mem: 15850
[22:26:46.855223] Test: Total time: 0:01:00 (0.1747 s / it)
[22:26:56.771983] Test:  [ 0/57]  eta: 0:00:25  loss: 0.5190 (0.5190)  time: 0.4503  data: 0.2863  max mem: 15850
[22:26:58.430529] Test:  [10/57]  eta: 0:00:09  loss: 0.4198 (0.4700)  time: 0.1916  data: 0.0261  max mem: 15850
[22:27:00.095217] Test:  [20/57]  eta: 0:00:06  loss: 0.4630 (0.4687)  time: 0.1661  data: 0.0001  max mem: 15850
[22:27:01.762840] Test:  [30/57]  eta: 0:00:04  loss: 0.2850 (0.3910)  time: 0.1666  data: 0.0001  max mem: 15850
[22:27:03.435135] Test:  [40/57]  eta: 0:00:02  loss: 0.2154 (0.3568)  time: 0.1669  data: 0.0001  max mem: 15850
[22:27:05.111901] Test:  [50/57]  eta: 0:00:01  loss: 0.2651 (0.3478)  time: 0.1674  data: 0.0001  max mem: 15850
[22:27:06.016451] Test:  [56/57]  eta: 0:00:00  loss: 0.2850 (0.3512)  time: 0.1625  data: 0.0000  max mem: 15850
[22:27:06.087390] Test: Total time: 0:00:09 (0.1713 s / it)
[22:27:07.711553] Dice score of the network on the train images: 0.871803, val images: 0.823083
[22:27:07.711787] saving best_prec_model_0 @ epoch 28
[22:27:08.991434] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:27:09.872565] Epoch: [29]  [  0/345]  eta: 0:05:03  lr: 0.000099  loss: 0.1334 (0.1334)  time: 0.8800  data: 0.2805  max mem: 15850
[22:27:21.820450] Epoch: [29]  [ 20/345]  eta: 0:03:18  lr: 0.000099  loss: 0.1514 (0.1538)  time: 0.5973  data: 0.0001  max mem: 15850
[22:27:33.796512] Epoch: [29]  [ 40/345]  eta: 0:03:04  lr: 0.000099  loss: 0.1383 (0.1488)  time: 0.5988  data: 0.0001  max mem: 15850
[22:27:45.802840] Epoch: [29]  [ 60/345]  eta: 0:02:51  lr: 0.000098  loss: 0.1389 (0.1472)  time: 0.6003  data: 0.0001  max mem: 15850
[22:27:57.825446] Epoch: [29]  [ 80/345]  eta: 0:02:39  lr: 0.000098  loss: 0.1419 (0.1469)  time: 0.6011  data: 0.0001  max mem: 15850
[22:28:09.860534] Epoch: [29]  [100/345]  eta: 0:02:27  lr: 0.000098  loss: 0.1499 (0.1485)  time: 0.6017  data: 0.0001  max mem: 15850
[22:28:21.910181] Epoch: [29]  [120/345]  eta: 0:02:15  lr: 0.000097  loss: 0.1492 (0.1489)  time: 0.6024  data: 0.0001  max mem: 15850
[22:28:33.972725] Epoch: [29]  [140/345]  eta: 0:02:03  lr: 0.000097  loss: 0.1523 (0.1501)  time: 0.6031  data: 0.0001  max mem: 15850
[22:28:46.045247] Epoch: [29]  [160/345]  eta: 0:01:51  lr: 0.000097  loss: 0.1537 (0.1505)  time: 0.6036  data: 0.0001  max mem: 15850
[22:28:58.118840] Epoch: [29]  [180/345]  eta: 0:01:39  lr: 0.000096  loss: 0.1556 (0.1513)  time: 0.6036  data: 0.0001  max mem: 15850
[22:29:10.191271] Epoch: [29]  [200/345]  eta: 0:01:27  lr: 0.000096  loss: 0.1450 (0.1512)  time: 0.6036  data: 0.0001  max mem: 15850
[22:29:22.265611] Epoch: [29]  [220/345]  eta: 0:01:15  lr: 0.000096  loss: 0.1379 (0.1509)  time: 0.6037  data: 0.0001  max mem: 15850
[22:29:34.336147] Epoch: [29]  [240/345]  eta: 0:01:03  lr: 0.000095  loss: 0.1475 (0.1506)  time: 0.6035  data: 0.0001  max mem: 15850
[22:29:46.403747] Epoch: [29]  [260/345]  eta: 0:00:51  lr: 0.000095  loss: 0.1530 (0.1510)  time: 0.6033  data: 0.0001  max mem: 15850
[22:29:58.454505] Epoch: [29]  [280/345]  eta: 0:00:39  lr: 0.000095  loss: 0.1507 (0.1513)  time: 0.6025  data: 0.0001  max mem: 15850
[22:30:10.493947] Epoch: [29]  [300/345]  eta: 0:00:27  lr: 0.000094  loss: 0.1583 (0.1514)  time: 0.6019  data: 0.0001  max mem: 15850
[22:30:22.530499] Epoch: [29]  [320/345]  eta: 0:00:15  lr: 0.000094  loss: 0.1489 (0.1515)  time: 0.6018  data: 0.0001  max mem: 15850
[22:30:34.560068] Epoch: [29]  [340/345]  eta: 0:00:03  lr: 0.000094  loss: 0.1463 (0.1510)  time: 0.6014  data: 0.0001  max mem: 15850
[22:30:36.963548] Epoch: [29]  [344/345]  eta: 0:00:00  lr: 0.000094  loss: 0.1443 (0.1509)  time: 0.6013  data: 0.0001  max mem: 15850
[22:30:37.031640] Epoch: [29] Total time: 0:03:28 (0.6030 s / it)
[22:30:37.032188] Averaged stats: lr: 0.000094  loss: 0.1443 (0.1509)
[22:30:37.587266] Test:  [  0/345]  eta: 0:03:09  loss: 0.1054 (0.1054)  time: 0.5497  data: 0.3844  max mem: 15850
[22:30:39.263781] Test:  [ 10/345]  eta: 0:01:07  loss: 0.1406 (0.1370)  time: 0.2023  data: 0.0350  max mem: 15850
[22:30:40.944502] Test:  [ 20/345]  eta: 0:01:00  loss: 0.1402 (0.1398)  time: 0.1678  data: 0.0001  max mem: 15850
[22:30:42.628686] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1429 (0.1442)  time: 0.1682  data: 0.0001  max mem: 15850
[22:30:44.316801] Test:  [ 40/345]  eta: 0:00:54  loss: 0.1483 (0.1451)  time: 0.1686  data: 0.0001  max mem: 15850
[22:30:46.007335] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1381 (0.1450)  time: 0.1689  data: 0.0001  max mem: 15850
[22:30:47.702698] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1357 (0.1433)  time: 0.1692  data: 0.0001  max mem: 15850
[22:30:49.400963] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1315 (0.1417)  time: 0.1696  data: 0.0001  max mem: 15850
[22:30:51.101509] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1315 (0.1418)  time: 0.1699  data: 0.0001  max mem: 15850
[22:30:52.806469] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1411 (0.1416)  time: 0.1702  data: 0.0001  max mem: 15850
[22:30:54.515068] Test:  [100/345]  eta: 0:00:42  loss: 0.1341 (0.1411)  time: 0.1706  data: 0.0001  max mem: 15850
[22:30:56.226300] Test:  [110/345]  eta: 0:00:40  loss: 0.1296 (0.1407)  time: 0.1709  data: 0.0001  max mem: 15850
[22:30:57.941323] Test:  [120/345]  eta: 0:00:38  loss: 0.1320 (0.1404)  time: 0.1713  data: 0.0001  max mem: 15850
[22:30:59.661048] Test:  [130/345]  eta: 0:00:37  loss: 0.1316 (0.1398)  time: 0.1717  data: 0.0001  max mem: 15850
[22:31:01.383382] Test:  [140/345]  eta: 0:00:35  loss: 0.1361 (0.1400)  time: 0.1720  data: 0.0001  max mem: 15850
[22:31:03.108589] Test:  [150/345]  eta: 0:00:33  loss: 0.1356 (0.1401)  time: 0.1723  data: 0.0001  max mem: 15850
[22:31:04.839928] Test:  [160/345]  eta: 0:00:31  loss: 0.1323 (0.1398)  time: 0.1728  data: 0.0001  max mem: 15850
[22:31:06.572764] Test:  [170/345]  eta: 0:00:30  loss: 0.1327 (0.1398)  time: 0.1731  data: 0.0001  max mem: 15850
[22:31:08.308372] Test:  [180/345]  eta: 0:00:28  loss: 0.1363 (0.1399)  time: 0.1734  data: 0.0001  max mem: 15850
[22:31:10.047593] Test:  [190/345]  eta: 0:00:26  loss: 0.1373 (0.1399)  time: 0.1737  data: 0.0001  max mem: 15850
[22:31:11.791251] Test:  [200/345]  eta: 0:00:25  loss: 0.1373 (0.1399)  time: 0.1741  data: 0.0001  max mem: 15850
[22:31:13.537510] Test:  [210/345]  eta: 0:00:23  loss: 0.1445 (0.1400)  time: 0.1744  data: 0.0001  max mem: 15850
[22:31:15.288388] Test:  [220/345]  eta: 0:00:21  loss: 0.1444 (0.1402)  time: 0.1748  data: 0.0001  max mem: 15850
[22:31:17.041712] Test:  [230/345]  eta: 0:00:19  loss: 0.1356 (0.1399)  time: 0.1751  data: 0.0001  max mem: 15850
[22:31:18.798969] Test:  [240/345]  eta: 0:00:18  loss: 0.1309 (0.1396)  time: 0.1755  data: 0.0001  max mem: 15850
[22:31:20.559100] Test:  [250/345]  eta: 0:00:16  loss: 0.1351 (0.1400)  time: 0.1758  data: 0.0001  max mem: 15850
[22:31:22.322881] Test:  [260/345]  eta: 0:00:14  loss: 0.1379 (0.1398)  time: 0.1761  data: 0.0001  max mem: 15850
[22:31:24.089410] Test:  [270/345]  eta: 0:00:13  loss: 0.1342 (0.1398)  time: 0.1765  data: 0.0001  max mem: 15850
[22:31:25.860660] Test:  [280/345]  eta: 0:00:11  loss: 0.1478 (0.1405)  time: 0.1768  data: 0.0001  max mem: 15850
[22:31:27.634616] Test:  [290/345]  eta: 0:00:09  loss: 0.1476 (0.1405)  time: 0.1772  data: 0.0001  max mem: 15850
[22:31:29.412147] Test:  [300/345]  eta: 0:00:07  loss: 0.1414 (0.1404)  time: 0.1775  data: 0.0001  max mem: 15850
[22:31:31.194316] Test:  [310/345]  eta: 0:00:06  loss: 0.1329 (0.1403)  time: 0.1779  data: 0.0001  max mem: 15850
[22:31:32.979299] Test:  [320/345]  eta: 0:00:04  loss: 0.1371 (0.1405)  time: 0.1783  data: 0.0001  max mem: 15850
[22:31:34.765637] Test:  [330/345]  eta: 0:00:02  loss: 0.1373 (0.1405)  time: 0.1785  data: 0.0001  max mem: 15850
[22:31:36.558349] Test:  [340/345]  eta: 0:00:00  loss: 0.1345 (0.1403)  time: 0.1789  data: 0.0001  max mem: 15850
[22:31:37.277409] Test:  [344/345]  eta: 0:00:00  loss: 0.1345 (0.1405)  time: 0.1791  data: 0.0001  max mem: 15850
[22:31:37.342394] Test: Total time: 0:01:00 (0.1748 s / it)
[22:31:47.403578] Test:  [ 0/57]  eta: 0:00:24  loss: 0.5167 (0.5167)  time: 0.4356  data: 0.2718  max mem: 15850
[22:31:49.062688] Test:  [10/57]  eta: 0:00:08  loss: 0.4076 (0.4568)  time: 0.1903  data: 0.0248  max mem: 15850
[22:31:50.727458] Test:  [20/57]  eta: 0:00:06  loss: 0.4305 (0.4511)  time: 0.1661  data: 0.0001  max mem: 15850
[22:31:52.397019] Test:  [30/57]  eta: 0:00:04  loss: 0.2665 (0.3759)  time: 0.1667  data: 0.0001  max mem: 15850
[22:31:54.071292] Test:  [40/57]  eta: 0:00:02  loss: 0.2163 (0.3439)  time: 0.1671  data: 0.0001  max mem: 15850
[22:31:55.748323] Test:  [50/57]  eta: 0:00:01  loss: 0.2602 (0.3365)  time: 0.1675  data: 0.0001  max mem: 15850
[22:31:56.652529] Test:  [56/57]  eta: 0:00:00  loss: 0.2817 (0.3401)  time: 0.1625  data: 0.0001  max mem: 15850
[22:31:56.727038] Test: Total time: 0:00:09 (0.1712 s / it)
[22:31:58.373626] Dice score of the network on the train images: 0.868631, val images: 0.826796
[22:31:58.377854] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:31:59.302803] Epoch: [30]  [  0/345]  eta: 0:05:18  lr: 0.000094  loss: 0.1644 (0.1644)  time: 0.9241  data: 0.3251  max mem: 15850
[22:32:11.244693] Epoch: [30]  [ 20/345]  eta: 0:03:19  lr: 0.000093  loss: 0.1396 (0.1465)  time: 0.5970  data: 0.0001  max mem: 15850
[22:32:23.202899] Epoch: [30]  [ 40/345]  eta: 0:03:04  lr: 0.000093  loss: 0.1540 (0.1487)  time: 0.5979  data: 0.0001  max mem: 15850
[22:32:35.200047] Epoch: [30]  [ 60/345]  eta: 0:02:52  lr: 0.000093  loss: 0.1522 (0.1498)  time: 0.5998  data: 0.0001  max mem: 15850
[22:32:47.199372] Epoch: [30]  [ 80/345]  eta: 0:02:39  lr: 0.000092  loss: 0.1428 (0.1485)  time: 0.5999  data: 0.0001  max mem: 15850
[22:32:59.200723] Epoch: [30]  [100/345]  eta: 0:02:27  lr: 0.000092  loss: 0.1416 (0.1485)  time: 0.6000  data: 0.0001  max mem: 15850
[22:33:11.232920] Epoch: [30]  [120/345]  eta: 0:02:15  lr: 0.000092  loss: 0.1400 (0.1482)  time: 0.6016  data: 0.0001  max mem: 15850
[22:33:23.274686] Epoch: [30]  [140/345]  eta: 0:02:03  lr: 0.000091  loss: 0.1450 (0.1490)  time: 0.6020  data: 0.0001  max mem: 15850
[22:33:35.323752] Epoch: [30]  [160/345]  eta: 0:01:51  lr: 0.000091  loss: 0.1433 (0.1487)  time: 0.6024  data: 0.0001  max mem: 15850
[22:33:47.370451] Epoch: [30]  [180/345]  eta: 0:01:39  lr: 0.000091  loss: 0.1498 (0.1486)  time: 0.6023  data: 0.0001  max mem: 15850
[22:33:59.425530] Epoch: [30]  [200/345]  eta: 0:01:27  lr: 0.000090  loss: 0.1481 (0.1481)  time: 0.6027  data: 0.0001  max mem: 15850
[22:34:11.475540] Epoch: [30]  [220/345]  eta: 0:01:15  lr: 0.000090  loss: 0.1415 (0.1479)  time: 0.6025  data: 0.0001  max mem: 15850
[22:34:23.524785] Epoch: [30]  [240/345]  eta: 0:01:03  lr: 0.000090  loss: 0.1500 (0.1481)  time: 0.6024  data: 0.0001  max mem: 15850
[22:34:35.572444] Epoch: [30]  [260/345]  eta: 0:00:51  lr: 0.000089  loss: 0.1369 (0.1476)  time: 0.6023  data: 0.0001  max mem: 15850
[22:34:47.614937] Epoch: [30]  [280/345]  eta: 0:00:39  lr: 0.000089  loss: 0.1402 (0.1474)  time: 0.6021  data: 0.0001  max mem: 15850
[22:34:59.659905] Epoch: [30]  [300/345]  eta: 0:00:27  lr: 0.000089  loss: 0.1378 (0.1473)  time: 0.6022  data: 0.0001  max mem: 15850
[22:35:11.701183] Epoch: [30]  [320/345]  eta: 0:00:15  lr: 0.000088  loss: 0.1534 (0.1480)  time: 0.6020  data: 0.0001  max mem: 15850
[22:35:23.734163] Epoch: [30]  [340/345]  eta: 0:00:03  lr: 0.000088  loss: 0.1400 (0.1475)  time: 0.6016  data: 0.0001  max mem: 15850
[22:35:26.142271] Epoch: [30]  [344/345]  eta: 0:00:00  lr: 0.000088  loss: 0.1390 (0.1475)  time: 0.6015  data: 0.0001  max mem: 15850
[22:35:26.210496] Epoch: [30] Total time: 0:03:27 (0.6024 s / it)
[22:35:26.211087] Averaged stats: lr: 0.000088  loss: 0.1390 (0.1475)
[22:35:26.680148] Test:  [  0/345]  eta: 0:02:39  loss: 0.1327 (0.1327)  time: 0.4633  data: 0.2982  max mem: 15850
[22:35:28.359231] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1456 (0.1443)  time: 0.1947  data: 0.0272  max mem: 15850
[22:35:30.041427] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1348 (0.1394)  time: 0.1680  data: 0.0001  max mem: 15850
[22:35:31.727649] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1418 (0.1435)  time: 0.1684  data: 0.0001  max mem: 15850
[22:35:33.415574] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1418 (0.1411)  time: 0.1686  data: 0.0001  max mem: 15850
[22:35:35.107155] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1364 (0.1419)  time: 0.1689  data: 0.0001  max mem: 15850
[22:35:36.803556] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1418 (0.1415)  time: 0.1693  data: 0.0001  max mem: 15850
[22:35:38.502344] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1417 (0.1410)  time: 0.1697  data: 0.0001  max mem: 15850
[22:35:40.204558] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1385 (0.1423)  time: 0.1700  data: 0.0001  max mem: 15850
[22:35:41.910020] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1377 (0.1417)  time: 0.1703  data: 0.0001  max mem: 15850
[22:35:43.621028] Test:  [100/345]  eta: 0:00:42  loss: 0.1377 (0.1412)  time: 0.1708  data: 0.0001  max mem: 15850
[22:35:45.333006] Test:  [110/345]  eta: 0:00:40  loss: 0.1373 (0.1409)  time: 0.1711  data: 0.0001  max mem: 15850
[22:35:47.048719] Test:  [120/345]  eta: 0:00:38  loss: 0.1338 (0.1409)  time: 0.1713  data: 0.0001  max mem: 15850
[22:35:48.768049] Test:  [130/345]  eta: 0:00:36  loss: 0.1348 (0.1411)  time: 0.1717  data: 0.0001  max mem: 15850
[22:35:50.491461] Test:  [140/345]  eta: 0:00:35  loss: 0.1348 (0.1414)  time: 0.1721  data: 0.0001  max mem: 15850
[22:35:52.218197] Test:  [150/345]  eta: 0:00:33  loss: 0.1447 (0.1415)  time: 0.1724  data: 0.0001  max mem: 15850
[22:35:53.948174] Test:  [160/345]  eta: 0:00:31  loss: 0.1416 (0.1415)  time: 0.1728  data: 0.0001  max mem: 15850
[22:35:55.681744] Test:  [170/345]  eta: 0:00:30  loss: 0.1354 (0.1409)  time: 0.1731  data: 0.0001  max mem: 15850
[22:35:57.418235] Test:  [180/345]  eta: 0:00:28  loss: 0.1323 (0.1411)  time: 0.1734  data: 0.0001  max mem: 15850
[22:35:59.158368] Test:  [190/345]  eta: 0:00:26  loss: 0.1435 (0.1415)  time: 0.1738  data: 0.0001  max mem: 15850
[22:36:00.902534] Test:  [200/345]  eta: 0:00:25  loss: 0.1410 (0.1414)  time: 0.1742  data: 0.0001  max mem: 15850
[22:36:02.650889] Test:  [210/345]  eta: 0:00:23  loss: 0.1346 (0.1413)  time: 0.1746  data: 0.0001  max mem: 15850
[22:36:04.403621] Test:  [220/345]  eta: 0:00:21  loss: 0.1348 (0.1410)  time: 0.1750  data: 0.0001  max mem: 15850
[22:36:06.158889] Test:  [230/345]  eta: 0:00:19  loss: 0.1348 (0.1408)  time: 0.1753  data: 0.0001  max mem: 15850
[22:36:07.915257] Test:  [240/345]  eta: 0:00:18  loss: 0.1376 (0.1408)  time: 0.1755  data: 0.0001  max mem: 15850
[22:36:09.676073] Test:  [250/345]  eta: 0:00:16  loss: 0.1376 (0.1407)  time: 0.1758  data: 0.0001  max mem: 15850
[22:36:11.439793] Test:  [260/345]  eta: 0:00:14  loss: 0.1378 (0.1409)  time: 0.1762  data: 0.0001  max mem: 15850
[22:36:13.206351] Test:  [270/345]  eta: 0:00:12  loss: 0.1428 (0.1410)  time: 0.1765  data: 0.0001  max mem: 15850
[22:36:14.977708] Test:  [280/345]  eta: 0:00:11  loss: 0.1412 (0.1411)  time: 0.1768  data: 0.0001  max mem: 15850
[22:36:16.751577] Test:  [290/345]  eta: 0:00:09  loss: 0.1461 (0.1415)  time: 0.1772  data: 0.0001  max mem: 15850
[22:36:18.529739] Test:  [300/345]  eta: 0:00:07  loss: 0.1446 (0.1415)  time: 0.1775  data: 0.0001  max mem: 15850
[22:36:20.312282] Test:  [310/345]  eta: 0:00:06  loss: 0.1318 (0.1413)  time: 0.1780  data: 0.0001  max mem: 15850
[22:36:22.097142] Test:  [320/345]  eta: 0:00:04  loss: 0.1384 (0.1415)  time: 0.1783  data: 0.0001  max mem: 15850
[22:36:23.885579] Test:  [330/345]  eta: 0:00:02  loss: 0.1399 (0.1415)  time: 0.1786  data: 0.0001  max mem: 15850
[22:36:25.677978] Test:  [340/345]  eta: 0:00:00  loss: 0.1343 (0.1413)  time: 0.1790  data: 0.0001  max mem: 15850
[22:36:26.395909] Test:  [344/345]  eta: 0:00:00  loss: 0.1397 (0.1412)  time: 0.1791  data: 0.0001  max mem: 15850
[22:36:26.467061] Test: Total time: 0:01:00 (0.1746 s / it)
[22:36:36.419765] Test:  [ 0/57]  eta: 0:00:24  loss: 0.5011 (0.5011)  time: 0.4331  data: 0.2693  max mem: 15850
[22:36:38.080226] Test:  [10/57]  eta: 0:00:08  loss: 0.3991 (0.4471)  time: 0.1902  data: 0.0246  max mem: 15850
[22:36:39.746050] Test:  [20/57]  eta: 0:00:06  loss: 0.4191 (0.4400)  time: 0.1662  data: 0.0001  max mem: 15850
[22:36:41.416456] Test:  [30/57]  eta: 0:00:04  loss: 0.2847 (0.3713)  time: 0.1667  data: 0.0001  max mem: 15850
[22:36:43.088874] Test:  [40/57]  eta: 0:00:02  loss: 0.2284 (0.3395)  time: 0.1671  data: 0.0001  max mem: 15850
[22:36:44.765880] Test:  [50/57]  eta: 0:00:01  loss: 0.2632 (0.3307)  time: 0.1674  data: 0.0001  max mem: 15850
[22:36:45.670206] Test:  [56/57]  eta: 0:00:00  loss: 0.2782 (0.3312)  time: 0.1625  data: 0.0000  max mem: 15850
[22:36:45.741524] Test: Total time: 0:00:09 (0.1711 s / it)
[22:36:47.399249] Dice score of the network on the train images: 0.859251, val images: 0.832237
[22:36:47.399470] saving best_dice_model_0 @ epoch 30
[22:36:48.694246] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:36:49.574744] Epoch: [31]  [  0/345]  eta: 0:05:03  lr: 0.000088  loss: 0.1128 (0.1128)  time: 0.8793  data: 0.2775  max mem: 15850
[22:37:01.524850] Epoch: [31]  [ 20/345]  eta: 0:03:18  lr: 0.000088  loss: 0.1380 (0.1407)  time: 0.5975  data: 0.0001  max mem: 15850
[22:37:13.501237] Epoch: [31]  [ 40/345]  eta: 0:03:04  lr: 0.000087  loss: 0.1370 (0.1414)  time: 0.5988  data: 0.0001  max mem: 15850
[22:37:25.489727] Epoch: [31]  [ 60/345]  eta: 0:02:51  lr: 0.000087  loss: 0.1392 (0.1431)  time: 0.5994  data: 0.0001  max mem: 15850
[22:37:37.499993] Epoch: [31]  [ 80/345]  eta: 0:02:39  lr: 0.000087  loss: 0.1427 (0.1460)  time: 0.6005  data: 0.0001  max mem: 15850
[22:37:49.522242] Epoch: [31]  [100/345]  eta: 0:02:27  lr: 0.000086  loss: 0.1409 (0.1465)  time: 0.6011  data: 0.0001  max mem: 15850
[22:38:01.563748] Epoch: [31]  [120/345]  eta: 0:02:15  lr: 0.000086  loss: 0.1500 (0.1472)  time: 0.6020  data: 0.0001  max mem: 15850
[22:38:13.617094] Epoch: [31]  [140/345]  eta: 0:02:03  lr: 0.000085  loss: 0.1360 (0.1458)  time: 0.6026  data: 0.0001  max mem: 15850
[22:38:25.682425] Epoch: [31]  [160/345]  eta: 0:01:51  lr: 0.000085  loss: 0.1353 (0.1451)  time: 0.6032  data: 0.0001  max mem: 15850
[22:38:37.748604] Epoch: [31]  [180/345]  eta: 0:01:39  lr: 0.000085  loss: 0.1394 (0.1450)  time: 0.6033  data: 0.0001  max mem: 15850
[22:38:49.810865] Epoch: [31]  [200/345]  eta: 0:01:27  lr: 0.000084  loss: 0.1372 (0.1447)  time: 0.6031  data: 0.0001  max mem: 15850
[22:39:01.875138] Epoch: [31]  [220/345]  eta: 0:01:15  lr: 0.000084  loss: 0.1433 (0.1442)  time: 0.6032  data: 0.0001  max mem: 15850
[22:39:13.933251] Epoch: [31]  [240/345]  eta: 0:01:03  lr: 0.000084  loss: 0.1429 (0.1444)  time: 0.6029  data: 0.0001  max mem: 15850
[22:39:25.987798] Epoch: [31]  [260/345]  eta: 0:00:51  lr: 0.000083  loss: 0.1375 (0.1442)  time: 0.6027  data: 0.0001  max mem: 15850
[22:39:38.035170] Epoch: [31]  [280/345]  eta: 0:00:39  lr: 0.000083  loss: 0.1421 (0.1443)  time: 0.6023  data: 0.0001  max mem: 15850
[22:39:50.082758] Epoch: [31]  [300/345]  eta: 0:00:27  lr: 0.000083  loss: 0.1428 (0.1440)  time: 0.6023  data: 0.0001  max mem: 15850
[22:40:02.127043] Epoch: [31]  [320/345]  eta: 0:00:15  lr: 0.000082  loss: 0.1472 (0.1442)  time: 0.6022  data: 0.0001  max mem: 15850
[22:40:14.158399] Epoch: [31]  [340/345]  eta: 0:00:03  lr: 0.000082  loss: 0.1454 (0.1441)  time: 0.6015  data: 0.0001  max mem: 15850
[22:40:16.564522] Epoch: [31]  [344/345]  eta: 0:00:00  lr: 0.000082  loss: 0.1428 (0.1440)  time: 0.6014  data: 0.0001  max mem: 15850
[22:40:16.638985] Epoch: [31] Total time: 0:03:27 (0.6027 s / it)
[22:40:16.639488] Averaged stats: lr: 0.000082  loss: 0.1428 (0.1440)
[22:40:17.105590] Test:  [  0/345]  eta: 0:02:38  loss: 0.1570 (0.1570)  time: 0.4605  data: 0.2956  max mem: 15850
[22:40:18.783502] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1508 (0.1483)  time: 0.1943  data: 0.0270  max mem: 15850
[22:40:20.465004] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1440 (0.1434)  time: 0.1679  data: 0.0001  max mem: 15850
[22:40:22.149244] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1294 (0.1399)  time: 0.1682  data: 0.0001  max mem: 15850
[22:40:23.837388] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1337 (0.1403)  time: 0.1686  data: 0.0001  max mem: 15850
[22:40:25.528970] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1359 (0.1393)  time: 0.1689  data: 0.0001  max mem: 15850
[22:40:27.224162] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1412 (0.1402)  time: 0.1693  data: 0.0001  max mem: 15850
[22:40:28.921396] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1468 (0.1404)  time: 0.1696  data: 0.0001  max mem: 15850
[22:40:30.622180] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1409 (0.1406)  time: 0.1698  data: 0.0001  max mem: 15850
[22:40:32.328239] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1479 (0.1422)  time: 0.1703  data: 0.0001  max mem: 15850
[22:40:34.035851] Test:  [100/345]  eta: 0:00:42  loss: 0.1434 (0.1416)  time: 0.1706  data: 0.0001  max mem: 15850
[22:40:35.748308] Test:  [110/345]  eta: 0:00:40  loss: 0.1415 (0.1421)  time: 0.1709  data: 0.0001  max mem: 15850
[22:40:37.463530] Test:  [120/345]  eta: 0:00:38  loss: 0.1428 (0.1426)  time: 0.1713  data: 0.0001  max mem: 15850
[22:40:39.181146] Test:  [130/345]  eta: 0:00:36  loss: 0.1397 (0.1422)  time: 0.1716  data: 0.0001  max mem: 15850
[22:40:40.905351] Test:  [140/345]  eta: 0:00:35  loss: 0.1320 (0.1419)  time: 0.1720  data: 0.0001  max mem: 15850
[22:40:42.631160] Test:  [150/345]  eta: 0:00:33  loss: 0.1307 (0.1411)  time: 0.1724  data: 0.0001  max mem: 15850
[22:40:44.362664] Test:  [160/345]  eta: 0:00:31  loss: 0.1325 (0.1415)  time: 0.1728  data: 0.0001  max mem: 15850
[22:40:46.095548] Test:  [170/345]  eta: 0:00:30  loss: 0.1362 (0.1415)  time: 0.1732  data: 0.0001  max mem: 15850
[22:40:47.831909] Test:  [180/345]  eta: 0:00:28  loss: 0.1371 (0.1412)  time: 0.1734  data: 0.0001  max mem: 15850
[22:40:49.571808] Test:  [190/345]  eta: 0:00:26  loss: 0.1380 (0.1409)  time: 0.1738  data: 0.0001  max mem: 15850
[22:40:51.314699] Test:  [200/345]  eta: 0:00:25  loss: 0.1341 (0.1407)  time: 0.1741  data: 0.0001  max mem: 15850
[22:40:53.062189] Test:  [210/345]  eta: 0:00:23  loss: 0.1360 (0.1409)  time: 0.1745  data: 0.0001  max mem: 15850
[22:40:54.812736] Test:  [220/345]  eta: 0:00:21  loss: 0.1360 (0.1411)  time: 0.1748  data: 0.0001  max mem: 15850
[22:40:56.566843] Test:  [230/345]  eta: 0:00:19  loss: 0.1439 (0.1417)  time: 0.1752  data: 0.0001  max mem: 15850
[22:40:58.324124] Test:  [240/345]  eta: 0:00:18  loss: 0.1453 (0.1421)  time: 0.1755  data: 0.0001  max mem: 15850
[22:41:00.085199] Test:  [250/345]  eta: 0:00:16  loss: 0.1453 (0.1421)  time: 0.1759  data: 0.0001  max mem: 15850
[22:41:01.849456] Test:  [260/345]  eta: 0:00:14  loss: 0.1480 (0.1424)  time: 0.1762  data: 0.0001  max mem: 15850
[22:41:03.615994] Test:  [270/345]  eta: 0:00:12  loss: 0.1410 (0.1424)  time: 0.1765  data: 0.0001  max mem: 15850
[22:41:05.387637] Test:  [280/345]  eta: 0:00:11  loss: 0.1396 (0.1425)  time: 0.1768  data: 0.0001  max mem: 15850
[22:41:07.163429] Test:  [290/345]  eta: 0:00:09  loss: 0.1358 (0.1422)  time: 0.1773  data: 0.0001  max mem: 15850
[22:41:08.942265] Test:  [300/345]  eta: 0:00:07  loss: 0.1361 (0.1425)  time: 0.1777  data: 0.0001  max mem: 15850
[22:41:10.725156] Test:  [310/345]  eta: 0:00:06  loss: 0.1402 (0.1424)  time: 0.1780  data: 0.0001  max mem: 15850
[22:41:12.509653] Test:  [320/345]  eta: 0:00:04  loss: 0.1402 (0.1424)  time: 0.1783  data: 0.0001  max mem: 15850
[22:41:14.297073] Test:  [330/345]  eta: 0:00:02  loss: 0.1448 (0.1425)  time: 0.1785  data: 0.0001  max mem: 15850
[22:41:16.089789] Test:  [340/345]  eta: 0:00:00  loss: 0.1454 (0.1425)  time: 0.1790  data: 0.0001  max mem: 15850
[22:41:16.808443] Test:  [344/345]  eta: 0:00:00  loss: 0.1394 (0.1426)  time: 0.1791  data: 0.0001  max mem: 15850
[22:41:16.880491] Test: Total time: 0:01:00 (0.1746 s / it)
[22:41:26.702412] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4698 (0.4698)  time: 0.4440  data: 0.2807  max mem: 15850
[22:41:28.363993] Test:  [10/57]  eta: 0:00:08  loss: 0.4125 (0.4401)  time: 0.1913  data: 0.0256  max mem: 15850
[22:41:30.029927] Test:  [20/57]  eta: 0:00:06  loss: 0.4343 (0.4444)  time: 0.1663  data: 0.0001  max mem: 15850
[22:41:31.699054] Test:  [30/57]  eta: 0:00:04  loss: 0.2901 (0.3734)  time: 0.1667  data: 0.0001  max mem: 15850
[22:41:33.372474] Test:  [40/57]  eta: 0:00:02  loss: 0.2181 (0.3411)  time: 0.1671  data: 0.0001  max mem: 15850
[22:41:35.049432] Test:  [50/57]  eta: 0:00:01  loss: 0.2560 (0.3310)  time: 0.1675  data: 0.0001  max mem: 15850
[22:41:35.954184] Test:  [56/57]  eta: 0:00:00  loss: 0.2569 (0.3330)  time: 0.1625  data: 0.0001  max mem: 15850
[22:41:36.026205] Test: Total time: 0:00:09 (0.1714 s / it)
[22:41:37.678205] Dice score of the network on the train images: 0.861950, val images: 0.830070
[22:41:37.682230] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:41:38.572629] Epoch: [32]  [  0/345]  eta: 0:05:06  lr: 0.000082  loss: 0.1295 (0.1295)  time: 0.8893  data: 0.2892  max mem: 15850
[22:41:50.538871] Epoch: [32]  [ 20/345]  eta: 0:03:18  lr: 0.000081  loss: 0.1407 (0.1426)  time: 0.5983  data: 0.0001  max mem: 15850
[22:42:02.536972] Epoch: [32]  [ 40/345]  eta: 0:03:04  lr: 0.000081  loss: 0.1382 (0.1419)  time: 0.5999  data: 0.0001  max mem: 15850
[22:42:14.557958] Epoch: [32]  [ 60/345]  eta: 0:02:52  lr: 0.000081  loss: 0.1365 (0.1415)  time: 0.6010  data: 0.0001  max mem: 15850
[22:42:26.589372] Epoch: [32]  [ 80/345]  eta: 0:02:39  lr: 0.000080  loss: 0.1414 (0.1422)  time: 0.6015  data: 0.0001  max mem: 15850
[22:42:38.628839] Epoch: [32]  [100/345]  eta: 0:02:27  lr: 0.000080  loss: 0.1359 (0.1425)  time: 0.6019  data: 0.0001  max mem: 15850
[22:42:50.679470] Epoch: [32]  [120/345]  eta: 0:02:15  lr: 0.000080  loss: 0.1386 (0.1426)  time: 0.6025  data: 0.0001  max mem: 15850
[22:43:02.740014] Epoch: [32]  [140/345]  eta: 0:02:03  lr: 0.000079  loss: 0.1306 (0.1413)  time: 0.6030  data: 0.0001  max mem: 15850
[22:43:14.805577] Epoch: [32]  [160/345]  eta: 0:01:51  lr: 0.000079  loss: 0.1437 (0.1417)  time: 0.6032  data: 0.0001  max mem: 15850
[22:43:26.881395] Epoch: [32]  [180/345]  eta: 0:01:39  lr: 0.000079  loss: 0.1362 (0.1419)  time: 0.6037  data: 0.0001  max mem: 15850
[22:43:38.958982] Epoch: [32]  [200/345]  eta: 0:01:27  lr: 0.000078  loss: 0.1408 (0.1419)  time: 0.6038  data: 0.0001  max mem: 15850
[22:43:51.024772] Epoch: [32]  [220/345]  eta: 0:01:15  lr: 0.000078  loss: 0.1369 (0.1416)  time: 0.6032  data: 0.0001  max mem: 15850
[22:44:03.069321] Epoch: [32]  [240/345]  eta: 0:01:03  lr: 0.000077  loss: 0.1371 (0.1414)  time: 0.6022  data: 0.0001  max mem: 15850
[22:44:15.112216] Epoch: [32]  [260/345]  eta: 0:00:51  lr: 0.000077  loss: 0.1316 (0.1407)  time: 0.6021  data: 0.0001  max mem: 15850
[22:44:27.147800] Epoch: [32]  [280/345]  eta: 0:00:39  lr: 0.000077  loss: 0.1416 (0.1409)  time: 0.6017  data: 0.0001  max mem: 15850
[22:44:39.176653] Epoch: [32]  [300/345]  eta: 0:00:27  lr: 0.000076  loss: 0.1366 (0.1410)  time: 0.6014  data: 0.0001  max mem: 15850
[22:44:51.207522] Epoch: [32]  [320/345]  eta: 0:00:15  lr: 0.000076  loss: 0.1362 (0.1409)  time: 0.6015  data: 0.0001  max mem: 15850
[22:45:03.236954] Epoch: [32]  [340/345]  eta: 0:00:03  lr: 0.000076  loss: 0.1433 (0.1411)  time: 0.6014  data: 0.0001  max mem: 15850
[22:45:05.639893] Epoch: [32]  [344/345]  eta: 0:00:00  lr: 0.000076  loss: 0.1450 (0.1410)  time: 0.6013  data: 0.0001  max mem: 15850
[22:45:05.711135] Epoch: [32] Total time: 0:03:28 (0.6030 s / it)
[22:45:05.711478] Averaged stats: lr: 0.000076  loss: 0.1450 (0.1410)
[22:45:06.209017] Test:  [  0/345]  eta: 0:02:49  loss: 0.1410 (0.1410)  time: 0.4922  data: 0.3269  max mem: 15850
[22:45:07.886811] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1281 (0.1416)  time: 0.1972  data: 0.0298  max mem: 15850
[22:45:09.569074] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1263 (0.1361)  time: 0.1679  data: 0.0001  max mem: 15850
[22:45:11.253697] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1263 (0.1355)  time: 0.1683  data: 0.0001  max mem: 15850
[22:45:12.941514] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1223 (0.1331)  time: 0.1686  data: 0.0001  max mem: 15850
[22:45:14.633310] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1280 (0.1347)  time: 0.1689  data: 0.0001  max mem: 15850
[22:45:16.326927] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1322 (0.1340)  time: 0.1692  data: 0.0001  max mem: 15850
[22:45:18.026403] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1267 (0.1337)  time: 0.1696  data: 0.0001  max mem: 15850
[22:45:19.727605] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1267 (0.1327)  time: 0.1700  data: 0.0001  max mem: 15850
[22:45:21.431681] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1284 (0.1324)  time: 0.1702  data: 0.0001  max mem: 15850
[22:45:23.140710] Test:  [100/345]  eta: 0:00:42  loss: 0.1284 (0.1322)  time: 0.1706  data: 0.0001  max mem: 15850
[22:45:24.854131] Test:  [110/345]  eta: 0:00:40  loss: 0.1254 (0.1323)  time: 0.1711  data: 0.0001  max mem: 15850
[22:45:26.570718] Test:  [120/345]  eta: 0:00:38  loss: 0.1273 (0.1334)  time: 0.1714  data: 0.0001  max mem: 15850
[22:45:28.289757] Test:  [130/345]  eta: 0:00:37  loss: 0.1378 (0.1337)  time: 0.1717  data: 0.0001  max mem: 15850
[22:45:30.011766] Test:  [140/345]  eta: 0:00:35  loss: 0.1317 (0.1337)  time: 0.1720  data: 0.0001  max mem: 15850
[22:45:31.737478] Test:  [150/345]  eta: 0:00:33  loss: 0.1279 (0.1342)  time: 0.1723  data: 0.0001  max mem: 15850
[22:45:33.466621] Test:  [160/345]  eta: 0:00:31  loss: 0.1233 (0.1335)  time: 0.1727  data: 0.0001  max mem: 15850
[22:45:35.199497] Test:  [170/345]  eta: 0:00:30  loss: 0.1277 (0.1338)  time: 0.1730  data: 0.0001  max mem: 15850
[22:45:36.935687] Test:  [180/345]  eta: 0:00:28  loss: 0.1334 (0.1344)  time: 0.1734  data: 0.0001  max mem: 15850
[22:45:38.676384] Test:  [190/345]  eta: 0:00:26  loss: 0.1346 (0.1347)  time: 0.1738  data: 0.0001  max mem: 15850
[22:45:40.419225] Test:  [200/345]  eta: 0:00:25  loss: 0.1341 (0.1349)  time: 0.1741  data: 0.0001  max mem: 15850
[22:45:42.166113] Test:  [210/345]  eta: 0:00:23  loss: 0.1308 (0.1346)  time: 0.1744  data: 0.0001  max mem: 15850
[22:45:43.916264] Test:  [220/345]  eta: 0:00:21  loss: 0.1255 (0.1344)  time: 0.1748  data: 0.0001  max mem: 15850
[22:45:45.670931] Test:  [230/345]  eta: 0:00:19  loss: 0.1258 (0.1343)  time: 0.1752  data: 0.0001  max mem: 15850
[22:45:47.426897] Test:  [240/345]  eta: 0:00:18  loss: 0.1290 (0.1342)  time: 0.1755  data: 0.0001  max mem: 15850
[22:45:49.187898] Test:  [250/345]  eta: 0:00:16  loss: 0.1290 (0.1341)  time: 0.1758  data: 0.0001  max mem: 15850
[22:45:50.952822] Test:  [260/345]  eta: 0:00:14  loss: 0.1250 (0.1338)  time: 0.1762  data: 0.0001  max mem: 15850
[22:45:52.720162] Test:  [270/345]  eta: 0:00:13  loss: 0.1295 (0.1338)  time: 0.1766  data: 0.0001  max mem: 15850
[22:45:54.491438] Test:  [280/345]  eta: 0:00:11  loss: 0.1278 (0.1335)  time: 0.1769  data: 0.0001  max mem: 15850
[22:45:56.265124] Test:  [290/345]  eta: 0:00:09  loss: 0.1278 (0.1337)  time: 0.1772  data: 0.0001  max mem: 15850
[22:45:58.042890] Test:  [300/345]  eta: 0:00:07  loss: 0.1355 (0.1336)  time: 0.1775  data: 0.0001  max mem: 15850
[22:45:59.825609] Test:  [310/345]  eta: 0:00:06  loss: 0.1355 (0.1337)  time: 0.1780  data: 0.0001  max mem: 15850
[22:46:01.612205] Test:  [320/345]  eta: 0:00:04  loss: 0.1360 (0.1338)  time: 0.1784  data: 0.0001  max mem: 15850
[22:46:03.399427] Test:  [330/345]  eta: 0:00:02  loss: 0.1301 (0.1336)  time: 0.1786  data: 0.0001  max mem: 15850
[22:46:05.193154] Test:  [340/345]  eta: 0:00:00  loss: 0.1279 (0.1337)  time: 0.1790  data: 0.0001  max mem: 15850
[22:46:05.911469] Test:  [344/345]  eta: 0:00:00  loss: 0.1306 (0.1337)  time: 0.1792  data: 0.0001  max mem: 15850
[22:46:05.988613] Test: Total time: 0:01:00 (0.1747 s / it)
[22:46:15.913257] Test:  [ 0/57]  eta: 0:00:29  loss: 0.4764 (0.4764)  time: 0.5214  data: 0.3577  max mem: 15850
[22:46:17.572934] Test:  [10/57]  eta: 0:00:09  loss: 0.4049 (0.4506)  time: 0.1982  data: 0.0326  max mem: 15850
[22:46:19.238189] Test:  [20/57]  eta: 0:00:06  loss: 0.4725 (0.4525)  time: 0.1662  data: 0.0001  max mem: 15850
[22:46:20.906918] Test:  [30/57]  eta: 0:00:04  loss: 0.2681 (0.3784)  time: 0.1666  data: 0.0001  max mem: 15850
[22:46:22.580195] Test:  [40/57]  eta: 0:00:02  loss: 0.2193 (0.3432)  time: 0.1670  data: 0.0001  max mem: 15850
[22:46:24.256911] Test:  [50/57]  eta: 0:00:01  loss: 0.2447 (0.3343)  time: 0.1674  data: 0.0001  max mem: 15850
[22:46:25.161257] Test:  [56/57]  eta: 0:00:00  loss: 0.2651 (0.3365)  time: 0.1625  data: 0.0000  max mem: 15850
[22:46:25.226779] Test: Total time: 0:00:09 (0.1726 s / it)
[22:46:26.896417] Dice score of the network on the train images: 0.870128, val images: 0.835684
[22:46:26.896654] saving best_dice_model_0 @ epoch 32
[22:46:28.192139] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:46:29.070972] Epoch: [33]  [  0/345]  eta: 0:05:02  lr: 0.000075  loss: 0.1642 (0.1642)  time: 0.8779  data: 0.2791  max mem: 15850
[22:46:41.003414] Epoch: [33]  [ 20/345]  eta: 0:03:18  lr: 0.000075  loss: 0.1289 (0.1410)  time: 0.5966  data: 0.0001  max mem: 15850
[22:46:52.968677] Epoch: [33]  [ 40/345]  eta: 0:03:04  lr: 0.000075  loss: 0.1295 (0.1372)  time: 0.5982  data: 0.0001  max mem: 15850
[22:47:04.962305] Epoch: [33]  [ 60/345]  eta: 0:02:51  lr: 0.000074  loss: 0.1247 (0.1344)  time: 0.5996  data: 0.0001  max mem: 15850
[22:47:16.981002] Epoch: [33]  [ 80/345]  eta: 0:02:39  lr: 0.000074  loss: 0.1290 (0.1352)  time: 0.6009  data: 0.0001  max mem: 15850
[22:47:29.024190] Epoch: [33]  [100/345]  eta: 0:02:27  lr: 0.000074  loss: 0.1341 (0.1352)  time: 0.6021  data: 0.0001  max mem: 15850
[22:47:41.082984] Epoch: [33]  [120/345]  eta: 0:02:15  lr: 0.000073  loss: 0.1363 (0.1358)  time: 0.6029  data: 0.0001  max mem: 15850
[22:47:53.146467] Epoch: [33]  [140/345]  eta: 0:02:03  lr: 0.000073  loss: 0.1363 (0.1359)  time: 0.6031  data: 0.0001  max mem: 15850
[22:48:05.211789] Epoch: [33]  [160/345]  eta: 0:01:51  lr: 0.000073  loss: 0.1335 (0.1367)  time: 0.6032  data: 0.0001  max mem: 15850
[22:48:17.286384] Epoch: [33]  [180/345]  eta: 0:01:39  lr: 0.000072  loss: 0.1396 (0.1373)  time: 0.6037  data: 0.0001  max mem: 15850
[22:48:29.364342] Epoch: [33]  [200/345]  eta: 0:01:27  lr: 0.000072  loss: 0.1288 (0.1368)  time: 0.6038  data: 0.0001  max mem: 15850
[22:48:41.444227] Epoch: [33]  [220/345]  eta: 0:01:15  lr: 0.000071  loss: 0.1339 (0.1366)  time: 0.6039  data: 0.0001  max mem: 15850
[22:48:53.513600] Epoch: [33]  [240/345]  eta: 0:01:03  lr: 0.000071  loss: 0.1282 (0.1362)  time: 0.6034  data: 0.0001  max mem: 15850
[22:49:05.582439] Epoch: [33]  [260/345]  eta: 0:00:51  lr: 0.000071  loss: 0.1386 (0.1365)  time: 0.6034  data: 0.0001  max mem: 15850
[22:49:17.640835] Epoch: [33]  [280/345]  eta: 0:00:39  lr: 0.000070  loss: 0.1369 (0.1365)  time: 0.6029  data: 0.0001  max mem: 15850
[22:49:29.707864] Epoch: [33]  [300/345]  eta: 0:00:27  lr: 0.000070  loss: 0.1394 (0.1368)  time: 0.6033  data: 0.0001  max mem: 15850
[22:49:41.765319] Epoch: [33]  [320/345]  eta: 0:00:15  lr: 0.000070  loss: 0.1381 (0.1369)  time: 0.6028  data: 0.0001  max mem: 15850
[22:49:53.818552] Epoch: [33]  [340/345]  eta: 0:00:03  lr: 0.000069  loss: 0.1365 (0.1372)  time: 0.6026  data: 0.0001  max mem: 15850
[22:49:56.228794] Epoch: [33]  [344/345]  eta: 0:00:00  lr: 0.000069  loss: 0.1365 (0.1372)  time: 0.6025  data: 0.0001  max mem: 15850
[22:49:56.294194] Epoch: [33] Total time: 0:03:28 (0.6032 s / it)
[22:49:56.294410] Averaged stats: lr: 0.000069  loss: 0.1365 (0.1372)
[22:49:56.764368] Test:  [  0/345]  eta: 0:02:40  loss: 0.1280 (0.1280)  time: 0.4642  data: 0.2988  max mem: 15850
[22:49:58.443383] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1141 (0.1213)  time: 0.1948  data: 0.0273  max mem: 15850
[22:50:00.125163] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1192 (0.1270)  time: 0.1680  data: 0.0001  max mem: 15850
[22:50:01.810752] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1305 (0.1288)  time: 0.1683  data: 0.0001  max mem: 15850
[22:50:03.500006] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1244 (0.1286)  time: 0.1687  data: 0.0001  max mem: 15850
[22:50:05.191172] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1267 (0.1297)  time: 0.1690  data: 0.0001  max mem: 15850
[22:50:06.886341] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1287 (0.1295)  time: 0.1693  data: 0.0001  max mem: 15850
[22:50:08.583946] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1249 (0.1298)  time: 0.1696  data: 0.0001  max mem: 15850
[22:50:10.284590] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1262 (0.1294)  time: 0.1699  data: 0.0001  max mem: 15850
[22:50:11.989261] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1226 (0.1287)  time: 0.1702  data: 0.0001  max mem: 15850
[22:50:13.697804] Test:  [100/345]  eta: 0:00:42  loss: 0.1206 (0.1278)  time: 0.1706  data: 0.0001  max mem: 15850
[22:50:15.410126] Test:  [110/345]  eta: 0:00:40  loss: 0.1241 (0.1286)  time: 0.1710  data: 0.0001  max mem: 15850
[22:50:17.125842] Test:  [120/345]  eta: 0:00:38  loss: 0.1244 (0.1284)  time: 0.1713  data: 0.0001  max mem: 15850
[22:50:18.844828] Test:  [130/345]  eta: 0:00:36  loss: 0.1232 (0.1282)  time: 0.1717  data: 0.0001  max mem: 15850
[22:50:20.568819] Test:  [140/345]  eta: 0:00:35  loss: 0.1267 (0.1288)  time: 0.1721  data: 0.0001  max mem: 15850
[22:50:22.295708] Test:  [150/345]  eta: 0:00:33  loss: 0.1306 (0.1293)  time: 0.1725  data: 0.0001  max mem: 15850
[22:50:24.024318] Test:  [160/345]  eta: 0:00:31  loss: 0.1295 (0.1294)  time: 0.1727  data: 0.0001  max mem: 15850
[22:50:25.758543] Test:  [170/345]  eta: 0:00:30  loss: 0.1257 (0.1291)  time: 0.1731  data: 0.0001  max mem: 15850
[22:50:27.494413] Test:  [180/345]  eta: 0:00:28  loss: 0.1246 (0.1296)  time: 0.1734  data: 0.0001  max mem: 15850
[22:50:29.234539] Test:  [190/345]  eta: 0:00:26  loss: 0.1207 (0.1291)  time: 0.1737  data: 0.0001  max mem: 15850
[22:50:30.977987] Test:  [200/345]  eta: 0:00:25  loss: 0.1188 (0.1288)  time: 0.1741  data: 0.0001  max mem: 15850
[22:50:32.724481] Test:  [210/345]  eta: 0:00:23  loss: 0.1238 (0.1290)  time: 0.1744  data: 0.0001  max mem: 15850
[22:50:34.475113] Test:  [220/345]  eta: 0:00:21  loss: 0.1287 (0.1292)  time: 0.1748  data: 0.0001  max mem: 15850
[22:50:36.228523] Test:  [230/345]  eta: 0:00:19  loss: 0.1246 (0.1292)  time: 0.1751  data: 0.0001  max mem: 15850
[22:50:37.985683] Test:  [240/345]  eta: 0:00:18  loss: 0.1308 (0.1296)  time: 0.1755  data: 0.0001  max mem: 15850
[22:50:39.747289] Test:  [250/345]  eta: 0:00:16  loss: 0.1366 (0.1297)  time: 0.1759  data: 0.0001  max mem: 15850
[22:50:41.512048] Test:  [260/345]  eta: 0:00:14  loss: 0.1340 (0.1300)  time: 0.1762  data: 0.0001  max mem: 15850
[22:50:43.278628] Test:  [270/345]  eta: 0:00:12  loss: 0.1310 (0.1300)  time: 0.1765  data: 0.0001  max mem: 15850
[22:50:45.048109] Test:  [280/345]  eta: 0:00:11  loss: 0.1261 (0.1299)  time: 0.1767  data: 0.0001  max mem: 15850
[22:50:46.823286] Test:  [290/345]  eta: 0:00:09  loss: 0.1344 (0.1302)  time: 0.1772  data: 0.0001  max mem: 15850
[22:50:48.601002] Test:  [300/345]  eta: 0:00:07  loss: 0.1342 (0.1302)  time: 0.1776  data: 0.0001  max mem: 15850
[22:50:50.381984] Test:  [310/345]  eta: 0:00:06  loss: 0.1213 (0.1299)  time: 0.1779  data: 0.0001  max mem: 15850
[22:50:52.165568] Test:  [320/345]  eta: 0:00:04  loss: 0.1184 (0.1298)  time: 0.1782  data: 0.0001  max mem: 15850
[22:50:53.955359] Test:  [330/345]  eta: 0:00:02  loss: 0.1208 (0.1296)  time: 0.1786  data: 0.0001  max mem: 15850
[22:50:55.745924] Test:  [340/345]  eta: 0:00:00  loss: 0.1266 (0.1297)  time: 0.1790  data: 0.0001  max mem: 15850
[22:50:56.464082] Test:  [344/345]  eta: 0:00:00  loss: 0.1266 (0.1297)  time: 0.1791  data: 0.0001  max mem: 15850
[22:50:56.546977] Test: Total time: 0:01:00 (0.1746 s / it)
[22:51:06.447900] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4813 (0.4813)  time: 0.4466  data: 0.2827  max mem: 15850
[22:51:08.109301] Test:  [10/57]  eta: 0:00:09  loss: 0.4128 (0.4512)  time: 0.1916  data: 0.0258  max mem: 15850
[22:51:09.774062] Test:  [20/57]  eta: 0:00:06  loss: 0.4685 (0.4559)  time: 0.1662  data: 0.0001  max mem: 15850
[22:51:11.443188] Test:  [30/57]  eta: 0:00:04  loss: 0.2862 (0.3807)  time: 0.1666  data: 0.0001  max mem: 15850
[22:51:13.115332] Test:  [40/57]  eta: 0:00:02  loss: 0.2330 (0.3476)  time: 0.1670  data: 0.0001  max mem: 15850
[22:51:14.794120] Test:  [50/57]  eta: 0:00:01  loss: 0.2568 (0.3391)  time: 0.1675  data: 0.0001  max mem: 15850
[22:51:15.699077] Test:  [56/57]  eta: 0:00:00  loss: 0.2781 (0.3416)  time: 0.1626  data: 0.0000  max mem: 15850
[22:51:15.764602] Test: Total time: 0:00:09 (0.1713 s / it)
[22:51:17.408231] Dice score of the network on the train images: 0.878483, val images: 0.831984
[22:51:17.412325] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:51:18.301304] Epoch: [34]  [  0/345]  eta: 0:05:06  lr: 0.000069  loss: 0.1273 (0.1273)  time: 0.8879  data: 0.2874  max mem: 15850
[22:51:30.263950] Epoch: [34]  [ 20/345]  eta: 0:03:18  lr: 0.000069  loss: 0.1317 (0.1290)  time: 0.5981  data: 0.0001  max mem: 15850
[22:51:42.226458] Epoch: [34]  [ 40/345]  eta: 0:03:04  lr: 0.000068  loss: 0.1282 (0.1348)  time: 0.5981  data: 0.0001  max mem: 15850
[22:51:54.223391] Epoch: [34]  [ 60/345]  eta: 0:02:51  lr: 0.000068  loss: 0.1194 (0.1317)  time: 0.5998  data: 0.0001  max mem: 15850
[22:52:06.225966] Epoch: [34]  [ 80/345]  eta: 0:02:39  lr: 0.000068  loss: 0.1311 (0.1331)  time: 0.6001  data: 0.0001  max mem: 15850
[22:52:18.240487] Epoch: [34]  [100/345]  eta: 0:02:27  lr: 0.000067  loss: 0.1335 (0.1333)  time: 0.6007  data: 0.0001  max mem: 15850
[22:52:30.264234] Epoch: [34]  [120/345]  eta: 0:02:15  lr: 0.000067  loss: 0.1269 (0.1327)  time: 0.6011  data: 0.0001  max mem: 15850
[22:52:42.303031] Epoch: [34]  [140/345]  eta: 0:02:03  lr: 0.000066  loss: 0.1300 (0.1321)  time: 0.6019  data: 0.0001  max mem: 15850
[22:52:54.355123] Epoch: [34]  [160/345]  eta: 0:01:51  lr: 0.000066  loss: 0.1286 (0.1322)  time: 0.6026  data: 0.0001  max mem: 15850
[22:53:06.405487] Epoch: [34]  [180/345]  eta: 0:01:39  lr: 0.000066  loss: 0.1349 (0.1329)  time: 0.6025  data: 0.0001  max mem: 15850
[22:53:18.444618] Epoch: [34]  [200/345]  eta: 0:01:27  lr: 0.000065  loss: 0.1351 (0.1329)  time: 0.6019  data: 0.0001  max mem: 15850
[22:53:30.480901] Epoch: [34]  [220/345]  eta: 0:01:15  lr: 0.000065  loss: 0.1259 (0.1324)  time: 0.6018  data: 0.0001  max mem: 15850
[22:53:42.516545] Epoch: [34]  [240/345]  eta: 0:01:03  lr: 0.000064  loss: 0.1349 (0.1326)  time: 0.6017  data: 0.0001  max mem: 15850
[22:53:54.550548] Epoch: [34]  [260/345]  eta: 0:00:51  lr: 0.000064  loss: 0.1317 (0.1326)  time: 0.6017  data: 0.0001  max mem: 15850
[22:54:06.588175] Epoch: [34]  [280/345]  eta: 0:00:39  lr: 0.000064  loss: 0.1315 (0.1327)  time: 0.6018  data: 0.0001  max mem: 15850
[22:54:18.620905] Epoch: [34]  [300/345]  eta: 0:00:27  lr: 0.000063  loss: 0.1320 (0.1327)  time: 0.6016  data: 0.0001  max mem: 15850
[22:54:30.649362] Epoch: [34]  [320/345]  eta: 0:00:15  lr: 0.000063  loss: 0.1349 (0.1331)  time: 0.6014  data: 0.0001  max mem: 15850
[22:54:42.672804] Epoch: [34]  [340/345]  eta: 0:00:03  lr: 0.000063  loss: 0.1332 (0.1334)  time: 0.6011  data: 0.0001  max mem: 15850
[22:54:45.076561] Epoch: [34]  [344/345]  eta: 0:00:00  lr: 0.000063  loss: 0.1332 (0.1334)  time: 0.6010  data: 0.0001  max mem: 15850
[22:54:45.149067] Epoch: [34] Total time: 0:03:27 (0.6021 s / it)
[22:54:45.149388] Averaged stats: lr: 0.000063  loss: 0.1332 (0.1334)
[22:54:45.672550] Test:  [  0/345]  eta: 0:02:58  loss: 0.1231 (0.1231)  time: 0.5182  data: 0.3530  max mem: 15850
[22:54:47.351064] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1195 (0.1206)  time: 0.1996  data: 0.0322  max mem: 15850
[22:54:49.031020] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1195 (0.1253)  time: 0.1678  data: 0.0001  max mem: 15850
[22:54:50.715536] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1275 (0.1279)  time: 0.1682  data: 0.0001  max mem: 15850
[22:54:52.404049] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1256 (0.1262)  time: 0.1686  data: 0.0001  max mem: 15850
[22:54:54.094768] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1162 (0.1266)  time: 0.1689  data: 0.0001  max mem: 15850
[22:54:55.789192] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1219 (0.1262)  time: 0.1692  data: 0.0001  max mem: 15850
[22:54:57.486370] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1194 (0.1262)  time: 0.1695  data: 0.0001  max mem: 15850
[22:54:59.187173] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1179 (0.1260)  time: 0.1698  data: 0.0001  max mem: 15850
[22:55:00.891426] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1279 (0.1263)  time: 0.1702  data: 0.0001  max mem: 15850
[22:55:02.599588] Test:  [100/345]  eta: 0:00:42  loss: 0.1221 (0.1259)  time: 0.1705  data: 0.0001  max mem: 15850
[22:55:04.311440] Test:  [110/345]  eta: 0:00:40  loss: 0.1221 (0.1261)  time: 0.1709  data: 0.0001  max mem: 15850
[22:55:06.026418] Test:  [120/345]  eta: 0:00:38  loss: 0.1260 (0.1264)  time: 0.1713  data: 0.0001  max mem: 15850
[22:55:07.745029] Test:  [130/345]  eta: 0:00:37  loss: 0.1221 (0.1259)  time: 0.1716  data: 0.0001  max mem: 15850
[22:55:09.468253] Test:  [140/345]  eta: 0:00:35  loss: 0.1210 (0.1259)  time: 0.1720  data: 0.0001  max mem: 15850
[22:55:11.195256] Test:  [150/345]  eta: 0:00:33  loss: 0.1182 (0.1253)  time: 0.1724  data: 0.0001  max mem: 15850
[22:55:12.924622] Test:  [160/345]  eta: 0:00:31  loss: 0.1179 (0.1255)  time: 0.1728  data: 0.0001  max mem: 15850
[22:55:14.656864] Test:  [170/345]  eta: 0:00:30  loss: 0.1264 (0.1254)  time: 0.1730  data: 0.0001  max mem: 15850
[22:55:16.393533] Test:  [180/345]  eta: 0:00:28  loss: 0.1252 (0.1258)  time: 0.1734  data: 0.0001  max mem: 15850
[22:55:18.133229] Test:  [190/345]  eta: 0:00:26  loss: 0.1208 (0.1260)  time: 0.1738  data: 0.0001  max mem: 15850
[22:55:19.876570] Test:  [200/345]  eta: 0:00:25  loss: 0.1262 (0.1259)  time: 0.1741  data: 0.0001  max mem: 15850
[22:55:21.624666] Test:  [210/345]  eta: 0:00:23  loss: 0.1249 (0.1259)  time: 0.1745  data: 0.0001  max mem: 15850
[22:55:23.373941] Test:  [220/345]  eta: 0:00:21  loss: 0.1251 (0.1263)  time: 0.1748  data: 0.0001  max mem: 15850
[22:55:25.128540] Test:  [230/345]  eta: 0:00:19  loss: 0.1262 (0.1264)  time: 0.1751  data: 0.0001  max mem: 15850
[22:55:26.885705] Test:  [240/345]  eta: 0:00:18  loss: 0.1212 (0.1263)  time: 0.1755  data: 0.0001  max mem: 15850
[22:55:28.646554] Test:  [250/345]  eta: 0:00:16  loss: 0.1260 (0.1264)  time: 0.1758  data: 0.0001  max mem: 15850
[22:55:30.409273] Test:  [260/345]  eta: 0:00:14  loss: 0.1294 (0.1266)  time: 0.1761  data: 0.0001  max mem: 15850
[22:55:32.177793] Test:  [270/345]  eta: 0:00:13  loss: 0.1294 (0.1266)  time: 0.1765  data: 0.0001  max mem: 15850
[22:55:33.949831] Test:  [280/345]  eta: 0:00:11  loss: 0.1249 (0.1268)  time: 0.1770  data: 0.0001  max mem: 15850
[22:55:35.724731] Test:  [290/345]  eta: 0:00:09  loss: 0.1247 (0.1268)  time: 0.1773  data: 0.0001  max mem: 15850
[22:55:37.503782] Test:  [300/345]  eta: 0:00:07  loss: 0.1192 (0.1267)  time: 0.1776  data: 0.0001  max mem: 15850
[22:55:39.286879] Test:  [310/345]  eta: 0:00:06  loss: 0.1241 (0.1269)  time: 0.1780  data: 0.0001  max mem: 15850
[22:55:41.073403] Test:  [320/345]  eta: 0:00:04  loss: 0.1293 (0.1271)  time: 0.1784  data: 0.0001  max mem: 15850
[22:55:42.863658] Test:  [330/345]  eta: 0:00:02  loss: 0.1284 (0.1275)  time: 0.1788  data: 0.0001  max mem: 15850
[22:55:44.653463] Test:  [340/345]  eta: 0:00:00  loss: 0.1327 (0.1275)  time: 0.1789  data: 0.0001  max mem: 15850
[22:55:45.372092] Test:  [344/345]  eta: 0:00:00  loss: 0.1327 (0.1276)  time: 0.1791  data: 0.0001  max mem: 15850
[22:55:45.453056] Test: Total time: 0:01:00 (0.1748 s / it)
[22:55:55.266924] Test:  [ 0/57]  eta: 0:00:24  loss: 0.4891 (0.4891)  time: 0.4347  data: 0.2708  max mem: 15850
[22:55:56.928062] Test:  [10/57]  eta: 0:00:08  loss: 0.4136 (0.4441)  time: 0.1905  data: 0.0247  max mem: 15850
[22:55:58.593229] Test:  [20/57]  eta: 0:00:06  loss: 0.4393 (0.4472)  time: 0.1662  data: 0.0001  max mem: 15850
[22:56:00.262744] Test:  [30/57]  eta: 0:00:04  loss: 0.2735 (0.3746)  time: 0.1667  data: 0.0001  max mem: 15850
[22:56:01.937474] Test:  [40/57]  eta: 0:00:02  loss: 0.2276 (0.3410)  time: 0.1671  data: 0.0001  max mem: 15850
[22:56:03.614200] Test:  [50/57]  eta: 0:00:01  loss: 0.2449 (0.3327)  time: 0.1675  data: 0.0001  max mem: 15850
[22:56:04.521163] Test:  [56/57]  eta: 0:00:00  loss: 0.2749 (0.3361)  time: 0.1626  data: 0.0000  max mem: 15850
[22:56:04.584370] Test: Total time: 0:00:09 (0.1711 s / it)
[22:56:06.244969] Dice score of the network on the train images: 0.875294, val images: 0.832986
[22:56:06.249028] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[22:56:07.128817] Epoch: [35]  [  0/345]  eta: 0:05:03  lr: 0.000063  loss: 0.1377 (0.1377)  time: 0.8787  data: 0.2782  max mem: 15850
[22:56:19.086035] Epoch: [35]  [ 20/345]  eta: 0:03:18  lr: 0.000062  loss: 0.1293 (0.1322)  time: 0.5978  data: 0.0001  max mem: 15850
[22:56:31.064793] Epoch: [35]  [ 40/345]  eta: 0:03:04  lr: 0.000062  loss: 0.1252 (0.1327)  time: 0.5989  data: 0.0001  max mem: 15850
[22:56:43.072748] Epoch: [35]  [ 60/345]  eta: 0:02:52  lr: 0.000061  loss: 0.1266 (0.1334)  time: 0.6004  data: 0.0001  max mem: 15850
[22:56:55.092974] Epoch: [35]  [ 80/345]  eta: 0:02:39  lr: 0.000061  loss: 0.1244 (0.1320)  time: 0.6010  data: 0.0001  max mem: 15850
[22:57:07.125295] Epoch: [35]  [100/345]  eta: 0:02:27  lr: 0.000061  loss: 0.1381 (0.1335)  time: 0.6016  data: 0.0001  max mem: 15850
[22:57:19.167103] Epoch: [35]  [120/345]  eta: 0:02:15  lr: 0.000060  loss: 0.1384 (0.1345)  time: 0.6020  data: 0.0001  max mem: 15850
[22:57:31.214082] Epoch: [35]  [140/345]  eta: 0:02:03  lr: 0.000060  loss: 0.1270 (0.1334)  time: 0.6023  data: 0.0001  max mem: 15850
[22:57:43.275091] Epoch: [35]  [160/345]  eta: 0:01:51  lr: 0.000059  loss: 0.1312 (0.1336)  time: 0.6030  data: 0.0001  max mem: 15850
[22:57:55.338539] Epoch: [35]  [180/345]  eta: 0:01:39  lr: 0.000059  loss: 0.1243 (0.1332)  time: 0.6031  data: 0.0001  max mem: 15850
[22:58:07.402201] Epoch: [35]  [200/345]  eta: 0:01:27  lr: 0.000059  loss: 0.1239 (0.1330)  time: 0.6031  data: 0.0001  max mem: 15850
[22:58:19.466816] Epoch: [35]  [220/345]  eta: 0:01:15  lr: 0.000058  loss: 0.1314 (0.1332)  time: 0.6032  data: 0.0001  max mem: 15850
[22:58:31.527179] Epoch: [35]  [240/345]  eta: 0:01:03  lr: 0.000058  loss: 0.1241 (0.1330)  time: 0.6030  data: 0.0001  max mem: 15850
[22:58:43.581626] Epoch: [35]  [260/345]  eta: 0:00:51  lr: 0.000058  loss: 0.1306 (0.1333)  time: 0.6027  data: 0.0001  max mem: 15850

[22:58:55.633746] Epoch: [35]  [280/345]  eta: 0:00:39  lr: 0.000057  loss: 0.1281 (0.1333)  time: 0.6026  data: 0.0001  max mem: 15850
[22:59:07.686681] Epoch: [35]  [300/345]  eta: 0:00:27  lr: 0.000057  loss: 0.1239 (0.1327)  time: 0.6026  data: 0.0001  max mem: 15850
[22:59:19.735866] Epoch: [35]  [320/345]  eta: 0:00:15  lr: 0.000056  loss: 0.1241 (0.1323)  time: 0.6024  data: 0.0001  max mem: 15850
[22:59:31.776943] Epoch: [35]  [340/345]  eta: 0:00:03  lr: 0.000056  loss: 0.1229 (0.1321)  time: 0.6020  data: 0.0001  max mem: 15850
[22:59:34.184276] Epoch: [35]  [344/345]  eta: 0:00:00  lr: 0.000056  loss: 0.1229 (0.1321)  time: 0.6019  data: 0.0001  max mem: 15850
[22:59:34.255929] Epoch: [35] Total time: 0:03:28 (0.6029 s / it)
[22:59:34.256181] Averaged stats: lr: 0.000056  loss: 0.1229 (0.1321)
[22:59:34.724916] Test:  [  0/345]  eta: 0:02:39  loss: 0.1305 (0.1305)  time: 0.4633  data: 0.2981  max mem: 15850
[22:59:36.403223] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1275 (0.1278)  time: 0.1946  data: 0.0272  max mem: 15850
[22:59:38.084507] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1188 (0.1257)  time: 0.1679  data: 0.0001  max mem: 15850
[22:59:39.768504] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1195 (0.1265)  time: 0.1682  data: 0.0001  max mem: 15850
[22:59:41.456855] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1231 (0.1255)  time: 0.1686  data: 0.0001  max mem: 15850
[22:59:43.147562] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1251 (0.1253)  time: 0.1689  data: 0.0001  max mem: 15850
[22:59:44.844584] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1187 (0.1250)  time: 0.1693  data: 0.0001  max mem: 15850
[22:59:46.542657] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1174 (0.1239)  time: 0.1697  data: 0.0001  max mem: 15850
[22:59:48.244603] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1166 (0.1228)  time: 0.1699  data: 0.0001  max mem: 15850
[22:59:49.949924] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1200 (0.1239)  time: 0.1703  data: 0.0001  max mem: 15850
[22:59:51.659055] Test:  [100/345]  eta: 0:00:42  loss: 0.1200 (0.1234)  time: 0.1707  data: 0.0001  max mem: 15850
[22:59:53.370832] Test:  [110/345]  eta: 0:00:40  loss: 0.1124 (0.1233)  time: 0.1710  data: 0.0001  max mem: 15850
[22:59:55.086140] Test:  [120/345]  eta: 0:00:38  loss: 0.1169 (0.1231)  time: 0.1713  data: 0.0001  max mem: 15850
[22:59:56.805170] Test:  [130/345]  eta: 0:00:36  loss: 0.1208 (0.1229)  time: 0.1717  data: 0.0001  max mem: 15850
[22:59:58.528158] Test:  [140/345]  eta: 0:00:35  loss: 0.1177 (0.1223)  time: 0.1720  data: 0.0001  max mem: 15850
[23:00:00.254654] Test:  [150/345]  eta: 0:00:33  loss: 0.1177 (0.1228)  time: 0.1724  data: 0.0001  max mem: 15850
[23:00:01.984361] Test:  [160/345]  eta: 0:00:31  loss: 0.1240 (0.1231)  time: 0.1727  data: 0.0001  max mem: 15850
[23:00:03.718262] Test:  [170/345]  eta: 0:00:30  loss: 0.1259 (0.1234)  time: 0.1731  data: 0.0001  max mem: 15850
[23:00:05.454491] Test:  [180/345]  eta: 0:00:28  loss: 0.1188 (0.1229)  time: 0.1734  data: 0.0001  max mem: 15850
[23:00:07.194926] Test:  [190/345]  eta: 0:00:26  loss: 0.1188 (0.1231)  time: 0.1738  data: 0.0001  max mem: 15850
[23:00:08.937030] Test:  [200/345]  eta: 0:00:25  loss: 0.1179 (0.1230)  time: 0.1741  data: 0.0001  max mem: 15850
[23:00:10.685227] Test:  [210/345]  eta: 0:00:23  loss: 0.1172 (0.1229)  time: 0.1744  data: 0.0001  max mem: 15850
[23:00:12.435808] Test:  [220/345]  eta: 0:00:21  loss: 0.1184 (0.1226)  time: 0.1749  data: 0.0001  max mem: 15850
[23:00:14.190643] Test:  [230/345]  eta: 0:00:19  loss: 0.1172 (0.1226)  time: 0.1752  data: 0.0001  max mem: 15850
[23:00:15.947524] Test:  [240/345]  eta: 0:00:18  loss: 0.1149 (0.1225)  time: 0.1755  data: 0.0001  max mem: 15850
[23:00:17.707701] Test:  [250/345]  eta: 0:00:16  loss: 0.1229 (0.1222)  time: 0.1758  data: 0.0001  max mem: 15850
[23:00:19.471558] Test:  [260/345]  eta: 0:00:14  loss: 0.1154 (0.1222)  time: 0.1761  data: 0.0001  max mem: 15850
[23:00:21.237835] Test:  [270/345]  eta: 0:00:12  loss: 0.1169 (0.1221)  time: 0.1764  data: 0.0001  max mem: 15850
[23:00:23.008600] Test:  [280/345]  eta: 0:00:11  loss: 0.1217 (0.1221)  time: 0.1768  data: 0.0001  max mem: 15850
[23:00:24.782093] Test:  [290/345]  eta: 0:00:09  loss: 0.1255 (0.1225)  time: 0.1771  data: 0.0001  max mem: 15850
[23:00:26.560165] Test:  [300/345]  eta: 0:00:07  loss: 0.1191 (0.1222)  time: 0.1775  data: 0.0001  max mem: 15850
[23:00:28.342834] Test:  [310/345]  eta: 0:00:06  loss: 0.1183 (0.1224)  time: 0.1780  data: 0.0001  max mem: 15850
[23:00:30.128741] Test:  [320/345]  eta: 0:00:04  loss: 0.1234 (0.1225)  time: 0.1784  data: 0.0001  max mem: 15850
[23:00:31.916565] Test:  [330/345]  eta: 0:00:02  loss: 0.1200 (0.1225)  time: 0.1786  data: 0.0001  max mem: 15850
[23:00:33.708775] Test:  [340/345]  eta: 0:00:00  loss: 0.1164 (0.1225)  time: 0.1789  data: 0.0001  max mem: 15850
[23:00:34.426747] Test:  [344/345]  eta: 0:00:00  loss: 0.1178 (0.1226)  time: 0.1791  data: 0.0001  max mem: 15850
[23:00:34.495716] Test: Total time: 0:01:00 (0.1746 s / it)
[23:00:44.337948] Test:  [ 0/57]  eta: 0:00:24  loss: 0.5041 (0.5041)  time: 0.4322  data: 0.2683  max mem: 15850
[23:00:45.997393] Test:  [10/57]  eta: 0:00:08  loss: 0.4227 (0.4602)  time: 0.1901  data: 0.0245  max mem: 15850
[23:00:47.662443] Test:  [20/57]  eta: 0:00:06  loss: 0.4531 (0.4593)  time: 0.1662  data: 0.0001  max mem: 15850
[23:00:49.331501] Test:  [30/57]  eta: 0:00:04  loss: 0.2681 (0.3826)  time: 0.1666  data: 0.0001  max mem: 15850
[23:00:51.003994] Test:  [40/57]  eta: 0:00:02  loss: 0.2252 (0.3464)  time: 0.1670  data: 0.0001  max mem: 15850
[23:00:52.681693] Test:  [50/57]  eta: 0:00:01  loss: 0.2289 (0.3363)  time: 0.1675  data: 0.0001  max mem: 15850
[23:00:53.586710] Test:  [56/57]  eta: 0:00:00  loss: 0.2722 (0.3412)  time: 0.1625  data: 0.0000  max mem: 15850
[23:00:53.652197] Test: Total time: 0:00:09 (0.1710 s / it)
[23:00:55.294868] Dice score of the network on the train images: 0.878454, val images: 0.830027
[23:00:55.298928] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[23:00:56.176318] Epoch: [36]  [  0/345]  eta: 0:05:02  lr: 0.000056  loss: 0.1234 (0.1234)  time: 0.8765  data: 0.2776  max mem: 15850
[23:01:08.109507] Epoch: [36]  [ 20/345]  eta: 0:03:18  lr: 0.000056  loss: 0.1181 (0.1226)  time: 0.5966  data: 0.0001  max mem: 15850
[23:01:20.074276] Epoch: [36]  [ 40/345]  eta: 0:03:04  lr: 0.000055  loss: 0.1382 (0.1298)  time: 0.5982  data: 0.0001  max mem: 15850
[23:01:32.062166] Epoch: [36]  [ 60/345]  eta: 0:02:51  lr: 0.000055  loss: 0.1250 (0.1307)  time: 0.5994  data: 0.0001  max mem: 15850
[23:01:44.054728] Epoch: [36]  [ 80/345]  eta: 0:02:39  lr: 0.000054  loss: 0.1205 (0.1299)  time: 0.5996  data: 0.0001  max mem: 15850
[23:01:56.058914] Epoch: [36]  [100/345]  eta: 0:02:27  lr: 0.000054  loss: 0.1250 (0.1292)  time: 0.6002  data: 0.0001  max mem: 15850
[23:02:08.083486] Epoch: [36]  [120/345]  eta: 0:02:15  lr: 0.000054  loss: 0.1180 (0.1289)  time: 0.6012  data: 0.0001  max mem: 15850
[23:02:20.118792] Epoch: [36]  [140/345]  eta: 0:02:03  lr: 0.000053  loss: 0.1277 (0.1285)  time: 0.6017  data: 0.0001  max mem: 15850
[23:02:32.165623] Epoch: [36]  [160/345]  eta: 0:01:51  lr: 0.000053  loss: 0.1204 (0.1280)  time: 0.6023  data: 0.0001  max mem: 15850
[23:02:44.214673] Epoch: [36]  [180/345]  eta: 0:01:39  lr: 0.000053  loss: 0.1212 (0.1278)  time: 0.6024  data: 0.0001  max mem: 15850
[23:02:56.257959] Epoch: [36]  [200/345]  eta: 0:01:27  lr: 0.000052  loss: 0.1244 (0.1274)  time: 0.6021  data: 0.0001  max mem: 15850
[23:03:08.303035] Epoch: [36]  [220/345]  eta: 0:01:15  lr: 0.000052  loss: 0.1224 (0.1276)  time: 0.6022  data: 0.0001  max mem: 15850
[23:03:20.345733] Epoch: [36]  [240/345]  eta: 0:01:03  lr: 0.000051  loss: 0.1246 (0.1276)  time: 0.6021  data: 0.0001  max mem: 15850
[23:03:32.395074] Epoch: [36]  [260/345]  eta: 0:00:51  lr: 0.000051  loss: 0.1153 (0.1271)  time: 0.6024  data: 0.0001  max mem: 15850
[23:03:44.533922] Epoch: [36]  [280/345]  eta: 0:00:39  lr: 0.000051  loss: 0.1188 (0.1268)  time: 0.6069  data: 0.0001  max mem: 15850
[23:03:56.580488] Epoch: [36]  [300/345]  eta: 0:00:27  lr: 0.000050  loss: 0.1162 (0.1266)  time: 0.6023  data: 0.0001  max mem: 15850
[23:04:08.625968] Epoch: [36]  [320/345]  eta: 0:00:15  lr: 0.000050  loss: 0.1266 (0.1269)  time: 0.6022  data: 0.0001  max mem: 15850
[23:04:20.662100] Epoch: [36]  [340/345]  eta: 0:00:03  lr: 0.000050  loss: 0.1195 (0.1267)  time: 0.6018  data: 0.0001  max mem: 15850
[23:04:23.070567] Epoch: [36]  [344/345]  eta: 0:00:00  lr: 0.000050  loss: 0.1231 (0.1267)  time: 0.6018  data: 0.0001  max mem: 15850
[23:04:23.144745] Epoch: [36] Total time: 0:03:27 (0.6025 s / it)
[23:04:23.145338] Averaged stats: lr: 0.000050  loss: 0.1231 (0.1267)
[23:04:23.612864] Test:  [  0/345]  eta: 0:02:39  loss: 0.1735 (0.1735)  time: 0.4619  data: 0.2960  max mem: 15850
[23:04:25.292277] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1228 (0.1282)  time: 0.1946  data: 0.0270  max mem: 15850
[23:04:26.973844] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1213 (0.1251)  time: 0.1680  data: 0.0001  max mem: 15850
[23:04:28.658362] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1191 (0.1242)  time: 0.1682  data: 0.0001  max mem: 15850
[23:04:30.346200] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1183 (0.1226)  time: 0.1685  data: 0.0001  max mem: 15850
[23:04:32.038917] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1212 (0.1232)  time: 0.1690  data: 0.0001  max mem: 15850
[23:04:33.733700] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1187 (0.1228)  time: 0.1693  data: 0.0001  max mem: 15850
[23:04:35.432549] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1126 (0.1215)  time: 0.1696  data: 0.0001  max mem: 15850
[23:04:37.134340] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1107 (0.1198)  time: 0.1700  data: 0.0001  max mem: 15850
[23:04:38.839442] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1109 (0.1204)  time: 0.1703  data: 0.0001  max mem: 15850
[23:04:40.546859] Test:  [100/345]  eta: 0:00:42  loss: 0.1162 (0.1200)  time: 0.1706  data: 0.0001  max mem: 15850
[23:04:42.258194] Test:  [110/345]  eta: 0:00:40  loss: 0.1144 (0.1195)  time: 0.1709  data: 0.0001  max mem: 15850
[23:04:43.973973] Test:  [120/345]  eta: 0:00:38  loss: 0.1193 (0.1199)  time: 0.1713  data: 0.0001  max mem: 15850
[23:04:45.693360] Test:  [130/345]  eta: 0:00:36  loss: 0.1234 (0.1202)  time: 0.1717  data: 0.0001  max mem: 15850
[23:04:47.417248] Test:  [140/345]  eta: 0:00:35  loss: 0.1229 (0.1207)  time: 0.1721  data: 0.0001  max mem: 15850
[23:04:49.143660] Test:  [150/345]  eta: 0:00:33  loss: 0.1169 (0.1209)  time: 0.1725  data: 0.0001  max mem: 15850
[23:04:50.875172] Test:  [160/345]  eta: 0:00:31  loss: 0.1154 (0.1207)  time: 0.1728  data: 0.0001  max mem: 15850
[23:04:52.608455] Test:  [170/345]  eta: 0:00:30  loss: 0.1102 (0.1201)  time: 0.1732  data: 0.0001  max mem: 15850
[23:04:54.345454] Test:  [180/345]  eta: 0:00:28  loss: 0.1112 (0.1198)  time: 0.1735  data: 0.0001  max mem: 15850
[23:04:56.085882] Test:  [190/345]  eta: 0:00:26  loss: 0.1141 (0.1195)  time: 0.1738  data: 0.0001  max mem: 15850
[23:04:57.829234] Test:  [200/345]  eta: 0:00:25  loss: 0.1182 (0.1199)  time: 0.1741  data: 0.0001  max mem: 15850
[23:04:59.575958] Test:  [210/345]  eta: 0:00:23  loss: 0.1150 (0.1197)  time: 0.1744  data: 0.0001  max mem: 15850
[23:05:01.325971] Test:  [220/345]  eta: 0:00:21  loss: 0.1110 (0.1195)  time: 0.1747  data: 0.0001  max mem: 15850
[23:05:03.080203] Test:  [230/345]  eta: 0:00:19  loss: 0.1117 (0.1193)  time: 0.1751  data: 0.0001  max mem: 15850
[23:05:04.836733] Test:  [240/345]  eta: 0:00:18  loss: 0.1172 (0.1195)  time: 0.1755  data: 0.0001  max mem: 15850
[23:05:06.598972] Test:  [250/345]  eta: 0:00:16  loss: 0.1200 (0.1195)  time: 0.1759  data: 0.0001  max mem: 15850
[23:05:08.362166] Test:  [260/345]  eta: 0:00:14  loss: 0.1150 (0.1195)  time: 0.1762  data: 0.0001  max mem: 15850
[23:05:10.129721] Test:  [270/345]  eta: 0:00:12  loss: 0.1183 (0.1198)  time: 0.1765  data: 0.0001  max mem: 15850
[23:05:11.901526] Test:  [280/345]  eta: 0:00:11  loss: 0.1183 (0.1198)  time: 0.1769  data: 0.0001  max mem: 15850
[23:05:13.676583] Test:  [290/345]  eta: 0:00:09  loss: 0.1180 (0.1200)  time: 0.1773  data: 0.0001  max mem: 15850
[23:05:15.454690] Test:  [300/345]  eta: 0:00:07  loss: 0.1112 (0.1199)  time: 0.1776  data: 0.0001  max mem: 15850
[23:05:17.236810] Test:  [310/345]  eta: 0:00:06  loss: 0.1112 (0.1199)  time: 0.1779  data: 0.0001  max mem: 15850
[23:05:19.021156] Test:  [320/345]  eta: 0:00:04  loss: 0.1149 (0.1200)  time: 0.1783  data: 0.0001  max mem: 15850
[23:05:20.810516] Test:  [330/345]  eta: 0:00:02  loss: 0.1106 (0.1198)  time: 0.1786  data: 0.0001  max mem: 15850
[23:05:22.602637] Test:  [340/345]  eta: 0:00:00  loss: 0.1131 (0.1197)  time: 0.1790  data: 0.0001  max mem: 15850
[23:05:23.320801] Test:  [344/345]  eta: 0:00:00  loss: 0.1131 (0.1196)  time: 0.1791  data: 0.0001  max mem: 15850
[23:05:23.387151] Test: Total time: 0:01:00 (0.1746 s / it)
[23:05:33.228583] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4886 (0.4886)  time: 0.4504  data: 0.2867  max mem: 15850
[23:05:34.888555] Test:  [10/57]  eta: 0:00:09  loss: 0.4083 (0.4489)  time: 0.1918  data: 0.0261  max mem: 15850
[23:05:36.555375] Test:  [20/57]  eta: 0:00:06  loss: 0.4516 (0.4520)  time: 0.1663  data: 0.0001  max mem: 15850
[23:05:38.223783] Test:  [30/57]  eta: 0:00:04  loss: 0.2710 (0.3764)  time: 0.1667  data: 0.0001  max mem: 15850
[23:05:39.897512] Test:  [40/57]  eta: 0:00:02  loss: 0.2193 (0.3408)  time: 0.1670  data: 0.0001  max mem: 15850
[23:05:41.574030] Test:  [50/57]  eta: 0:00:01  loss: 0.2365 (0.3299)  time: 0.1675  data: 0.0001  max mem: 15850
[23:05:42.479054] Test:  [56/57]  eta: 0:00:00  loss: 0.2559 (0.3312)  time: 0.1625  data: 0.0000  max mem: 15850
[23:05:42.552190] Test: Total time: 0:00:09 (0.1715 s / it)
[23:05:44.209596] Dice score of the network on the train images: 0.875080, val images: 0.836029
[23:05:44.209823] saving best_dice_model_0 @ epoch 36
[23:05:45.472588] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[23:05:46.375738] Epoch: [37]  [  0/345]  eta: 0:05:11  lr: 0.000050  loss: 0.1140 (0.1140)  time: 0.9023  data: 0.3037  max mem: 15850
[23:05:58.305199] Epoch: [37]  [ 20/345]  eta: 0:03:18  lr: 0.000049  loss: 0.1135 (0.1199)  time: 0.5964  data: 0.0001  max mem: 15850
[23:06:10.260248] Epoch: [37]  [ 40/345]  eta: 0:03:04  lr: 0.000049  loss: 0.1201 (0.1220)  time: 0.5977  data: 0.0001  max mem: 15850
[23:06:22.255835] Epoch: [37]  [ 60/345]  eta: 0:02:51  lr: 0.000048  loss: 0.1216 (0.1228)  time: 0.5997  data: 0.0001  max mem: 15850
[23:06:34.254112] Epoch: [37]  [ 80/345]  eta: 0:02:39  lr: 0.000048  loss: 0.1131 (0.1227)  time: 0.5999  data: 0.0001  max mem: 15850

[23:06:46.262625] Epoch: [37]  [100/345]  eta: 0:02:27  lr: 0.000048  loss: 0.1421 (0.1268)  time: 0.6004  data: 0.0001  max mem: 15850
[23:06:58.290353] Epoch: [37]  [120/345]  eta: 0:02:15  lr: 0.000047  loss: 0.1259 (0.1269)  time: 0.6013  data: 0.0001  max mem: 15850
[23:07:10.324323] Epoch: [37]  [140/345]  eta: 0:02:03  lr: 0.000047  loss: 0.1327 (0.1276)  time: 0.6017  data: 0.0001  max mem: 15850
[23:07:22.375865] Epoch: [37]  [160/345]  eta: 0:01:51  lr: 0.000047  loss: 0.1238 (0.1277)  time: 0.6025  data: 0.0001  max mem: 15850
[23:07:34.425215] Epoch: [37]  [180/345]  eta: 0:01:39  lr: 0.000046  loss: 0.1236 (0.1277)  time: 0.6024  data: 0.0001  max mem: 15850
[23:07:46.480999] Epoch: [37]  [200/345]  eta: 0:01:27  lr: 0.000046  loss: 0.1266 (0.1280)  time: 0.6027  data: 0.0001  max mem: 15850
[23:07:58.532182] Epoch: [37]  [220/345]  eta: 0:01:15  lr: 0.000045  loss: 0.1211 (0.1276)  time: 0.6025  data: 0.0001  max mem: 15850
[23:08:10.577369] Epoch: [37]  [240/345]  eta: 0:01:03  lr: 0.000045  loss: 0.1169 (0.1271)  time: 0.6022  data: 0.0001  max mem: 15850
[23:08:22.618901] Epoch: [37]  [260/345]  eta: 0:00:51  lr: 0.000045  loss: 0.1187 (0.1270)  time: 0.6020  data: 0.0001  max mem: 15850
[23:08:34.656097] Epoch: [37]  [280/345]  eta: 0:00:39  lr: 0.000044  loss: 0.1268 (0.1269)  time: 0.6018  data: 0.0001  max mem: 15850
[23:08:46.684016] Epoch: [37]  [300/345]  eta: 0:00:27  lr: 0.000044  loss: 0.1222 (0.1266)  time: 0.6014  data: 0.0001  max mem: 15850
[23:08:58.713728] Epoch: [37]  [320/345]  eta: 0:00:15  lr: 0.000044  loss: 0.1251 (0.1262)  time: 0.6014  data: 0.0001  max mem: 15850
[23:09:10.735051] Epoch: [37]  [340/345]  eta: 0:00:03  lr: 0.000043  loss: 0.1234 (0.1262)  time: 0.6010  data: 0.0001  max mem: 15850
[23:09:13.138688] Epoch: [37]  [344/345]  eta: 0:00:00  lr: 0.000043  loss: 0.1202 (0.1260)  time: 0.6010  data: 0.0001  max mem: 15850
[23:09:13.218743] Epoch: [37] Total time: 0:03:27 (0.6022 s / it)
[23:09:13.219094] Averaged stats: lr: 0.000043  loss: 0.1202 (0.1260)
[23:09:13.740879] Test:  [  0/345]  eta: 0:02:58  loss: 0.0913 (0.0913)  time: 0.5167  data: 0.3515  max mem: 15850
[23:09:15.419485] Test:  [ 10/345]  eta: 0:01:06  loss: 0.0998 (0.1066)  time: 0.1995  data: 0.0320  max mem: 15850
[23:09:17.101972] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1122 (0.1127)  time: 0.1680  data: 0.0001  max mem: 15850
[23:09:18.787759] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1140 (0.1131)  time: 0.1683  data: 0.0001  max mem: 15850
[23:09:20.475612] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1132 (0.1158)  time: 0.1686  data: 0.0001  max mem: 15850
[23:09:22.168167] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1132 (0.1159)  time: 0.1690  data: 0.0001  max mem: 15850
[23:09:23.863817] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1105 (0.1142)  time: 0.1693  data: 0.0001  max mem: 15850
[23:09:25.561713] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1129 (0.1161)  time: 0.1696  data: 0.0001  max mem: 15850
[23:09:27.263441] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1279 (0.1164)  time: 0.1699  data: 0.0001  max mem: 15850
[23:09:28.968435] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1213 (0.1172)  time: 0.1703  data: 0.0001  max mem: 15850
[23:09:30.676360] Test:  [100/345]  eta: 0:00:42  loss: 0.1107 (0.1164)  time: 0.1706  data: 0.0001  max mem: 15850
[23:09:32.388740] Test:  [110/345]  eta: 0:00:40  loss: 0.1093 (0.1164)  time: 0.1710  data: 0.0001  max mem: 15850
[23:09:34.104700] Test:  [120/345]  eta: 0:00:38  loss: 0.1135 (0.1161)  time: 0.1713  data: 0.0001  max mem: 15850
[23:09:35.825259] Test:  [130/345]  eta: 0:00:37  loss: 0.1140 (0.1159)  time: 0.1717  data: 0.0001  max mem: 15850
[23:09:37.547365] Test:  [140/345]  eta: 0:00:35  loss: 0.1136 (0.1159)  time: 0.1721  data: 0.0001  max mem: 15850
[23:09:39.273333] Test:  [150/345]  eta: 0:00:33  loss: 0.1075 (0.1158)  time: 0.1723  data: 0.0001  max mem: 15850
[23:09:41.003172] Test:  [160/345]  eta: 0:00:31  loss: 0.1120 (0.1161)  time: 0.1727  data: 0.0001  max mem: 15850
[23:09:42.736543] Test:  [170/345]  eta: 0:00:30  loss: 0.1140 (0.1163)  time: 0.1731  data: 0.0001  max mem: 15850
[23:09:44.473916] Test:  [180/345]  eta: 0:00:28  loss: 0.1190 (0.1162)  time: 0.1735  data: 0.0001  max mem: 15850
[23:09:46.213956] Test:  [190/345]  eta: 0:00:26  loss: 0.1140 (0.1161)  time: 0.1738  data: 0.0001  max mem: 15850
[23:09:47.957756] Test:  [200/345]  eta: 0:00:25  loss: 0.1140 (0.1161)  time: 0.1741  data: 0.0001  max mem: 15850
[23:09:49.705917] Test:  [210/345]  eta: 0:00:23  loss: 0.1080 (0.1156)  time: 0.1745  data: 0.0001  max mem: 15850
[23:09:51.456551] Test:  [220/345]  eta: 0:00:21  loss: 0.1126 (0.1159)  time: 0.1749  data: 0.0001  max mem: 15850
[23:09:53.210815] Test:  [230/345]  eta: 0:00:19  loss: 0.1126 (0.1159)  time: 0.1752  data: 0.0001  max mem: 15850
[23:09:54.968525] Test:  [240/345]  eta: 0:00:18  loss: 0.1108 (0.1160)  time: 0.1755  data: 0.0001  max mem: 15850
[23:09:56.729269] Test:  [250/345]  eta: 0:00:16  loss: 0.1113 (0.1160)  time: 0.1758  data: 0.0001  max mem: 15850
[23:09:58.493774] Test:  [260/345]  eta: 0:00:14  loss: 0.1113 (0.1157)  time: 0.1762  data: 0.0001  max mem: 15850
[23:10:00.261372] Test:  [270/345]  eta: 0:00:13  loss: 0.1136 (0.1157)  time: 0.1765  data: 0.0001  max mem: 15850
[23:10:02.032692] Test:  [280/345]  eta: 0:00:11  loss: 0.1196 (0.1160)  time: 0.1769  data: 0.0001  max mem: 15850
[23:10:03.807361] Test:  [290/345]  eta: 0:00:09  loss: 0.1090 (0.1158)  time: 0.1772  data: 0.0001  max mem: 15850
[23:10:05.586247] Test:  [300/345]  eta: 0:00:07  loss: 0.1090 (0.1157)  time: 0.1776  data: 0.0001  max mem: 15850
[23:10:07.367684] Test:  [310/345]  eta: 0:00:06  loss: 0.1090 (0.1158)  time: 0.1780  data: 0.0001  max mem: 15850
[23:10:09.152972] Test:  [320/345]  eta: 0:00:04  loss: 0.1102 (0.1158)  time: 0.1783  data: 0.0001  max mem: 15850
[23:10:10.943098] Test:  [330/345]  eta: 0:00:02  loss: 0.1108 (0.1162)  time: 0.1787  data: 0.0001  max mem: 15850
[23:10:12.733344] Test:  [340/345]  eta: 0:00:00  loss: 0.1227 (0.1164)  time: 0.1790  data: 0.0001  max mem: 15850
[23:10:13.451326] Test:  [344/345]  eta: 0:00:00  loss: 0.1209 (0.1164)  time: 0.1791  data: 0.0001  max mem: 15850
[23:10:13.510969] Test: Total time: 0:01:00 (0.1747 s / it)
[23:10:23.477302] Test:  [ 0/57]  eta: 0:00:28  loss: 0.4968 (0.4968)  time: 0.4970  data: 0.3335  max mem: 15850
[23:10:25.136889] Test:  [10/57]  eta: 0:00:09  loss: 0.4344 (0.4556)  time: 0.1960  data: 0.0304  max mem: 15850
[23:10:26.804547] Test:  [20/57]  eta: 0:00:06  loss: 0.4726 (0.4602)  time: 0.1663  data: 0.0001  max mem: 15850
[23:10:28.474761] Test:  [30/57]  eta: 0:00:04  loss: 0.2656 (0.3845)  time: 0.1668  data: 0.0001  max mem: 15850
[23:10:30.148402] Test:  [40/57]  eta: 0:00:02  loss: 0.2192 (0.3488)  time: 0.1671  data: 0.0001  max mem: 15850
[23:10:31.826546] Test:  [50/57]  eta: 0:00:01  loss: 0.2337 (0.3381)  time: 0.1675  data: 0.0001  max mem: 15850
[23:10:32.731941] Test:  [56/57]  eta: 0:00:00  loss: 0.2688 (0.3399)  time: 0.1626  data: 0.0000  max mem: 15850
[23:10:32.804806] Test: Total time: 0:00:09 (0.1724 s / it)
[23:10:34.448789] Dice score of the network on the train images: 0.881742, val images: 0.832455
[23:10:34.453316] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[23:10:35.334476] Epoch: [38]  [  0/345]  eta: 0:05:03  lr: 0.000043  loss: 0.1058 (0.1058)  time: 0.8802  data: 0.2790  max mem: 15850
[23:10:47.289520] Epoch: [38]  [ 20/345]  eta: 0:03:18  lr: 0.000043  loss: 0.1204 (0.1229)  time: 0.5977  data: 0.0001  max mem: 15850
[23:10:59.274185] Epoch: [38]  [ 40/345]  eta: 0:03:04  lr: 0.000042  loss: 0.1139 (0.1218)  time: 0.5992  data: 0.0001  max mem: 15850
[23:11:11.268557] Epoch: [38]  [ 60/345]  eta: 0:02:51  lr: 0.000042  loss: 0.1169 (0.1212)  time: 0.5997  data: 0.0001  max mem: 15850
[23:11:23.268363] Epoch: [38]  [ 80/345]  eta: 0:02:39  lr: 0.000042  loss: 0.1192 (0.1220)  time: 0.6000  data: 0.0001  max mem: 15850
[23:11:35.267974] Epoch: [38]  [100/345]  eta: 0:02:27  lr: 0.000041  loss: 0.1076 (0.1207)  time: 0.5999  data: 0.0001  max mem: 15850
[23:11:47.289769] Epoch: [38]  [120/345]  eta: 0:02:15  lr: 0.000041  loss: 0.1241 (0.1211)  time: 0.6010  data: 0.0001  max mem: 15850
[23:11:59.332043] Epoch: [38]  [140/345]  eta: 0:02:03  lr: 0.000041  loss: 0.1139 (0.1210)  time: 0.6021  data: 0.0001  max mem: 15850
[23:12:11.392031] Epoch: [38]  [160/345]  eta: 0:01:51  lr: 0.000040  loss: 0.1219 (0.1213)  time: 0.6030  data: 0.0001  max mem: 15850
[23:12:23.456284] Epoch: [38]  [180/345]  eta: 0:01:39  lr: 0.000040  loss: 0.1247 (0.1215)  time: 0.6032  data: 0.0001  max mem: 15850
[23:12:35.516941] Epoch: [38]  [200/345]  eta: 0:01:27  lr: 0.000040  loss: 0.1269 (0.1218)  time: 0.6030  data: 0.0001  max mem: 15850
[23:12:47.579581] Epoch: [38]  [220/345]  eta: 0:01:15  lr: 0.000039  loss: 0.1247 (0.1223)  time: 0.6031  data: 0.0001  max mem: 15850
[23:12:59.638475] Epoch: [38]  [240/345]  eta: 0:01:03  lr: 0.000039  loss: 0.1172 (0.1222)  time: 0.6029  data: 0.0001  max mem: 15850
[23:13:11.698335] Epoch: [38]  [260/345]  eta: 0:00:51  lr: 0.000039  loss: 0.1204 (0.1222)  time: 0.6029  data: 0.0001  max mem: 15850
[23:13:23.748678] Epoch: [38]  [280/345]  eta: 0:00:39  lr: 0.000038  loss: 0.1124 (0.1217)  time: 0.6025  data: 0.0001  max mem: 15850
[23:13:35.793088] Epoch: [38]  [300/345]  eta: 0:00:27  lr: 0.000038  loss: 0.1194 (0.1217)  time: 0.6022  data: 0.0001  max mem: 15850
[23:13:47.836699] Epoch: [38]  [320/345]  eta: 0:00:15  lr: 0.000038  loss: 0.1150 (0.1212)  time: 0.6021  data: 0.0001  max mem: 15850
[23:13:59.877740] Epoch: [38]  [340/345]  eta: 0:00:03  lr: 0.000037  loss: 0.1201 (0.1212)  time: 0.6020  data: 0.0001  max mem: 15850
[23:14:02.283344] Epoch: [38]  [344/345]  eta: 0:00:00  lr: 0.000037  loss: 0.1118 (0.1210)  time: 0.6019  data: 0.0001  max mem: 15850
[23:14:02.364063] Epoch: [38] Total time: 0:03:27 (0.6026 s / it)
[23:14:02.364298] Averaged stats: lr: 0.000037  loss: 0.1118 (0.1210)
[23:14:02.835028] Test:  [  0/345]  eta: 0:02:40  loss: 0.1190 (0.1190)  time: 0.4647  data: 0.2995  max mem: 15850
[23:14:04.512853] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1227 (0.1189)  time: 0.1947  data: 0.0273  max mem: 15850
[23:14:06.194002] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1188 (0.1184)  time: 0.1679  data: 0.0001  max mem: 15850
[23:14:07.879006] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1162 (0.1190)  time: 0.1682  data: 0.0001  max mem: 15850
[23:14:09.567213] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1165 (0.1185)  time: 0.1686  data: 0.0001  max mem: 15850
[23:14:11.258185] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1154 (0.1173)  time: 0.1689  data: 0.0001  max mem: 15850
[23:14:12.954186] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1154 (0.1178)  time: 0.1693  data: 0.0001  max mem: 15850
[23:14:14.651590] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1115 (0.1174)  time: 0.1696  data: 0.0001  max mem: 15850
[23:14:16.352051] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1172 (0.1186)  time: 0.1698  data: 0.0001  max mem: 15850
[23:14:18.057837] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1181 (0.1187)  time: 0.1702  data: 0.0001  max mem: 15850
[23:14:19.767037] Test:  [100/345]  eta: 0:00:42  loss: 0.1188 (0.1189)  time: 0.1707  data: 0.0001  max mem: 15850
[23:14:21.478940] Test:  [110/345]  eta: 0:00:40  loss: 0.1144 (0.1182)  time: 0.1710  data: 0.0001  max mem: 15850
[23:14:23.195528] Test:  [120/345]  eta: 0:00:38  loss: 0.1133 (0.1182)  time: 0.1714  data: 0.0001  max mem: 15850
[23:14:24.915304] Test:  [130/345]  eta: 0:00:36  loss: 0.1106 (0.1176)  time: 0.1718  data: 0.0001  max mem: 15850
[23:14:26.637844] Test:  [140/345]  eta: 0:00:35  loss: 0.1137 (0.1179)  time: 0.1721  data: 0.0001  max mem: 15850
[23:14:28.363751] Test:  [150/345]  eta: 0:00:33  loss: 0.1151 (0.1178)  time: 0.1724  data: 0.0001  max mem: 15850
[23:14:30.094441] Test:  [160/345]  eta: 0:00:31  loss: 0.1035 (0.1175)  time: 0.1728  data: 0.0001  max mem: 15850
[23:14:31.828171] Test:  [170/345]  eta: 0:00:30  loss: 0.1086 (0.1174)  time: 0.1732  data: 0.0001  max mem: 15850
[23:14:33.563796] Test:  [180/345]  eta: 0:00:28  loss: 0.1193 (0.1175)  time: 0.1734  data: 0.0001  max mem: 15850
[23:14:35.302501] Test:  [190/345]  eta: 0:00:26  loss: 0.1112 (0.1171)  time: 0.1737  data: 0.0001  max mem: 15850
[23:14:37.046759] Test:  [200/345]  eta: 0:00:25  loss: 0.1104 (0.1169)  time: 0.1741  data: 0.0001  max mem: 15850
[23:14:38.793279] Test:  [210/345]  eta: 0:00:23  loss: 0.1104 (0.1167)  time: 0.1745  data: 0.0001  max mem: 15850
[23:14:40.543529] Test:  [220/345]  eta: 0:00:21  loss: 0.1143 (0.1167)  time: 0.1748  data: 0.0001  max mem: 15850
[23:14:42.296604] Test:  [230/345]  eta: 0:00:19  loss: 0.1199 (0.1171)  time: 0.1751  data: 0.0001  max mem: 15850
[23:14:44.054485] Test:  [240/345]  eta: 0:00:18  loss: 0.1139 (0.1169)  time: 0.1755  data: 0.0001  max mem: 15850
[23:14:45.815132] Test:  [250/345]  eta: 0:00:16  loss: 0.1132 (0.1170)  time: 0.1759  data: 0.0001  max mem: 15850
[23:14:47.580238] Test:  [260/345]  eta: 0:00:14  loss: 0.1033 (0.1167)  time: 0.1762  data: 0.0001  max mem: 15850
[23:14:49.347725] Test:  [270/345]  eta: 0:00:12  loss: 0.1033 (0.1167)  time: 0.1766  data: 0.0001  max mem: 15850
[23:14:51.118425] Test:  [280/345]  eta: 0:00:11  loss: 0.1089 (0.1166)  time: 0.1768  data: 0.0001  max mem: 15850
[23:14:52.893146] Test:  [290/345]  eta: 0:00:09  loss: 0.1186 (0.1167)  time: 0.1772  data: 0.0001  max mem: 15850
[23:14:54.671943] Test:  [300/345]  eta: 0:00:07  loss: 0.1170 (0.1166)  time: 0.1776  data: 0.0001  max mem: 15850
[23:14:56.453680] Test:  [310/345]  eta: 0:00:06  loss: 0.1129 (0.1165)  time: 0.1780  data: 0.0001  max mem: 15850
[23:14:58.238690] Test:  [320/345]  eta: 0:00:04  loss: 0.1077 (0.1165)  time: 0.1783  data: 0.0001  max mem: 15850
[23:15:00.028246] Test:  [330/345]  eta: 0:00:02  loss: 0.1210 (0.1167)  time: 0.1787  data: 0.0001  max mem: 15850
[23:15:01.819080] Test:  [340/345]  eta: 0:00:00  loss: 0.1218 (0.1170)  time: 0.1790  data: 0.0001  max mem: 15850
[23:15:02.537377] Test:  [344/345]  eta: 0:00:00  loss: 0.1218 (0.1170)  time: 0.1791  data: 0.0001  max mem: 15850
[23:15:02.613683] Test: Total time: 0:01:00 (0.1746 s / it)
[23:15:12.405003] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4792 (0.4792)  time: 0.4489  data: 0.2858  max mem: 15850
[23:15:14.065287] Test:  [10/57]  eta: 0:00:09  loss: 0.4109 (0.4382)  time: 0.1916  data: 0.0261  max mem: 15850
[23:15:15.729912] Test:  [20/57]  eta: 0:00:06  loss: 0.4515 (0.4445)  time: 0.1662  data: 0.0001  max mem: 15850
[23:15:17.398059] Test:  [30/57]  eta: 0:00:04  loss: 0.2523 (0.3711)  time: 0.1666  data: 0.0001  max mem: 15850
[23:15:19.071885] Test:  [40/57]  eta: 0:00:02  loss: 0.2188 (0.3365)  time: 0.1670  data: 0.0001  max mem: 15850
[23:15:20.749066] Test:  [50/57]  eta: 0:00:01  loss: 0.2295 (0.3265)  time: 0.1675  data: 0.0001  max mem: 15850
[23:15:21.653107] Test:  [56/57]  eta: 0:00:00  loss: 0.2698 (0.3299)  time: 0.1625  data: 0.0000  max mem: 15850
[23:15:21.707514] Test: Total time: 0:00:09 (0.1711 s / it)
[23:15:23.356981] Dice score of the network on the train images: 0.879507, val images: 0.831140
[23:15:23.361156] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[23:15:24.299336] Epoch: [39]  [  0/345]  eta: 0:05:23  lr: 0.000037  loss: 0.1216 (0.1216)  time: 0.9370  data: 0.3359  max mem: 15850
[23:15:36.250680] Epoch: [39]  [ 20/345]  eta: 0:03:19  lr: 0.000037  loss: 0.1211 (0.1230)  time: 0.5975  data: 0.0001  max mem: 15850
[23:15:48.219205] Epoch: [39]  [ 40/345]  eta: 0:03:04  lr: 0.000036  loss: 0.1096 (0.1197)  time: 0.5984  data: 0.0001  max mem: 15850
[23:16:00.220469] Epoch: [39]  [ 60/345]  eta: 0:02:52  lr: 0.000036  loss: 0.1162 (0.1195)  time: 0.6000  data: 0.0001  max mem: 15850
[23:16:12.236005] Epoch: [39]  [ 80/345]  eta: 0:02:39  lr: 0.000036  loss: 0.1189 (0.1198)  time: 0.6007  data: 0.0001  max mem: 15850
[23:16:24.260338] Epoch: [39]  [100/345]  eta: 0:02:27  lr: 0.000035  loss: 0.1037 (0.1172)  time: 0.6012  data: 0.0001  max mem: 15850
[23:16:36.299909] Epoch: [39]  [120/345]  eta: 0:02:15  lr: 0.000035  loss: 0.1105 (0.1165)  time: 0.6019  data: 0.0001  max mem: 15850
[23:16:48.347372] Epoch: [39]  [140/345]  eta: 0:02:03  lr: 0.000035  loss: 0.1121 (0.1163)  time: 0.6023  data: 0.0001  max mem: 15850
[23:17:00.399458] Epoch: [39]  [160/345]  eta: 0:01:51  lr: 0.000034  loss: 0.1166 (0.1163)  time: 0.6025  data: 0.0001  max mem: 15850
[23:17:12.454386] Epoch: [39]  [180/345]  eta: 0:01:39  lr: 0.000034  loss: 0.1182 (0.1163)  time: 0.6027  data: 0.0001  max mem: 15850
[23:17:24.491177] Epoch: [39]  [200/345]  eta: 0:01:27  lr: 0.000034  loss: 0.1183 (0.1173)  time: 0.6018  data: 0.0001  max mem: 15850
[23:17:36.529163] Epoch: [39]  [220/345]  eta: 0:01:15  lr: 0.000033  loss: 0.1262 (0.1182)  time: 0.6019  data: 0.0001  max mem: 15850
[23:17:48.566089] Epoch: [39]  [240/345]  eta: 0:01:03  lr: 0.000033  loss: 0.1098 (0.1177)  time: 0.6018  data: 0.0001  max mem: 15850
[23:18:00.599400] Epoch: [39]  [260/345]  eta: 0:00:51  lr: 0.000033  loss: 0.1099 (0.1173)  time: 0.6016  data: 0.0001  max mem: 15850
[23:18:12.622572] Epoch: [39]  [280/345]  eta: 0:00:39  lr: 0.000032  loss: 0.1164 (0.1174)  time: 0.6011  data: 0.0001  max mem: 15850
[23:18:24.651757] Epoch: [39]  [300/345]  eta: 0:00:27  lr: 0.000032  loss: 0.1219 (0.1175)  time: 0.6014  data: 0.0001  max mem: 15850
[23:18:36.672589] Epoch: [39]  [320/345]  eta: 0:00:15  lr: 0.000032  loss: 0.1188 (0.1178)  time: 0.6010  data: 0.0001  max mem: 15850
[23:18:48.686112] Epoch: [39]  [340/345]  eta: 0:00:03  lr: 0.000031  loss: 0.1143 (0.1180)  time: 0.6006  data: 0.0001  max mem: 15850
[23:18:51.089056] Epoch: [39]  [344/345]  eta: 0:00:00  lr: 0.000031  loss: 0.1104 (0.1179)  time: 0.6007  data: 0.0001  max mem: 15850
[23:18:51.163709] Epoch: [39] Total time: 0:03:27 (0.6023 s / it)
[23:18:51.163906] Averaged stats: lr: 0.000031  loss: 0.1104 (0.1179)
[23:18:51.658645] Test:  [  0/345]  eta: 0:02:48  loss: 0.0976 (0.0976)  time: 0.4894  data: 0.3244  max mem: 15850
[23:18:53.337934] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1182 (0.1174)  time: 0.1971  data: 0.0296  max mem: 15850
[23:18:55.020358] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1182 (0.1182)  time: 0.1680  data: 0.0001  max mem: 15850
[23:18:56.705378] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1316 (0.1236)  time: 0.1683  data: 0.0001  max mem: 15850
[23:18:58.393733] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1290 (0.1234)  time: 0.1686  data: 0.0001  max mem: 15850
[23:19:00.085004] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1178 (0.1254)  time: 0.1689  data: 0.0001  max mem: 15850
[23:19:01.780232] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1268 (0.1255)  time: 0.1693  data: 0.0001  max mem: 15850
[23:19:03.478211] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1215 (0.1257)  time: 0.1696  data: 0.0001  max mem: 15850
[23:19:05.179200] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1188 (0.1252)  time: 0.1699  data: 0.0001  max mem: 15850
[23:19:06.884120] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1292 (0.1256)  time: 0.1702  data: 0.0001  max mem: 15850
[23:19:08.593598] Test:  [100/345]  eta: 0:00:42  loss: 0.1284 (0.1253)  time: 0.1707  data: 0.0001  max mem: 15850
[23:19:10.305090] Test:  [110/345]  eta: 0:00:40  loss: 0.1239 (0.1265)  time: 0.1710  data: 0.0001  max mem: 15850
[23:19:12.020572] Test:  [120/345]  eta: 0:00:38  loss: 0.1236 (0.1261)  time: 0.1713  data: 0.0001  max mem: 15850
[23:19:13.740894] Test:  [130/345]  eta: 0:00:37  loss: 0.1203 (0.1262)  time: 0.1717  data: 0.0001  max mem: 15850
[23:19:15.463050] Test:  [140/345]  eta: 0:00:35  loss: 0.1238 (0.1260)  time: 0.1721  data: 0.0001  max mem: 15850
[23:19:17.189604] Test:  [150/345]  eta: 0:00:33  loss: 0.1238 (0.1262)  time: 0.1724  data: 0.0001  max mem: 15850
[23:19:18.921074] Test:  [160/345]  eta: 0:00:31  loss: 0.1178 (0.1258)  time: 0.1728  data: 0.0001  max mem: 15850
[23:19:20.655404] Test:  [170/345]  eta: 0:00:30  loss: 0.1220 (0.1261)  time: 0.1732  data: 0.0001  max mem: 15850
[23:19:22.392801] Test:  [180/345]  eta: 0:00:28  loss: 0.1272 (0.1260)  time: 0.1735  data: 0.0001  max mem: 15850
[23:19:24.133809] Test:  [190/345]  eta: 0:00:26  loss: 0.1272 (0.1264)  time: 0.1739  data: 0.0001  max mem: 15850
[23:19:25.877031] Test:  [200/345]  eta: 0:00:25  loss: 0.1239 (0.1261)  time: 0.1741  data: 0.0001  max mem: 15850
[23:19:27.623549] Test:  [210/345]  eta: 0:00:23  loss: 0.1150 (0.1259)  time: 0.1744  data: 0.0001  max mem: 15850
[23:19:29.374321] Test:  [220/345]  eta: 0:00:21  loss: 0.1237 (0.1259)  time: 0.1748  data: 0.0001  max mem: 15850
[23:19:31.127371] Test:  [230/345]  eta: 0:00:19  loss: 0.1242 (0.1261)  time: 0.1751  data: 0.0001  max mem: 15850
[23:19:32.884554] Test:  [240/345]  eta: 0:00:18  loss: 0.1186 (0.1256)  time: 0.1755  data: 0.0001  max mem: 15850
[23:19:34.644313] Test:  [250/345]  eta: 0:00:16  loss: 0.1204 (0.1259)  time: 0.1758  data: 0.0001  max mem: 15850
[23:19:36.409952] Test:  [260/345]  eta: 0:00:14  loss: 0.1308 (0.1260)  time: 0.1762  data: 0.0001  max mem: 15850
[23:19:38.177193] Test:  [270/345]  eta: 0:00:13  loss: 0.1276 (0.1260)  time: 0.1766  data: 0.0001  max mem: 15850
[23:19:39.947931] Test:  [280/345]  eta: 0:00:11  loss: 0.1252 (0.1260)  time: 0.1768  data: 0.0001  max mem: 15850
[23:19:41.723007] Test:  [290/345]  eta: 0:00:09  loss: 0.1252 (0.1261)  time: 0.1772  data: 0.0001  max mem: 15850
[23:19:43.501548] Test:  [300/345]  eta: 0:00:07  loss: 0.1239 (0.1259)  time: 0.1776  data: 0.0001  max mem: 15850
[23:19:45.283561] Test:  [310/345]  eta: 0:00:06  loss: 0.1151 (0.1256)  time: 0.1780  data: 0.0001  max mem: 15850
[23:19:47.069967] Test:  [320/345]  eta: 0:00:04  loss: 0.1195 (0.1257)  time: 0.1784  data: 0.0001  max mem: 15850
[23:19:48.858821] Test:  [330/345]  eta: 0:00:02  loss: 0.1229 (0.1258)  time: 0.1787  data: 0.0001  max mem: 15850
[23:19:50.650339] Test:  [340/345]  eta: 0:00:00  loss: 0.1141 (0.1257)  time: 0.1790  data: 0.0001  max mem: 15850
[23:19:51.366269] Test:  [344/345]  eta: 0:00:00  loss: 0.1129 (0.1258)  time: 0.1791  data: 0.0001  max mem: 15850
[23:19:51.448289] Test: Total time: 0:01:00 (0.1747 s / it)
[23:20:01.254304] Test:  [ 0/57]  eta: 0:00:26  loss: 0.5162 (0.5162)  time: 0.4615  data: 0.2976  max mem: 15850
[23:20:02.914318] Test:  [10/57]  eta: 0:00:09  loss: 0.4449 (0.4765)  time: 0.1928  data: 0.0271  max mem: 15850
[23:20:04.579725] Test:  [20/57]  eta: 0:00:06  loss: 0.5114 (0.4829)  time: 0.1662  data: 0.0001  max mem: 15850
[23:20:06.248555] Test:  [30/57]  eta: 0:00:04  loss: 0.2873 (0.4046)  time: 0.1666  data: 0.0001  max mem: 15850
[23:20:07.920562] Test:  [40/57]  eta: 0:00:02  loss: 0.2413 (0.3715)  time: 0.1670  data: 0.0001  max mem: 15850
[23:20:09.597972] Test:  [50/57]  eta: 0:00:01  loss: 0.2729 (0.3614)  time: 0.1674  data: 0.0001  max mem: 15850
[23:20:10.502607] Test:  [56/57]  eta: 0:00:00  loss: 0.2955 (0.3641)  time: 0.1625  data: 0.0000  max mem: 15850
[23:20:10.567834] Test: Total time: 0:00:09 (0.1715 s / it)
[23:20:12.215234] Dice score of the network on the train images: 0.895717, val images: 0.822169
[23:20:12.215460] saving best_prec_model_0 @ epoch 39
[23:20:13.594503] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[23:20:14.491685] Epoch: [40]  [  0/345]  eta: 0:05:09  lr: 0.000031  loss: 0.1156 (0.1156)  time: 0.8962  data: 0.2973  max mem: 15850
[23:20:26.416784] Epoch: [40]  [ 20/345]  eta: 0:03:18  lr: 0.000031  loss: 0.1141 (0.1158)  time: 0.5962  data: 0.0001  max mem: 15850
[23:20:38.361933] Epoch: [40]  [ 40/345]  eta: 0:03:04  lr: 0.000031  loss: 0.1116 (0.1167)  time: 0.5972  data: 0.0001  max mem: 15850
[23:20:50.348798] Epoch: [40]  [ 60/345]  eta: 0:02:51  lr: 0.000030  loss: 0.1099 (0.1152)  time: 0.5993  data: 0.0001  max mem: 15850
[23:21:02.340304] Epoch: [40]  [ 80/345]  eta: 0:02:39  lr: 0.000030  loss: 0.1058 (0.1144)  time: 0.5995  data: 0.0001  max mem: 15850
[23:21:14.340370] Epoch: [40]  [100/345]  eta: 0:02:27  lr: 0.000030  loss: 0.1192 (0.1153)  time: 0.6000  data: 0.0001  max mem: 15850
[23:21:26.358987] Epoch: [40]  [120/345]  eta: 0:02:15  lr: 0.000029  loss: 0.1217 (0.1159)  time: 0.6009  data: 0.0001  max mem: 15850
[23:21:38.397866] Epoch: [40]  [140/345]  eta: 0:02:03  lr: 0.000029  loss: 0.1105 (0.1156)  time: 0.6019  data: 0.0001  max mem: 15850
[23:21:50.448967] Epoch: [40]  [160/345]  eta: 0:01:51  lr: 0.000029  loss: 0.1109 (0.1154)  time: 0.6025  data: 0.0001  max mem: 15850
[23:22:02.502144] Epoch: [40]  [180/345]  eta: 0:01:39  lr: 0.000028  loss: 0.1103 (0.1151)  time: 0.6026  data: 0.0001  max mem: 15850
[23:22:14.559294] Epoch: [40]  [200/345]  eta: 0:01:27  lr: 0.000028  loss: 0.1204 (0.1157)  time: 0.6028  data: 0.0001  max mem: 15850
[23:22:26.610475] Epoch: [40]  [220/345]  eta: 0:01:15  lr: 0.000028  loss: 0.1184 (0.1157)  time: 0.6025  data: 0.0001  max mem: 15850
[23:22:38.654928] Epoch: [40]  [240/345]  eta: 0:01:03  lr: 0.000027  loss: 0.1192 (0.1161)  time: 0.6022  data: 0.0001  max mem: 15850
[23:22:50.699795] Epoch: [40]  [260/345]  eta: 0:00:51  lr: 0.000027  loss: 0.1176 (0.1162)  time: 0.6022  data: 0.0001  max mem: 15850
[23:23:02.740979] Epoch: [40]  [280/345]  eta: 0:00:39  lr: 0.000027  loss: 0.1092 (0.1160)  time: 0.6020  data: 0.0001  max mem: 15850
[23:23:14.777043] Epoch: [40]  [300/345]  eta: 0:00:27  lr: 0.000026  loss: 0.1104 (0.1155)  time: 0.6018  data: 0.0001  max mem: 15850
[23:23:26.813178] Epoch: [40]  [320/345]  eta: 0:00:15  lr: 0.000026  loss: 0.1077 (0.1154)  time: 0.6018  data: 0.0001  max mem: 15850
[23:23:38.854117] Epoch: [40]  [340/345]  eta: 0:00:03  lr: 0.000026  loss: 0.1119 (0.1154)  time: 0.6020  data: 0.0001  max mem: 15850
[23:23:41.263969] Epoch: [40]  [344/345]  eta: 0:00:00  lr: 0.000026  loss: 0.1159 (0.1155)  time: 0.6020  data: 0.0001  max mem: 15850
[23:23:41.334161] Epoch: [40] Total time: 0:03:27 (0.6021 s / it)
[23:23:41.334374] Averaged stats: lr: 0.000026  loss: 0.1159 (0.1155)
[23:23:41.892337] Test:  [  0/345]  eta: 0:03:10  loss: 0.1284 (0.1284)  time: 0.5527  data: 0.3862  max mem: 15850
[23:23:43.569665] Test:  [ 10/345]  eta: 0:01:07  loss: 0.1075 (0.1097)  time: 0.2027  data: 0.0352  max mem: 15850
[23:23:45.251956] Test:  [ 20/345]  eta: 0:01:00  loss: 0.1075 (0.1100)  time: 0.1679  data: 0.0001  max mem: 15850
[23:23:46.936710] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1022 (0.1081)  time: 0.1683  data: 0.0001  max mem: 15850
[23:23:48.624547] Test:  [ 40/345]  eta: 0:00:54  loss: 0.1074 (0.1090)  time: 0.1686  data: 0.0001  max mem: 15850
[23:23:50.315305] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1097 (0.1092)  time: 0.1688  data: 0.0001  max mem: 15850
[23:23:52.010808] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1139 (0.1110)  time: 0.1692  data: 0.0001  max mem: 15850
[23:23:53.708818] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1124 (0.1110)  time: 0.1696  data: 0.0001  max mem: 15850
[23:23:55.409168] Test:  [ 80/345]  eta: 0:00:46  loss: 0.1067 (0.1111)  time: 0.1699  data: 0.0001  max mem: 15850
[23:23:57.114194] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1050 (0.1102)  time: 0.1702  data: 0.0001  max mem: 15850
[23:23:58.822978] Test:  [100/345]  eta: 0:00:42  loss: 0.1042 (0.1101)  time: 0.1706  data: 0.0001  max mem: 15850
[23:24:00.535163] Test:  [110/345]  eta: 0:00:40  loss: 0.1017 (0.1097)  time: 0.1710  data: 0.0001  max mem: 15850
[23:24:02.249528] Test:  [120/345]  eta: 0:00:38  loss: 0.1017 (0.1095)  time: 0.1713  data: 0.0001  max mem: 15850
[23:24:03.967946] Test:  [130/345]  eta: 0:00:37  loss: 0.1053 (0.1092)  time: 0.1716  data: 0.0001  max mem: 15850
[23:24:05.690467] Test:  [140/345]  eta: 0:00:35  loss: 0.1096 (0.1090)  time: 0.1720  data: 0.0001  max mem: 15850
[23:24:07.416814] Test:  [150/345]  eta: 0:00:33  loss: 0.1096 (0.1091)  time: 0.1724  data: 0.0001  max mem: 15850
[23:24:09.145838] Test:  [160/345]  eta: 0:00:31  loss: 0.1110 (0.1091)  time: 0.1727  data: 0.0001  max mem: 15850
[23:24:10.878807] Test:  [170/345]  eta: 0:00:30  loss: 0.1114 (0.1097)  time: 0.1730  data: 0.0001  max mem: 15850
[23:24:12.614932] Test:  [180/345]  eta: 0:00:28  loss: 0.1198 (0.1103)  time: 0.1734  data: 0.0001  max mem: 15850
[23:24:14.356318] Test:  [190/345]  eta: 0:00:26  loss: 0.1183 (0.1107)  time: 0.1738  data: 0.0001  max mem: 15850
[23:24:16.099839] Test:  [200/345]  eta: 0:00:25  loss: 0.1148 (0.1106)  time: 0.1742  data: 0.0001  max mem: 15850
[23:24:17.846777] Test:  [210/345]  eta: 0:00:23  loss: 0.1040 (0.1102)  time: 0.1745  data: 0.0001  max mem: 15850
[23:24:19.596838] Test:  [220/345]  eta: 0:00:21  loss: 0.1006 (0.1100)  time: 0.1748  data: 0.0001  max mem: 15850
[23:24:21.350488] Test:  [230/345]  eta: 0:00:19  loss: 0.1050 (0.1099)  time: 0.1751  data: 0.0001  max mem: 15850
[23:24:23.107357] Test:  [240/345]  eta: 0:00:18  loss: 0.1049 (0.1100)  time: 0.1755  data: 0.0001  max mem: 15850
[23:24:24.868025] Test:  [250/345]  eta: 0:00:16  loss: 0.1048 (0.1099)  time: 0.1758  data: 0.0001  max mem: 15850
[23:24:26.632940] Test:  [260/345]  eta: 0:00:14  loss: 0.1080 (0.1101)  time: 0.1762  data: 0.0001  max mem: 15850
[23:24:28.401281] Test:  [270/345]  eta: 0:00:13  loss: 0.1036 (0.1099)  time: 0.1766  data: 0.0001  max mem: 15850
[23:24:30.170061] Test:  [280/345]  eta: 0:00:11  loss: 0.1044 (0.1104)  time: 0.1768  data: 0.0001  max mem: 15850
[23:24:31.945157] Test:  [290/345]  eta: 0:00:09  loss: 0.1181 (0.1104)  time: 0.1771  data: 0.0001  max mem: 15850
[23:24:33.723447] Test:  [300/345]  eta: 0:00:07  loss: 0.1083 (0.1106)  time: 0.1776  data: 0.0001  max mem: 15850
[23:24:35.505304] Test:  [310/345]  eta: 0:00:06  loss: 0.1125 (0.1106)  time: 0.1779  data: 0.0001  max mem: 15850
[23:24:37.289623] Test:  [320/345]  eta: 0:00:04  loss: 0.1081 (0.1104)  time: 0.1782  data: 0.0001  max mem: 15850
[23:24:39.079474] Test:  [330/345]  eta: 0:00:02  loss: 0.1015 (0.1102)  time: 0.1786  data: 0.0001  max mem: 15850
[23:24:40.869487] Test:  [340/345]  eta: 0:00:00  loss: 0.1039 (0.1104)  time: 0.1789  data: 0.0001  max mem: 15850
[23:24:41.587039] Test:  [344/345]  eta: 0:00:00  loss: 0.1030 (0.1106)  time: 0.1790  data: 0.0001  max mem: 15850
[23:24:41.665052] Test: Total time: 0:01:00 (0.1749 s / it)
[23:24:51.529701] Test:  [ 0/57]  eta: 0:00:24  loss: 0.4929 (0.4929)  time: 0.4341  data: 0.2704  max mem: 15850
[23:24:53.190099] Test:  [10/57]  eta: 0:00:08  loss: 0.4370 (0.4541)  time: 0.1903  data: 0.0247  max mem: 15850
[23:24:54.854879] Test:  [20/57]  eta: 0:00:06  loss: 0.4719 (0.4596)  time: 0.1662  data: 0.0001  max mem: 15850
[23:24:56.525924] Test:  [30/57]  eta: 0:00:04  loss: 0.2637 (0.3830)  time: 0.1667  data: 0.0001  max mem: 15850
[23:24:58.199465] Test:  [40/57]  eta: 0:00:02  loss: 0.2289 (0.3490)  time: 0.1672  data: 0.0001  max mem: 15850
[23:24:59.876255] Test:  [50/57]  eta: 0:00:01  loss: 0.2508 (0.3395)  time: 0.1675  data: 0.0001  max mem: 15850
[23:25:00.781658] Test:  [56/57]  eta: 0:00:00  loss: 0.2664 (0.3426)  time: 0.1626  data: 0.0001  max mem: 15850
[23:25:00.854740] Test: Total time: 0:00:09 (0.1712 s / it)
[23:25:02.507866] Dice score of the network on the train images: 0.893507, val images: 0.829271
[23:25:02.512016] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[23:25:03.418649] Epoch: [41]  [  0/345]  eta: 0:05:12  lr: 0.000026  loss: 0.1153 (0.1153)  time: 0.9055  data: 0.3052  max mem: 15850
[23:25:15.380604] Epoch: [41]  [ 20/345]  eta: 0:03:19  lr: 0.000025  loss: 0.1104 (0.1143)  time: 0.5980  data: 0.0001  max mem: 15850
[23:25:27.363351] Epoch: [41]  [ 40/345]  eta: 0:03:04  lr: 0.000025  loss: 0.1022 (0.1119)  time: 0.5991  data: 0.0001  max mem: 15850
[23:25:39.368924] Epoch: [41]  [ 60/345]  eta: 0:02:52  lr: 0.000025  loss: 0.1036 (0.1103)  time: 0.6002  data: 0.0001  max mem: 15850
[23:25:51.388435] Epoch: [41]  [ 80/345]  eta: 0:02:39  lr: 0.000025  loss: 0.1067 (0.1099)  time: 0.6009  data: 0.0001  max mem: 15850
[23:26:03.417903] Epoch: [41]  [100/345]  eta: 0:02:27  lr: 0.000024  loss: 0.1110 (0.1119)  time: 0.6014  data: 0.0001  max mem: 15850
[23:26:15.457715] Epoch: [41]  [120/345]  eta: 0:02:15  lr: 0.000024  loss: 0.1134 (0.1124)  time: 0.6019  data: 0.0001  max mem: 15850
[23:26:27.514439] Epoch: [41]  [140/345]  eta: 0:02:03  lr: 0.000024  loss: 0.1068 (0.1122)  time: 0.6028  data: 0.0001  max mem: 15850
[23:26:39.573519] Epoch: [41]  [160/345]  eta: 0:01:51  lr: 0.000023  loss: 0.1102 (0.1121)  time: 0.6029  data: 0.0001  max mem: 15850
[23:26:51.639795] Epoch: [41]  [180/345]  eta: 0:01:39  lr: 0.000023  loss: 0.1143 (0.1128)  time: 0.6033  data: 0.0001  max mem: 15850

[23:27:03.702441] Epoch: [41]  [200/345]  eta: 0:01:27  lr: 0.000023  loss: 0.1128 (0.1127)  time: 0.6031  data: 0.0001  max mem: 15850
[23:27:15.766607] Epoch: [41]  [220/345]  eta: 0:01:15  lr: 0.000022  loss: 0.1107 (0.1132)  time: 0.6032  data: 0.0001  max mem: 15850
[23:27:27.828773] Epoch: [41]  [240/345]  eta: 0:01:03  lr: 0.000022  loss: 0.1163 (0.1138)  time: 0.6031  data: 0.0001  max mem: 15850
[23:27:39.889497] Epoch: [41]  [260/345]  eta: 0:00:51  lr: 0.000022  loss: 0.1116 (0.1136)  time: 0.6030  data: 0.0001  max mem: 15850
[23:27:51.945097] Epoch: [41]  [280/345]  eta: 0:00:39  lr: 0.000022  loss: 0.1072 (0.1133)  time: 0.6027  data: 0.0001  max mem: 15850
[23:28:03.999270] Epoch: [41]  [300/345]  eta: 0:00:27  lr: 0.000021  loss: 0.1032 (0.1132)  time: 0.6027  data: 0.0001  max mem: 15850
[23:28:16.129025] Epoch: [41]  [320/345]  eta: 0:00:15  lr: 0.000021  loss: 0.1128 (0.1134)  time: 0.6064  data: 0.0001  max mem: 15850
[23:28:28.154906] Epoch: [41]  [340/345]  eta: 0:00:03  lr: 0.000021  loss: 0.1119 (0.1135)  time: 0.6013  data: 0.0001  max mem: 15850
[23:28:30.556678] Epoch: [41]  [344/345]  eta: 0:00:00  lr: 0.000021  loss: 0.1069 (0.1134)  time: 0.6010  data: 0.0001  max mem: 15850
[23:28:30.630012] Epoch: [41] Total time: 0:03:28 (0.6032 s / it)
[23:28:30.630238] Averaged stats: lr: 0.000021  loss: 0.1069 (0.1134)
[23:28:31.121393] Test:  [  0/345]  eta: 0:02:47  loss: 0.1266 (0.1266)  time: 0.4854  data: 0.3201  max mem: 15850
[23:28:32.800764] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1033 (0.1079)  time: 0.1967  data: 0.0292  max mem: 15850
[23:28:34.481539] Test:  [ 20/345]  eta: 0:00:59  loss: 0.0994 (0.1074)  time: 0.1679  data: 0.0001  max mem: 15850
[23:28:36.165683] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1100 (0.1106)  time: 0.1682  data: 0.0001  max mem: 15850
[23:28:37.852899] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1138 (0.1104)  time: 0.1685  data: 0.0001  max mem: 15850
[23:28:39.544045] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1078 (0.1106)  time: 0.1689  data: 0.0001  max mem: 15850
[23:28:41.239657] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1042 (0.1094)  time: 0.1693  data: 0.0001  max mem: 15850
[23:28:42.938958] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1042 (0.1099)  time: 0.1697  data: 0.0001  max mem: 15850
[23:28:44.640004] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1026 (0.1085)  time: 0.1700  data: 0.0001  max mem: 15850
[23:28:46.346108] Test:  [ 90/345]  eta: 0:00:44  loss: 0.1014 (0.1084)  time: 0.1703  data: 0.0001  max mem: 15850
[23:28:48.054183] Test:  [100/345]  eta: 0:00:42  loss: 0.1064 (0.1080)  time: 0.1706  data: 0.0001  max mem: 15850
[23:28:49.767086] Test:  [110/345]  eta: 0:00:40  loss: 0.1028 (0.1085)  time: 0.1710  data: 0.0001  max mem: 15850
[23:28:51.482745] Test:  [120/345]  eta: 0:00:38  loss: 0.1042 (0.1085)  time: 0.1714  data: 0.0001  max mem: 15850
[23:28:53.202006] Test:  [130/345]  eta: 0:00:37  loss: 0.1037 (0.1081)  time: 0.1717  data: 0.0001  max mem: 15850
[23:28:54.925324] Test:  [140/345]  eta: 0:00:35  loss: 0.1017 (0.1081)  time: 0.1721  data: 0.0001  max mem: 15850
[23:28:56.651808] Test:  [150/345]  eta: 0:00:33  loss: 0.1067 (0.1083)  time: 0.1724  data: 0.0001  max mem: 15850
[23:28:58.381142] Test:  [160/345]  eta: 0:00:31  loss: 0.1036 (0.1080)  time: 0.1727  data: 0.0001  max mem: 15850
[23:29:00.113373] Test:  [170/345]  eta: 0:00:30  loss: 0.1028 (0.1088)  time: 0.1730  data: 0.0001  max mem: 15850
[23:29:01.849724] Test:  [180/345]  eta: 0:00:28  loss: 0.1073 (0.1092)  time: 0.1734  data: 0.0001  max mem: 15850
[23:29:03.589676] Test:  [190/345]  eta: 0:00:26  loss: 0.1117 (0.1093)  time: 0.1738  data: 0.0001  max mem: 15850
[23:29:05.333544] Test:  [200/345]  eta: 0:00:25  loss: 0.1057 (0.1089)  time: 0.1741  data: 0.0001  max mem: 15850
[23:29:07.080298] Test:  [210/345]  eta: 0:00:23  loss: 0.1008 (0.1089)  time: 0.1745  data: 0.0001  max mem: 15850
[23:29:08.831665] Test:  [220/345]  eta: 0:00:21  loss: 0.1101 (0.1090)  time: 0.1748  data: 0.0001  max mem: 15850
[23:29:10.584857] Test:  [230/345]  eta: 0:00:19  loss: 0.1114 (0.1092)  time: 0.1752  data: 0.0001  max mem: 15850
[23:29:12.341465] Test:  [240/345]  eta: 0:00:18  loss: 0.1091 (0.1092)  time: 0.1754  data: 0.0001  max mem: 15850
[23:29:14.102524] Test:  [250/345]  eta: 0:00:16  loss: 0.1067 (0.1091)  time: 0.1758  data: 0.0001  max mem: 15850
[23:29:15.867036] Test:  [260/345]  eta: 0:00:14  loss: 0.1060 (0.1090)  time: 0.1762  data: 0.0001  max mem: 15850
[23:29:17.634476] Test:  [270/345]  eta: 0:00:13  loss: 0.1038 (0.1090)  time: 0.1765  data: 0.0001  max mem: 15850
[23:29:19.404908] Test:  [280/345]  eta: 0:00:11  loss: 0.1051 (0.1092)  time: 0.1768  data: 0.0001  max mem: 15850
[23:29:21.179021] Test:  [290/345]  eta: 0:00:09  loss: 0.1038 (0.1091)  time: 0.1772  data: 0.0001  max mem: 15850
[23:29:22.957291] Test:  [300/345]  eta: 0:00:07  loss: 0.1011 (0.1089)  time: 0.1776  data: 0.0001  max mem: 15850
[23:29:24.740646] Test:  [310/345]  eta: 0:00:06  loss: 0.1053 (0.1090)  time: 0.1780  data: 0.0001  max mem: 15850
[23:29:26.525785] Test:  [320/345]  eta: 0:00:04  loss: 0.1107 (0.1091)  time: 0.1784  data: 0.0001  max mem: 15850
[23:29:28.313644] Test:  [330/345]  eta: 0:00:02  loss: 0.1094 (0.1090)  time: 0.1786  data: 0.0001  max mem: 15850
[23:29:30.107361] Test:  [340/345]  eta: 0:00:00  loss: 0.1061 (0.1091)  time: 0.1790  data: 0.0001  max mem: 15850
[23:29:30.825788] Test:  [344/345]  eta: 0:00:00  loss: 0.1036 (0.1092)  time: 0.1792  data: 0.0001  max mem: 15850
[23:29:30.883687] Test: Total time: 0:01:00 (0.1746 s / it)
[23:29:40.724990] Test:  [ 0/57]  eta: 0:00:27  loss: 0.5024 (0.5024)  time: 0.4743  data: 0.3111  max mem: 15850
[23:29:42.385958] Test:  [10/57]  eta: 0:00:09  loss: 0.4337 (0.4547)  time: 0.1940  data: 0.0284  max mem: 15850
[23:29:44.050775] Test:  [20/57]  eta: 0:00:06  loss: 0.4665 (0.4621)  time: 0.1662  data: 0.0001  max mem: 15850
[23:29:45.720023] Test:  [30/57]  eta: 0:00:04  loss: 0.2633 (0.3842)  time: 0.1666  data: 0.0001  max mem: 15850
[23:29:47.395887] Test:  [40/57]  eta: 0:00:02  loss: 0.2199 (0.3478)  time: 0.1672  data: 0.0001  max mem: 15850
[23:29:49.073261] Test:  [50/57]  eta: 0:00:01  loss: 0.2374 (0.3377)  time: 0.1676  data: 0.0001  max mem: 15850
[23:29:49.978646] Test:  [56/57]  eta: 0:00:00  loss: 0.2727 (0.3396)  time: 0.1626  data: 0.0000  max mem: 15850
[23:29:50.047316] Test: Total time: 0:00:09 (0.1719 s / it)
[23:29:51.695470] Dice score of the network on the train images: 0.887218, val images: 0.834673
[23:29:51.699637] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[23:29:52.580325] Epoch: [42]  [  0/345]  eta: 0:05:03  lr: 0.000021  loss: 0.1283 (0.1283)  time: 0.8795  data: 0.2784  max mem: 15850
[23:30:04.536752] Epoch: [42]  [ 20/345]  eta: 0:03:18  lr: 0.000020  loss: 0.1079 (0.1127)  time: 0.5978  data: 0.0001  max mem: 15850
[23:30:16.513884] Epoch: [42]  [ 40/345]  eta: 0:03:04  lr: 0.000020  loss: 0.1155 (0.1157)  time: 0.5988  data: 0.0001  max mem: 15850
[23:30:28.519885] Epoch: [42]  [ 60/345]  eta: 0:02:52  lr: 0.000020  loss: 0.1111 (0.1150)  time: 0.6003  data: 0.0001  max mem: 15850
[23:30:40.539213] Epoch: [42]  [ 80/345]  eta: 0:02:39  lr: 0.000020  loss: 0.1174 (0.1154)  time: 0.6009  data: 0.0001  max mem: 15850
[23:30:52.570304] Epoch: [42]  [100/345]  eta: 0:02:27  lr: 0.000019  loss: 0.1058 (0.1143)  time: 0.6015  data: 0.0001  max mem: 15850
[23:31:04.616994] Epoch: [42]  [120/345]  eta: 0:02:15  lr: 0.000019  loss: 0.1065 (0.1132)  time: 0.6023  data: 0.0001  max mem: 15850
[23:31:16.664776] Epoch: [42]  [140/345]  eta: 0:02:03  lr: 0.000019  loss: 0.1096 (0.1131)  time: 0.6023  data: 0.0001  max mem: 15850
[23:31:28.727710] Epoch: [42]  [160/345]  eta: 0:01:51  lr: 0.000018  loss: 0.1056 (0.1128)  time: 0.6031  data: 0.0001  max mem: 15850
[23:31:40.793753] Epoch: [42]  [180/345]  eta: 0:01:39  lr: 0.000018  loss: 0.1086 (0.1133)  time: 0.6033  data: 0.0001  max mem: 15850
[23:31:52.863269] Epoch: [42]  [200/345]  eta: 0:01:27  lr: 0.000018  loss: 0.1129 (0.1135)  time: 0.6034  data: 0.0001  max mem: 15850
[23:32:04.926642] Epoch: [42]  [220/345]  eta: 0:01:15  lr: 0.000018  loss: 0.1091 (0.1135)  time: 0.6031  data: 0.0001  max mem: 15850
[23:32:16.990574] Epoch: [42]  [240/345]  eta: 0:01:03  lr: 0.000017  loss: 0.1072 (0.1133)  time: 0.6032  data: 0.0001  max mem: 15850
[23:32:29.043024] Epoch: [42]  [260/345]  eta: 0:00:51  lr: 0.000017  loss: 0.1081 (0.1129)  time: 0.6026  data: 0.0001  max mem: 15850
[23:32:41.094459] Epoch: [42]  [280/345]  eta: 0:00:39  lr: 0.000017  loss: 0.1061 (0.1128)  time: 0.6025  data: 0.0001  max mem: 15850
[23:32:53.144004] Epoch: [42]  [300/345]  eta: 0:00:27  lr: 0.000017  loss: 0.1081 (0.1123)  time: 0.6024  data: 0.0001  max mem: 15850
[23:33:05.192885] Epoch: [42]  [320/345]  eta: 0:00:15  lr: 0.000016  loss: 0.1020 (0.1118)  time: 0.6024  data: 0.0001  max mem: 15850
[23:33:17.237082] Epoch: [42]  [340/345]  eta: 0:00:03  lr: 0.000016  loss: 0.1090 (0.1121)  time: 0.6022  data: 0.0001  max mem: 15850
[23:33:19.644750] Epoch: [42]  [344/345]  eta: 0:00:00  lr: 0.000016  loss: 0.1068 (0.1120)  time: 0.6020  data: 0.0001  max mem: 15850
[23:33:19.719195] Epoch: [42] Total time: 0:03:28 (0.6030 s / it)
[23:33:19.719464] Averaged stats: lr: 0.000016  loss: 0.1068 (0.1120)
[23:33:20.204853] Test:  [  0/345]  eta: 0:02:45  loss: 0.6483 (0.6483)  time: 0.4807  data: 0.3154  max mem: 15850
[23:33:21.883949] Test:  [ 10/345]  eta: 0:01:05  loss: 0.6525 (0.6605)  time: 0.1963  data: 0.0288  max mem: 15850
[23:33:23.564278] Test:  [ 20/345]  eta: 0:00:59  loss: 0.6558 (0.6585)  time: 0.1679  data: 0.0001  max mem: 15850
[23:33:25.249037] Test:  [ 30/345]  eta: 0:00:56  loss: 0.6522 (0.6562)  time: 0.1682  data: 0.0001  max mem: 15850
[23:33:26.937301] Test:  [ 40/345]  eta: 0:00:53  loss: 0.6522 (0.6580)  time: 0.1686  data: 0.0001  max mem: 15850
[23:33:28.629510] Test:  [ 50/345]  eta: 0:00:51  loss: 0.6642 (0.6598)  time: 0.1690  data: 0.0001  max mem: 15850
[23:33:30.324806] Test:  [ 60/345]  eta: 0:00:49  loss: 0.6642 (0.6593)  time: 0.1693  data: 0.0001  max mem: 15850
[23:33:32.024013] Test:  [ 70/345]  eta: 0:00:47  loss: 0.6548 (0.6601)  time: 0.1697  data: 0.0001  max mem: 15850
[23:33:33.726346] Test:  [ 80/345]  eta: 0:00:45  loss: 0.6753 (0.6606)  time: 0.1700  data: 0.0001  max mem: 15850
[23:33:35.432625] Test:  [ 90/345]  eta: 0:00:44  loss: 0.6665 (0.6602)  time: 0.1704  data: 0.0001  max mem: 15850
[23:33:37.141391] Test:  [100/345]  eta: 0:00:42  loss: 0.6665 (0.6612)  time: 0.1707  data: 0.0001  max mem: 15850
[23:33:38.853991] Test:  [110/345]  eta: 0:00:40  loss: 0.6720 (0.6609)  time: 0.1710  data: 0.0001  max mem: 15850
[23:33:40.569999] Test:  [120/345]  eta: 0:00:38  loss: 0.6487 (0.6603)  time: 0.1714  data: 0.0001  max mem: 15850
[23:33:42.290274] Test:  [130/345]  eta: 0:00:37  loss: 0.6642 (0.6624)  time: 0.1717  data: 0.0001  max mem: 15850
[23:33:44.013557] Test:  [140/345]  eta: 0:00:35  loss: 0.6746 (0.6630)  time: 0.1721  data: 0.0001  max mem: 15850
[23:33:45.739309] Test:  [150/345]  eta: 0:00:33  loss: 0.6635 (0.6626)  time: 0.1724  data: 0.0001  max mem: 15850
[23:33:47.470897] Test:  [160/345]  eta: 0:00:31  loss: 0.6525 (0.6623)  time: 0.1728  data: 0.0001  max mem: 15850
[23:33:49.203541] Test:  [170/345]  eta: 0:00:30  loss: 0.6583 (0.6628)  time: 0.1731  data: 0.0001  max mem: 15850
[23:33:50.939904] Test:  [180/345]  eta: 0:00:28  loss: 0.6690 (0.6631)  time: 0.1734  data: 0.0001  max mem: 15850
[23:33:52.679723] Test:  [190/345]  eta: 0:00:26  loss: 0.6691 (0.6635)  time: 0.1737  data: 0.0001  max mem: 15850
[23:33:54.423627] Test:  [200/345]  eta: 0:00:25  loss: 0.6663 (0.6636)  time: 0.1741  data: 0.0001  max mem: 15850
[23:33:56.170740] Test:  [210/345]  eta: 0:00:23  loss: 0.6622 (0.6633)  time: 0.1745  data: 0.0001  max mem: 15850
[23:33:57.920442] Test:  [220/345]  eta: 0:00:21  loss: 0.6395 (0.6622)  time: 0.1748  data: 0.0001  max mem: 15850
[23:33:59.673426] Test:  [230/345]  eta: 0:00:19  loss: 0.6524 (0.6627)  time: 0.1751  data: 0.0001  max mem: 15850
[23:34:01.430937] Test:  [240/345]  eta: 0:00:18  loss: 0.6594 (0.6629)  time: 0.1755  data: 0.0001  max mem: 15850
[23:34:03.190620] Test:  [250/345]  eta: 0:00:16  loss: 0.6537 (0.6628)  time: 0.1758  data: 0.0001  max mem: 15850
[23:34:04.954494] Test:  [260/345]  eta: 0:00:14  loss: 0.6532 (0.6624)  time: 0.1761  data: 0.0001  max mem: 15850
[23:34:06.720931] Test:  [270/345]  eta: 0:00:13  loss: 0.6508 (0.6621)  time: 0.1765  data: 0.0001  max mem: 15850
[23:34:08.493327] Test:  [280/345]  eta: 0:00:11  loss: 0.6390 (0.6610)  time: 0.1768  data: 0.0001  max mem: 15850
[23:34:10.268233] Test:  [290/345]  eta: 0:00:09  loss: 0.6392 (0.6605)  time: 0.1773  data: 0.0001  max mem: 15850
[23:34:12.046361] Test:  [300/345]  eta: 0:00:07  loss: 0.6607 (0.6608)  time: 0.1776  data: 0.0001  max mem: 15850
[23:34:13.828710] Test:  [310/345]  eta: 0:00:06  loss: 0.6601 (0.6608)  time: 0.1780  data: 0.0001  max mem: 15850
[23:34:15.615019] Test:  [320/345]  eta: 0:00:04  loss: 0.6577 (0.6606)  time: 0.1784  data: 0.0001  max mem: 15850
[23:34:17.402850] Test:  [330/345]  eta: 0:00:02  loss: 0.6513 (0.6604)  time: 0.1786  data: 0.0001  max mem: 15850
[23:34:19.196079] Test:  [340/345]  eta: 0:00:00  loss: 0.6511 (0.6604)  time: 0.1790  data: 0.0001  max mem: 15850
[23:34:19.914687] Test:  [344/345]  eta: 0:00:00  loss: 0.6632 (0.6606)  time: 0.1792  data: 0.0001  max mem: 15850
[23:34:19.984698] Test: Total time: 0:01:00 (0.1747 s / it)
[23:34:29.820471] Test:  [ 0/57]  eta: 0:00:24  loss: 0.8085 (0.8085)  time: 0.4353  data: 0.2714  max mem: 15850
[23:34:31.479720] Test:  [10/57]  eta: 0:00:08  loss: 0.8085 (0.7887)  time: 0.1903  data: 0.0248  max mem: 15850
[23:34:33.145115] Test:  [20/57]  eta: 0:00:06  loss: 0.7806 (0.7843)  time: 0.1661  data: 0.0001  max mem: 15850
[23:34:34.814080] Test:  [30/57]  eta: 0:00:04  loss: 0.7496 (0.7554)  time: 0.1667  data: 0.0001  max mem: 15850
[23:34:36.486825] Test:  [40/57]  eta: 0:00:02  loss: 0.7132 (0.7568)  time: 0.1670  data: 0.0001  max mem: 15850
[23:34:38.164870] Test:  [50/57]  eta: 0:00:01  loss: 0.7272 (0.7558)  time: 0.1675  data: 0.0001  max mem: 15850
[23:34:39.070003] Test:  [56/57]  eta: 0:00:00  loss: 0.7544 (0.7617)  time: 0.1626  data: 0.0000  max mem: 15850
[23:34:39.142591] Test: Total time: 0:00:09 (0.1712 s / it)
[23:34:40.805138] Dice score of the network on the train images: 0.584652, val images: 0.495694
[23:34:40.805364] saving best_prec_model_0 @ epoch 42
[23:34:42.178590] log_dir: /root/seg_framework/MS-Mamba/output_dir/maskenc/train_ft
[23:34:43.059315] Epoch: [43]  [  0/345]  eta: 0:05:03  lr: 0.000016  loss: 0.1366 (0.1366)  time: 0.8795  data: 0.2793  max mem: 15850
[23:34:55.005755] Epoch: [43]  [ 20/345]  eta: 0:03:18  lr: 0.000016  loss: 0.1073 (0.1124)  time: 0.5973  data: 0.0001  max mem: 15850
[23:35:06.978449] Epoch: [43]  [ 40/345]  eta: 0:03:04  lr: 0.000016  loss: 0.1096 (0.1109)  time: 0.5986  data: 0.0001  max mem: 15850
[23:35:14.351588] Loss is nan, stopping training
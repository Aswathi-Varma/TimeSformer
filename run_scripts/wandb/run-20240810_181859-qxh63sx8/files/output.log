Not using distributed mode
[18:19:01.504509] job dir: /root/seg_framework/MS-Mamba/run_scripts
[18:19:01.504641] Namespace(accum_iter=1,
model='SegFormer3D',
in_channels=1,
lr=None,
blr=0.001,
min_lr=0,
dist_on_itp=False,
warmup_epochs=20,
device='cuda:0',
seed=42,
layer_decay=0.75,
clip_grad=None,
num_workers=8,
pin_mem=True,
resume='',
world_size=1,
embed_dim=1,
local_rank=-1,
dist_url='env://',
nb_classes=2,
data_dir='/root/MSLesSeg24/data',
datalist=None,
preprocess=False,
dim=2,
distributed=False)
[18:19:01.504747] device  cuda:0
[18:19:01.505833] Starting for fold 0
[18:19:01.700071] Elements in data_dir_paths: 11052
[18:19:01.734814] Elements in data_dir_paths: 1803
[18:19:03.178564] number of params: 59620439
[18:19:03.178814] model: Vivim2D(
  (encoder): mamba_block(
    (downsample_layers): SegformerEncoder(
      (patch_embeddings): ModuleList(
        (0): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (3): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (block): ModuleList(
        (0): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): Identity()
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.003703703870996833)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.007407407741993666)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.011111111380159855)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.014814815483987331)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.018518518656492233)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.02222222276031971)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.025925926864147186)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.029629630967974663)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03333333507180214)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03703703731298447)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04074074327945709)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04444444552063942)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.048148151487112045)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.051851850003004074)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0555555559694767)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.05925925821065903)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06296296417713165)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06666667014360428)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07037036865949631)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07407407462596893)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07777778059244156)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08148147910833359)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08518518507480621)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08888889104127884)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.09259259700775146)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0962962955236435)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.10000000149011612)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (layer_norm): ModuleList(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (2): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (3): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (decoder): SegformerDecodeHead(
    (linear_c): ModuleList(
      (0): SegformerMLP(
        (proj): Linear(in_features=64, out_features=768, bias=True)
      )
      (1): SegformerMLP(
        (proj): Linear(in_features=128, out_features=768, bias=True)
      )
      (2): SegformerMLP(
        (proj): Linear(in_features=320, out_features=768, bias=True)
      )
      (3): SegformerMLP(
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
    )
    (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Conv2d(768, 150, kernel_size=(1, 1), stride=(1, 1))
  )
  (out): Conv2d(768, 1, kernel_size=(1, 1), stride=(1, 1))
)
[18:19:03.182080] base lr: 1.00e-03
[18:19:03.182144] actual lr: 1.25e-04
[18:19:03.182195] accumulate grad iterations: 1
[18:19:03.182254] effective batch size: 32
[18:19:03.184168] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.000125
    maximize: False
    weight_decay: 0.01
)
[18:19:03.186198] Start training for 250 epochs
[18:19:03.186289] Number of samples in train dataloader:  345
[18:19:03.188077] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
[18:19:13.083427] Epoch: [0]  [  0/345]  eta: 0:56:53  lr: 0.000000  loss: 1.5696 (1.5696)  time: 9.8943  data: 0.2925  max mem: 15821
[18:19:24.888064] Epoch: [0]  [ 20/345]  eta: 0:05:35  lr: 0.000000  loss: 1.5682 (1.5690)  time: 0.5902  data: 0.0001  max mem: 15821
[18:19:36.920287] Epoch: [0]  [ 40/345]  eta: 0:04:10  lr: 0.000001  loss: 1.5626 (1.5672)  time: 0.6016  data: 0.0001  max mem: 15821
[18:19:48.875269] Epoch: [0]  [ 60/345]  eta: 0:03:33  lr: 0.000001  loss: 1.5453 (1.5605)  time: 0.5977  data: 0.0001  max mem: 15821
[18:20:00.887521] Epoch: [0]  [ 80/345]  eta: 0:03:08  lr: 0.000001  loss: 1.5377 (1.5551)  time: 0.6006  data: 0.0001  max mem: 15821
[18:20:12.954353] Epoch: [0]  [100/345]  eta: 0:02:49  lr: 0.000002  loss: 1.5304 (1.5500)  time: 0.6033  data: 0.0001  max mem: 15821
[18:20:25.043288] Epoch: [0]  [120/345]  eta: 0:02:32  lr: 0.000002  loss: 1.5248 (1.5457)  time: 0.6044  data: 0.0001  max mem: 15821
[18:20:37.149361] Epoch: [0]  [140/345]  eta: 0:02:16  lr: 0.000003  loss: 1.5065 (1.5403)  time: 0.6053  data: 0.0001  max mem: 15821
[18:20:49.284348] Epoch: [0]  [160/345]  eta: 0:02:01  lr: 0.000003  loss: 1.4792 (1.5329)  time: 0.6067  data: 0.0001  max mem: 15821
[18:21:01.425820] Epoch: [0]  [180/345]  eta: 0:01:47  lr: 0.000003  loss: 1.4274 (1.5217)  time: 0.6070  data: 0.0001  max mem: 15821
[18:21:13.583505] Epoch: [0]  [200/345]  eta: 0:01:34  lr: 0.000004  loss: 1.3853 (1.5083)  time: 0.6078  data: 0.0001  max mem: 15821
[18:21:25.740304] Epoch: [0]  [220/345]  eta: 0:01:20  lr: 0.000004  loss: 1.3624 (1.4953)  time: 0.6078  data: 0.0001  max mem: 15821
[18:21:37.884242] Epoch: [0]  [240/345]  eta: 0:01:07  lr: 0.000004  loss: 1.3396 (1.4825)  time: 0.6072  data: 0.0001  max mem: 15821
[18:21:50.013369] Epoch: [0]  [260/345]  eta: 0:00:54  lr: 0.000005  loss: 1.3092 (1.4694)  time: 0.6064  data: 0.0001  max mem: 15821
[18:22:02.155079] Epoch: [0]  [280/345]  eta: 0:00:41  lr: 0.000005  loss: 1.2797 (1.4560)  time: 0.6070  data: 0.0001  max mem: 15821
[18:22:14.307279] Epoch: [0]  [300/345]  eta: 0:00:28  lr: 0.000005  loss: 1.2477 (1.4424)  time: 0.6076  data: 0.0001  max mem: 15821
[18:22:26.453076] Epoch: [0]  [320/345]  eta: 0:00:15  lr: 0.000006  loss: 1.2142 (1.4286)  time: 0.6072  data: 0.0001  max mem: 15821
[18:22:38.579191] Epoch: [0]  [340/345]  eta: 0:00:03  lr: 0.000006  loss: 1.1799 (1.4143)  time: 0.6063  data: 0.0001  max mem: 15821
[18:22:41.008460] Epoch: [0]  [344/345]  eta: 0:00:00  lr: 0.000006  loss: 1.1728 (1.4112)  time: 0.6064  data: 0.0001  max mem: 15821
[18:22:41.078661] Epoch: [0] Total time: 0:03:37 (0.6316 s / it)
[18:22:41.079107] Averaged stats: lr: 0.000006  loss: 1.1728 (1.4112)
[18:22:41.554022] Test:  [  0/345]  eta: 0:02:42  loss: 1.2052 (1.2052)  time: 0.4714  data: 0.3059  max mem: 15821
[18:22:43.224956] Test:  [ 10/345]  eta: 0:01:05  loss: 1.2090 (1.2092)  time: 0.1947  data: 0.0279  max mem: 15821
[18:22:44.899212] Test:  [ 20/345]  eta: 0:00:59  loss: 1.2086 (1.2088)  time: 0.1672  data: 0.0001  max mem: 15821
[18:22:46.575476] Test:  [ 30/345]  eta: 0:00:55  loss: 1.2082 (1.2087)  time: 0.1675  data: 0.0001  max mem: 15821
[18:22:48.256111] Test:  [ 40/345]  eta: 0:00:53  loss: 1.2075 (1.2082)  time: 0.1678  data: 0.0001  max mem: 15821
[18:22:49.941682] Test:  [ 50/345]  eta: 0:00:51  loss: 1.2075 (1.2081)  time: 0.1682  data: 0.0001  max mem: 15821
[18:22:51.628543] Test:  [ 60/345]  eta: 0:00:49  loss: 1.2087 (1.2082)  time: 0.1686  data: 0.0001  max mem: 15821
[18:22:53.318780] Test:  [ 70/345]  eta: 0:00:47  loss: 1.2091 (1.2082)  time: 0.1688  data: 0.0001  max mem: 15821
[18:22:55.012399] Test:  [ 80/345]  eta: 0:00:45  loss: 1.2081 (1.2083)  time: 0.1691  data: 0.0001  max mem: 15821
[18:22:56.709372] Test:  [ 90/345]  eta: 0:00:43  loss: 1.2081 (1.2083)  time: 0.1695  data: 0.0001  max mem: 15821
[18:22:58.409216] Test:  [100/345]  eta: 0:00:42  loss: 1.2077 (1.2081)  time: 0.1698  data: 0.0001  max mem: 15821
[18:23:00.112982] Test:  [110/345]  eta: 0:00:40  loss: 1.2060 (1.2081)  time: 0.1701  data: 0.0001  max mem: 15821
[18:23:01.820649] Test:  [120/345]  eta: 0:00:38  loss: 1.2076 (1.2081)  time: 0.1705  data: 0.0001  max mem: 15821
[18:23:03.532015] Test:  [130/345]  eta: 0:00:36  loss: 1.2086 (1.2082)  time: 0.1709  data: 0.0001  max mem: 15821
[18:23:05.247653] Test:  [140/345]  eta: 0:00:35  loss: 1.2088 (1.2082)  time: 0.1713  data: 0.0001  max mem: 15821
[18:23:06.966319] Test:  [150/345]  eta: 0:00:33  loss: 1.2088 (1.2082)  time: 0.1717  data: 0.0001  max mem: 15821
[18:23:08.688409] Test:  [160/345]  eta: 0:00:31  loss: 1.2070 (1.2081)  time: 0.1720  data: 0.0001  max mem: 15821
[18:23:10.412842] Test:  [170/345]  eta: 0:00:30  loss: 1.2070 (1.2082)  time: 0.1723  data: 0.0001  max mem: 15821
[18:23:12.142701] Test:  [180/345]  eta: 0:00:28  loss: 1.2098 (1.2082)  time: 0.1727  data: 0.0001  max mem: 15821
[18:23:13.875188] Test:  [190/345]  eta: 0:00:26  loss: 1.2094 (1.2082)  time: 0.1731  data: 0.0001  max mem: 15821
[18:23:15.610979] Test:  [200/345]  eta: 0:00:24  loss: 1.2082 (1.2082)  time: 0.1734  data: 0.0001  max mem: 15821
[18:23:17.735144] Test:  [210/345]  eta: 0:00:23  loss: 1.2078 (1.2082)  time: 0.1929  data: 0.0001  max mem: 15821
[18:23:19.489287] Test:  [220/345]  eta: 0:00:21  loss: 1.2078 (1.2082)  time: 0.1939  data: 0.0001  max mem: 15821
[18:23:21.370357] Test:  [230/345]  eta: 0:00:20  loss: 1.2079 (1.2082)  time: 0.1817  data: 0.0001  max mem: 15821
[18:23:23.121839] Test:  [240/345]  eta: 0:00:18  loss: 1.2069 (1.2081)  time: 0.1816  data: 0.0001  max mem: 15821
[18:23:25.111273] Test:  [250/345]  eta: 0:00:16  loss: 1.2070 (1.2082)  time: 0.1870  data: 0.0001  max mem: 15821
[18:23:27.111159] Test:  [260/345]  eta: 0:00:14  loss: 1.2088 (1.2082)  time: 0.1994  data: 0.0001  max mem: 15821
[18:23:29.025440] Test:  [270/345]  eta: 0:00:13  loss: 1.2073 (1.2081)  time: 0.1957  data: 0.0001  max mem: 15821
[18:23:30.815319] Test:  [280/345]  eta: 0:00:11  loss: 1.2075 (1.2081)  time: 0.1851  data: 0.0001  max mem: 15821
[18:23:32.771223] Test:  [290/345]  eta: 0:00:09  loss: 1.2087 (1.2082)  time: 0.1872  data: 0.0001  max mem: 15821
[18:23:34.796461] Test:  [300/345]  eta: 0:00:08  loss: 1.2096 (1.2082)  time: 0.1990  data: 0.0001  max mem: 15821
[18:23:36.798572] Test:  [310/345]  eta: 0:00:06  loss: 1.2092 (1.2083)  time: 0.2013  data: 0.0001  max mem: 15821
[18:23:38.821639] Test:  [320/345]  eta: 0:00:04  loss: 1.2082 (1.2083)  time: 0.2012  data: 0.0001  max mem: 15821
[18:23:40.843848] Test:  [330/345]  eta: 0:00:02  loss: 1.2082 (1.2083)  time: 0.2022  data: 0.0001  max mem: 15821
[18:23:42.649785] Test:  [340/345]  eta: 0:00:00  loss: 1.2072 (1.2083)  time: 0.1913  data: 0.0001  max mem: 15821
[18:23:43.550140] Test:  [344/345]  eta: 0:00:00  loss: 1.2072 (1.2083)  time: 0.1998  data: 0.0001  max mem: 15821
[18:23:43.604536] Test: Total time: 0:01:02 (0.1812 s / it)
[18:23:53.543075] Test:  [ 0/57]  eta: 0:00:28  loss: 1.2008 (1.2008)  time: 0.5037  data: 0.3409  max mem: 15821
[18:23:55.194929] Test:  [10/57]  eta: 0:00:09  loss: 1.2117 (1.2087)  time: 0.1959  data: 0.0311  max mem: 15821
[18:23:56.851360] Test:  [20/57]  eta: 0:00:06  loss: 1.2112 (1.2073)  time: 0.1653  data: 0.0001  max mem: 15821
[18:23:58.511284] Test:  [30/57]  eta: 0:00:04  loss: 1.1873 (1.1982)  time: 0.1658  data: 0.0001  max mem: 15821
[18:24:00.175015] Test:  [40/57]  eta: 0:00:02  loss: 1.1787 (1.1929)  time: 0.1661  data: 0.0001  max mem: 15821
[18:24:01.843197] Test:  [50/57]  eta: 0:00:01  loss: 1.1826 (1.1913)  time: 0.1665  data: 0.0001  max mem: 15821
[18:24:03.525119] Test:  [56/57]  eta: 0:00:00  loss: 1.1848 (1.1917)  time: 0.2007  data: 0.0000  max mem: 15821
[18:24:03.596370] Test: Total time: 0:00:10 (0.1852 s / it)
[18:24:05.271262] Dice score of the network on the train images: 0.000000, val images: 0.000000
[18:24:05.271500] saving best_dice_model_0 @ epoch 0
[18:24:06.504322] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[18:24:07.410559] Epoch: [1]  [  0/345]  eta: 0:05:12  lr: 0.000006  loss: 1.1532 (1.1532)  time: 0.9051  data: 0.3020  max mem: 15821
[18:24:19.445674] Epoch: [1]  [ 20/345]  eta: 0:03:20  lr: 0.000007  loss: 1.1178 (1.1213)  time: 0.6017  data: 0.0001  max mem: 15821
[18:24:31.538216] Epoch: [1]  [ 40/345]  eta: 0:03:06  lr: 0.000007  loss: 1.0842 (1.1034)  time: 0.6046  data: 0.0001  max mem: 15821
[18:24:43.635503] Epoch: [1]  [ 60/345]  eta: 0:02:53  lr: 0.000007  loss: 1.0638 (1.0900)  time: 0.6048  data: 0.0001  max mem: 15821
[18:24:55.749818] Epoch: [1]  [ 80/345]  eta: 0:02:41  lr: 0.000008  loss: 1.0332 (1.0767)  time: 0.6057  data: 0.0001  max mem: 15821
[18:25:07.877581] Epoch: [1]  [100/345]  eta: 0:02:28  lr: 0.000008  loss: 1.0238 (1.0661)  time: 0.6063  data: 0.0001  max mem: 15821
[18:25:20.030647] Epoch: [1]  [120/345]  eta: 0:02:16  lr: 0.000008  loss: 1.0022 (1.0554)  time: 0.6076  data: 0.0001  max mem: 15821
[18:25:32.195294] Epoch: [1]  [140/345]  eta: 0:02:04  lr: 0.000009  loss: 0.9855 (1.0452)  time: 0.6082  data: 0.0001  max mem: 15821
[18:25:44.367968] Epoch: [1]  [160/345]  eta: 0:01:52  lr: 0.000009  loss: 0.9612 (1.0345)  time: 0.6086  data: 0.0001  max mem: 15821
[18:25:56.542858] Epoch: [1]  [180/345]  eta: 0:01:40  lr: 0.000010  loss: 0.9444 (1.0243)  time: 0.6087  data: 0.0001  max mem: 15821
[18:26:08.709474] Epoch: [1]  [200/345]  eta: 0:01:28  lr: 0.000010  loss: 0.9199 (1.0140)  time: 0.6083  data: 0.0001  max mem: 15821
[18:26:20.875057] Epoch: [1]  [220/345]  eta: 0:01:15  lr: 0.000010  loss: 0.9059 (1.0047)  time: 0.6082  data: 0.0001  max mem: 15821
[18:26:33.031614] Epoch: [1]  [240/345]  eta: 0:01:03  lr: 0.000011  loss: 0.9016 (0.9958)  time: 0.6078  data: 0.0001  max mem: 15821
[18:26:45.178962] Epoch: [1]  [260/345]  eta: 0:00:51  lr: 0.000011  loss: 0.8752 (0.9871)  time: 0.6073  data: 0.0001  max mem: 15821
[18:26:57.320315] Epoch: [1]  [280/345]  eta: 0:00:39  lr: 0.000011  loss: 0.8773 (0.9792)  time: 0.6070  data: 0.0001  max mem: 15821
[18:27:09.460615] Epoch: [1]  [300/345]  eta: 0:00:27  lr: 0.000012  loss: 0.8619 (0.9714)  time: 0.6070  data: 0.0001  max mem: 15821
[18:27:21.597139] Epoch: [1]  [320/345]  eta: 0:00:15  lr: 0.000012  loss: 0.8404 (0.9633)  time: 0.6068  data: 0.0001  max mem: 15821
[18:27:33.730720] Epoch: [1]  [340/345]  eta: 0:00:03  lr: 0.000012  loss: 0.8262 (0.9555)  time: 0.6066  data: 0.0001  max mem: 15821
[18:27:36.155910] Epoch: [1]  [344/345]  eta: 0:00:00  lr: 0.000012  loss: 0.8260 (0.9540)  time: 0.6065  data: 0.0001  max mem: 15821
[18:27:36.215608] Epoch: [1] Total time: 0:03:29 (0.6079 s / it)
[18:27:36.216055] Averaged stats: lr: 0.000012  loss: 0.8260 (0.9540)
[18:27:36.698898] Test:  [  0/345]  eta: 0:02:44  loss: 0.8639 (0.8639)  time: 0.4772  data: 0.3119  max mem: 15821
[18:27:38.369144] Test:  [ 10/345]  eta: 0:01:05  loss: 0.8639 (0.8574)  time: 0.1951  data: 0.0284  max mem: 15821
[18:27:40.041807] Test:  [ 20/345]  eta: 0:00:59  loss: 0.8447 (0.8455)  time: 0.1671  data: 0.0001  max mem: 15821
[18:27:41.718453] Test:  [ 30/345]  eta: 0:00:55  loss: 0.8378 (0.8454)  time: 0.1674  data: 0.0001  max mem: 15821
[18:27:43.399029] Test:  [ 40/345]  eta: 0:00:53  loss: 0.8359 (0.8427)  time: 0.1678  data: 0.0001  max mem: 15821
[18:27:45.083440] Test:  [ 50/345]  eta: 0:00:51  loss: 0.8277 (0.8416)  time: 0.1682  data: 0.0001  max mem: 15821
[18:27:46.769901] Test:  [ 60/345]  eta: 0:00:49  loss: 0.8343 (0.8399)  time: 0.1685  data: 0.0001  max mem: 15821
[18:27:48.460866] Test:  [ 70/345]  eta: 0:00:47  loss: 0.8420 (0.8393)  time: 0.1688  data: 0.0001  max mem: 15821
[18:27:50.154489] Test:  [ 80/345]  eta: 0:00:45  loss: 0.8436 (0.8406)  time: 0.1692  data: 0.0001  max mem: 15821
[18:27:51.851138] Test:  [ 90/345]  eta: 0:00:43  loss: 0.8423 (0.8397)  time: 0.1694  data: 0.0001  max mem: 15821
[18:27:53.551775] Test:  [100/345]  eta: 0:00:42  loss: 0.8386 (0.8400)  time: 0.1698  data: 0.0001  max mem: 15821
[18:27:55.256145] Test:  [110/345]  eta: 0:00:40  loss: 0.8386 (0.8404)  time: 0.1702  data: 0.0001  max mem: 15821
[18:27:56.962948] Test:  [120/345]  eta: 0:00:38  loss: 0.8509 (0.8419)  time: 0.1705  data: 0.0001  max mem: 15821
[18:27:58.673365] Test:  [130/345]  eta: 0:00:36  loss: 0.8411 (0.8404)  time: 0.1708  data: 0.0001  max mem: 15821
[18:28:00.387972] Test:  [140/345]  eta: 0:00:35  loss: 0.8203 (0.8402)  time: 0.1712  data: 0.0001  max mem: 15821
[18:28:02.105498] Test:  [150/345]  eta: 0:00:33  loss: 0.8373 (0.8408)  time: 0.1715  data: 0.0001  max mem: 15821
[18:28:03.828072] Test:  [160/345]  eta: 0:00:31  loss: 0.8484 (0.8413)  time: 0.1719  data: 0.0001  max mem: 15821
[18:28:05.552956] Test:  [170/345]  eta: 0:00:30  loss: 0.8426 (0.8406)  time: 0.1723  data: 0.0001  max mem: 15821
[18:28:07.282753] Test:  [180/345]  eta: 0:00:28  loss: 0.8318 (0.8406)  time: 0.1727  data: 0.0001  max mem: 15821
[18:28:09.013932] Test:  [190/345]  eta: 0:00:26  loss: 0.8461 (0.8413)  time: 0.1730  data: 0.0001  max mem: 15821
[18:28:10.748388] Test:  [200/345]  eta: 0:00:24  loss: 0.8482 (0.8413)  time: 0.1732  data: 0.0001  max mem: 15821
[18:28:12.488033] Test:  [210/345]  eta: 0:00:23  loss: 0.8482 (0.8417)  time: 0.1736  data: 0.0001  max mem: 15821
[18:28:14.594362] Test:  [220/345]  eta: 0:00:21  loss: 0.8447 (0.8409)  time: 0.1922  data: 0.0001  max mem: 15821
[18:28:16.339901] Test:  [230/345]  eta: 0:00:19  loss: 0.8272 (0.8403)  time: 0.1925  data: 0.0001  max mem: 15821
[18:28:18.254563] Test:  [240/345]  eta: 0:00:18  loss: 0.8359 (0.8404)  time: 0.1830  data: 0.0001  max mem: 15821
[18:28:20.025613] Test:  [250/345]  eta: 0:00:16  loss: 0.8584 (0.8409)  time: 0.1842  data: 0.0001  max mem: 15821
[18:28:21.989995] Test:  [260/345]  eta: 0:00:14  loss: 0.8395 (0.8405)  time: 0.1867  data: 0.0001  max mem: 15821
[18:28:23.778546] Test:  [270/345]  eta: 0:00:13  loss: 0.8386 (0.8406)  time: 0.1876  data: 0.0001  max mem: 15821
[18:28:25.681162] Test:  [280/345]  eta: 0:00:11  loss: 0.8554 (0.8411)  time: 0.1845  data: 0.0001  max mem: 15821
[18:28:27.471101] Test:  [290/345]  eta: 0:00:09  loss: 0.8256 (0.8407)  time: 0.1846  data: 0.0001  max mem: 15821
[18:28:29.416152] Test:  [300/345]  eta: 0:00:07  loss: 0.8235 (0.8410)  time: 0.1867  data: 0.0001  max mem: 15821
[18:28:31.203881] Test:  [310/345]  eta: 0:00:06  loss: 0.8425 (0.8408)  time: 0.1866  data: 0.0001  max mem: 15821
[18:28:33.241414] Test:  [320/345]  eta: 0:00:04  loss: 0.8199 (0.8407)  time: 0.1912  data: 0.0001  max mem: 15821
[18:28:35.022492] Test:  [330/345]  eta: 0:00:02  loss: 0.8251 (0.8403)  time: 0.1909  data: 0.0001  max mem: 15821
[18:28:36.914016] Test:  [340/345]  eta: 0:00:00  loss: 0.8251 (0.8401)  time: 0.1836  data: 0.0001  max mem: 15821
[18:28:37.628664] Test:  [344/345]  eta: 0:00:00  loss: 0.8254 (0.8400)  time: 0.1836  data: 0.0001  max mem: 15821
[18:28:37.694738] Test: Total time: 0:01:01 (0.1782 s / it)
[18:28:47.618343] Test:  [ 0/57]  eta: 0:00:26  loss: 0.9305 (0.9305)  time: 0.4638  data: 0.3016  max mem: 15821
[18:28:49.270362] Test:  [10/57]  eta: 0:00:09  loss: 0.8973 (0.8954)  time: 0.1923  data: 0.0275  max mem: 15821
[18:28:50.925376] Test:  [20/57]  eta: 0:00:06  loss: 0.8574 (0.8861)  time: 0.1653  data: 0.0001  max mem: 15821
[18:28:52.584676] Test:  [30/57]  eta: 0:00:04  loss: 0.7815 (0.8209)  time: 0.1656  data: 0.0001  max mem: 15821
[18:28:54.248338] Test:  [40/57]  eta: 0:00:02  loss: 0.6825 (0.7840)  time: 0.1661  data: 0.0001  max mem: 15821
[18:28:55.914894] Test:  [50/57]  eta: 0:00:01  loss: 0.7030 (0.7749)  time: 0.1665  data: 0.0001  max mem: 15821
[18:28:56.814801] Test:  [56/57]  eta: 0:00:00  loss: 0.7618 (0.7823)  time: 0.1616  data: 0.0000  max mem: 15821
[18:28:56.885060] Test: Total time: 0:00:09 (0.1707 s / it)
[18:28:58.581391] Dice score of the network on the train images: 0.469273, val images: 0.584248
[18:28:58.581686] saving best_prec_model_0 @ epoch 1
[18:28:59.788417] saving best_rec_model_0 @ epoch 1
[18:29:00.954591] saving best_dice_model_0 @ epoch 1
[18:29:02.172444] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[18:29:03.066402] Epoch: [2]  [  0/345]  eta: 0:05:08  lr: 0.000013  loss: 0.8069 (0.8069)  time: 0.8928  data: 0.2938  max mem: 15821
[18:29:15.032889] Epoch: [2]  [ 20/345]  eta: 0:03:18  lr: 0.000013  loss: 0.8145 (0.8151)  time: 0.5983  data: 0.0001  max mem: 15821
[18:29:27.038976] Epoch: [2]  [ 40/345]  eta: 0:03:04  lr: 0.000013  loss: 0.7819 (0.8022)  time: 0.6003  data: 0.0001  max mem: 15821
[18:29:39.079733] Epoch: [2]  [ 60/345]  eta: 0:02:52  lr: 0.000014  loss: 0.7879 (0.7957)  time: 0.6020  data: 0.0001  max mem: 15821
[18:29:51.141380] Epoch: [2]  [ 80/345]  eta: 0:02:40  lr: 0.000014  loss: 0.7727 (0.7902)  time: 0.6030  data: 0.0001  max mem: 15821
[18:30:03.232162] Epoch: [2]  [100/345]  eta: 0:02:28  lr: 0.000014  loss: 0.7504 (0.7826)  time: 0.6045  data: 0.0001  max mem: 15821
[18:30:15.354901] Epoch: [2]  [120/345]  eta: 0:02:16  lr: 0.000015  loss: 0.7469 (0.7764)  time: 0.6061  data: 0.0001  max mem: 15821
[18:30:27.483923] Epoch: [2]  [140/345]  eta: 0:02:04  lr: 0.000015  loss: 0.7233 (0.7685)  time: 0.6064  data: 0.0001  max mem: 15821
[18:30:39.615731] Epoch: [2]  [160/345]  eta: 0:01:51  lr: 0.000015  loss: 0.7091 (0.7616)  time: 0.6065  data: 0.0001  max mem: 15821
[18:30:51.742025] Epoch: [2]  [180/345]  eta: 0:01:39  lr: 0.000016  loss: 0.6996 (0.7555)  time: 0.6063  data: 0.0001  max mem: 15821
[18:31:03.868854] Epoch: [2]  [200/345]  eta: 0:01:27  lr: 0.000016  loss: 0.6955 (0.7493)  time: 0.6063  data: 0.0001  max mem: 15821
[18:31:16.003404] Epoch: [2]  [220/345]  eta: 0:01:15  lr: 0.000016  loss: 0.6836 (0.7432)  time: 0.6067  data: 0.0001  max mem: 15821
[18:31:28.132235] Epoch: [2]  [240/345]  eta: 0:01:03  lr: 0.000017  loss: 0.6528 (0.7364)  time: 0.6064  data: 0.0001  max mem: 15821
[18:31:40.339291] Epoch: [2]  [260/345]  eta: 0:00:51  lr: 0.000017  loss: 0.6549 (0.7301)  time: 0.6103  data: 0.0001  max mem: 15821
[18:31:52.451777] Epoch: [2]  [280/345]  eta: 0:00:39  lr: 0.000018  loss: 0.6318 (0.7238)  time: 0.6056  data: 0.0001  max mem: 15821
[18:32:04.568130] Epoch: [2]  [300/345]  eta: 0:00:27  lr: 0.000018  loss: 0.6152 (0.7173)  time: 0.6058  data: 0.0001  max mem: 15821
[18:32:16.683598] Epoch: [2]  [320/345]  eta: 0:00:15  lr: 0.000018  loss: 0.6150 (0.7109)  time: 0.6057  data: 0.0001  max mem: 15821
[18:32:28.779222] Epoch: [2]  [340/345]  eta: 0:00:03  lr: 0.000019  loss: 0.6051 (0.7045)  time: 0.6047  data: 0.0001  max mem: 15821
[18:32:31.197792] Epoch: [2]  [344/345]  eta: 0:00:00  lr: 0.000019  loss: 0.6037 (0.7031)  time: 0.6046  data: 0.0001  max mem: 15821
[18:32:31.273650] Epoch: [2] Total time: 0:03:29 (0.6061 s / it)
[18:32:31.274126] Averaged stats: lr: 0.000019  loss: 0.6037 (0.7031)
[18:32:31.755849] Test:  [  0/345]  eta: 0:02:44  loss: 0.6275 (0.6275)  time: 0.4764  data: 0.3125  max mem: 15821
[18:32:33.424183] Test:  [ 10/345]  eta: 0:01:05  loss: 0.5615 (0.5686)  time: 0.1949  data: 0.0285  max mem: 15821
[18:32:35.095465] Test:  [ 20/345]  eta: 0:00:59  loss: 0.5559 (0.5636)  time: 0.1669  data: 0.0001  max mem: 15821
[18:32:36.771059] Test:  [ 30/345]  eta: 0:00:55  loss: 0.5602 (0.5642)  time: 0.1673  data: 0.0001  max mem: 15821
[18:32:38.449145] Test:  [ 40/345]  eta: 0:00:53  loss: 0.5810 (0.5737)  time: 0.1676  data: 0.0001  max mem: 15821
[18:32:40.129860] Test:  [ 50/345]  eta: 0:00:51  loss: 0.5833 (0.5755)  time: 0.1679  data: 0.0001  max mem: 15821
[18:32:41.814413] Test:  [ 60/345]  eta: 0:00:49  loss: 0.5831 (0.5769)  time: 0.1682  data: 0.0001  max mem: 15821
[18:32:43.502356] Test:  [ 70/345]  eta: 0:00:47  loss: 0.5933 (0.5783)  time: 0.1686  data: 0.0001  max mem: 15821
[18:32:45.194222] Test:  [ 80/345]  eta: 0:00:45  loss: 0.5639 (0.5775)  time: 0.1689  data: 0.0001  max mem: 15821
[18:32:46.890546] Test:  [ 90/345]  eta: 0:00:43  loss: 0.5649 (0.5775)  time: 0.1693  data: 0.0001  max mem: 15821
[18:32:48.591309] Test:  [100/345]  eta: 0:00:41  loss: 0.5666 (0.5773)  time: 0.1698  data: 0.0001  max mem: 15821
[18:32:50.292900] Test:  [110/345]  eta: 0:00:40  loss: 0.5672 (0.5765)  time: 0.1701  data: 0.0001  max mem: 15821
[18:32:51.999129] Test:  [120/345]  eta: 0:00:38  loss: 0.5804 (0.5781)  time: 0.1703  data: 0.0001  max mem: 15821
[18:32:53.708963] Test:  [130/345]  eta: 0:00:36  loss: 0.5804 (0.5767)  time: 0.1707  data: 0.0001  max mem: 15821
[18:32:55.421365] Test:  [140/345]  eta: 0:00:35  loss: 0.5606 (0.5772)  time: 0.1710  data: 0.0001  max mem: 15821
[18:32:57.136722] Test:  [150/345]  eta: 0:00:33  loss: 0.5737 (0.5767)  time: 0.1713  data: 0.0001  max mem: 15821
[18:32:58.855712] Test:  [160/345]  eta: 0:00:31  loss: 0.5815 (0.5789)  time: 0.1717  data: 0.0001  max mem: 15821
[18:33:00.580385] Test:  [170/345]  eta: 0:00:29  loss: 0.5815 (0.5785)  time: 0.1721  data: 0.0001  max mem: 15821
[18:33:02.307482] Test:  [180/345]  eta: 0:00:28  loss: 0.5781 (0.5789)  time: 0.1725  data: 0.0001  max mem: 15821
[18:33:04.036670] Test:  [190/345]  eta: 0:00:26  loss: 0.5590 (0.5773)  time: 0.1728  data: 0.0001  max mem: 15821
[18:33:05.770524] Test:  [200/345]  eta: 0:00:24  loss: 0.5590 (0.5784)  time: 0.1731  data: 0.0001  max mem: 15821
[18:33:07.508359] Test:  [210/345]  eta: 0:00:23  loss: 0.5725 (0.5774)  time: 0.1735  data: 0.0001  max mem: 15821
[18:33:09.248059] Test:  [220/345]  eta: 0:00:21  loss: 0.5620 (0.5773)  time: 0.1738  data: 0.0001  max mem: 15821
[18:33:10.991555] Test:  [230/345]  eta: 0:00:19  loss: 0.5715 (0.5774)  time: 0.1741  data: 0.0001  max mem: 15821
[18:33:12.739325] Test:  [240/345]  eta: 0:00:18  loss: 0.5715 (0.5772)  time: 0.1745  data: 0.0001  max mem: 15821
[18:33:14.490745] Test:  [250/345]  eta: 0:00:16  loss: 0.5679 (0.5768)  time: 0.1749  data: 0.0001  max mem: 15821
[18:33:16.244334] Test:  [260/345]  eta: 0:00:14  loss: 0.5573 (0.5764)  time: 0.1752  data: 0.0001  max mem: 15821
[18:33:18.001370] Test:  [270/345]  eta: 0:00:12  loss: 0.5705 (0.5764)  time: 0.1755  data: 0.0001  max mem: 15821
[18:33:19.761105] Test:  [280/345]  eta: 0:00:11  loss: 0.5800 (0.5763)  time: 0.1758  data: 0.0001  max mem: 15821
[18:33:21.525054] Test:  [290/345]  eta: 0:00:09  loss: 0.5740 (0.5757)  time: 0.1761  data: 0.0001  max mem: 15821
[18:33:23.291327] Test:  [300/345]  eta: 0:00:07  loss: 0.5740 (0.5757)  time: 0.1765  data: 0.0001  max mem: 15821
[18:33:25.063791] Test:  [310/345]  eta: 0:00:06  loss: 0.5726 (0.5755)  time: 0.1769  data: 0.0001  max mem: 15821
[18:33:26.838286] Test:  [320/345]  eta: 0:00:04  loss: 0.5704 (0.5750)  time: 0.1773  data: 0.0001  max mem: 15821
[18:33:28.616801] Test:  [330/345]  eta: 0:00:02  loss: 0.5424 (0.5742)  time: 0.1776  data: 0.0001  max mem: 15821
[18:33:30.400653] Test:  [340/345]  eta: 0:00:00  loss: 0.5621 (0.5745)  time: 0.1781  data: 0.0001  max mem: 15821
[18:33:31.113401] Test:  [344/345]  eta: 0:00:00  loss: 0.5641 (0.5746)  time: 0.1781  data: 0.0001  max mem: 15821
[18:33:31.176528] Test: Total time: 0:00:59 (0.1736 s / it)
[18:33:41.158598] Test:  [ 0/57]  eta: 0:00:27  loss: 0.6846 (0.6846)  time: 0.4793  data: 0.3163  max mem: 15821
[18:33:42.810567] Test:  [10/57]  eta: 0:00:09  loss: 0.6268 (0.6316)  time: 0.1937  data: 0.0288  max mem: 15821
[18:33:44.466710] Test:  [20/57]  eta: 0:00:06  loss: 0.5746 (0.6055)  time: 0.1653  data: 0.0001  max mem: 15821
[18:33:46.125918] Test:  [30/57]  eta: 0:00:04  loss: 0.4789 (0.5482)  time: 0.1657  data: 0.0001  max mem: 15821
[18:33:47.788100] Test:  [40/57]  eta: 0:00:02  loss: 0.4236 (0.5171)  time: 0.1660  data: 0.0001  max mem: 15821
[18:33:49.455501] Test:  [50/57]  eta: 0:00:01  loss: 0.4236 (0.5131)  time: 0.1664  data: 0.0001  max mem: 15821
[18:33:50.354489] Test:  [56/57]  eta: 0:00:00  loss: 0.5038 (0.5178)  time: 0.1615  data: 0.0001  max mem: 15821
[18:33:50.418772] Test: Total time: 0:00:09 (0.1709 s / it)
[18:33:52.110302] Dice score of the network on the train images: 0.649875, val images: 0.712263
[18:33:52.110538] saving best_prec_model_0 @ epoch 2
[18:33:53.342433] saving best_dice_model_0 @ epoch 2
[18:33:54.550126] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[18:33:55.446420] Epoch: [3]  [  0/345]  eta: 0:05:08  lr: 0.000019  loss: 0.6134 (0.6134)  time: 0.8951  data: 0.2949  max mem: 15821
[18:34:07.441643] Epoch: [3]  [ 20/345]  eta: 0:03:19  lr: 0.000019  loss: 0.5726 (0.5823)  time: 0.5997  data: 0.0001  max mem: 15821
[18:34:19.475291] Epoch: [3]  [ 40/345]  eta: 0:03:05  lr: 0.000019  loss: 0.5733 (0.5785)  time: 0.6016  data: 0.0001  max mem: 15821
[18:34:31.532302] Epoch: [3]  [ 60/345]  eta: 0:02:52  lr: 0.000020  loss: 0.5585 (0.5730)  time: 0.6028  data: 0.0001  max mem: 15821
[18:34:43.585938] Epoch: [3]  [ 80/345]  eta: 0:02:40  lr: 0.000020  loss: 0.5616 (0.5690)  time: 0.6026  data: 0.0001  max mem: 15821
[18:34:55.679602] Epoch: [3]  [100/345]  eta: 0:02:28  lr: 0.000021  loss: 0.5554 (0.5669)  time: 0.6046  data: 0.0001  max mem: 15821
[18:35:07.791576] Epoch: [3]  [120/345]  eta: 0:02:16  lr: 0.000021  loss: 0.5228 (0.5609)  time: 0.6055  data: 0.0001  max mem: 15821
[18:35:19.920271] Epoch: [3]  [140/345]  eta: 0:02:04  lr: 0.000021  loss: 0.5356 (0.5571)  time: 0.6064  data: 0.0001  max mem: 15821
[18:35:32.069398] Epoch: [3]  [160/345]  eta: 0:01:52  lr: 0.000022  loss: 0.5325 (0.5539)  time: 0.6073  data: 0.0001  max mem: 15821
[18:35:44.209318] Epoch: [3]  [180/345]  eta: 0:01:39  lr: 0.000022  loss: 0.5066 (0.5490)  time: 0.6070  data: 0.0001  max mem: 15821
[18:35:56.349047] Epoch: [3]  [200/345]  eta: 0:01:27  lr: 0.000022  loss: 0.4996 (0.5441)  time: 0.6069  data: 0.0001  max mem: 15821
[18:36:08.482255] Epoch: [3]  [220/345]  eta: 0:01:15  lr: 0.000023  loss: 0.5060 (0.5404)  time: 0.6066  data: 0.0001  max mem: 15821
[18:36:20.611670] Epoch: [3]  [240/345]  eta: 0:01:03  lr: 0.000023  loss: 0.4913 (0.5361)  time: 0.6064  data: 0.0001  max mem: 15821
[18:36:32.743861] Epoch: [3]  [260/345]  eta: 0:00:51  lr: 0.000023  loss: 0.4738 (0.5321)  time: 0.6066  data: 0.0001  max mem: 15821
[18:36:44.865444] Epoch: [3]  [280/345]  eta: 0:00:39  lr: 0.000024  loss: 0.4714 (0.5283)  time: 0.6060  data: 0.0001  max mem: 15821
[18:36:56.972351] Epoch: [3]  [300/345]  eta: 0:00:27  lr: 0.000024  loss: 0.4536 (0.5236)  time: 0.6053  data: 0.0001  max mem: 15821
[18:37:09.066026] Epoch: [3]  [320/345]  eta: 0:00:15  lr: 0.000025  loss: 0.4629 (0.5194)  time: 0.6046  data: 0.0001  max mem: 15821

[18:37:21.158607] Epoch: [3]  [340/345]  eta: 0:00:03  lr: 0.000025  loss: 0.4452 (0.5164)  time: 0.6046  data: 0.0001  max mem: 15821
[18:37:23.576955] Epoch: [3]  [344/345]  eta: 0:00:00  lr: 0.000025  loss: 0.4419 (0.5156)  time: 0.6045  data: 0.0001  max mem: 15821
[18:37:23.649715] Epoch: [3] Total time: 0:03:29 (0.6061 s / it)
[18:37:23.650037] Averaged stats: lr: 0.000025  loss: 0.4419 (0.5156)
[18:37:24.174567] Test:  [  0/345]  eta: 0:02:59  loss: 0.4803 (0.4803)  time: 0.5190  data: 0.3550  max mem: 15821
[18:37:25.842895] Test:  [ 10/345]  eta: 0:01:06  loss: 0.4372 (0.4393)  time: 0.1987  data: 0.0324  max mem: 15821
[18:37:27.513346] Test:  [ 20/345]  eta: 0:00:59  loss: 0.4341 (0.4341)  time: 0.1669  data: 0.0001  max mem: 15821
[18:37:29.187722] Test:  [ 30/345]  eta: 0:00:56  loss: 0.4488 (0.4421)  time: 0.1672  data: 0.0001  max mem: 15821
[18:37:30.865585] Test:  [ 40/345]  eta: 0:00:53  loss: 0.4486 (0.4421)  time: 0.1675  data: 0.0001  max mem: 15821
[18:37:32.546825] Test:  [ 50/345]  eta: 0:00:51  loss: 0.4481 (0.4427)  time: 0.1679  data: 0.0001  max mem: 15821
[18:37:34.231223] Test:  [ 60/345]  eta: 0:00:49  loss: 0.4440 (0.4397)  time: 0.1682  data: 0.0001  max mem: 15821
[18:37:35.919327] Test:  [ 70/345]  eta: 0:00:47  loss: 0.4287 (0.4389)  time: 0.1686  data: 0.0001  max mem: 15821
[18:37:37.610428] Test:  [ 80/345]  eta: 0:00:45  loss: 0.4424 (0.4394)  time: 0.1689  data: 0.0001  max mem: 15821
[18:37:39.304968] Test:  [ 90/345]  eta: 0:00:43  loss: 0.4338 (0.4389)  time: 0.1692  data: 0.0001  max mem: 15821
[18:37:41.004732] Test:  [100/345]  eta: 0:00:42  loss: 0.4289 (0.4386)  time: 0.1697  data: 0.0001  max mem: 15821
[18:37:42.708192] Test:  [110/345]  eta: 0:00:40  loss: 0.4339 (0.4386)  time: 0.1701  data: 0.0001  max mem: 15821
[18:37:44.413156] Test:  [120/345]  eta: 0:00:38  loss: 0.4339 (0.4389)  time: 0.1704  data: 0.0001  max mem: 15821
[18:37:46.121444] Test:  [130/345]  eta: 0:00:36  loss: 0.4422 (0.4385)  time: 0.1706  data: 0.0001  max mem: 15821
[18:37:47.834354] Test:  [140/345]  eta: 0:00:35  loss: 0.4540 (0.4415)  time: 0.1710  data: 0.0001  max mem: 15821
[18:37:49.550501] Test:  [150/345]  eta: 0:00:33  loss: 0.4540 (0.4413)  time: 0.1714  data: 0.0001  max mem: 15821
[18:37:51.270163] Test:  [160/345]  eta: 0:00:31  loss: 0.4409 (0.4410)  time: 0.1717  data: 0.0001  max mem: 15821
[18:37:52.993489] Test:  [170/345]  eta: 0:00:30  loss: 0.4342 (0.4404)  time: 0.1721  data: 0.0001  max mem: 15821
[18:37:54.720316] Test:  [180/345]  eta: 0:00:28  loss: 0.4238 (0.4396)  time: 0.1724  data: 0.0001  max mem: 15821
[18:37:56.451509] Test:  [190/345]  eta: 0:00:26  loss: 0.4280 (0.4407)  time: 0.1728  data: 0.0001  max mem: 15821
[18:37:58.183900] Test:  [200/345]  eta: 0:00:24  loss: 0.4457 (0.4398)  time: 0.1731  data: 0.0001  max mem: 15821
[18:37:59.920406] Test:  [210/345]  eta: 0:00:23  loss: 0.4360 (0.4401)  time: 0.1734  data: 0.0001  max mem: 15821
[18:38:01.660301] Test:  [220/345]  eta: 0:00:21  loss: 0.4424 (0.4404)  time: 0.1738  data: 0.0001  max mem: 15821
[18:38:03.404463] Test:  [230/345]  eta: 0:00:19  loss: 0.4424 (0.4408)  time: 0.1741  data: 0.0001  max mem: 15821
[18:38:05.151701] Test:  [240/345]  eta: 0:00:18  loss: 0.4519 (0.4415)  time: 0.1745  data: 0.0001  max mem: 15821
[18:38:06.902061] Test:  [250/345]  eta: 0:00:16  loss: 0.4491 (0.4417)  time: 0.1748  data: 0.0001  max mem: 15821
[18:38:08.656278] Test:  [260/345]  eta: 0:00:14  loss: 0.4491 (0.4423)  time: 0.1752  data: 0.0001  max mem: 15821
[18:38:10.413909] Test:  [270/345]  eta: 0:00:12  loss: 0.4463 (0.4424)  time: 0.1755  data: 0.0001  max mem: 15821
[18:38:12.173999] Test:  [280/345]  eta: 0:00:11  loss: 0.4257 (0.4419)  time: 0.1758  data: 0.0001  max mem: 15821
[18:38:13.938138] Test:  [290/345]  eta: 0:00:09  loss: 0.4447 (0.4424)  time: 0.1761  data: 0.0001  max mem: 15821
[18:38:15.706480] Test:  [300/345]  eta: 0:00:07  loss: 0.4447 (0.4423)  time: 0.1766  data: 0.0001  max mem: 15821
[18:38:17.478307] Test:  [310/345]  eta: 0:00:06  loss: 0.4334 (0.4421)  time: 0.1769  data: 0.0001  max mem: 15821
[18:38:19.254348] Test:  [320/345]  eta: 0:00:04  loss: 0.4340 (0.4419)  time: 0.1773  data: 0.0001  max mem: 15821
[18:38:21.034234] Test:  [330/345]  eta: 0:00:02  loss: 0.4380 (0.4424)  time: 0.1777  data: 0.0001  max mem: 15821
[18:38:22.816413] Test:  [340/345]  eta: 0:00:00  loss: 0.4330 (0.4421)  time: 0.1780  data: 0.0001  max mem: 15821
[18:38:23.531698] Test:  [344/345]  eta: 0:00:00  loss: 0.4382 (0.4422)  time: 0.1782  data: 0.0001  max mem: 15821
[18:38:23.597075] Test: Total time: 0:00:59 (0.1737 s / it)
[18:38:33.458762] Test:  [ 0/57]  eta: 0:00:27  loss: 0.5286 (0.5286)  time: 0.4752  data: 0.3127  max mem: 15821
[18:38:35.109224] Test:  [10/57]  eta: 0:00:09  loss: 0.4759 (0.4845)  time: 0.1932  data: 0.0285  max mem: 15821
[18:38:36.764965] Test:  [20/57]  eta: 0:00:06  loss: 0.4514 (0.4620)  time: 0.1652  data: 0.0001  max mem: 15821
[18:38:38.424114] Test:  [30/57]  eta: 0:00:04  loss: 0.3572 (0.4186)  time: 0.1657  data: 0.0001  max mem: 15821
[18:38:40.088157] Test:  [40/57]  eta: 0:00:02  loss: 0.3295 (0.3933)  time: 0.1661  data: 0.0001  max mem: 15821
[18:38:41.755832] Test:  [50/57]  eta: 0:00:01  loss: 0.3270 (0.3902)  time: 0.1665  data: 0.0001  max mem: 15821
[18:38:42.654748] Test:  [56/57]  eta: 0:00:00  loss: 0.3829 (0.3967)  time: 0.1616  data: 0.0000  max mem: 15821
[18:38:42.723599] Test: Total time: 0:00:09 (0.1709 s / it)
[18:38:44.419412] Dice score of the network on the train images: 0.687657, val images: 0.750500
[18:38:44.419646] saving best_prec_model_0 @ epoch 3
[18:38:45.628848] saving best_rec_model_0 @ epoch 3
[18:38:46.907026] saving best_dice_model_0 @ epoch 3
[18:38:48.131528] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[18:38:49.027524] Epoch: [4]  [  0/345]  eta: 0:05:08  lr: 0.000025  loss: 0.4093 (0.4093)  time: 0.8950  data: 0.2943  max mem: 15821
[18:39:01.022128] Epoch: [4]  [ 20/345]  eta: 0:03:19  lr: 0.000025  loss: 0.4546 (0.4557)  time: 0.5997  data: 0.0001  max mem: 15821
[18:39:13.026726] Epoch: [4]  [ 40/345]  eta: 0:03:05  lr: 0.000026  loss: 0.4318 (0.4485)  time: 0.6002  data: 0.0001  max mem: 15821
[18:39:25.063306] Epoch: [4]  [ 60/345]  eta: 0:02:52  lr: 0.000026  loss: 0.4192 (0.4427)  time: 0.6018  data: 0.0001  max mem: 15821
[18:39:37.136415] Epoch: [4]  [ 80/345]  eta: 0:02:40  lr: 0.000026  loss: 0.4182 (0.4390)  time: 0.6036  data: 0.0001  max mem: 15821
[18:39:49.214912] Epoch: [4]  [100/345]  eta: 0:02:28  lr: 0.000027  loss: 0.4212 (0.4363)  time: 0.6039  data: 0.0001  max mem: 15821
[18:40:01.315759] Epoch: [4]  [120/345]  eta: 0:02:16  lr: 0.000027  loss: 0.4180 (0.4330)  time: 0.6050  data: 0.0001  max mem: 15821
[18:40:13.436490] Epoch: [4]  [140/345]  eta: 0:02:04  lr: 0.000028  loss: 0.4138 (0.4314)  time: 0.6060  data: 0.0001  max mem: 15821
[18:40:25.568176] Epoch: [4]  [160/345]  eta: 0:01:51  lr: 0.000028  loss: 0.4124 (0.4292)  time: 0.6065  data: 0.0001  max mem: 15821
[18:40:37.702414] Epoch: [4]  [180/345]  eta: 0:01:39  lr: 0.000028  loss: 0.4141 (0.4275)  time: 0.6067  data: 0.0001  max mem: 15821
[18:40:49.854848] Epoch: [4]  [200/345]  eta: 0:01:27  lr: 0.000029  loss: 0.3970 (0.4244)  time: 0.6076  data: 0.0001  max mem: 15821
[18:41:01.996101] Epoch: [4]  [220/345]  eta: 0:01:15  lr: 0.000029  loss: 0.4040 (0.4235)  time: 0.6070  data: 0.0001  max mem: 15821
[18:41:14.126364] Epoch: [4]  [240/345]  eta: 0:01:03  lr: 0.000029  loss: 0.4005 (0.4220)  time: 0.6065  data: 0.0001  max mem: 15821
[18:41:26.243833] Epoch: [4]  [260/345]  eta: 0:00:51  lr: 0.000030  loss: 0.4069 (0.4206)  time: 0.6058  data: 0.0001  max mem: 15821
[18:41:38.354294] Epoch: [4]  [280/345]  eta: 0:00:39  lr: 0.000030  loss: 0.3828 (0.4172)  time: 0.6055  data: 0.0001  max mem: 15821
[18:41:50.447218] Epoch: [4]  [300/345]  eta: 0:00:27  lr: 0.000030  loss: 0.3953 (0.4165)  time: 0.6046  data: 0.0001  max mem: 15821
[18:42:02.545891] Epoch: [4]  [320/345]  eta: 0:00:15  lr: 0.000031  loss: 0.3962 (0.4149)  time: 0.6049  data: 0.0001  max mem: 15821
[18:42:14.634786] Epoch: [4]  [340/345]  eta: 0:00:03  lr: 0.000031  loss: 0.3578 (0.4120)  time: 0.6044  data: 0.0001  max mem: 15821
[18:42:17.052956] Epoch: [4]  [344/345]  eta: 0:00:00  lr: 0.000031  loss: 0.3582 (0.4115)  time: 0.6042  data: 0.0001  max mem: 15821
[18:42:17.120322] Epoch: [4] Total time: 0:03:28 (0.6058 s / it)
[18:42:17.120537] Averaged stats: lr: 0.000031  loss: 0.3582 (0.4115)
[18:42:17.606576] Test:  [  0/345]  eta: 0:02:45  loss: 0.3681 (0.3681)  time: 0.4810  data: 0.3164  max mem: 15821
[18:42:19.276330] Test:  [ 10/345]  eta: 0:01:05  loss: 0.3692 (0.3779)  time: 0.1954  data: 0.0289  max mem: 15821
[18:42:20.947624] Test:  [ 20/345]  eta: 0:00:59  loss: 0.3692 (0.3707)  time: 0.1670  data: 0.0001  max mem: 15821
[18:42:22.622713] Test:  [ 30/345]  eta: 0:00:55  loss: 0.3776 (0.3723)  time: 0.1673  data: 0.0001  max mem: 15821
[18:42:24.302011] Test:  [ 40/345]  eta: 0:00:53  loss: 0.3650 (0.3722)  time: 0.1677  data: 0.0001  max mem: 15821
[18:42:25.983639] Test:  [ 50/345]  eta: 0:00:51  loss: 0.3636 (0.3718)  time: 0.1680  data: 0.0001  max mem: 15821
[18:42:27.668224] Test:  [ 60/345]  eta: 0:00:49  loss: 0.3591 (0.3707)  time: 0.1682  data: 0.0001  max mem: 15821
[18:42:29.358124] Test:  [ 70/345]  eta: 0:00:47  loss: 0.3453 (0.3682)  time: 0.1687  data: 0.0001  max mem: 15821
[18:42:31.049900] Test:  [ 80/345]  eta: 0:00:45  loss: 0.3637 (0.3708)  time: 0.1690  data: 0.0001  max mem: 15821
[18:42:32.745528] Test:  [ 90/345]  eta: 0:00:43  loss: 0.3901 (0.3735)  time: 0.1693  data: 0.0001  max mem: 15821
[18:42:34.443944] Test:  [100/345]  eta: 0:00:41  loss: 0.3901 (0.3745)  time: 0.1696  data: 0.0001  max mem: 15821
[18:42:36.146430] Test:  [110/345]  eta: 0:00:40  loss: 0.3804 (0.3752)  time: 0.1700  data: 0.0001  max mem: 15821
[18:42:37.852744] Test:  [120/345]  eta: 0:00:38  loss: 0.3610 (0.3742)  time: 0.1704  data: 0.0001  max mem: 15821
[18:42:39.562047] Test:  [130/345]  eta: 0:00:36  loss: 0.3473 (0.3712)  time: 0.1707  data: 0.0001  max mem: 15821
[18:42:41.275915] Test:  [140/345]  eta: 0:00:35  loss: 0.3473 (0.3707)  time: 0.1711  data: 0.0001  max mem: 15821
[18:42:42.992332] Test:  [150/345]  eta: 0:00:33  loss: 0.3601 (0.3704)  time: 0.1714  data: 0.0001  max mem: 15821
[18:42:44.712890] Test:  [160/345]  eta: 0:00:31  loss: 0.3604 (0.3708)  time: 0.1718  data: 0.0001  max mem: 15821
[18:42:46.436817] Test:  [170/345]  eta: 0:00:29  loss: 0.3614 (0.3708)  time: 0.1722  data: 0.0001  max mem: 15821
[18:42:48.163721] Test:  [180/345]  eta: 0:00:28  loss: 0.3614 (0.3699)  time: 0.1725  data: 0.0001  max mem: 15821
[18:42:49.895637] Test:  [190/345]  eta: 0:00:26  loss: 0.3642 (0.3707)  time: 0.1729  data: 0.0001  max mem: 15821
[18:42:51.629730] Test:  [200/345]  eta: 0:00:24  loss: 0.3856 (0.3719)  time: 0.1732  data: 0.0001  max mem: 15821
[18:42:53.366471] Test:  [210/345]  eta: 0:00:23  loss: 0.3817 (0.3722)  time: 0.1735  data: 0.0001  max mem: 15821
[18:42:55.107683] Test:  [220/345]  eta: 0:00:21  loss: 0.3909 (0.3728)  time: 0.1738  data: 0.0001  max mem: 15821
[18:42:56.851895] Test:  [230/345]  eta: 0:00:19  loss: 0.3666 (0.3725)  time: 0.1742  data: 0.0001  max mem: 15821
[18:42:58.598790] Test:  [240/345]  eta: 0:00:18  loss: 0.3527 (0.3718)  time: 0.1745  data: 0.0001  max mem: 15821
[18:43:00.349629] Test:  [250/345]  eta: 0:00:16  loss: 0.3464 (0.3709)  time: 0.1748  data: 0.0001  max mem: 15821
[18:43:02.104919] Test:  [260/345]  eta: 0:00:14  loss: 0.3499 (0.3701)  time: 0.1752  data: 0.0001  max mem: 15821
[18:43:03.863082] Test:  [270/345]  eta: 0:00:12  loss: 0.3534 (0.3697)  time: 0.1756  data: 0.0001  max mem: 15821
[18:43:05.625284] Test:  [280/345]  eta: 0:00:11  loss: 0.3626 (0.3697)  time: 0.1760  data: 0.0001  max mem: 15821
[18:43:07.391097] Test:  [290/345]  eta: 0:00:09  loss: 0.3488 (0.3694)  time: 0.1763  data: 0.0001  max mem: 15821
[18:43:09.159492] Test:  [300/345]  eta: 0:00:07  loss: 0.3583 (0.3693)  time: 0.1766  data: 0.0001  max mem: 15821
[18:43:10.930863] Test:  [310/345]  eta: 0:00:06  loss: 0.3630 (0.3691)  time: 0.1769  data: 0.0001  max mem: 15821
[18:43:12.706709] Test:  [320/345]  eta: 0:00:04  loss: 0.3693 (0.3695)  time: 0.1773  data: 0.0001  max mem: 15821
[18:43:14.486349] Test:  [330/345]  eta: 0:00:02  loss: 0.3769 (0.3697)  time: 0.1777  data: 0.0001  max mem: 15821
[18:43:16.268985] Test:  [340/345]  eta: 0:00:00  loss: 0.3528 (0.3697)  time: 0.1781  data: 0.0001  max mem: 15821
[18:43:16.981381] Test:  [344/345]  eta: 0:00:00  loss: 0.3464 (0.3694)  time: 0.1781  data: 0.0001  max mem: 15821
[18:43:17.055665] Test: Total time: 0:00:59 (0.1737 s / it)
[18:43:26.835491] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4556 (0.4556)  time: 0.4505  data: 0.2875  max mem: 15821
[18:43:28.487002] Test:  [10/57]  eta: 0:00:08  loss: 0.4157 (0.4198)  time: 0.1910  data: 0.0262  max mem: 15821
[18:43:30.142534] Test:  [20/57]  eta: 0:00:06  loss: 0.3926 (0.4014)  time: 0.1653  data: 0.0001  max mem: 15821
[18:43:31.802517] Test:  [30/57]  eta: 0:00:04  loss: 0.3002 (0.3595)  time: 0.1657  data: 0.0001  max mem: 15821
[18:43:33.466640] Test:  [40/57]  eta: 0:00:02  loss: 0.2690 (0.3387)  time: 0.1661  data: 0.0001  max mem: 15821
[18:43:35.134985] Test:  [50/57]  eta: 0:00:01  loss: 0.2728 (0.3357)  time: 0.1666  data: 0.0001  max mem: 15821
[18:43:36.033755] Test:  [56/57]  eta: 0:00:00  loss: 0.3172 (0.3408)  time: 0.1616  data: 0.0001  max mem: 15821
[18:43:36.108791] Test: Total time: 0:00:09 (0.1706 s / it)
[18:43:37.842081] Dice score of the network on the train images: 0.709476, val images: 0.773334
[18:43:37.842315] saving best_prec_model_0 @ epoch 4
[18:43:39.042652] saving best_rec_model_0 @ epoch 4
[18:43:40.221460] saving best_dice_model_0 @ epoch 4
[18:43:41.517187] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[18:43:42.458974] Epoch: [5]  [  0/345]  eta: 0:05:24  lr: 0.000031  loss: 0.3482 (0.3482)  time: 0.9407  data: 0.3396  max mem: 15821
[18:43:54.447325] Epoch: [5]  [ 20/345]  eta: 0:03:20  lr: 0.000032  loss: 0.3703 (0.3753)  time: 0.5994  data: 0.0001  max mem: 15821
[18:44:06.473619] Epoch: [5]  [ 40/345]  eta: 0:03:05  lr: 0.000032  loss: 0.3727 (0.3736)  time: 0.6013  data: 0.0001  max mem: 15821
[18:44:18.521259] Epoch: [5]  [ 60/345]  eta: 0:02:52  lr: 0.000032  loss: 0.3853 (0.3776)  time: 0.6023  data: 0.0001  max mem: 15821
[18:44:30.589120] Epoch: [5]  [ 80/345]  eta: 0:02:40  lr: 0.000033  loss: 0.3480 (0.3717)  time: 0.6033  data: 0.0001  max mem: 15821
[18:44:42.678908] Epoch: [5]  [100/345]  eta: 0:02:28  lr: 0.000033  loss: 0.3712 (0.3731)  time: 0.6045  data: 0.0001  max mem: 15821
[18:44:54.799667] Epoch: [5]  [120/345]  eta: 0:02:16  lr: 0.000033  loss: 0.3734 (0.3736)  time: 0.6060  data: 0.0001  max mem: 15821
[18:45:06.953111] Epoch: [5]  [140/345]  eta: 0:02:04  lr: 0.000034  loss: 0.3530 (0.3714)  time: 0.6076  data: 0.0001  max mem: 15821
[18:45:19.108297] Epoch: [5]  [160/345]  eta: 0:01:52  lr: 0.000034  loss: 0.3538 (0.3701)  time: 0.6077  data: 0.0001  max mem: 15821
[18:45:31.250405] Epoch: [5]  [180/345]  eta: 0:01:40  lr: 0.000035  loss: 0.3515 (0.3696)  time: 0.6071  data: 0.0001  max mem: 15821
[18:45:43.373958] Epoch: [5]  [200/345]  eta: 0:01:27  lr: 0.000035  loss: 0.3440 (0.3675)  time: 0.6061  data: 0.0001  max mem: 15821
[18:45:55.509982] Epoch: [5]  [220/345]  eta: 0:01:15  lr: 0.000035  loss: 0.3536 (0.3670)  time: 0.6068  data: 0.0001  max mem: 15821
[18:46:07.653912] Epoch: [5]  [240/345]  eta: 0:01:03  lr: 0.000036  loss: 0.3528 (0.3667)  time: 0.6072  data: 0.0001  max mem: 15821

[18:46:19.781229] Epoch: [5]  [260/345]  eta: 0:00:51  lr: 0.000036  loss: 0.3380 (0.3655)  time: 0.6063  data: 0.0001  max mem: 15821
[18:46:31.881780] Epoch: [5]  [280/345]  eta: 0:00:39  lr: 0.000036  loss: 0.3516 (0.3653)  time: 0.6050  data: 0.0001  max mem: 15821
[18:46:43.971449] Epoch: [5]  [300/345]  eta: 0:00:27  lr: 0.000037  loss: 0.3381 (0.3637)  time: 0.6044  data: 0.0001  max mem: 15821
[18:46:56.057508] Epoch: [5]  [320/345]  eta: 0:00:15  lr: 0.000037  loss: 0.3252 (0.3618)  time: 0.6043  data: 0.0001  max mem: 15821
[18:47:08.138238] Epoch: [5]  [340/345]  eta: 0:00:03  lr: 0.000037  loss: 0.3473 (0.3614)  time: 0.6040  data: 0.0001  max mem: 15821
[18:47:10.554763] Epoch: [5]  [344/345]  eta: 0:00:00  lr: 0.000037  loss: 0.3390 (0.3610)  time: 0.6040  data: 0.0001  max mem: 15821
[18:47:10.625479] Epoch: [5] Total time: 0:03:29 (0.6061 s / it)
[18:47:10.625675] Averaged stats: lr: 0.000037  loss: 0.3390 (0.3610)
[18:47:11.140697] Test:  [  0/345]  eta: 0:02:55  loss: 0.2917 (0.2917)  time: 0.5096  data: 0.3455  max mem: 15821
[18:47:12.810507] Test:  [ 10/345]  eta: 0:01:06  loss: 0.3495 (0.3421)  time: 0.1980  data: 0.0315  max mem: 15821
[18:47:14.482067] Test:  [ 20/345]  eta: 0:00:59  loss: 0.3195 (0.3319)  time: 0.1670  data: 0.0001  max mem: 15821
[18:47:16.157292] Test:  [ 30/345]  eta: 0:00:56  loss: 0.3222 (0.3326)  time: 0.1673  data: 0.0001  max mem: 15821
[18:47:17.836220] Test:  [ 40/345]  eta: 0:00:53  loss: 0.3313 (0.3307)  time: 0.1676  data: 0.0001  max mem: 15821
[18:47:19.518508] Test:  [ 50/345]  eta: 0:00:51  loss: 0.3287 (0.3325)  time: 0.1680  data: 0.0001  max mem: 15821
[18:47:21.203402] Test:  [ 60/345]  eta: 0:00:49  loss: 0.3253 (0.3310)  time: 0.1683  data: 0.0001  max mem: 15821
[18:47:22.893062] Test:  [ 70/345]  eta: 0:00:47  loss: 0.3182 (0.3294)  time: 0.1687  data: 0.0001  max mem: 15821
[18:47:24.585153] Test:  [ 80/345]  eta: 0:00:45  loss: 0.3235 (0.3294)  time: 0.1690  data: 0.0001  max mem: 15821
[18:47:26.281912] Test:  [ 90/345]  eta: 0:00:43  loss: 0.3246 (0.3293)  time: 0.1694  data: 0.0001  max mem: 15821
[18:47:27.982562] Test:  [100/345]  eta: 0:00:42  loss: 0.3205 (0.3275)  time: 0.1698  data: 0.0001  max mem: 15821
[18:47:29.685102] Test:  [110/345]  eta: 0:00:40  loss: 0.3172 (0.3269)  time: 0.1701  data: 0.0001  max mem: 15821
[18:47:31.390813] Test:  [120/345]  eta: 0:00:38  loss: 0.3313 (0.3285)  time: 0.1703  data: 0.0001  max mem: 15821
[18:47:33.101846] Test:  [130/345]  eta: 0:00:36  loss: 0.3339 (0.3297)  time: 0.1708  data: 0.0001  max mem: 15821
[18:47:34.815722] Test:  [140/345]  eta: 0:00:35  loss: 0.3266 (0.3289)  time: 0.1712  data: 0.0001  max mem: 15821
[18:47:36.532740] Test:  [150/345]  eta: 0:00:33  loss: 0.3197 (0.3292)  time: 0.1715  data: 0.0001  max mem: 15821
[18:47:38.254084] Test:  [160/345]  eta: 0:00:31  loss: 0.3321 (0.3300)  time: 0.1718  data: 0.0001  max mem: 15821
[18:47:39.978006] Test:  [170/345]  eta: 0:00:30  loss: 0.3270 (0.3296)  time: 0.1722  data: 0.0001  max mem: 15821
[18:47:41.705816] Test:  [180/345]  eta: 0:00:28  loss: 0.3268 (0.3299)  time: 0.1725  data: 0.0001  max mem: 15821
[18:47:43.437065] Test:  [190/345]  eta: 0:00:26  loss: 0.3268 (0.3289)  time: 0.1729  data: 0.0001  max mem: 15821
[18:47:45.170971] Test:  [200/345]  eta: 0:00:24  loss: 0.3163 (0.3291)  time: 0.1732  data: 0.0001  max mem: 15821
[18:47:46.908596] Test:  [210/345]  eta: 0:00:23  loss: 0.3238 (0.3290)  time: 0.1735  data: 0.0001  max mem: 15821
[18:47:48.650569] Test:  [220/345]  eta: 0:00:21  loss: 0.3238 (0.3292)  time: 0.1739  data: 0.0001  max mem: 15821
[18:47:50.395225] Test:  [230/345]  eta: 0:00:19  loss: 0.3146 (0.3292)  time: 0.1743  data: 0.0001  max mem: 15821
[18:47:52.143593] Test:  [240/345]  eta: 0:00:18  loss: 0.3266 (0.3296)  time: 0.1746  data: 0.0001  max mem: 15821
[18:47:53.894874] Test:  [250/345]  eta: 0:00:16  loss: 0.3333 (0.3299)  time: 0.1749  data: 0.0001  max mem: 15821
[18:47:55.650010] Test:  [260/345]  eta: 0:00:14  loss: 0.3262 (0.3299)  time: 0.1753  data: 0.0001  max mem: 15821
[18:47:57.406892] Test:  [270/345]  eta: 0:00:12  loss: 0.3250 (0.3301)  time: 0.1755  data: 0.0001  max mem: 15821
[18:47:59.169421] Test:  [280/345]  eta: 0:00:11  loss: 0.3250 (0.3297)  time: 0.1759  data: 0.0001  max mem: 15821
[18:48:00.935143] Test:  [290/345]  eta: 0:00:09  loss: 0.3099 (0.3291)  time: 0.1763  data: 0.0001  max mem: 15821
[18:48:02.703637] Test:  [300/345]  eta: 0:00:07  loss: 0.3276 (0.3299)  time: 0.1766  data: 0.0001  max mem: 15821
[18:48:04.476417] Test:  [310/345]  eta: 0:00:06  loss: 0.3329 (0.3294)  time: 0.1770  data: 0.0001  max mem: 15821
[18:48:06.252628] Test:  [320/345]  eta: 0:00:04  loss: 0.3234 (0.3293)  time: 0.1774  data: 0.0001  max mem: 15821
[18:48:08.032936] Test:  [330/345]  eta: 0:00:02  loss: 0.3144 (0.3289)  time: 0.1778  data: 0.0001  max mem: 15821
[18:48:09.817396] Test:  [340/345]  eta: 0:00:00  loss: 0.3199 (0.3292)  time: 0.1782  data: 0.0001  max mem: 15821
[18:48:10.530715] Test:  [344/345]  eta: 0:00:00  loss: 0.3260 (0.3293)  time: 0.1783  data: 0.0001  max mem: 15821
[18:48:10.592982] Test: Total time: 0:00:59 (0.1738 s / it)
[18:48:20.432230] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4553 (0.4553)  time: 0.4480  data: 0.2853  max mem: 15821
[18:48:22.085366] Test:  [10/57]  eta: 0:00:08  loss: 0.3870 (0.4104)  time: 0.1909  data: 0.0260  max mem: 15821
[18:48:23.742084] Test:  [20/57]  eta: 0:00:06  loss: 0.3815 (0.3934)  time: 0.1654  data: 0.0001  max mem: 15821
[18:48:25.402028] Test:  [30/57]  eta: 0:00:04  loss: 0.2816 (0.3498)  time: 0.1658  data: 0.0001  max mem: 15821
[18:48:27.065322] Test:  [40/57]  eta: 0:00:02  loss: 0.2754 (0.3304)  time: 0.1661  data: 0.0001  max mem: 15821
[18:48:28.732095] Test:  [50/57]  eta: 0:00:01  loss: 0.2796 (0.3293)  time: 0.1664  data: 0.0001  max mem: 15821
[18:48:29.631434] Test:  [56/57]  eta: 0:00:00  loss: 0.3107 (0.3336)  time: 0.1615  data: 0.0001  max mem: 15821
[18:48:29.688019] Test: Total time: 0:00:09 (0.1703 s / it)
[18:48:31.426890] Dice score of the network on the train images: 0.741684, val images: 0.781842
[18:48:31.427099] saving best_prec_model_0 @ epoch 5
[18:48:32.674269] saving best_dice_model_0 @ epoch 5
[18:48:33.997467] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[18:48:34.963253] Epoch: [6]  [  0/345]  eta: 0:05:32  lr: 0.000038  loss: 0.3155 (0.3155)  time: 0.9646  data: 0.3636  max mem: 15821
[18:48:46.952269] Epoch: [6]  [ 20/345]  eta: 0:03:20  lr: 0.000038  loss: 0.3384 (0.3364)  time: 0.5994  data: 0.0001  max mem: 15821
[18:48:58.978592] Epoch: [6]  [ 40/345]  eta: 0:03:05  lr: 0.000038  loss: 0.3262 (0.3368)  time: 0.6013  data: 0.0001  max mem: 15821
[18:49:11.040522] Epoch: [6]  [ 60/345]  eta: 0:02:53  lr: 0.000039  loss: 0.3353 (0.3364)  time: 0.6030  data: 0.0001  max mem: 15821
[18:49:23.111895] Epoch: [6]  [ 80/345]  eta: 0:02:40  lr: 0.000039  loss: 0.3443 (0.3368)  time: 0.6035  data: 0.0001  max mem: 15821
[18:49:35.205996] Epoch: [6]  [100/345]  eta: 0:02:28  lr: 0.000039  loss: 0.3304 (0.3370)  time: 0.6047  data: 0.0001  max mem: 15821
[18:49:47.309134] Epoch: [6]  [120/345]  eta: 0:02:16  lr: 0.000040  loss: 0.3405 (0.3379)  time: 0.6051  data: 0.0001  max mem: 15821
[18:49:59.443467] Epoch: [6]  [140/345]  eta: 0:02:04  lr: 0.000040  loss: 0.3164 (0.3358)  time: 0.6067  data: 0.0001  max mem: 15821
[18:50:11.592199] Epoch: [6]  [160/345]  eta: 0:01:52  lr: 0.000040  loss: 0.3111 (0.3348)  time: 0.6074  data: 0.0001  max mem: 15821
[18:50:23.735149] Epoch: [6]  [180/345]  eta: 0:01:40  lr: 0.000041  loss: 0.3252 (0.3349)  time: 0.6071  data: 0.0001  max mem: 15821
[18:50:35.874295] Epoch: [6]  [200/345]  eta: 0:01:27  lr: 0.000041  loss: 0.3154 (0.3343)  time: 0.6069  data: 0.0001  max mem: 15821
[18:50:48.017839] Epoch: [6]  [220/345]  eta: 0:01:15  lr: 0.000041  loss: 0.3141 (0.3328)  time: 0.6071  data: 0.0001  max mem: 15821
[18:51:00.149183] Epoch: [6]  [240/345]  eta: 0:01:03  lr: 0.000042  loss: 0.3114 (0.3319)  time: 0.6065  data: 0.0001  max mem: 15821
[18:51:12.278542] Epoch: [6]  [260/345]  eta: 0:00:51  lr: 0.000042  loss: 0.3365 (0.3322)  time: 0.6064  data: 0.0001  max mem: 15821
[18:51:24.398496] Epoch: [6]  [280/345]  eta: 0:00:39  lr: 0.000043  loss: 0.3126 (0.3311)  time: 0.6060  data: 0.0001  max mem: 15821
[18:51:36.522208] Epoch: [6]  [300/345]  eta: 0:00:27  lr: 0.000043  loss: 0.3260 (0.3298)  time: 0.6061  data: 0.0001  max mem: 15821
[18:51:48.636442] Epoch: [6]  [320/345]  eta: 0:00:15  lr: 0.000043  loss: 0.3253 (0.3292)  time: 0.6057  data: 0.0001  max mem: 15821
[18:52:00.741376] Epoch: [6]  [340/345]  eta: 0:00:03  lr: 0.000044  loss: 0.3287 (0.3293)  time: 0.6052  data: 0.0001  max mem: 15821
[18:52:03.164310] Epoch: [6]  [344/345]  eta: 0:00:00  lr: 0.000044  loss: 0.3240 (0.3291)  time: 0.6053  data: 0.0001  max mem: 15821
[18:52:03.229602] Epoch: [6] Total time: 0:03:29 (0.6065 s / it)
[18:52:03.230109] Averaged stats: lr: 0.000044  loss: 0.3240 (0.3291)
[18:52:03.779020] Test:  [  0/345]  eta: 0:03:07  loss: 0.3009 (0.3009)  time: 0.5433  data: 0.3788  max mem: 15821
[18:52:05.448558] Test:  [ 10/345]  eta: 0:01:07  loss: 0.3009 (0.3041)  time: 0.2011  data: 0.0345  max mem: 15821
[18:52:07.120467] Test:  [ 20/345]  eta: 0:01:00  loss: 0.3000 (0.3014)  time: 0.1670  data: 0.0001  max mem: 15821
[18:52:08.796881] Test:  [ 30/345]  eta: 0:00:56  loss: 0.3005 (0.3058)  time: 0.1673  data: 0.0001  max mem: 15821
[18:52:10.476314] Test:  [ 40/345]  eta: 0:00:53  loss: 0.3003 (0.3052)  time: 0.1677  data: 0.0001  max mem: 15821
[18:52:12.158551] Test:  [ 50/345]  eta: 0:00:51  loss: 0.3003 (0.3070)  time: 0.1680  data: 0.0001  max mem: 15821
[18:52:13.845167] Test:  [ 60/345]  eta: 0:00:49  loss: 0.3064 (0.3066)  time: 0.1684  data: 0.0001  max mem: 15821
[18:52:15.533725] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2950 (0.3048)  time: 0.1687  data: 0.0001  max mem: 15821
[18:52:17.227214] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2950 (0.3053)  time: 0.1690  data: 0.0001  max mem: 15821
[18:52:18.923032] Test:  [ 90/345]  eta: 0:00:43  loss: 0.3127 (0.3057)  time: 0.1694  data: 0.0001  max mem: 15821
[18:52:20.623962] Test:  [100/345]  eta: 0:00:42  loss: 0.3102 (0.3063)  time: 0.1698  data: 0.0001  max mem: 15821
[18:52:22.325558] Test:  [110/345]  eta: 0:00:40  loss: 0.3102 (0.3070)  time: 0.1701  data: 0.0001  max mem: 15821
[18:52:24.032512] Test:  [120/345]  eta: 0:00:38  loss: 0.3114 (0.3072)  time: 0.1704  data: 0.0001  max mem: 15821
[18:52:25.743746] Test:  [130/345]  eta: 0:00:36  loss: 0.3069 (0.3093)  time: 0.1708  data: 0.0001  max mem: 15821
[18:52:27.458478] Test:  [140/345]  eta: 0:00:35  loss: 0.2976 (0.3085)  time: 0.1712  data: 0.0001  max mem: 15821
[18:52:29.176381] Test:  [150/345]  eta: 0:00:33  loss: 0.3036 (0.3090)  time: 0.1716  data: 0.0001  max mem: 15821
[18:52:30.897597] Test:  [160/345]  eta: 0:00:31  loss: 0.3037 (0.3083)  time: 0.1719  data: 0.0001  max mem: 15821
[18:52:32.623361] Test:  [170/345]  eta: 0:00:30  loss: 0.2994 (0.3090)  time: 0.1723  data: 0.0001  max mem: 15821
[18:52:34.351074] Test:  [180/345]  eta: 0:00:28  loss: 0.2955 (0.3082)  time: 0.1726  data: 0.0001  max mem: 15821
[18:52:36.081906] Test:  [190/345]  eta: 0:00:26  loss: 0.2955 (0.3082)  time: 0.1729  data: 0.0001  max mem: 15821
[18:52:37.817075] Test:  [200/345]  eta: 0:00:24  loss: 0.2979 (0.3077)  time: 0.1732  data: 0.0001  max mem: 15821
[18:52:39.555404] Test:  [210/345]  eta: 0:00:23  loss: 0.3064 (0.3080)  time: 0.1736  data: 0.0001  max mem: 15821
[18:52:41.297243] Test:  [220/345]  eta: 0:00:21  loss: 0.3144 (0.3086)  time: 0.1739  data: 0.0001  max mem: 15821
[18:52:43.043023] Test:  [230/345]  eta: 0:00:19  loss: 0.3012 (0.3080)  time: 0.1743  data: 0.0001  max mem: 15821
[18:52:44.790383] Test:  [240/345]  eta: 0:00:18  loss: 0.2958 (0.3077)  time: 0.1746  data: 0.0001  max mem: 15821
[18:52:46.544046] Test:  [250/345]  eta: 0:00:16  loss: 0.2953 (0.3072)  time: 0.1750  data: 0.0001  max mem: 15821
[18:52:48.299081] Test:  [260/345]  eta: 0:00:14  loss: 0.2994 (0.3075)  time: 0.1754  data: 0.0001  max mem: 15821
[18:52:50.056976] Test:  [270/345]  eta: 0:00:12  loss: 0.3103 (0.3081)  time: 0.1756  data: 0.0001  max mem: 15821
[18:52:51.820379] Test:  [280/345]  eta: 0:00:11  loss: 0.3103 (0.3078)  time: 0.1760  data: 0.0001  max mem: 15821
[18:52:53.585349] Test:  [290/345]  eta: 0:00:09  loss: 0.2921 (0.3074)  time: 0.1764  data: 0.0001  max mem: 15821
[18:52:55.353584] Test:  [300/345]  eta: 0:00:07  loss: 0.3027 (0.3079)  time: 0.1766  data: 0.0001  max mem: 15821
[18:52:57.128125] Test:  [310/345]  eta: 0:00:06  loss: 0.2950 (0.3074)  time: 0.1771  data: 0.0001  max mem: 15821
[18:52:58.904690] Test:  [320/345]  eta: 0:00:04  loss: 0.3018 (0.3076)  time: 0.1775  data: 0.0001  max mem: 15821
[18:53:00.685978] Test:  [330/345]  eta: 0:00:02  loss: 0.3070 (0.3073)  time: 0.1778  data: 0.0001  max mem: 15821
[18:53:02.469188] Test:  [340/345]  eta: 0:00:00  loss: 0.3053 (0.3073)  time: 0.1781  data: 0.0001  max mem: 15821
[18:53:03.185057] Test:  [344/345]  eta: 0:00:00  loss: 0.3037 (0.3070)  time: 0.1784  data: 0.0001  max mem: 15821
[18:53:03.259205] Test: Total time: 0:01:00 (0.1740 s / it)
[18:53:13.165975] Test:  [ 0/57]  eta: 0:00:26  loss: 0.4638 (0.4638)  time: 0.4647  data: 0.3018  max mem: 15821
[18:53:14.818046] Test:  [10/57]  eta: 0:00:09  loss: 0.3919 (0.4144)  time: 0.1923  data: 0.0275  max mem: 15821
[18:53:16.475452] Test:  [20/57]  eta: 0:00:06  loss: 0.3919 (0.4055)  time: 0.1654  data: 0.0001  max mem: 15821
[18:53:18.136026] Test:  [30/57]  eta: 0:00:04  loss: 0.3127 (0.3629)  time: 0.1658  data: 0.0001  max mem: 15821
[18:53:19.801045] Test:  [40/57]  eta: 0:00:02  loss: 0.2902 (0.3489)  time: 0.1662  data: 0.0001  max mem: 15821
[18:53:21.469213] Test:  [50/57]  eta: 0:00:01  loss: 0.3227 (0.3497)  time: 0.1666  data: 0.0001  max mem: 15821
[18:53:22.368509] Test:  [56/57]  eta: 0:00:00  loss: 0.3381 (0.3538)  time: 0.1616  data: 0.0000  max mem: 15821
[18:53:22.432955] Test: Total time: 0:00:09 (0.1707 s / it)
[18:53:24.157205] Dice score of the network on the train images: 0.766598, val images: 0.763389
[18:53:24.157435] saving best_prec_model_0 @ epoch 6
[18:53:25.377990] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[18:53:26.275818] Epoch: [7]  [  0/345]  eta: 0:05:09  lr: 0.000044  loss: 0.3467 (0.3467)  time: 0.8970  data: 0.2962  max mem: 15821
[18:53:38.402416] Epoch: [7]  [ 20/345]  eta: 0:03:21  lr: 0.000044  loss: 0.3052 (0.3111)  time: 0.6063  data: 0.0001  max mem: 15821
[18:53:50.455399] Epoch: [7]  [ 40/345]  eta: 0:03:06  lr: 0.000044  loss: 0.3140 (0.3162)  time: 0.6026  data: 0.0001  max mem: 15821
[18:54:02.515087] Epoch: [7]  [ 60/345]  eta: 0:02:53  lr: 0.000045  loss: 0.3007 (0.3112)  time: 0.6029  data: 0.0001  max mem: 15821
[18:54:14.595174] Epoch: [7]  [ 80/345]  eta: 0:02:41  lr: 0.000045  loss: 0.3141 (0.3114)  time: 0.6040  data: 0.0001  max mem: 15821
[18:54:26.686687] Epoch: [7]  [100/345]  eta: 0:02:28  lr: 0.000046  loss: 0.2958 (0.3084)  time: 0.6045  data: 0.0001  max mem: 15821
[18:54:38.794782] Epoch: [7]  [120/345]  eta: 0:02:16  lr: 0.000046  loss: 0.2989 (0.3074)  time: 0.6054  data: 0.0001  max mem: 15821
[18:54:50.915446] Epoch: [7]  [140/345]  eta: 0:02:04  lr: 0.000046  loss: 0.2979 (0.3073)  time: 0.6060  data: 0.0001  max mem: 15821
[18:55:03.041777] Epoch: [7]  [160/345]  eta: 0:01:52  lr: 0.000047  loss: 0.3009 (0.3068)  time: 0.6063  data: 0.0001  max mem: 15821
[18:55:15.174423] Epoch: [7]  [180/345]  eta: 0:01:40  lr: 0.000047  loss: 0.2896 (0.3056)  time: 0.6066  data: 0.0001  max mem: 15821
[18:55:27.305580] Epoch: [7]  [200/345]  eta: 0:01:27  lr: 0.000047  loss: 0.2806 (0.3050)  time: 0.6065  data: 0.0001  max mem: 15821
[18:55:39.435409] Epoch: [7]  [220/345]  eta: 0:01:15  lr: 0.000048  loss: 0.3004 (0.3052)  time: 0.6064  data: 0.0001  max mem: 15821
[18:55:51.570061] Epoch: [7]  [240/345]  eta: 0:01:03  lr: 0.000048  loss: 0.3140 (0.3054)  time: 0.6067  data: 0.0001  max mem: 15821
[18:56:03.775178] Epoch: [7]  [260/345]  eta: 0:00:51  lr: 0.000048  loss: 0.3006 (0.3052)  time: 0.6102  data: 0.0001  max mem: 15821
[18:56:15.901905] Epoch: [7]  [280/345]  eta: 0:00:39  lr: 0.000049  loss: 0.3269 (0.3060)  time: 0.6063  data: 0.0001  max mem: 15821
[18:56:27.999617] Epoch: [7]  [300/345]  eta: 0:00:27  lr: 0.000049  loss: 0.3086 (0.3059)  time: 0.6048  data: 0.0001  max mem: 15821
[18:56:40.098576] Epoch: [7]  [320/345]  eta: 0:00:15  lr: 0.000050  loss: 0.2939 (0.3052)  time: 0.6049  data: 0.0001  max mem: 15821
[18:56:52.191307] Epoch: [7]  [340/345]  eta: 0:00:03  lr: 0.000050  loss: 0.2816 (0.3046)  time: 0.6046  data: 0.0001  max mem: 15821
[18:56:54.612016] Epoch: [7]  [344/345]  eta: 0:00:00  lr: 0.000050  loss: 0.3053 (0.3046)  time: 0.6047  data: 0.0001  max mem: 15821
[18:56:54.679695] Epoch: [7] Total time: 0:03:29 (0.6067 s / it)
[18:56:54.680142] Averaged stats: lr: 0.000050  loss: 0.3053 (0.3046)
[18:56:55.155638] Test:  [  0/345]  eta: 0:02:42  loss: 0.3057 (0.3057)  time: 0.4701  data: 0.3058  max mem: 15821
[18:56:56.824928] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2876 (0.2861)  time: 0.1944  data: 0.0279  max mem: 15821
[18:56:58.497476] Test:  [ 20/345]  eta: 0:00:58  loss: 0.2867 (0.2953)  time: 0.1670  data: 0.0001  max mem: 15821
[18:57:00.174269] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2810 (0.2911)  time: 0.1674  data: 0.0001  max mem: 15821
[18:57:01.854069] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2830 (0.2918)  time: 0.1678  data: 0.0001  max mem: 15821
[18:57:03.536052] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2875 (0.2892)  time: 0.1680  data: 0.0001  max mem: 15821
[18:57:05.222414] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2883 (0.2903)  time: 0.1684  data: 0.0001  max mem: 15821
[18:57:06.911883] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2780 (0.2900)  time: 0.1687  data: 0.0001  max mem: 15821
[18:57:08.605789] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2780 (0.2895)  time: 0.1691  data: 0.0001  max mem: 15821
[18:57:10.303544] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2877 (0.2901)  time: 0.1695  data: 0.0001  max mem: 15821
[18:57:12.004582] Test:  [100/345]  eta: 0:00:41  loss: 0.3032 (0.2911)  time: 0.1699  data: 0.0001  max mem: 15821
[18:57:13.708625] Test:  [110/345]  eta: 0:00:40  loss: 0.3013 (0.2913)  time: 0.1702  data: 0.0001  max mem: 15821
[18:57:15.415897] Test:  [120/345]  eta: 0:00:38  loss: 0.3013 (0.2932)  time: 0.1705  data: 0.0001  max mem: 15821
[18:57:17.126096] Test:  [130/345]  eta: 0:00:36  loss: 0.3013 (0.2930)  time: 0.1708  data: 0.0001  max mem: 15821
[18:57:18.840451] Test:  [140/345]  eta: 0:00:35  loss: 0.2889 (0.2928)  time: 0.1712  data: 0.0001  max mem: 15821
[18:57:20.558352] Test:  [150/345]  eta: 0:00:33  loss: 0.2793 (0.2924)  time: 0.1715  data: 0.0001  max mem: 15821
[18:57:22.279756] Test:  [160/345]  eta: 0:00:31  loss: 0.2842 (0.2928)  time: 0.1719  data: 0.0001  max mem: 15821
[18:57:24.003930] Test:  [170/345]  eta: 0:00:29  loss: 0.2893 (0.2932)  time: 0.1722  data: 0.0001  max mem: 15821
[18:57:25.733695] Test:  [180/345]  eta: 0:00:28  loss: 0.2933 (0.2937)  time: 0.1726  data: 0.0001  max mem: 15821
[18:57:27.465771] Test:  [190/345]  eta: 0:00:26  loss: 0.3036 (0.2945)  time: 0.1730  data: 0.0001  max mem: 15821
[18:57:29.200388] Test:  [200/345]  eta: 0:00:24  loss: 0.2959 (0.2937)  time: 0.1733  data: 0.0001  max mem: 15821
[18:57:30.939091] Test:  [210/345]  eta: 0:00:23  loss: 0.2831 (0.2935)  time: 0.1736  data: 0.0001  max mem: 15821
[18:57:32.679830] Test:  [220/345]  eta: 0:00:21  loss: 0.2826 (0.2931)  time: 0.1739  data: 0.0001  max mem: 15821
[18:57:34.425791] Test:  [230/345]  eta: 0:00:19  loss: 0.2826 (0.2932)  time: 0.1743  data: 0.0001  max mem: 15821
[18:57:36.175293] Test:  [240/345]  eta: 0:00:18  loss: 0.2998 (0.2939)  time: 0.1747  data: 0.0001  max mem: 15821
[18:57:37.927801] Test:  [250/345]  eta: 0:00:16  loss: 0.2901 (0.2936)  time: 0.1750  data: 0.0001  max mem: 15821
[18:57:39.682845] Test:  [260/345]  eta: 0:00:14  loss: 0.2878 (0.2935)  time: 0.1753  data: 0.0001  max mem: 15821
[18:57:41.441824] Test:  [270/345]  eta: 0:00:12  loss: 0.2777 (0.2930)  time: 0.1756  data: 0.0001  max mem: 15821
[18:57:43.205399] Test:  [280/345]  eta: 0:00:11  loss: 0.2882 (0.2929)  time: 0.1761  data: 0.0001  max mem: 15821
[18:57:44.970271] Test:  [290/345]  eta: 0:00:09  loss: 0.2955 (0.2930)  time: 0.1764  data: 0.0001  max mem: 15821
[18:57:46.739089] Test:  [300/345]  eta: 0:00:07  loss: 0.2923 (0.2927)  time: 0.1766  data: 0.0001  max mem: 15821
[18:57:48.513436] Test:  [310/345]  eta: 0:00:06  loss: 0.2928 (0.2934)  time: 0.1771  data: 0.0001  max mem: 15821
[18:57:50.291719] Test:  [320/345]  eta: 0:00:04  loss: 0.2900 (0.2932)  time: 0.1776  data: 0.0001  max mem: 15821
[18:57:52.071793] Test:  [330/345]  eta: 0:00:02  loss: 0.2778 (0.2931)  time: 0.1779  data: 0.0001  max mem: 15821
[18:57:53.856187] Test:  [340/345]  eta: 0:00:00  loss: 0.2897 (0.2935)  time: 0.1782  data: 0.0001  max mem: 15821
[18:57:54.571634] Test:  [344/345]  eta: 0:00:00  loss: 0.2849 (0.2934)  time: 0.1784  data: 0.0001  max mem: 15821
[18:57:54.643016] Test: Total time: 0:00:59 (0.1738 s / it)
[18:58:04.661344] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4689 (0.4689)  time: 0.4517  data: 0.2888  max mem: 15821
[18:58:06.313511] Test:  [10/57]  eta: 0:00:08  loss: 0.3934 (0.4185)  time: 0.1912  data: 0.0263  max mem: 15821
[18:58:07.970337] Test:  [20/57]  eta: 0:00:06  loss: 0.3934 (0.4087)  time: 0.1654  data: 0.0001  max mem: 15821
[18:58:09.631357] Test:  [30/57]  eta: 0:00:04  loss: 0.3180 (0.3642)  time: 0.1658  data: 0.0001  max mem: 15821
[18:58:11.295931] Test:  [40/57]  eta: 0:00:02  loss: 0.2874 (0.3481)  time: 0.1662  data: 0.0001  max mem: 15821
[18:58:12.963680] Test:  [50/57]  eta: 0:00:01  loss: 0.3201 (0.3477)  time: 0.1666  data: 0.0001  max mem: 15821
[18:58:13.865028] Test:  [56/57]  eta: 0:00:00  loss: 0.3242 (0.3495)  time: 0.1617  data: 0.0001  max mem: 15821
[18:58:13.931644] Test: Total time: 0:00:09 (0.1706 s / it)
[18:58:15.668020] Dice score of the network on the train images: 0.774019, val images: 0.775523
[18:58:15.672150] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[18:58:16.587893] Epoch: [8]  [  0/345]  eta: 0:05:15  lr: 0.000050  loss: 0.2956 (0.2956)  time: 0.9147  data: 0.3114  max mem: 15821
[18:58:28.598505] Epoch: [8]  [ 20/345]  eta: 0:03:20  lr: 0.000050  loss: 0.2852 (0.2950)  time: 0.6005  data: 0.0001  max mem: 15821

[18:58:40.654971] Epoch: [8]  [ 40/345]  eta: 0:03:05  lr: 0.000051  loss: 0.2840 (0.2900)  time: 0.6028  data: 0.0001  max mem: 15821
[18:58:52.718515] Epoch: [8]  [ 60/345]  eta: 0:02:53  lr: 0.000051  loss: 0.2951 (0.2916)  time: 0.6031  data: 0.0001  max mem: 15821
[18:59:04.790112] Epoch: [8]  [ 80/345]  eta: 0:02:40  lr: 0.000051  loss: 0.2722 (0.2880)  time: 0.6035  data: 0.0001  max mem: 15821
[18:59:16.878509] Epoch: [8]  [100/345]  eta: 0:02:28  lr: 0.000052  loss: 0.2914 (0.2888)  time: 0.6044  data: 0.0001  max mem: 15821
[18:59:28.968030] Epoch: [8]  [120/345]  eta: 0:02:16  lr: 0.000052  loss: 0.2866 (0.2884)  time: 0.6044  data: 0.0001  max mem: 15821
[18:59:41.068653] Epoch: [8]  [140/345]  eta: 0:02:04  lr: 0.000053  loss: 0.2754 (0.2875)  time: 0.6050  data: 0.0001  max mem: 15821
[18:59:53.173925] Epoch: [8]  [160/345]  eta: 0:01:52  lr: 0.000053  loss: 0.2673 (0.2865)  time: 0.6052  data: 0.0001  max mem: 15821
[19:00:05.405065] Epoch: [8]  [180/345]  eta: 0:01:40  lr: 0.000053  loss: 0.2689 (0.2852)  time: 0.6115  data: 0.0001  max mem: 15821
[19:00:17.525985] Epoch: [8]  [200/345]  eta: 0:01:27  lr: 0.000054  loss: 0.2636 (0.2840)  time: 0.6060  data: 0.0001  max mem: 15821
[19:00:29.660184] Epoch: [8]  [220/345]  eta: 0:01:15  lr: 0.000054  loss: 0.2763 (0.2834)  time: 0.6067  data: 0.0001  max mem: 15821
[19:00:41.783820] Epoch: [8]  [240/345]  eta: 0:01:03  lr: 0.000054  loss: 0.2667 (0.2823)  time: 0.6061  data: 0.0001  max mem: 15821
[19:00:53.904118] Epoch: [8]  [260/345]  eta: 0:00:51  lr: 0.000055  loss: 0.2940 (0.2832)  time: 0.6060  data: 0.0001  max mem: 15821
[19:01:06.023217] Epoch: [8]  [280/345]  eta: 0:00:39  lr: 0.000055  loss: 0.2801 (0.2837)  time: 0.6059  data: 0.0001  max mem: 15821
[19:01:18.124028] Epoch: [8]  [300/345]  eta: 0:00:27  lr: 0.000055  loss: 0.2841 (0.2843)  time: 0.6050  data: 0.0001  max mem: 15821
[19:01:30.215259] Epoch: [8]  [320/345]  eta: 0:00:15  lr: 0.000056  loss: 0.2727 (0.2839)  time: 0.6045  data: 0.0001  max mem: 15821
[19:01:42.313638] Epoch: [8]  [340/345]  eta: 0:00:03  lr: 0.000056  loss: 0.2852 (0.2842)  time: 0.6049  data: 0.0001  max mem: 15821
[19:01:44.730761] Epoch: [8]  [344/345]  eta: 0:00:00  lr: 0.000056  loss: 0.2633 (0.2839)  time: 0.6046  data: 0.0001  max mem: 15821
[19:01:44.813876] Epoch: [8] Total time: 0:03:29 (0.6062 s / it)
[19:01:44.814107] Averaged stats: lr: 0.000056  loss: 0.2633 (0.2839)
[19:01:45.356951] Test:  [  0/345]  eta: 0:03:05  loss: 0.2939 (0.2939)  time: 0.5374  data: 0.3734  max mem: 15821
[19:01:47.027196] Test:  [ 10/345]  eta: 0:01:07  loss: 0.2714 (0.2651)  time: 0.2006  data: 0.0340  max mem: 15821
[19:01:48.698805] Test:  [ 20/345]  eta: 0:01:00  loss: 0.2590 (0.2674)  time: 0.1670  data: 0.0001  max mem: 15821
[19:01:50.375009] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2590 (0.2683)  time: 0.1673  data: 0.0001  max mem: 15821
[19:01:52.053400] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2774 (0.2723)  time: 0.1677  data: 0.0001  max mem: 15821
[19:01:53.735024] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2813 (0.2728)  time: 0.1679  data: 0.0001  max mem: 15821
[19:01:55.420608] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2784 (0.2722)  time: 0.1683  data: 0.0001  max mem: 15821
[19:01:57.109798] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2742 (0.2721)  time: 0.1687  data: 0.0001  max mem: 15821
[19:01:58.803523] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2735 (0.2717)  time: 0.1691  data: 0.0001  max mem: 15821
[19:02:00.500739] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2775 (0.2724)  time: 0.1695  data: 0.0001  max mem: 15821
[19:02:02.201204] Test:  [100/345]  eta: 0:00:42  loss: 0.2855 (0.2745)  time: 0.1698  data: 0.0001  max mem: 15821
[19:02:03.904947] Test:  [110/345]  eta: 0:00:40  loss: 0.2902 (0.2758)  time: 0.1701  data: 0.0001  max mem: 15821
[19:02:05.612025] Test:  [120/345]  eta: 0:00:38  loss: 0.2753 (0.2754)  time: 0.1705  data: 0.0001  max mem: 15821
[19:02:07.322246] Test:  [130/345]  eta: 0:00:36  loss: 0.2734 (0.2744)  time: 0.1708  data: 0.0001  max mem: 15821
[19:02:09.036477] Test:  [140/345]  eta: 0:00:35  loss: 0.2661 (0.2733)  time: 0.1712  data: 0.0001  max mem: 15821
[19:02:10.753460] Test:  [150/345]  eta: 0:00:33  loss: 0.2652 (0.2736)  time: 0.1715  data: 0.0001  max mem: 15821
[19:02:12.473357] Test:  [160/345]  eta: 0:00:31  loss: 0.2652 (0.2733)  time: 0.1718  data: 0.0001  max mem: 15821
[19:02:14.197477] Test:  [170/345]  eta: 0:00:30  loss: 0.2659 (0.2730)  time: 0.1721  data: 0.0001  max mem: 15821
[19:02:15.925309] Test:  [180/345]  eta: 0:00:28  loss: 0.2659 (0.2730)  time: 0.1725  data: 0.0001  max mem: 15821
[19:02:17.657068] Test:  [190/345]  eta: 0:00:26  loss: 0.2823 (0.2738)  time: 0.1729  data: 0.0001  max mem: 15821
[19:02:19.390402] Test:  [200/345]  eta: 0:00:24  loss: 0.2823 (0.2738)  time: 0.1732  data: 0.0001  max mem: 15821
[19:02:21.127654] Test:  [210/345]  eta: 0:00:23  loss: 0.2765 (0.2743)  time: 0.1735  data: 0.0001  max mem: 15821
[19:02:22.868694] Test:  [220/345]  eta: 0:00:21  loss: 0.2727 (0.2742)  time: 0.1738  data: 0.0001  max mem: 15821
[19:02:24.614514] Test:  [230/345]  eta: 0:00:19  loss: 0.2656 (0.2742)  time: 0.1743  data: 0.0001  max mem: 15821
[19:02:26.362423] Test:  [240/345]  eta: 0:00:18  loss: 0.2656 (0.2739)  time: 0.1746  data: 0.0001  max mem: 15821
[19:02:28.114935] Test:  [250/345]  eta: 0:00:16  loss: 0.2649 (0.2744)  time: 0.1749  data: 0.0001  max mem: 15821
[19:02:29.868741] Test:  [260/345]  eta: 0:00:14  loss: 0.2719 (0.2749)  time: 0.1753  data: 0.0001  max mem: 15821
[19:02:31.626897] Test:  [270/345]  eta: 0:00:12  loss: 0.2719 (0.2749)  time: 0.1755  data: 0.0001  max mem: 15821
[19:02:33.389374] Test:  [280/345]  eta: 0:00:11  loss: 0.2771 (0.2753)  time: 0.1760  data: 0.0001  max mem: 15821
[19:02:35.155439] Test:  [290/345]  eta: 0:00:09  loss: 0.2848 (0.2751)  time: 0.1764  data: 0.0001  max mem: 15821
[19:02:36.922725] Test:  [300/345]  eta: 0:00:07  loss: 0.2789 (0.2752)  time: 0.1766  data: 0.0001  max mem: 15821
[19:02:38.696991] Test:  [310/345]  eta: 0:00:06  loss: 0.2713 (0.2747)  time: 0.1770  data: 0.0001  max mem: 15821
[19:02:40.472707] Test:  [320/345]  eta: 0:00:04  loss: 0.2738 (0.2746)  time: 0.1774  data: 0.0001  max mem: 15821
[19:02:42.253055] Test:  [330/345]  eta: 0:00:02  loss: 0.2765 (0.2747)  time: 0.1777  data: 0.0001  max mem: 15821
[19:02:44.036502] Test:  [340/345]  eta: 0:00:00  loss: 0.2765 (0.2747)  time: 0.1781  data: 0.0001  max mem: 15821
[19:02:44.749890] Test:  [344/345]  eta: 0:00:00  loss: 0.2803 (0.2750)  time: 0.1783  data: 0.0001  max mem: 15821
[19:02:44.822552] Test: Total time: 0:01:00 (0.1739 s / it)
[19:02:54.710361] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4587 (0.4587)  time: 0.4519  data: 0.2892  max mem: 15821
[19:02:56.362611] Test:  [10/57]  eta: 0:00:08  loss: 0.4216 (0.4348)  time: 0.1912  data: 0.0264  max mem: 15821
[19:02:58.019622] Test:  [20/57]  eta: 0:00:06  loss: 0.3789 (0.4074)  time: 0.1654  data: 0.0001  max mem: 15821
[19:02:59.679384] Test:  [30/57]  eta: 0:00:04  loss: 0.2737 (0.3530)  time: 0.1658  data: 0.0001  max mem: 15821
[19:03:01.343987] Test:  [40/57]  eta: 0:00:02  loss: 0.2409 (0.3287)  time: 0.1662  data: 0.0001  max mem: 15821
[19:03:03.012284] Test:  [50/57]  eta: 0:00:01  loss: 0.2670 (0.3283)  time: 0.1666  data: 0.0001  max mem: 15821
[19:03:03.912881] Test:  [56/57]  eta: 0:00:00  loss: 0.2818 (0.3327)  time: 0.1617  data: 0.0001  max mem: 15821
[19:03:03.979212] Test: Total time: 0:00:09 (0.1706 s / it)
[19:03:05.724676] Dice score of the network on the train images: 0.780171, val images: 0.804218
[19:03:05.724884] saving best_prec_model_0 @ epoch 8
[19:03:06.965686] saving best_rec_model_0 @ epoch 8
[19:03:08.224028] saving best_dice_model_0 @ epoch 8
[19:03:09.443827] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:03:10.341458] Epoch: [9]  [  0/345]  eta: 0:05:09  lr: 0.000056  loss: 0.2880 (0.2880)  time: 0.8965  data: 0.2952  max mem: 15821
[19:03:22.332662] Epoch: [9]  [ 20/345]  eta: 0:03:19  lr: 0.000057  loss: 0.2778 (0.2754)  time: 0.5995  data: 0.0001  max mem: 15821
[19:03:34.353168] Epoch: [9]  [ 40/345]  eta: 0:03:05  lr: 0.000057  loss: 0.2671 (0.2707)  time: 0.6010  data: 0.0001  max mem: 15821
[19:03:46.407561] Epoch: [9]  [ 60/345]  eta: 0:02:52  lr: 0.000057  loss: 0.2591 (0.2681)  time: 0.6027  data: 0.0001  max mem: 15821
[19:03:58.478740] Epoch: [9]  [ 80/345]  eta: 0:02:40  lr: 0.000058  loss: 0.2531 (0.2678)  time: 0.6035  data: 0.0001  max mem: 15821
[19:04:10.579524] Epoch: [9]  [100/345]  eta: 0:02:28  lr: 0.000058  loss: 0.2571 (0.2668)  time: 0.6050  data: 0.0001  max mem: 15821
[19:04:22.706025] Epoch: [9]  [120/345]  eta: 0:02:16  lr: 0.000058  loss: 0.2718 (0.2678)  time: 0.6063  data: 0.0001  max mem: 15821
[19:04:34.834341] Epoch: [9]  [140/345]  eta: 0:02:04  lr: 0.000059  loss: 0.2724 (0.2690)  time: 0.6064  data: 0.0001  max mem: 15821
[19:04:46.972998] Epoch: [9]  [160/345]  eta: 0:01:52  lr: 0.000059  loss: 0.2646 (0.2689)  time: 0.6069  data: 0.0001  max mem: 15821

[19:04:59.118307] Epoch: [9]  [180/345]  eta: 0:01:39  lr: 0.000060  loss: 0.2536 (0.2688)  time: 0.6072  data: 0.0001  max mem: 15821
[19:05:11.269819] Epoch: [9]  [200/345]  eta: 0:01:27  lr: 0.000060  loss: 0.2649 (0.2684)  time: 0.6075  data: 0.0001  max mem: 15821
[19:05:23.412798] Epoch: [9]  [220/345]  eta: 0:01:15  lr: 0.000060  loss: 0.2695 (0.2693)  time: 0.6071  data: 0.0001  max mem: 15821
[19:05:35.543418] Epoch: [9]  [240/345]  eta: 0:01:03  lr: 0.000061  loss: 0.2626 (0.2689)  time: 0.6065  data: 0.0001  max mem: 15821
[19:05:47.650695] Epoch: [9]  [260/345]  eta: 0:00:51  lr: 0.000061  loss: 0.2589 (0.2687)  time: 0.6053  data: 0.0001  max mem: 15821
[19:05:59.746992] Epoch: [9]  [280/345]  eta: 0:00:39  lr: 0.000061  loss: 0.2639 (0.2685)  time: 0.6048  data: 0.0001  max mem: 15821
[19:06:11.842225] Epoch: [9]  [300/345]  eta: 0:00:27  lr: 0.000062  loss: 0.2701 (0.2686)  time: 0.6047  data: 0.0001  max mem: 15821
[19:06:23.930350] Epoch: [9]  [320/345]  eta: 0:00:15  lr: 0.000062  loss: 0.2631 (0.2685)  time: 0.6044  data: 0.0001  max mem: 15821
[19:06:36.014279] Epoch: [9]  [340/345]  eta: 0:00:03  lr: 0.000062  loss: 0.2583 (0.2682)  time: 0.6042  data: 0.0001  max mem: 15821
[19:06:38.431478] Epoch: [9]  [344/345]  eta: 0:00:00  lr: 0.000062  loss: 0.2529 (0.2680)  time: 0.6043  data: 0.0001  max mem: 15821
[19:06:38.499666] Epoch: [9] Total time: 0:03:29 (0.6060 s / it)
[19:06:38.499878] Averaged stats: lr: 0.000062  loss: 0.2529 (0.2680)
[19:06:39.043200] Test:  [  0/345]  eta: 0:03:05  loss: 0.2895 (0.2895)  time: 0.5380  data: 0.3744  max mem: 15821
[19:06:40.712558] Test:  [ 10/345]  eta: 0:01:07  loss: 0.2667 (0.2650)  time: 0.2006  data: 0.0341  max mem: 15821
[19:06:42.385199] Test:  [ 20/345]  eta: 0:01:00  loss: 0.2596 (0.2594)  time: 0.1670  data: 0.0001  max mem: 15821
[19:06:44.060899] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2544 (0.2578)  time: 0.1673  data: 0.0001  max mem: 15821
[19:06:45.740549] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2593 (0.2615)  time: 0.1677  data: 0.0001  max mem: 15821
[19:06:47.423168] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2623 (0.2615)  time: 0.1680  data: 0.0001  max mem: 15821
[19:06:49.109046] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2507 (0.2606)  time: 0.1684  data: 0.0001  max mem: 15821
[19:06:50.799360] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2526 (0.2621)  time: 0.1687  data: 0.0001  max mem: 15821
[19:06:52.493103] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2657 (0.2619)  time: 0.1691  data: 0.0001  max mem: 15821
[19:06:54.189463] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2530 (0.2618)  time: 0.1694  data: 0.0001  max mem: 15821
[19:06:55.889810] Test:  [100/345]  eta: 0:00:42  loss: 0.2530 (0.2614)  time: 0.1698  data: 0.0001  max mem: 15821
[19:06:57.593448] Test:  [110/345]  eta: 0:00:40  loss: 0.2452 (0.2600)  time: 0.1701  data: 0.0001  max mem: 15821
[19:06:59.300245] Test:  [120/345]  eta: 0:00:38  loss: 0.2562 (0.2605)  time: 0.1705  data: 0.0001  max mem: 15821
[19:07:01.010903] Test:  [130/345]  eta: 0:00:36  loss: 0.2639 (0.2608)  time: 0.1708  data: 0.0001  max mem: 15821
[19:07:02.724949] Test:  [140/345]  eta: 0:00:35  loss: 0.2639 (0.2613)  time: 0.1712  data: 0.0001  max mem: 15821
[19:07:04.442283] Test:  [150/345]  eta: 0:00:33  loss: 0.2536 (0.2595)  time: 0.1715  data: 0.0001  max mem: 15821
[19:07:06.163445] Test:  [160/345]  eta: 0:00:31  loss: 0.2368 (0.2596)  time: 0.1719  data: 0.0001  max mem: 15821
[19:07:07.887995] Test:  [170/345]  eta: 0:00:30  loss: 0.2494 (0.2590)  time: 0.1722  data: 0.0001  max mem: 15821
[19:07:09.616941] Test:  [180/345]  eta: 0:00:28  loss: 0.2511 (0.2587)  time: 0.1726  data: 0.0001  max mem: 15821
[19:07:11.348953] Test:  [190/345]  eta: 0:00:26  loss: 0.2544 (0.2582)  time: 0.1730  data: 0.0001  max mem: 15821
[19:07:13.084138] Test:  [200/345]  eta: 0:00:24  loss: 0.2612 (0.2587)  time: 0.1733  data: 0.0001  max mem: 15821
[19:07:14.822707] Test:  [210/345]  eta: 0:00:23  loss: 0.2660 (0.2588)  time: 0.1736  data: 0.0001  max mem: 15821
[19:07:16.564605] Test:  [220/345]  eta: 0:00:21  loss: 0.2655 (0.2589)  time: 0.1740  data: 0.0001  max mem: 15821
[19:07:18.309826] Test:  [230/345]  eta: 0:00:19  loss: 0.2655 (0.2586)  time: 0.1743  data: 0.0001  max mem: 15821
[19:07:20.058418] Test:  [240/345]  eta: 0:00:18  loss: 0.2657 (0.2592)  time: 0.1746  data: 0.0001  max mem: 15821
[19:07:21.810554] Test:  [250/345]  eta: 0:00:16  loss: 0.2671 (0.2597)  time: 0.1750  data: 0.0001  max mem: 15821
[19:07:23.567190] Test:  [260/345]  eta: 0:00:14  loss: 0.2728 (0.2605)  time: 0.1754  data: 0.0001  max mem: 15821
[19:07:25.326809] Test:  [270/345]  eta: 0:00:12  loss: 0.2722 (0.2606)  time: 0.1757  data: 0.0001  max mem: 15821
[19:07:27.088377] Test:  [280/345]  eta: 0:00:11  loss: 0.2583 (0.2608)  time: 0.1760  data: 0.0001  max mem: 15821
[19:07:28.853731] Test:  [290/345]  eta: 0:00:09  loss: 0.2692 (0.2609)  time: 0.1763  data: 0.0001  max mem: 15821
[19:07:30.624120] Test:  [300/345]  eta: 0:00:07  loss: 0.2612 (0.2612)  time: 0.1767  data: 0.0001  max mem: 15821
[19:07:32.397781] Test:  [310/345]  eta: 0:00:06  loss: 0.2612 (0.2612)  time: 0.1771  data: 0.0001  max mem: 15821
[19:07:34.173441] Test:  [320/345]  eta: 0:00:04  loss: 0.2545 (0.2610)  time: 0.1774  data: 0.0001  max mem: 15821
[19:07:35.955396] Test:  [330/345]  eta: 0:00:02  loss: 0.2545 (0.2612)  time: 0.1778  data: 0.0001  max mem: 15821
[19:07:37.739989] Test:  [340/345]  eta: 0:00:00  loss: 0.2480 (0.2608)  time: 0.1783  data: 0.0001  max mem: 15821
[19:07:38.453491] Test:  [344/345]  eta: 0:00:00  loss: 0.2480 (0.2611)  time: 0.1783  data: 0.0001  max mem: 15821
[19:07:38.519312] Test: Total time: 0:01:00 (0.1740 s / it)
[19:07:48.516286] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4648 (0.4648)  time: 0.4523  data: 0.2900  max mem: 15821
[19:07:50.168874] Test:  [10/57]  eta: 0:00:08  loss: 0.4065 (0.4452)  time: 0.1913  data: 0.0264  max mem: 15821
[19:07:51.825359] Test:  [20/57]  eta: 0:00:06  loss: 0.4065 (0.4291)  time: 0.1654  data: 0.0001  max mem: 15821
[19:07:53.485998] Test:  [30/57]  eta: 0:00:04  loss: 0.2904 (0.3748)  time: 0.1658  data: 0.0001  max mem: 15821
[19:07:55.150496] Test:  [40/57]  eta: 0:00:02  loss: 0.2838 (0.3557)  time: 0.1662  data: 0.0001  max mem: 15821
[19:07:56.819587] Test:  [50/57]  eta: 0:00:01  loss: 0.2958 (0.3549)  time: 0.1666  data: 0.0001  max mem: 15821
[19:07:57.718260] Test:  [56/57]  eta: 0:00:00  loss: 0.3088 (0.3592)  time: 0.1617  data: 0.0001  max mem: 15821
[19:07:57.786744] Test: Total time: 0:00:09 (0.1706 s / it)
[19:07:59.518467] Dice score of the network on the train images: 0.804153, val images: 0.796064
[19:07:59.518691] saving best_prec_model_0 @ epoch 9
[19:08:00.735311] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:08:01.643926] Epoch: [10]  [  0/345]  eta: 0:05:13  lr: 0.000063  loss: 0.2240 (0.2240)  time: 0.9074  data: 0.3063  max mem: 15821
[19:08:13.634252] Epoch: [10]  [ 20/345]  eta: 0:03:19  lr: 0.000063  loss: 0.2482 (0.2532)  time: 0.5995  data: 0.0001  max mem: 15821
[19:08:25.671579] Epoch: [10]  [ 40/345]  eta: 0:03:05  lr: 0.000063  loss: 0.2496 (0.2508)  time: 0.6018  data: 0.0001  max mem: 15821
[19:08:37.731818] Epoch: [10]  [ 60/345]  eta: 0:02:52  lr: 0.000064  loss: 0.2469 (0.2498)  time: 0.6030  data: 0.0001  max mem: 15821
[19:08:49.806480] Epoch: [10]  [ 80/345]  eta: 0:02:40  lr: 0.000064  loss: 0.2565 (0.2536)  time: 0.6037  data: 0.0001  max mem: 15821
[19:09:01.902195] Epoch: [10]  [100/345]  eta: 0:02:28  lr: 0.000064  loss: 0.2407 (0.2524)  time: 0.6047  data: 0.0001  max mem: 15821
[19:09:14.006385] Epoch: [10]  [120/345]  eta: 0:02:16  lr: 0.000065  loss: 0.2479 (0.2517)  time: 0.6052  data: 0.0001  max mem: 15821
[19:09:26.130742] Epoch: [10]  [140/345]  eta: 0:02:04  lr: 0.000065  loss: 0.2661 (0.2539)  time: 0.6062  data: 0.0001  max mem: 15821
[19:09:38.257484] Epoch: [10]  [160/345]  eta: 0:01:52  lr: 0.000065  loss: 0.2599 (0.2548)  time: 0.6063  data: 0.0001  max mem: 15821
[19:09:50.392277] Epoch: [10]  [180/345]  eta: 0:01:39  lr: 0.000066  loss: 0.2686 (0.2559)  time: 0.6067  data: 0.0001  max mem: 15821
[19:10:02.528500] Epoch: [10]  [200/345]  eta: 0:01:27  lr: 0.000066  loss: 0.2482 (0.2555)  time: 0.6068  data: 0.0001  max mem: 15821
[19:10:14.662761] Epoch: [10]  [220/345]  eta: 0:01:15  lr: 0.000066  loss: 0.2332 (0.2543)  time: 0.6067  data: 0.0001  max mem: 15821
[19:10:26.791105] Epoch: [10]  [240/345]  eta: 0:01:03  lr: 0.000067  loss: 0.2477 (0.2542)  time: 0.6064  data: 0.0001  max mem: 15821
[19:10:38.906239] Epoch: [10]  [260/345]  eta: 0:00:51  lr: 0.000067  loss: 0.2766 (0.2551)  time: 0.6057  data: 0.0001  max mem: 15821
[19:10:51.000702] Epoch: [10]  [280/345]  eta: 0:00:39  lr: 0.000068  loss: 0.2591 (0.2553)  time: 0.6047  data: 0.0001  max mem: 15821
[19:11:03.174070] Epoch: [10]  [300/345]  eta: 0:00:27  lr: 0.000068  loss: 0.2624 (0.2561)  time: 0.6086  data: 0.0001  max mem: 15821
[19:11:15.258387] Epoch: [10]  [320/345]  eta: 0:00:15  lr: 0.000068  loss: 0.2500 (0.2551)  time: 0.6042  data: 0.0001  max mem: 15821
[19:11:27.348147] Epoch: [10]  [340/345]  eta: 0:00:03  lr: 0.000069  loss: 0.2491 (0.2550)  time: 0.6044  data: 0.0001  max mem: 15821
[19:11:29.766600] Epoch: [10]  [344/345]  eta: 0:00:00  lr: 0.000069  loss: 0.2475 (0.2545)  time: 0.6045  data: 0.0001  max mem: 15821
[19:11:29.838275] Epoch: [10] Total time: 0:03:29 (0.6061 s / it)
[19:11:29.838797] Averaged stats: lr: 0.000069  loss: 0.2475 (0.2545)
[19:11:30.338740] Test:  [  0/345]  eta: 0:02:50  loss: 0.2579 (0.2579)  time: 0.4946  data: 0.3299  max mem: 15821
[19:11:32.008927] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2365 (0.2327)  time: 0.1967  data: 0.0301  max mem: 15821
[19:11:33.681168] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2354 (0.2369)  time: 0.1670  data: 0.0001  max mem: 15821
[19:11:35.357746] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2351 (0.2375)  time: 0.1674  data: 0.0001  max mem: 15821
[19:11:37.036975] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2370 (0.2394)  time: 0.1677  data: 0.0001  max mem: 15821
[19:11:38.718475] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2405 (0.2424)  time: 0.1680  data: 0.0001  max mem: 15821
[19:11:40.403646] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2309 (0.2413)  time: 0.1683  data: 0.0001  max mem: 15821
[19:11:42.093898] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2286 (0.2422)  time: 0.1687  data: 0.0001  max mem: 15821
[19:11:43.786783] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2377 (0.2411)  time: 0.1691  data: 0.0001  max mem: 15821
[19:11:45.483450] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2377 (0.2414)  time: 0.1694  data: 0.0001  max mem: 15821
[19:11:47.183597] Test:  [100/345]  eta: 0:00:42  loss: 0.2446 (0.2412)  time: 0.1698  data: 0.0001  max mem: 15821
[19:11:48.886976] Test:  [110/345]  eta: 0:00:40  loss: 0.2240 (0.2402)  time: 0.1701  data: 0.0001  max mem: 15821
[19:11:50.593536] Test:  [120/345]  eta: 0:00:38  loss: 0.2332 (0.2411)  time: 0.1704  data: 0.0001  max mem: 15821
[19:11:52.303141] Test:  [130/345]  eta: 0:00:36  loss: 0.2413 (0.2410)  time: 0.1707  data: 0.0001  max mem: 15821
[19:11:54.016915] Test:  [140/345]  eta: 0:00:35  loss: 0.2324 (0.2405)  time: 0.1711  data: 0.0001  max mem: 15821
[19:11:55.733605] Test:  [150/345]  eta: 0:00:33  loss: 0.2334 (0.2404)  time: 0.1715  data: 0.0001  max mem: 15821
[19:11:57.454861] Test:  [160/345]  eta: 0:00:31  loss: 0.2364 (0.2399)  time: 0.1718  data: 0.0001  max mem: 15821
[19:11:59.179794] Test:  [170/345]  eta: 0:00:30  loss: 0.2301 (0.2394)  time: 0.1722  data: 0.0001  max mem: 15821
[19:12:00.907592] Test:  [180/345]  eta: 0:00:28  loss: 0.2299 (0.2394)  time: 0.1726  data: 0.0001  max mem: 15821
[19:12:02.638320] Test:  [190/345]  eta: 0:00:26  loss: 0.2247 (0.2386)  time: 0.1729  data: 0.0001  max mem: 15821
[19:12:04.373449] Test:  [200/345]  eta: 0:00:24  loss: 0.2408 (0.2393)  time: 0.1732  data: 0.0001  max mem: 15821
[19:12:06.110721] Test:  [210/345]  eta: 0:00:23  loss: 0.2453 (0.2397)  time: 0.1736  data: 0.0001  max mem: 15821
[19:12:07.851523] Test:  [220/345]  eta: 0:00:21  loss: 0.2385 (0.2398)  time: 0.1738  data: 0.0001  max mem: 15821
[19:12:09.596374] Test:  [230/345]  eta: 0:00:19  loss: 0.2330 (0.2399)  time: 0.1742  data: 0.0001  max mem: 15821
[19:12:11.346465] Test:  [240/345]  eta: 0:00:18  loss: 0.2351 (0.2400)  time: 0.1747  data: 0.0001  max mem: 15821
[19:12:13.098401] Test:  [250/345]  eta: 0:00:16  loss: 0.2417 (0.2401)  time: 0.1750  data: 0.0001  max mem: 15821
[19:12:14.853591] Test:  [260/345]  eta: 0:00:14  loss: 0.2422 (0.2407)  time: 0.1753  data: 0.0001  max mem: 15821
[19:12:16.613461] Test:  [270/345]  eta: 0:00:12  loss: 0.2333 (0.2405)  time: 0.1757  data: 0.0001  max mem: 15821
[19:12:18.376878] Test:  [280/345]  eta: 0:00:11  loss: 0.2282 (0.2401)  time: 0.1761  data: 0.0001  max mem: 15821
[19:12:20.142974] Test:  [290/345]  eta: 0:00:09  loss: 0.2305 (0.2401)  time: 0.1764  data: 0.0001  max mem: 15821
[19:12:21.912789] Test:  [300/345]  eta: 0:00:07  loss: 0.2360 (0.2401)  time: 0.1767  data: 0.0001  max mem: 15821
[19:12:23.687117] Test:  [310/345]  eta: 0:00:06  loss: 0.2360 (0.2403)  time: 0.1771  data: 0.0001  max mem: 15821
[19:12:25.464916] Test:  [320/345]  eta: 0:00:04  loss: 0.2371 (0.2403)  time: 0.1775  data: 0.0001  max mem: 15821
[19:12:27.245625] Test:  [330/345]  eta: 0:00:02  loss: 0.2371 (0.2401)  time: 0.1779  data: 0.0001  max mem: 15821
[19:12:29.029049] Test:  [340/345]  eta: 0:00:00  loss: 0.2345 (0.2401)  time: 0.1782  data: 0.0001  max mem: 15821
[19:12:29.744780] Test:  [344/345]  eta: 0:00:00  loss: 0.2246 (0.2398)  time: 0.1784  data: 0.0001  max mem: 15821
[19:12:29.806132] Test: Total time: 0:00:59 (0.1738 s / it)
[19:12:39.639905] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4675 (0.4675)  time: 0.4478  data: 0.2852  max mem: 15821
[19:12:41.294085] Test:  [10/57]  eta: 0:00:08  loss: 0.4355 (0.4410)  time: 0.1910  data: 0.0260  max mem: 15821
[19:12:42.951710] Test:  [20/57]  eta: 0:00:06  loss: 0.4140 (0.4254)  time: 0.1655  data: 0.0001  max mem: 15821
[19:12:44.613617] Test:  [30/57]  eta: 0:00:04  loss: 0.2692 (0.3658)  time: 0.1659  data: 0.0001  max mem: 15821
[19:12:46.278631] Test:  [40/57]  eta: 0:00:02  loss: 0.2539 (0.3384)  time: 0.1663  data: 0.0001  max mem: 15821
[19:12:47.948675] Test:  [50/57]  eta: 0:00:01  loss: 0.2724 (0.3355)  time: 0.1667  data: 0.0001  max mem: 15821
[19:12:48.848456] Test:  [56/57]  eta: 0:00:00  loss: 0.2795 (0.3416)  time: 0.1618  data: 0.0001  max mem: 15821
[19:12:48.910078] Test: Total time: 0:00:09 (0.1705 s / it)
[19:12:50.644872] Dice score of the network on the train images: 0.796640, val images: 0.795886
[19:12:50.657309] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:12:51.553573] Epoch: [11]  [  0/345]  eta: 0:05:08  lr: 0.000069  loss: 0.2118 (0.2118)  time: 0.8953  data: 0.2912  max mem: 15821
[19:13:03.574816] Epoch: [11]  [ 20/345]  eta: 0:03:19  lr: 0.000069  loss: 0.2290 (0.2338)  time: 0.6010  data: 0.0001  max mem: 15821
[19:13:15.618274] Epoch: [11]  [ 40/345]  eta: 0:03:05  lr: 0.000069  loss: 0.2390 (0.2354)  time: 0.6021  data: 0.0001  max mem: 15821

[19:13:27.684697] Epoch: [11]  [ 60/345]  eta: 0:02:52  lr: 0.000070  loss: 0.2544 (0.2422)  time: 0.6033  data: 0.0001  max mem: 15821
[19:13:39.749515] Epoch: [11]  [ 80/345]  eta: 0:02:40  lr: 0.000070  loss: 0.2361 (0.2415)  time: 0.6032  data: 0.0001  max mem: 15821
[19:13:51.826687] Epoch: [11]  [100/345]  eta: 0:02:28  lr: 0.000071  loss: 0.2464 (0.2432)  time: 0.6038  data: 0.0001  max mem: 15821
[19:14:03.933408] Epoch: [11]  [120/345]  eta: 0:02:16  lr: 0.000071  loss: 0.2428 (0.2432)  time: 0.6053  data: 0.0001  max mem: 15821
[19:14:16.064208] Epoch: [11]  [140/345]  eta: 0:02:04  lr: 0.000071  loss: 0.2477 (0.2441)  time: 0.6065  data: 0.0001  max mem: 15821
[19:14:28.191237] Epoch: [11]  [160/345]  eta: 0:01:52  lr: 0.000072  loss: 0.2380 (0.2441)  time: 0.6063  data: 0.0001  max mem: 15821
[19:14:40.295055] Epoch: [11]  [180/345]  eta: 0:01:39  lr: 0.000072  loss: 0.2302 (0.2432)  time: 0.6051  data: 0.0001  max mem: 15821
[19:14:52.404803] Epoch: [11]  [200/345]  eta: 0:01:27  lr: 0.000072  loss: 0.2431 (0.2430)  time: 0.6054  data: 0.0001  max mem: 15821
[19:15:04.510339] Epoch: [11]  [220/345]  eta: 0:01:15  lr: 0.000073  loss: 0.2352 (0.2425)  time: 0.6052  data: 0.0001  max mem: 15821
[19:15:16.626158] Epoch: [11]  [240/345]  eta: 0:01:03  lr: 0.000073  loss: 0.2418 (0.2419)  time: 0.6057  data: 0.0001  max mem: 15821
[19:15:28.748341] Epoch: [11]  [260/345]  eta: 0:00:51  lr: 0.000073  loss: 0.2431 (0.2424)  time: 0.6061  data: 0.0001  max mem: 15821
[19:15:40.863343] Epoch: [11]  [280/345]  eta: 0:00:39  lr: 0.000074  loss: 0.2463 (0.2427)  time: 0.6057  data: 0.0001  max mem: 15821
[19:15:52.975252] Epoch: [11]  [300/345]  eta: 0:00:27  lr: 0.000074  loss: 0.2390 (0.2430)  time: 0.6055  data: 0.0001  max mem: 15821
[19:16:05.082007] Epoch: [11]  [320/345]  eta: 0:00:15  lr: 0.000075  loss: 0.2346 (0.2423)  time: 0.6053  data: 0.0001  max mem: 15821
[19:16:17.184304] Epoch: [11]  [340/345]  eta: 0:00:03  lr: 0.000075  loss: 0.2383 (0.2426)  time: 0.6051  data: 0.0001  max mem: 15821
[19:16:19.608058] Epoch: [11]  [344/345]  eta: 0:00:00  lr: 0.000075  loss: 0.2397 (0.2426)  time: 0.6052  data: 0.0001  max mem: 15821
[19:16:19.679206] Epoch: [11] Total time: 0:03:29 (0.6059 s / it)
[19:16:19.679660] Averaged stats: lr: 0.000075  loss: 0.2397 (0.2426)
[19:16:20.199442] Test:  [  0/345]  eta: 0:02:57  loss: 0.2739 (0.2739)  time: 0.5141  data: 0.3497  max mem: 15821
[19:16:21.867594] Test:  [ 10/345]  eta: 0:01:06  loss: 0.2193 (0.2339)  time: 0.1983  data: 0.0319  max mem: 15821
[19:16:23.540148] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2213 (0.2365)  time: 0.1670  data: 0.0001  max mem: 15821
[19:16:25.215273] Test:  [ 30/345]  eta: 0:00:56  loss: 0.2241 (0.2319)  time: 0.1673  data: 0.0001  max mem: 15821
[19:16:26.893313] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2275 (0.2333)  time: 0.1676  data: 0.0001  max mem: 15821
[19:16:28.573875] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2253 (0.2301)  time: 0.1679  data: 0.0001  max mem: 15821
[19:16:30.259320] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2168 (0.2283)  time: 0.1682  data: 0.0001  max mem: 15821
[19:16:31.947149] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2168 (0.2274)  time: 0.1686  data: 0.0001  max mem: 15821
[19:16:33.638737] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2272 (0.2286)  time: 0.1689  data: 0.0001  max mem: 15821
[19:16:35.335453] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2271 (0.2279)  time: 0.1694  data: 0.0001  max mem: 15821
[19:16:37.034875] Test:  [100/345]  eta: 0:00:42  loss: 0.2240 (0.2279)  time: 0.1697  data: 0.0001  max mem: 15821
[19:16:38.738091] Test:  [110/345]  eta: 0:00:40  loss: 0.2306 (0.2285)  time: 0.1701  data: 0.0001  max mem: 15821
[19:16:40.444435] Test:  [120/345]  eta: 0:00:38  loss: 0.2271 (0.2288)  time: 0.1704  data: 0.0001  max mem: 15821
[19:16:42.155139] Test:  [130/345]  eta: 0:00:36  loss: 0.2215 (0.2289)  time: 0.1708  data: 0.0001  max mem: 15821
[19:16:43.869767] Test:  [140/345]  eta: 0:00:35  loss: 0.2277 (0.2290)  time: 0.1712  data: 0.0001  max mem: 15821
[19:16:45.587433] Test:  [150/345]  eta: 0:00:33  loss: 0.2331 (0.2293)  time: 0.1715  data: 0.0001  max mem: 15821
[19:16:47.308317] Test:  [160/345]  eta: 0:00:31  loss: 0.2355 (0.2290)  time: 0.1719  data: 0.0001  max mem: 15821
[19:16:49.033459] Test:  [170/345]  eta: 0:00:30  loss: 0.2249 (0.2291)  time: 0.1722  data: 0.0001  max mem: 15821
[19:16:50.760842] Test:  [180/345]  eta: 0:00:28  loss: 0.2253 (0.2287)  time: 0.1726  data: 0.0001  max mem: 15821
[19:16:52.493272] Test:  [190/345]  eta: 0:00:26  loss: 0.2253 (0.2289)  time: 0.1729  data: 0.0001  max mem: 15821
[19:16:54.227681] Test:  [200/345]  eta: 0:00:24  loss: 0.2270 (0.2290)  time: 0.1733  data: 0.0001  max mem: 15821
[19:16:55.965735] Test:  [210/345]  eta: 0:00:23  loss: 0.2280 (0.2294)  time: 0.1736  data: 0.0001  max mem: 15821
[19:16:57.708133] Test:  [220/345]  eta: 0:00:21  loss: 0.2287 (0.2297)  time: 0.1740  data: 0.0001  max mem: 15821
[19:16:59.453745] Test:  [230/345]  eta: 0:00:19  loss: 0.2218 (0.2299)  time: 0.1743  data: 0.0001  max mem: 15821
[19:17:01.201083] Test:  [240/345]  eta: 0:00:18  loss: 0.2228 (0.2300)  time: 0.1745  data: 0.0001  max mem: 15821
[19:17:02.953446] Test:  [250/345]  eta: 0:00:16  loss: 0.2259 (0.2303)  time: 0.1749  data: 0.0001  max mem: 15821
[19:17:04.709659] Test:  [260/345]  eta: 0:00:14  loss: 0.2309 (0.2303)  time: 0.1754  data: 0.0001  max mem: 15821
[19:17:06.468454] Test:  [270/345]  eta: 0:00:12  loss: 0.2238 (0.2302)  time: 0.1757  data: 0.0001  max mem: 15821
[19:17:08.231579] Test:  [280/345]  eta: 0:00:11  loss: 0.2270 (0.2307)  time: 0.1760  data: 0.0001  max mem: 15821
[19:17:09.996840] Test:  [290/345]  eta: 0:00:09  loss: 0.2351 (0.2310)  time: 0.1764  data: 0.0001  max mem: 15821
[19:17:11.765989] Test:  [300/345]  eta: 0:00:07  loss: 0.2292 (0.2307)  time: 0.1767  data: 0.0001  max mem: 15821
[19:17:13.537959] Test:  [310/345]  eta: 0:00:06  loss: 0.2138 (0.2305)  time: 0.1770  data: 0.0001  max mem: 15821
[19:17:15.314594] Test:  [320/345]  eta: 0:00:04  loss: 0.2130 (0.2305)  time: 0.1774  data: 0.0001  max mem: 15821
[19:17:17.096150] Test:  [330/345]  eta: 0:00:02  loss: 0.2178 (0.2304)  time: 0.1778  data: 0.0001  max mem: 15821
[19:17:18.879205] Test:  [340/345]  eta: 0:00:00  loss: 0.2295 (0.2305)  time: 0.1782  data: 0.0001  max mem: 15821
[19:17:19.594844] Test:  [344/345]  eta: 0:00:00  loss: 0.2266 (0.2306)  time: 0.1783  data: 0.0001  max mem: 15821
[19:17:19.666961] Test: Total time: 0:00:59 (0.1739 s / it)
[19:17:29.683597] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4656 (0.4656)  time: 0.4496  data: 0.2868  max mem: 15821
[19:17:31.336265] Test:  [10/57]  eta: 0:00:08  loss: 0.4308 (0.4372)  time: 0.1910  data: 0.0262  max mem: 15821
[19:17:32.992163] Test:  [20/57]  eta: 0:00:06  loss: 0.4100 (0.4178)  time: 0.1653  data: 0.0001  max mem: 15821
[19:17:34.651967] Test:  [30/57]  eta: 0:00:04  loss: 0.2626 (0.3552)  time: 0.1657  data: 0.0001  max mem: 15821
[19:17:36.315777] Test:  [40/57]  eta: 0:00:02  loss: 0.2374 (0.3285)  time: 0.1661  data: 0.0001  max mem: 15821
[19:17:37.982909] Test:  [50/57]  eta: 0:00:01  loss: 0.2582 (0.3228)  time: 0.1665  data: 0.0001  max mem: 15821
[19:17:38.883254] Test:  [56/57]  eta: 0:00:00  loss: 0.2860 (0.3243)  time: 0.1616  data: 0.0001  max mem: 15821
[19:17:38.946281] Test: Total time: 0:00:09 (0.1704 s / it)
[19:17:40.679490] Dice score of the network on the train images: 0.796352, val images: 0.815458
[19:17:40.679720] saving best_rec_model_0 @ epoch 11
[19:17:41.933427] saving best_dice_model_0 @ epoch 11
[19:17:43.150406] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:17:44.046613] Epoch: [12]  [  0/345]  eta: 0:05:08  lr: 0.000075  loss: 0.2353 (0.2353)  time: 0.8953  data: 0.2927  max mem: 15821
[19:17:56.047630] Epoch: [12]  [ 20/345]  eta: 0:03:19  lr: 0.000075  loss: 0.2353 (0.2409)  time: 0.6000  data: 0.0001  max mem: 15821
[19:18:08.082759] Epoch: [12]  [ 40/345]  eta: 0:03:05  lr: 0.000076  loss: 0.2351 (0.2388)  time: 0.6017  data: 0.0001  max mem: 15821
[19:18:20.151816] Epoch: [12]  [ 60/345]  eta: 0:02:52  lr: 0.000076  loss: 0.2382 (0.2390)  time: 0.6034  data: 0.0001  max mem: 15821
[19:18:32.232402] Epoch: [12]  [ 80/345]  eta: 0:02:40  lr: 0.000076  loss: 0.2286 (0.2364)  time: 0.6040  data: 0.0001  max mem: 15821
[19:18:44.333938] Epoch: [12]  [100/345]  eta: 0:02:28  lr: 0.000077  loss: 0.2301 (0.2360)  time: 0.6050  data: 0.0001  max mem: 15821
[19:18:56.461107] Epoch: [12]  [120/345]  eta: 0:02:16  lr: 0.000077  loss: 0.2192 (0.2341)  time: 0.6063  data: 0.0001  max mem: 15821
[19:19:08.611316] Epoch: [12]  [140/345]  eta: 0:02:04  lr: 0.000078  loss: 0.2261 (0.2340)  time: 0.6075  data: 0.0001  max mem: 15821
[19:19:20.767521] Epoch: [12]  [160/345]  eta: 0:01:52  lr: 0.000078  loss: 0.2343 (0.2348)  time: 0.6078  data: 0.0001  max mem: 15821
[19:19:32.921347] Epoch: [12]  [180/345]  eta: 0:01:40  lr: 0.000078  loss: 0.2437 (0.2356)  time: 0.6076  data: 0.0001  max mem: 15821
[19:19:45.071481] Epoch: [12]  [200/345]  eta: 0:01:27  lr: 0.000079  loss: 0.2431 (0.2366)  time: 0.6075  data: 0.0001  max mem: 15821
[19:19:57.226952] Epoch: [12]  [220/345]  eta: 0:01:15  lr: 0.000079  loss: 0.2267 (0.2359)  time: 0.6077  data: 0.0001  max mem: 15821
[19:20:09.365319] Epoch: [12]  [240/345]  eta: 0:01:03  lr: 0.000079  loss: 0.2491 (0.2369)  time: 0.6069  data: 0.0001  max mem: 15821
[19:20:21.499161] Epoch: [12]  [260/345]  eta: 0:00:51  lr: 0.000080  loss: 0.2256 (0.2362)  time: 0.6066  data: 0.0001  max mem: 15821
[19:20:33.635035] Epoch: [12]  [280/345]  eta: 0:00:39  lr: 0.000080  loss: 0.2283 (0.2358)  time: 0.6068  data: 0.0001  max mem: 15821
[19:20:45.763445] Epoch: [12]  [300/345]  eta: 0:00:27  lr: 0.000080  loss: 0.2183 (0.2350)  time: 0.6064  data: 0.0001  max mem: 15821
[19:20:57.888186] Epoch: [12]  [320/345]  eta: 0:00:15  lr: 0.000081  loss: 0.2250 (0.2344)  time: 0.6062  data: 0.0001  max mem: 15821
[19:21:09.993417] Epoch: [12]  [340/345]  eta: 0:00:03  lr: 0.000081  loss: 0.2212 (0.2339)  time: 0.6052  data: 0.0001  max mem: 15821
[19:21:12.411643] Epoch: [12]  [344/345]  eta: 0:00:00  lr: 0.000081  loss: 0.2275 (0.2339)  time: 0.6050  data: 0.0001  max mem: 15821
[19:21:12.483683] Epoch: [12] Total time: 0:03:29 (0.6068 s / it)
[19:21:12.484310] Averaged stats: lr: 0.000081  loss: 0.2275 (0.2339)
[19:21:12.972933] Test:  [  0/345]  eta: 0:02:46  loss: 0.3021 (0.3021)  time: 0.4833  data: 0.3192  max mem: 15821
[19:21:14.643870] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2242 (0.2319)  time: 0.1957  data: 0.0291  max mem: 15821
[19:21:16.317608] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2211 (0.2252)  time: 0.1671  data: 0.0001  max mem: 15821
[19:21:17.994390] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2164 (0.2230)  time: 0.1675  data: 0.0001  max mem: 15821
[19:21:19.673980] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2211 (0.2249)  time: 0.1678  data: 0.0001  max mem: 15821
[19:21:21.356714] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2294 (0.2245)  time: 0.1680  data: 0.0001  max mem: 15821
[19:21:23.042934] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2185 (0.2229)  time: 0.1684  data: 0.0001  max mem: 15821
[19:21:24.732776] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2118 (0.2209)  time: 0.1687  data: 0.0001  max mem: 15821
[19:21:26.425794] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2084 (0.2209)  time: 0.1691  data: 0.0001  max mem: 15821
[19:21:28.123822] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2128 (0.2200)  time: 0.1695  data: 0.0001  max mem: 15821
[19:21:29.823822] Test:  [100/345]  eta: 0:00:42  loss: 0.2075 (0.2194)  time: 0.1698  data: 0.0001  max mem: 15821
[19:21:31.528528] Test:  [110/345]  eta: 0:00:40  loss: 0.2165 (0.2207)  time: 0.1702  data: 0.0001  max mem: 15821
[19:21:33.236373] Test:  [120/345]  eta: 0:00:38  loss: 0.2227 (0.2199)  time: 0.1706  data: 0.0001  max mem: 15821
[19:21:34.946894] Test:  [130/345]  eta: 0:00:36  loss: 0.2173 (0.2206)  time: 0.1709  data: 0.0001  max mem: 15821
[19:21:36.662230] Test:  [140/345]  eta: 0:00:35  loss: 0.2237 (0.2213)  time: 0.1712  data: 0.0001  max mem: 15821
[19:21:38.380009] Test:  [150/345]  eta: 0:00:33  loss: 0.2241 (0.2218)  time: 0.1716  data: 0.0001  max mem: 15821
[19:21:40.101811] Test:  [160/345]  eta: 0:00:31  loss: 0.2325 (0.2222)  time: 0.1719  data: 0.0001  max mem: 15821
[19:21:41.827512] Test:  [170/345]  eta: 0:00:30  loss: 0.2202 (0.2220)  time: 0.1723  data: 0.0001  max mem: 15821
[19:21:43.557057] Test:  [180/345]  eta: 0:00:28  loss: 0.2114 (0.2216)  time: 0.1727  data: 0.0001  max mem: 15821
[19:21:45.288364] Test:  [190/345]  eta: 0:00:26  loss: 0.2268 (0.2218)  time: 0.1730  data: 0.0001  max mem: 15821
[19:21:47.023041] Test:  [200/345]  eta: 0:00:24  loss: 0.2170 (0.2211)  time: 0.1732  data: 0.0001  max mem: 15821
[19:21:48.761235] Test:  [210/345]  eta: 0:00:23  loss: 0.2115 (0.2212)  time: 0.1736  data: 0.0001  max mem: 15821
[19:21:50.503096] Test:  [220/345]  eta: 0:00:21  loss: 0.2125 (0.2213)  time: 0.1739  data: 0.0001  max mem: 15821
[19:21:52.248774] Test:  [230/345]  eta: 0:00:19  loss: 0.2211 (0.2216)  time: 0.1743  data: 0.0001  max mem: 15821
[19:21:53.997896] Test:  [240/345]  eta: 0:00:18  loss: 0.2154 (0.2215)  time: 0.1747  data: 0.0001  max mem: 15821
[19:21:55.751456] Test:  [250/345]  eta: 0:00:16  loss: 0.2056 (0.2212)  time: 0.1751  data: 0.0001  max mem: 15821
[19:21:57.507246] Test:  [260/345]  eta: 0:00:14  loss: 0.2031 (0.2206)  time: 0.1754  data: 0.0001  max mem: 15821
[19:21:59.268112] Test:  [270/345]  eta: 0:00:12  loss: 0.2028 (0.2203)  time: 0.1758  data: 0.0001  max mem: 15821
[19:22:01.031608] Test:  [280/345]  eta: 0:00:11  loss: 0.2192 (0.2208)  time: 0.1762  data: 0.0001  max mem: 15821
[19:22:02.797305] Test:  [290/345]  eta: 0:00:09  loss: 0.2257 (0.2210)  time: 0.1764  data: 0.0001  max mem: 15821
[19:22:04.566329] Test:  [300/345]  eta: 0:00:07  loss: 0.2247 (0.2210)  time: 0.1767  data: 0.0001  max mem: 15821
[19:22:06.340777] Test:  [310/345]  eta: 0:00:06  loss: 0.2168 (0.2210)  time: 0.1771  data: 0.0001  max mem: 15821
[19:22:08.118354] Test:  [320/345]  eta: 0:00:04  loss: 0.2168 (0.2214)  time: 0.1775  data: 0.0001  max mem: 15821
[19:22:09.899462] Test:  [330/345]  eta: 0:00:02  loss: 0.2347 (0.2217)  time: 0.1779  data: 0.0001  max mem: 15821
[19:22:11.682705] Test:  [340/345]  eta: 0:00:00  loss: 0.2190 (0.2215)  time: 0.1782  data: 0.0001  max mem: 15821
[19:22:12.397764] Test:  [344/345]  eta: 0:00:00  loss: 0.2244 (0.2215)  time: 0.1783  data: 0.0001  max mem: 15821
[19:22:12.479678] Test: Total time: 0:00:59 (0.1739 s / it)
[19:22:22.575111] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4682 (0.4682)  time: 0.4534  data: 0.2908  max mem: 15821
[19:22:24.227488] Test:  [10/57]  eta: 0:00:08  loss: 0.4250 (0.4316)  time: 0.1913  data: 0.0265  max mem: 15821
[19:22:25.885451] Test:  [20/57]  eta: 0:00:06  loss: 0.4085 (0.4044)  time: 0.1654  data: 0.0001  max mem: 15821
[19:22:27.545331] Test:  [30/57]  eta: 0:00:04  loss: 0.2556 (0.3502)  time: 0.1658  data: 0.0001  max mem: 15821
[19:22:29.210092] Test:  [40/57]  eta: 0:00:02  loss: 0.2391 (0.3266)  time: 0.1662  data: 0.0001  max mem: 15821
[19:22:30.878828] Test:  [50/57]  eta: 0:00:01  loss: 0.2604 (0.3235)  time: 0.1666  data: 0.0001  max mem: 15821
[19:22:31.778954] Test:  [56/57]  eta: 0:00:00  loss: 0.2861 (0.3280)  time: 0.1617  data: 0.0000  max mem: 15821
[19:22:31.858397] Test: Total time: 0:00:09 (0.1708 s / it)
[19:22:33.601925] Dice score of the network on the train images: 0.798087, val images: 0.802288
[19:22:33.606915] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:22:34.505069] Epoch: [13]  [  0/345]  eta: 0:05:09  lr: 0.000081  loss: 0.2144 (0.2144)  time: 0.8971  data: 0.2945  max mem: 15821
[19:22:46.523679] Epoch: [13]  [ 20/345]  eta: 0:03:19  lr: 0.000082  loss: 0.2169 (0.2259)  time: 0.6009  data: 0.0001  max mem: 15821
[19:22:58.580556] Epoch: [13]  [ 40/345]  eta: 0:03:05  lr: 0.000082  loss: 0.2308 (0.2299)  time: 0.6028  data: 0.0001  max mem: 15821
[19:23:10.631238] Epoch: [13]  [ 60/345]  eta: 0:02:52  lr: 0.000082  loss: 0.2274 (0.2289)  time: 0.6025  data: 0.0001  max mem: 15821
[19:23:22.694161] Epoch: [13]  [ 80/345]  eta: 0:02:40  lr: 0.000083  loss: 0.2200 (0.2272)  time: 0.6031  data: 0.0001  max mem: 15821
[19:23:34.773730] Epoch: [13]  [100/345]  eta: 0:02:28  lr: 0.000083  loss: 0.2226 (0.2269)  time: 0.6039  data: 0.0001  max mem: 15821
[19:23:46.859624] Epoch: [13]  [120/345]  eta: 0:02:16  lr: 0.000083  loss: 0.2205 (0.2267)  time: 0.6043  data: 0.0001  max mem: 15821
[19:23:58.967050] Epoch: [13]  [140/345]  eta: 0:02:04  lr: 0.000084  loss: 0.2190 (0.2264)  time: 0.6053  data: 0.0001  max mem: 15821
[19:24:11.091695] Epoch: [13]  [160/345]  eta: 0:01:52  lr: 0.000084  loss: 0.2316 (0.2277)  time: 0.6062  data: 0.0001  max mem: 15821
[19:24:23.231010] Epoch: [13]  [180/345]  eta: 0:01:39  lr: 0.000085  loss: 0.2198 (0.2269)  time: 0.6069  data: 0.0001  max mem: 15821
[19:24:35.368383] Epoch: [13]  [200/345]  eta: 0:01:27  lr: 0.000085  loss: 0.2326 (0.2272)  time: 0.6068  data: 0.0001  max mem: 15821
[19:24:47.488139] Epoch: [13]  [220/345]  eta: 0:01:15  lr: 0.000085  loss: 0.2246 (0.2278)  time: 0.6059  data: 0.0001  max mem: 15821
[19:24:59.607194] Epoch: [13]  [240/345]  eta: 0:01:03  lr: 0.000086  loss: 0.2163 (0.2278)  time: 0.6059  data: 0.0001  max mem: 15821
[19:25:11.721223] Epoch: [13]  [260/345]  eta: 0:00:51  lr: 0.000086  loss: 0.2281 (0.2286)  time: 0.6057  data: 0.0001  max mem: 15821
[19:25:23.826796] Epoch: [13]  [280/345]  eta: 0:00:39  lr: 0.000086  loss: 0.2129 (0.2280)  time: 0.6052  data: 0.0001  max mem: 15821
[19:25:35.934413] Epoch: [13]  [300/345]  eta: 0:00:27  lr: 0.000087  loss: 0.2318 (0.2281)  time: 0.6053  data: 0.0001  max mem: 15821
[19:25:48.038568] Epoch: [13]  [320/345]  eta: 0:00:15  lr: 0.000087  loss: 0.2163 (0.2279)  time: 0.6052  data: 0.0001  max mem: 15821
[19:26:00.149210] Epoch: [13]  [340/345]  eta: 0:00:03  lr: 0.000087  loss: 0.2199 (0.2278)  time: 0.6055  data: 0.0001  max mem: 15821
[19:26:02.574057] Epoch: [13]  [344/345]  eta: 0:00:00  lr: 0.000087  loss: 0.2199 (0.2277)  time: 0.6056  data: 0.0001  max mem: 15821
[19:26:02.645235] Epoch: [13] Total time: 0:03:29 (0.6059 s / it)
[19:26:02.645678] Averaged stats: lr: 0.000087  loss: 0.2199 (0.2277)
[19:26:03.130183] Test:  [  0/345]  eta: 0:02:45  loss: 0.2045 (0.2045)  time: 0.4784  data: 0.3134  max mem: 15821
[19:26:04.802073] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2137 (0.2199)  time: 0.1954  data: 0.0286  max mem: 15821
[19:26:06.476170] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2173 (0.2239)  time: 0.1672  data: 0.0001  max mem: 15821
[19:26:08.153550] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2084 (0.2174)  time: 0.1675  data: 0.0001  max mem: 15821
[19:26:09.834281] Test:  [ 40/345]  eta: 0:00:53  loss: 0.2010 (0.2161)  time: 0.1678  data: 0.0001  max mem: 15821
[19:26:11.518572] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2087 (0.2156)  time: 0.1682  data: 0.0001  max mem: 15821
[19:26:13.207284] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2141 (0.2156)  time: 0.1686  data: 0.0001  max mem: 15821
[19:26:14.897528] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2055 (0.2149)  time: 0.1689  data: 0.0001  max mem: 15821
[19:26:16.592053] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2055 (0.2162)  time: 0.1692  data: 0.0001  max mem: 15821
[19:26:18.290471] Test:  [ 90/345]  eta: 0:00:43  loss: 0.2283 (0.2195)  time: 0.1696  data: 0.0001  max mem: 15821
[19:26:19.991141] Test:  [100/345]  eta: 0:00:42  loss: 0.2201 (0.2197)  time: 0.1699  data: 0.0001  max mem: 15821
[19:26:21.695957] Test:  [110/345]  eta: 0:00:40  loss: 0.2150 (0.2194)  time: 0.1702  data: 0.0001  max mem: 15821
[19:26:23.404435] Test:  [120/345]  eta: 0:00:38  loss: 0.2080 (0.2186)  time: 0.1706  data: 0.0001  max mem: 15821
[19:26:25.114471] Test:  [130/345]  eta: 0:00:36  loss: 0.1977 (0.2183)  time: 0.1709  data: 0.0001  max mem: 15821
[19:26:26.830323] Test:  [140/345]  eta: 0:00:35  loss: 0.2006 (0.2170)  time: 0.1712  data: 0.0001  max mem: 15821
[19:26:28.548925] Test:  [150/345]  eta: 0:00:33  loss: 0.1972 (0.2161)  time: 0.1717  data: 0.0001  max mem: 15821
[19:26:30.271409] Test:  [160/345]  eta: 0:00:31  loss: 0.2091 (0.2172)  time: 0.1720  data: 0.0001  max mem: 15821
[19:26:31.998213] Test:  [170/345]  eta: 0:00:30  loss: 0.2129 (0.2166)  time: 0.1724  data: 0.0001  max mem: 15821
[19:26:33.726647] Test:  [180/345]  eta: 0:00:28  loss: 0.2170 (0.2171)  time: 0.1727  data: 0.0001  max mem: 15821
[19:26:35.460301] Test:  [190/345]  eta: 0:00:26  loss: 0.2264 (0.2174)  time: 0.1730  data: 0.0001  max mem: 15821
[19:26:37.195941] Test:  [200/345]  eta: 0:00:24  loss: 0.2212 (0.2174)  time: 0.1734  data: 0.0001  max mem: 15821
[19:26:38.934789] Test:  [210/345]  eta: 0:00:23  loss: 0.2212 (0.2179)  time: 0.1737  data: 0.0001  max mem: 15821
[19:26:40.677150] Test:  [220/345]  eta: 0:00:21  loss: 0.2295 (0.2181)  time: 0.1740  data: 0.0001  max mem: 15821
[19:26:42.422954] Test:  [230/345]  eta: 0:00:19  loss: 0.2295 (0.2190)  time: 0.1743  data: 0.0001  max mem: 15821
[19:26:44.172721] Test:  [240/345]  eta: 0:00:18  loss: 0.2196 (0.2188)  time: 0.1747  data: 0.0001  max mem: 15821
[19:26:45.926083] Test:  [250/345]  eta: 0:00:16  loss: 0.2182 (0.2187)  time: 0.1751  data: 0.0001  max mem: 15821
[19:26:47.682594] Test:  [260/345]  eta: 0:00:14  loss: 0.2223 (0.2188)  time: 0.1754  data: 0.0001  max mem: 15821
[19:26:49.441845] Test:  [270/345]  eta: 0:00:12  loss: 0.2238 (0.2189)  time: 0.1757  data: 0.0001  max mem: 15821
[19:26:51.204135] Test:  [280/345]  eta: 0:00:11  loss: 0.2202 (0.2190)  time: 0.1760  data: 0.0001  max mem: 15821
[19:26:52.970255] Test:  [290/345]  eta: 0:00:09  loss: 0.2127 (0.2189)  time: 0.1764  data: 0.0001  max mem: 15821
[19:26:54.741901] Test:  [300/345]  eta: 0:00:07  loss: 0.2167 (0.2191)  time: 0.1768  data: 0.0001  max mem: 15821
[19:26:56.517250] Test:  [310/345]  eta: 0:00:06  loss: 0.2176 (0.2192)  time: 0.1773  data: 0.0001  max mem: 15821
[19:26:58.295681] Test:  [320/345]  eta: 0:00:04  loss: 0.2133 (0.2191)  time: 0.1776  data: 0.0001  max mem: 15821
[19:27:00.078924] Test:  [330/345]  eta: 0:00:02  loss: 0.2129 (0.2190)  time: 0.1780  data: 0.0001  max mem: 15821
[19:27:01.864217] Test:  [340/345]  eta: 0:00:00  loss: 0.2228 (0.2191)  time: 0.1784  data: 0.0001  max mem: 15821
[19:27:02.580445] Test:  [344/345]  eta: 0:00:00  loss: 0.2193 (0.2192)  time: 0.1786  data: 0.0001  max mem: 15821
[19:27:02.653548] Test: Total time: 0:01:00 (0.1739 s / it)
[19:27:12.632958] Test:  [ 0/57]  eta: 0:00:26  loss: 0.3864 (0.3864)  time: 0.4727  data: 0.3096  max mem: 15821
[19:27:14.286547] Test:  [10/57]  eta: 0:00:09  loss: 0.3864 (0.3970)  time: 0.1932  data: 0.0283  max mem: 15821
[19:27:15.944519] Test:  [20/57]  eta: 0:00:06  loss: 0.3758 (0.3790)  time: 0.1655  data: 0.0001  max mem: 15821
[19:27:17.605575] Test:  [30/57]  eta: 0:00:04  loss: 0.2721 (0.3354)  time: 0.1659  data: 0.0001  max mem: 15821
[19:27:19.271192] Test:  [40/57]  eta: 0:00:02  loss: 0.2476 (0.3160)  time: 0.1663  data: 0.0001  max mem: 15821
[19:27:20.940148] Test:  [50/57]  eta: 0:00:01  loss: 0.2654 (0.3168)  time: 0.1667  data: 0.0001  max mem: 15821
[19:27:21.840387] Test:  [56/57]  eta: 0:00:00  loss: 0.3099 (0.3253)  time: 0.1617  data: 0.0001  max mem: 15821
[19:27:21.910490] Test: Total time: 0:00:09 (0.1711 s / it)
[19:27:23.669839] Dice score of the network on the train images: 0.799994, val images: 0.775733
[19:27:23.674927] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:27:24.579674] Epoch: [14]  [  0/345]  eta: 0:05:11  lr: 0.000087  loss: 0.2652 (0.2652)  time: 0.9036  data: 0.2973  max mem: 15821
[19:27:36.602540] Epoch: [14]  [ 20/345]  eta: 0:03:20  lr: 0.000088  loss: 0.2139 (0.2186)  time: 0.6011  data: 0.0001  max mem: 15821
[19:27:48.657806] Epoch: [14]  [ 40/345]  eta: 0:03:05  lr: 0.000088  loss: 0.2241 (0.2231)  time: 0.6027  data: 0.0001  max mem: 15821
[19:28:00.744111] Epoch: [14]  [ 60/345]  eta: 0:02:53  lr: 0.000089  loss: 0.2141 (0.2196)  time: 0.6043  data: 0.0001  max mem: 15821
[19:28:12.827777] Epoch: [14]  [ 80/345]  eta: 0:02:40  lr: 0.000089  loss: 0.2182 (0.2195)  time: 0.6041  data: 0.0001  max mem: 15821
[19:28:24.930883] Epoch: [14]  [100/345]  eta: 0:02:28  lr: 0.000089  loss: 0.2138 (0.2195)  time: 0.6051  data: 0.0001  max mem: 15821
[19:28:37.047818] Epoch: [14]  [120/345]  eta: 0:02:16  lr: 0.000090  loss: 0.2112 (0.2187)  time: 0.6058  data: 0.0001  max mem: 15821
[19:28:49.176099] Epoch: [14]  [140/345]  eta: 0:02:04  lr: 0.000090  loss: 0.2111 (0.2186)  time: 0.6064  data: 0.0001  max mem: 15821
[19:29:01.320791] Epoch: [14]  [160/345]  eta: 0:01:52  lr: 0.000090  loss: 0.2172 (0.2186)  time: 0.6072  data: 0.0001  max mem: 15821
[19:29:13.459885] Epoch: [14]  [180/345]  eta: 0:01:40  lr: 0.000091  loss: 0.2083 (0.2177)  time: 0.6069  data: 0.0001  max mem: 15821
[19:29:25.598410] Epoch: [14]  [200/345]  eta: 0:01:27  lr: 0.000091  loss: 0.2155 (0.2177)  time: 0.6069  data: 0.0001  max mem: 15821
[19:29:37.724537] Epoch: [14]  [220/345]  eta: 0:01:15  lr: 0.000091  loss: 0.2194 (0.2180)  time: 0.6063  data: 0.0001  max mem: 15821
[19:29:49.842391] Epoch: [14]  [240/345]  eta: 0:01:03  lr: 0.000092  loss: 0.2074 (0.2180)  time: 0.6058  data: 0.0001  max mem: 15821
[19:30:01.955311] Epoch: [14]  [260/345]  eta: 0:00:51  lr: 0.000092  loss: 0.2272 (0.2186)  time: 0.6056  data: 0.0001  max mem: 15821
[19:30:14.064946] Epoch: [14]  [280/345]  eta: 0:00:39  lr: 0.000093  loss: 0.2228 (0.2189)  time: 0.6054  data: 0.0001  max mem: 15821
[19:30:26.168719] Epoch: [14]  [300/345]  eta: 0:00:27  lr: 0.000093  loss: 0.2146 (0.2187)  time: 0.6051  data: 0.0001  max mem: 15821
[19:30:38.268162] Epoch: [14]  [320/345]  eta: 0:00:15  lr: 0.000093  loss: 0.2052 (0.2183)  time: 0.6049  data: 0.0001  max mem: 15821
[19:30:50.359893] Epoch: [14]  [340/345]  eta: 0:00:03  lr: 0.000094  loss: 0.2182 (0.2185)  time: 0.6045  data: 0.0001  max mem: 15821
[19:30:52.780693] Epoch: [14]  [344/345]  eta: 0:00:00  lr: 0.000094  loss: 0.2246 (0.2184)  time: 0.6046  data: 0.0001  max mem: 15821
[19:30:52.851089] Epoch: [14] Total time: 0:03:29 (0.6063 s / it)
[19:30:52.851292] Averaged stats: lr: 0.000094  loss: 0.2246 (0.2184)
[19:30:53.339918] Test:  [  0/345]  eta: 0:02:46  loss: 0.1679 (0.1679)  time: 0.4830  data: 0.3184  max mem: 15821
[19:30:55.010428] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2155 (0.2095)  time: 0.1957  data: 0.0290  max mem: 15821
[19:30:56.685122] Test:  [ 20/345]  eta: 0:00:59  loss: 0.2155 (0.2099)  time: 0.1672  data: 0.0001  max mem: 15821
[19:30:58.362220] Test:  [ 30/345]  eta: 0:00:55  loss: 0.2061 (0.2071)  time: 0.1675  data: 0.0001  max mem: 15821
[19:31:00.042213] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1884 (0.2034)  time: 0.1678  data: 0.0001  max mem: 15821
[19:31:01.727132] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1938 (0.2043)  time: 0.1682  data: 0.0001  max mem: 15821
[19:31:03.414333] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1999 (0.2033)  time: 0.1685  data: 0.0001  max mem: 15821
[19:31:05.105079] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2001 (0.2043)  time: 0.1688  data: 0.0001  max mem: 15821
[19:31:06.799666] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1983 (0.2031)  time: 0.1692  data: 0.0001  max mem: 15821
[19:31:08.498692] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1938 (0.2029)  time: 0.1696  data: 0.0001  max mem: 15821
[19:31:10.201165] Test:  [100/345]  eta: 0:00:42  loss: 0.2050 (0.2042)  time: 0.1700  data: 0.0001  max mem: 15821
[19:31:11.907038] Test:  [110/345]  eta: 0:00:40  loss: 0.2054 (0.2046)  time: 0.1704  data: 0.0001  max mem: 15821
[19:31:13.614467] Test:  [120/345]  eta: 0:00:38  loss: 0.2054 (0.2045)  time: 0.1706  data: 0.0001  max mem: 15821
[19:31:15.326500] Test:  [130/345]  eta: 0:00:36  loss: 0.1928 (0.2038)  time: 0.1709  data: 0.0001  max mem: 15821
[19:31:17.041276] Test:  [140/345]  eta: 0:00:35  loss: 0.1914 (0.2034)  time: 0.1713  data: 0.0001  max mem: 15821
[19:31:18.759927] Test:  [150/345]  eta: 0:00:33  loss: 0.1943 (0.2038)  time: 0.1716  data: 0.0001  max mem: 15821
[19:31:20.482257] Test:  [160/345]  eta: 0:00:31  loss: 0.2010 (0.2038)  time: 0.1720  data: 0.0001  max mem: 15821
[19:31:22.209400] Test:  [170/345]  eta: 0:00:30  loss: 0.1996 (0.2031)  time: 0.1724  data: 0.0001  max mem: 15821
[19:31:23.937972] Test:  [180/345]  eta: 0:00:28  loss: 0.2039 (0.2038)  time: 0.1727  data: 0.0001  max mem: 15821
[19:31:25.671802] Test:  [190/345]  eta: 0:00:26  loss: 0.2033 (0.2035)  time: 0.1731  data: 0.0001  max mem: 15821
[19:31:27.408631] Test:  [200/345]  eta: 0:00:24  loss: 0.1988 (0.2030)  time: 0.1735  data: 0.0001  max mem: 15821
[19:31:29.147746] Test:  [210/345]  eta: 0:00:23  loss: 0.1952 (0.2032)  time: 0.1737  data: 0.0001  max mem: 15821
[19:31:30.890433] Test:  [220/345]  eta: 0:00:21  loss: 0.1974 (0.2030)  time: 0.1740  data: 0.0001  max mem: 15821
[19:31:32.637260] Test:  [230/345]  eta: 0:00:19  loss: 0.2072 (0.2034)  time: 0.1744  data: 0.0001  max mem: 15821
[19:31:34.387325] Test:  [240/345]  eta: 0:00:18  loss: 0.2033 (0.2037)  time: 0.1748  data: 0.0001  max mem: 15821
[19:31:36.141019] Test:  [250/345]  eta: 0:00:16  loss: 0.1948 (0.2035)  time: 0.1751  data: 0.0001  max mem: 15821
[19:31:37.897101] Test:  [260/345]  eta: 0:00:14  loss: 0.1965 (0.2036)  time: 0.1754  data: 0.0001  max mem: 15821
[19:31:39.658336] Test:  [270/345]  eta: 0:00:12  loss: 0.1983 (0.2040)  time: 0.1758  data: 0.0001  max mem: 15821
[19:31:41.422597] Test:  [280/345]  eta: 0:00:11  loss: 0.2060 (0.2040)  time: 0.1762  data: 0.0001  max mem: 15821
[19:31:43.189408] Test:  [290/345]  eta: 0:00:09  loss: 0.1926 (0.2037)  time: 0.1765  data: 0.0001  max mem: 15821
[19:31:44.960079] Test:  [300/345]  eta: 0:00:07  loss: 0.1926 (0.2036)  time: 0.1768  data: 0.0001  max mem: 15821
[19:31:46.734296] Test:  [310/345]  eta: 0:00:06  loss: 0.1997 (0.2036)  time: 0.1772  data: 0.0001  max mem: 15821
[19:31:48.512579] Test:  [320/345]  eta: 0:00:04  loss: 0.2029 (0.2036)  time: 0.1776  data: 0.0001  max mem: 15821
[19:31:50.294718] Test:  [330/345]  eta: 0:00:02  loss: 0.1978 (0.2034)  time: 0.1780  data: 0.0001  max mem: 15821
[19:31:52.080297] Test:  [340/345]  eta: 0:00:00  loss: 0.2124 (0.2038)  time: 0.1783  data: 0.0001  max mem: 15821
[19:31:52.794975] Test:  [344/345]  eta: 0:00:00  loss: 0.2123 (0.2038)  time: 0.1784  data: 0.0001  max mem: 15821
[19:31:52.874331] Test: Total time: 0:01:00 (0.1740 s / it)
[19:32:02.715601] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4723 (0.4723)  time: 0.4534  data: 0.2902  max mem: 15821
[19:32:04.369114] Test:  [10/57]  eta: 0:00:08  loss: 0.4065 (0.4243)  time: 0.1914  data: 0.0265  max mem: 15821
[19:32:06.028498] Test:  [20/57]  eta: 0:00:06  loss: 0.4065 (0.4159)  time: 0.1656  data: 0.0001  max mem: 15821
[19:32:07.690617] Test:  [30/57]  eta: 0:00:04  loss: 0.2795 (0.3631)  time: 0.1660  data: 0.0001  max mem: 15821
[19:32:09.357222] Test:  [40/57]  eta: 0:00:02  loss: 0.2623 (0.3427)  time: 0.1664  data: 0.0001  max mem: 15821
[19:32:11.029056] Test:  [50/57]  eta: 0:00:01  loss: 0.2772 (0.3413)  time: 0.1669  data: 0.0001  max mem: 15821
[19:32:11.929905] Test:  [56/57]  eta: 0:00:00  loss: 0.3136 (0.3485)  time: 0.1619  data: 0.0001  max mem: 15821
[19:32:12.001114] Test: Total time: 0:00:09 (0.1709 s / it)
[19:32:13.774424] Dice score of the network on the train images: 0.830799, val images: 0.792674
[19:32:13.778566] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:32:14.753690] Epoch: [15]  [  0/345]  eta: 0:05:36  lr: 0.000094  loss: 0.2489 (0.2489)  time: 0.9741  data: 0.3720  max mem: 15821
[19:32:26.905847] Epoch: [15]  [ 20/345]  eta: 0:03:23  lr: 0.000094  loss: 0.2073 (0.2148)  time: 0.6076  data: 0.0001  max mem: 15821
[19:32:38.950687] Epoch: [15]  [ 40/345]  eta: 0:03:07  lr: 0.000094  loss: 0.2026 (0.2100)  time: 0.6022  data: 0.0001  max mem: 15821

[19:32:51.034777] Epoch: [15]  [ 60/345]  eta: 0:02:54  lr: 0.000095  loss: 0.1999 (0.2072)  time: 0.6042  data: 0.0001  max mem: 15821
[19:33:03.129459] Epoch: [15]  [ 80/345]  eta: 0:02:41  lr: 0.000095  loss: 0.2177 (0.2095)  time: 0.6047  data: 0.0001  max mem: 15821
[19:33:15.236793] Epoch: [15]  [100/345]  eta: 0:02:29  lr: 0.000096  loss: 0.2037 (0.2082)  time: 0.6053  data: 0.0001  max mem: 15821
[19:33:27.355915] Epoch: [15]  [120/345]  eta: 0:02:16  lr: 0.000096  loss: 0.1980 (0.2078)  time: 0.6059  data: 0.0001  max mem: 15821
[19:33:39.481159] Epoch: [15]  [140/345]  eta: 0:02:04  lr: 0.000096  loss: 0.2051 (0.2084)  time: 0.6062  data: 0.0001  max mem: 15821
[19:33:51.619053] Epoch: [15]  [160/345]  eta: 0:01:52  lr: 0.000097  loss: 0.1971 (0.2079)  time: 0.6068  data: 0.0001  max mem: 15821
[19:34:03.758920] Epoch: [15]  [180/345]  eta: 0:01:40  lr: 0.000097  loss: 0.2072 (0.2075)  time: 0.6069  data: 0.0001  max mem: 15821
[19:34:15.902985] Epoch: [15]  [200/345]  eta: 0:01:28  lr: 0.000097  loss: 0.2050 (0.2083)  time: 0.6072  data: 0.0001  max mem: 15821
[19:34:28.039494] Epoch: [15]  [220/345]  eta: 0:01:15  lr: 0.000098  loss: 0.2088 (0.2089)  time: 0.6068  data: 0.0001  max mem: 15821
[19:34:40.177266] Epoch: [15]  [240/345]  eta: 0:01:03  lr: 0.000098  loss: 0.2042 (0.2090)  time: 0.6068  data: 0.0001  max mem: 15821
[19:34:52.309170] Epoch: [15]  [260/345]  eta: 0:00:51  lr: 0.000098  loss: 0.2074 (0.2091)  time: 0.6065  data: 0.0001  max mem: 15821
[19:35:04.443287] Epoch: [15]  [280/345]  eta: 0:00:39  lr: 0.000099  loss: 0.2007 (0.2089)  time: 0.6067  data: 0.0001  max mem: 15821
[19:35:16.572525] Epoch: [15]  [300/345]  eta: 0:00:27  lr: 0.000099  loss: 0.2046 (0.2093)  time: 0.6064  data: 0.0001  max mem: 15821
[19:35:28.691857] Epoch: [15]  [320/345]  eta: 0:00:15  lr: 0.000100  loss: 0.2067 (0.2095)  time: 0.6059  data: 0.0001  max mem: 15821
[19:35:40.808382] Epoch: [15]  [340/345]  eta: 0:00:03  lr: 0.000100  loss: 0.2078 (0.2098)  time: 0.6058  data: 0.0001  max mem: 15821
[19:35:43.232775] Epoch: [15]  [344/345]  eta: 0:00:00  lr: 0.000100  loss: 0.2101 (0.2099)  time: 0.6058  data: 0.0001  max mem: 15821
[19:35:43.303807] Epoch: [15] Total time: 0:03:29 (0.6073 s / it)
[19:35:43.304316] Averaged stats: lr: 0.000100  loss: 0.2101 (0.2099)
[19:35:43.806450] Test:  [  0/345]  eta: 0:02:51  loss: 0.2023 (0.2023)  time: 0.4965  data: 0.3317  max mem: 15821
[19:35:45.478011] Test:  [ 10/345]  eta: 0:01:05  loss: 0.2023 (0.2061)  time: 0.1970  data: 0.0302  max mem: 15821
[19:35:47.152430] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1994 (0.2024)  time: 0.1672  data: 0.0001  max mem: 15821
[19:35:48.831293] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1929 (0.2011)  time: 0.1676  data: 0.0001  max mem: 15821
[19:35:50.512477] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1965 (0.2015)  time: 0.1679  data: 0.0001  max mem: 15821
[19:35:52.196008] Test:  [ 50/345]  eta: 0:00:51  loss: 0.2020 (0.2019)  time: 0.1682  data: 0.0001  max mem: 15821
[19:35:53.883522] Test:  [ 60/345]  eta: 0:00:49  loss: 0.2052 (0.2033)  time: 0.1685  data: 0.0001  max mem: 15821
[19:35:55.574283] Test:  [ 70/345]  eta: 0:00:47  loss: 0.2095 (0.2051)  time: 0.1689  data: 0.0001  max mem: 15821
[19:35:57.267571] Test:  [ 80/345]  eta: 0:00:45  loss: 0.2016 (0.2034)  time: 0.1691  data: 0.0001  max mem: 15821
[19:35:58.966144] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1984 (0.2038)  time: 0.1695  data: 0.0001  max mem: 15821
[19:36:00.667588] Test:  [100/345]  eta: 0:00:42  loss: 0.2163 (0.2049)  time: 0.1699  data: 0.0001  max mem: 15821
[19:36:02.371780] Test:  [110/345]  eta: 0:00:40  loss: 0.2003 (0.2042)  time: 0.1702  data: 0.0001  max mem: 15821
[19:36:04.080529] Test:  [120/345]  eta: 0:00:38  loss: 0.1928 (0.2041)  time: 0.1706  data: 0.0001  max mem: 15821
[19:36:05.792375] Test:  [130/345]  eta: 0:00:36  loss: 0.2027 (0.2041)  time: 0.1710  data: 0.0001  max mem: 15821
[19:36:07.506732] Test:  [140/345]  eta: 0:00:35  loss: 0.2013 (0.2040)  time: 0.1712  data: 0.0001  max mem: 15821
[19:36:09.225390] Test:  [150/345]  eta: 0:00:33  loss: 0.1965 (0.2038)  time: 0.1716  data: 0.0001  max mem: 15821
[19:36:10.948857] Test:  [160/345]  eta: 0:00:31  loss: 0.1970 (0.2040)  time: 0.1720  data: 0.0001  max mem: 15821
[19:36:12.675691] Test:  [170/345]  eta: 0:00:30  loss: 0.2048 (0.2044)  time: 0.1724  data: 0.0001  max mem: 15821
[19:36:14.404378] Test:  [180/345]  eta: 0:00:28  loss: 0.2029 (0.2038)  time: 0.1727  data: 0.0001  max mem: 15821
[19:36:16.137638] Test:  [190/345]  eta: 0:00:26  loss: 0.1976 (0.2041)  time: 0.1730  data: 0.0001  max mem: 15821
[19:36:17.872561] Test:  [200/345]  eta: 0:00:24  loss: 0.2074 (0.2044)  time: 0.1733  data: 0.0001  max mem: 15821
[19:36:19.612713] Test:  [210/345]  eta: 0:00:23  loss: 0.1986 (0.2040)  time: 0.1737  data: 0.0001  max mem: 15821
[19:36:21.354701] Test:  [220/345]  eta: 0:00:21  loss: 0.1969 (0.2042)  time: 0.1740  data: 0.0001  max mem: 15821
[19:36:23.101004] Test:  [230/345]  eta: 0:00:19  loss: 0.2030 (0.2044)  time: 0.1743  data: 0.0001  max mem: 15821
[19:36:24.850469] Test:  [240/345]  eta: 0:00:18  loss: 0.2015 (0.2044)  time: 0.1747  data: 0.0001  max mem: 15821
[19:36:26.603470] Test:  [250/345]  eta: 0:00:16  loss: 0.1951 (0.2040)  time: 0.1751  data: 0.0001  max mem: 15821
[19:36:28.359798] Test:  [260/345]  eta: 0:00:14  loss: 0.1981 (0.2040)  time: 0.1754  data: 0.0001  max mem: 15821
[19:36:30.120740] Test:  [270/345]  eta: 0:00:12  loss: 0.2033 (0.2044)  time: 0.1758  data: 0.0001  max mem: 15821
[19:36:31.884955] Test:  [280/345]  eta: 0:00:11  loss: 0.2023 (0.2042)  time: 0.1762  data: 0.0001  max mem: 15821
[19:36:33.652201] Test:  [290/345]  eta: 0:00:09  loss: 0.2050 (0.2047)  time: 0.1765  data: 0.0001  max mem: 15821
[19:36:35.421681] Test:  [300/345]  eta: 0:00:07  loss: 0.2021 (0.2045)  time: 0.1768  data: 0.0001  max mem: 15821
[19:36:37.196594] Test:  [310/345]  eta: 0:00:06  loss: 0.1980 (0.2047)  time: 0.1772  data: 0.0001  max mem: 15821
[19:36:38.974998] Test:  [320/345]  eta: 0:00:04  loss: 0.1980 (0.2047)  time: 0.1776  data: 0.0001  max mem: 15821
[19:36:40.757389] Test:  [330/345]  eta: 0:00:02  loss: 0.1954 (0.2044)  time: 0.1780  data: 0.0001  max mem: 15821
[19:36:42.540779] Test:  [340/345]  eta: 0:00:00  loss: 0.1954 (0.2043)  time: 0.1782  data: 0.0001  max mem: 15821
[19:36:43.256565] Test:  [344/345]  eta: 0:00:00  loss: 0.2005 (0.2044)  time: 0.1784  data: 0.0001  max mem: 15821
[19:36:43.326504] Test: Total time: 0:01:00 (0.1740 s / it)
[19:36:53.271139] Test:  [ 0/57]  eta: 0:00:29  loss: 0.4357 (0.4357)  time: 0.5165  data: 0.3534  max mem: 15821
[19:36:54.925387] Test:  [10/57]  eta: 0:00:09  loss: 0.3792 (0.4060)  time: 0.1973  data: 0.0322  max mem: 15821
[19:36:56.582887] Test:  [20/57]  eta: 0:00:06  loss: 0.3792 (0.3912)  time: 0.1655  data: 0.0001  max mem: 15821
[19:36:58.244139] Test:  [30/57]  eta: 0:00:04  loss: 0.2515 (0.3351)  time: 0.1659  data: 0.0001  max mem: 15821
[19:36:59.910127] Test:  [40/57]  eta: 0:00:02  loss: 0.2258 (0.3089)  time: 0.1663  data: 0.0001  max mem: 15821
[19:37:01.578938] Test:  [50/57]  eta: 0:00:01  loss: 0.2466 (0.3058)  time: 0.1667  data: 0.0001  max mem: 15821
[19:37:02.480143] Test:  [56/57]  eta: 0:00:00  loss: 0.2638 (0.3094)  time: 0.1618  data: 0.0000  max mem: 15821
[19:37:02.546469] Test: Total time: 0:00:09 (0.1718 s / it)
[19:37:04.302661] Dice score of the network on the train images: 0.805611, val images: 0.812385
[19:37:04.302816] saving best_rec_model_0 @ epoch 15
[19:37:05.498635] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:37:06.453754] Epoch: [16]  [  0/345]  eta: 0:05:29  lr: 0.000100  loss: 0.2089 (0.2089)  time: 0.9539  data: 0.3536  max mem: 15821
[19:37:18.463248] Epoch: [16]  [ 20/345]  eta: 0:03:20  lr: 0.000100  loss: 0.2062 (0.2107)  time: 0.6004  data: 0.0001  max mem: 15821
[19:37:30.614795] Epoch: [16]  [ 40/345]  eta: 0:03:06  lr: 0.000101  loss: 0.2137 (0.2101)  time: 0.6075  data: 0.0001  max mem: 15821
[19:37:42.672459] Epoch: [16]  [ 60/345]  eta: 0:02:53  lr: 0.000101  loss: 0.2087 (0.2091)  time: 0.6028  data: 0.0001  max mem: 15821
[19:37:54.738032] Epoch: [16]  [ 80/345]  eta: 0:02:41  lr: 0.000101  loss: 0.2015 (0.2085)  time: 0.6032  data: 0.0001  max mem: 15821
[19:38:06.834611] Epoch: [16]  [100/345]  eta: 0:02:28  lr: 0.000102  loss: 0.2033 (0.2077)  time: 0.6048  data: 0.0001  max mem: 15821
[19:38:18.954896] Epoch: [16]  [120/345]  eta: 0:02:16  lr: 0.000102  loss: 0.1972 (0.2063)  time: 0.6060  data: 0.0001  max mem: 15821
[19:38:31.080068] Epoch: [16]  [140/345]  eta: 0:02:04  lr: 0.000103  loss: 0.2039 (0.2063)  time: 0.6062  data: 0.0001  max mem: 15821
[19:38:43.200736] Epoch: [16]  [160/345]  eta: 0:01:52  lr: 0.000103  loss: 0.2105 (0.2069)  time: 0.6060  data: 0.0001  max mem: 15821
[19:38:55.323323] Epoch: [16]  [180/345]  eta: 0:01:40  lr: 0.000103  loss: 0.2029 (0.2068)  time: 0.6061  data: 0.0001  max mem: 15821
[19:39:07.451015] Epoch: [16]  [200/345]  eta: 0:01:27  lr: 0.000104  loss: 0.1933 (0.2060)  time: 0.6063  data: 0.0001  max mem: 15821
[19:39:19.578237] Epoch: [16]  [220/345]  eta: 0:01:15  lr: 0.000104  loss: 0.2097 (0.2061)  time: 0.6063  data: 0.0001  max mem: 15821
[19:39:31.706173] Epoch: [16]  [240/345]  eta: 0:01:03  lr: 0.000104  loss: 0.2118 (0.2063)  time: 0.6064  data: 0.0001  max mem: 15821
[19:39:43.842542] Epoch: [16]  [260/345]  eta: 0:00:51  lr: 0.000105  loss: 0.2107 (0.2064)  time: 0.6068  data: 0.0001  max mem: 15821
[19:39:55.972042] Epoch: [16]  [280/345]  eta: 0:00:39  lr: 0.000105  loss: 0.2091 (0.2067)  time: 0.6064  data: 0.0001  max mem: 15821
[19:40:08.103654] Epoch: [16]  [300/345]  eta: 0:00:27  lr: 0.000105  loss: 0.2126 (0.2068)  time: 0.6065  data: 0.0001  max mem: 15821
[19:40:20.229266] Epoch: [16]  [320/345]  eta: 0:00:15  lr: 0.000106  loss: 0.1937 (0.2062)  time: 0.6062  data: 0.0001  max mem: 15821
[19:40:32.348067] Epoch: [16]  [340/345]  eta: 0:00:03  lr: 0.000106  loss: 0.1977 (0.2059)  time: 0.6059  data: 0.0001  max mem: 15821
[19:40:34.771501] Epoch: [16]  [344/345]  eta: 0:00:00  lr: 0.000106  loss: 0.2057 (0.2058)  time: 0.6058  data: 0.0001  max mem: 15821
[19:40:34.840355] Epoch: [16] Total time: 0:03:29 (0.6068 s / it)
[19:40:34.840748] Averaged stats: lr: 0.000106  loss: 0.2057 (0.2058)
[19:40:35.369115] Test:  [  0/345]  eta: 0:03:00  loss: 0.1969 (0.1969)  time: 0.5231  data: 0.3587  max mem: 15821
[19:40:37.040505] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1972 (0.1997)  time: 0.1994  data: 0.0327  max mem: 15821
[19:40:38.714836] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1935 (0.1948)  time: 0.1672  data: 0.0001  max mem: 15821
[19:40:40.392257] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1893 (0.1954)  time: 0.1675  data: 0.0001  max mem: 15821
[19:40:42.072317] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1951 (0.1978)  time: 0.1678  data: 0.0001  max mem: 15821
[19:40:43.755356] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1935 (0.1967)  time: 0.1681  data: 0.0001  max mem: 15821
[19:40:45.442209] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1910 (0.1964)  time: 0.1684  data: 0.0001  max mem: 15821
[19:40:47.132292] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1944 (0.1978)  time: 0.1688  data: 0.0001  max mem: 15821
[19:40:48.826611] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1921 (0.1970)  time: 0.1692  data: 0.0001  max mem: 15821
[19:40:50.525547] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1873 (0.1968)  time: 0.1696  data: 0.0001  max mem: 15821
[19:40:52.226277] Test:  [100/345]  eta: 0:00:42  loss: 0.1891 (0.1965)  time: 0.1699  data: 0.0001  max mem: 15821
[19:40:53.930928] Test:  [110/345]  eta: 0:00:40  loss: 0.1999 (0.1972)  time: 0.1702  data: 0.0001  max mem: 15821
[19:40:55.639940] Test:  [120/345]  eta: 0:00:38  loss: 0.2033 (0.1984)  time: 0.1706  data: 0.0001  max mem: 15821
[19:40:57.351877] Test:  [130/345]  eta: 0:00:36  loss: 0.1894 (0.1973)  time: 0.1710  data: 0.0001  max mem: 15821
[19:40:59.067019] Test:  [140/345]  eta: 0:00:35  loss: 0.1861 (0.1979)  time: 0.1713  data: 0.0001  max mem: 15821
[19:41:00.785887] Test:  [150/345]  eta: 0:00:33  loss: 0.1861 (0.1972)  time: 0.1716  data: 0.0001  max mem: 15821
[19:41:02.508415] Test:  [160/345]  eta: 0:00:31  loss: 0.1884 (0.1971)  time: 0.1720  data: 0.0001  max mem: 15821
[19:41:04.234441] Test:  [170/345]  eta: 0:00:30  loss: 0.1926 (0.1968)  time: 0.1724  data: 0.0001  max mem: 15821
[19:41:05.965457] Test:  [180/345]  eta: 0:00:28  loss: 0.1926 (0.1966)  time: 0.1728  data: 0.0001  max mem: 15821
[19:41:07.698641] Test:  [190/345]  eta: 0:00:26  loss: 0.1908 (0.1964)  time: 0.1731  data: 0.0001  max mem: 15821
[19:41:09.435237] Test:  [200/345]  eta: 0:00:24  loss: 0.1864 (0.1958)  time: 0.1734  data: 0.0001  max mem: 15821
[19:41:11.174672] Test:  [210/345]  eta: 0:00:23  loss: 0.1865 (0.1958)  time: 0.1737  data: 0.0001  max mem: 15821
[19:41:12.918400] Test:  [220/345]  eta: 0:00:21  loss: 0.1865 (0.1954)  time: 0.1741  data: 0.0001  max mem: 15821
[19:41:14.664395] Test:  [230/345]  eta: 0:00:19  loss: 0.1860 (0.1954)  time: 0.1744  data: 0.0001  max mem: 15821
[19:41:16.414917] Test:  [240/345]  eta: 0:00:18  loss: 0.1902 (0.1953)  time: 0.1748  data: 0.0001  max mem: 15821
[19:41:18.167543] Test:  [250/345]  eta: 0:00:16  loss: 0.1930 (0.1957)  time: 0.1751  data: 0.0001  max mem: 15821
[19:41:19.924384] Test:  [260/345]  eta: 0:00:14  loss: 0.2009 (0.1960)  time: 0.1754  data: 0.0001  max mem: 15821
[19:41:21.685074] Test:  [270/345]  eta: 0:00:12  loss: 0.2009 (0.1963)  time: 0.1758  data: 0.0001  max mem: 15821
[19:41:23.448264] Test:  [280/345]  eta: 0:00:11  loss: 0.2015 (0.1965)  time: 0.1761  data: 0.0001  max mem: 15821
[19:41:25.215113] Test:  [290/345]  eta: 0:00:09  loss: 0.1971 (0.1964)  time: 0.1764  data: 0.0001  max mem: 15821
[19:41:26.984915] Test:  [300/345]  eta: 0:00:07  loss: 0.1849 (0.1964)  time: 0.1768  data: 0.0001  max mem: 15821
[19:41:28.759177] Test:  [310/345]  eta: 0:00:06  loss: 0.1863 (0.1962)  time: 0.1771  data: 0.0001  max mem: 15821
[19:41:30.536364] Test:  [320/345]  eta: 0:00:04  loss: 0.1868 (0.1963)  time: 0.1775  data: 0.0001  max mem: 15821
[19:41:32.317376] Test:  [330/345]  eta: 0:00:02  loss: 0.1946 (0.1963)  time: 0.1779  data: 0.0001  max mem: 15821
[19:41:34.102693] Test:  [340/345]  eta: 0:00:00  loss: 0.1946 (0.1963)  time: 0.1783  data: 0.0001  max mem: 15821
[19:41:34.818221] Test:  [344/345]  eta: 0:00:00  loss: 0.2022 (0.1964)  time: 0.1784  data: 0.0001  max mem: 15821
[19:41:34.886438] Test: Total time: 0:01:00 (0.1740 s / it)
[19:41:44.927382] Test:  [ 0/57]  eta: 0:00:26  loss: 0.4605 (0.4605)  time: 0.4574  data: 0.2944  max mem: 15821
[19:41:46.579578] Test:  [10/57]  eta: 0:00:09  loss: 0.3998 (0.4361)  time: 0.1917  data: 0.0269  max mem: 15821
[19:41:48.238905] Test:  [20/57]  eta: 0:00:06  loss: 0.3998 (0.4164)  time: 0.1655  data: 0.0001  max mem: 15821
[19:41:49.900800] Test:  [30/57]  eta: 0:00:04  loss: 0.2844 (0.3600)  time: 0.1660  data: 0.0001  max mem: 15821
[19:41:51.567151] Test:  [40/57]  eta: 0:00:02  loss: 0.2419 (0.3354)  time: 0.1664  data: 0.0001  max mem: 15821
[19:41:53.236763] Test:  [50/57]  eta: 0:00:01  loss: 0.2667 (0.3344)  time: 0.1667  data: 0.0001  max mem: 15821
[19:41:54.137500] Test:  [56/57]  eta: 0:00:00  loss: 0.2845 (0.3391)  time: 0.1618  data: 0.0001  max mem: 15821
[19:41:54.198790] Test: Total time: 0:00:09 (0.1707 s / it)
[19:41:55.960802] Dice score of the network on the train images: 0.821404, val images: 0.797925
[19:41:55.964731] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:41:56.870221] Epoch: [17]  [  0/345]  eta: 0:05:12  lr: 0.000106  loss: 0.2089 (0.2089)  time: 0.9044  data: 0.2994  max mem: 15821
[19:42:08.889393] Epoch: [17]  [ 20/345]  eta: 0:03:19  lr: 0.000107  loss: 0.1861 (0.1989)  time: 0.6009  data: 0.0001  max mem: 15821
[19:42:20.955659] Epoch: [17]  [ 40/345]  eta: 0:03:05  lr: 0.000107  loss: 0.2058 (0.2045)  time: 0.6033  data: 0.0001  max mem: 15821
[19:42:33.034243] Epoch: [17]  [ 60/345]  eta: 0:02:53  lr: 0.000107  loss: 0.1913 (0.2009)  time: 0.6039  data: 0.0001  max mem: 15821
[19:42:45.122809] Epoch: [17]  [ 80/345]  eta: 0:02:40  lr: 0.000108  loss: 0.1962 (0.1997)  time: 0.6044  data: 0.0001  max mem: 15821
[19:42:57.220778] Epoch: [17]  [100/345]  eta: 0:02:28  lr: 0.000108  loss: 0.1894 (0.1985)  time: 0.6048  data: 0.0001  max mem: 15821
[19:43:09.335795] Epoch: [17]  [120/345]  eta: 0:02:16  lr: 0.000108  loss: 0.1962 (0.1993)  time: 0.6057  data: 0.0001  max mem: 15821
[19:43:21.463581] Epoch: [17]  [140/345]  eta: 0:02:04  lr: 0.000109  loss: 0.2029 (0.2001)  time: 0.6063  data: 0.0001  max mem: 15821
[19:43:33.579698] Epoch: [17]  [160/345]  eta: 0:01:52  lr: 0.000109  loss: 0.1895 (0.1991)  time: 0.6058  data: 0.0001  max mem: 15821
[19:43:45.713809] Epoch: [17]  [180/345]  eta: 0:01:40  lr: 0.000110  loss: 0.1875 (0.1982)  time: 0.6067  data: 0.0001  max mem: 15821
[19:43:57.845023] Epoch: [17]  [200/345]  eta: 0:01:27  lr: 0.000110  loss: 0.1912 (0.1979)  time: 0.6065  data: 0.0001  max mem: 15821
[19:44:09.954421] Epoch: [17]  [220/345]  eta: 0:01:15  lr: 0.000110  loss: 0.2095 (0.1992)  time: 0.6054  data: 0.0001  max mem: 15821
[19:44:22.068550] Epoch: [17]  [240/345]  eta: 0:01:03  lr: 0.000111  loss: 0.2054 (0.1996)  time: 0.6057  data: 0.0001  max mem: 15821
[19:44:34.181105] Epoch: [17]  [260/345]  eta: 0:00:51  lr: 0.000111  loss: 0.1927 (0.1992)  time: 0.6056  data: 0.0001  max mem: 15821
[19:44:46.286056] Epoch: [17]  [280/345]  eta: 0:00:39  lr: 0.000111  loss: 0.1852 (0.1988)  time: 0.6052  data: 0.0001  max mem: 15821

[19:44:58.389423] Epoch: [17]  [300/345]  eta: 0:00:27  lr: 0.000112  loss: 0.2081 (0.1994)  time: 0.6051  data: 0.0001  max mem: 15821
[19:45:10.494451] Epoch: [17]  [320/345]  eta: 0:00:15  lr: 0.000112  loss: 0.1900 (0.1995)  time: 0.6052  data: 0.0001  max mem: 15821
[19:45:22.589972] Epoch: [17]  [340/345]  eta: 0:00:03  lr: 0.000112  loss: 0.1917 (0.1996)  time: 0.6047  data: 0.0001  max mem: 15821
[19:45:25.012835] Epoch: [17]  [344/345]  eta: 0:00:00  lr: 0.000112  loss: 0.1923 (0.1996)  time: 0.6049  data: 0.0001  max mem: 15821
[19:45:25.074901] Epoch: [17] Total time: 0:03:29 (0.6061 s / it)
[19:45:25.075212] Averaged stats: lr: 0.000112  loss: 0.1923 (0.1996)
[19:45:25.561941] Test:  [  0/345]  eta: 0:02:46  loss: 0.1537 (0.1537)  time: 0.4813  data: 0.3173  max mem: 15821
[19:45:27.233188] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1688 (0.1729)  time: 0.1956  data: 0.0289  max mem: 15821
[19:45:28.906791] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1834 (0.1800)  time: 0.1672  data: 0.0001  max mem: 15821
[19:45:30.584398] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1878 (0.1845)  time: 0.1675  data: 0.0001  max mem: 15821
[19:45:32.264600] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1901 (0.1871)  time: 0.1678  data: 0.0001  max mem: 15821
[19:45:33.948267] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1810 (0.1853)  time: 0.1681  data: 0.0001  max mem: 15821
[19:45:35.635537] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1810 (0.1880)  time: 0.1685  data: 0.0001  max mem: 15821
[19:45:37.325979] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1889 (0.1889)  time: 0.1688  data: 0.0001  max mem: 15821
[19:45:39.021821] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1841 (0.1891)  time: 0.1692  data: 0.0001  max mem: 15821
[19:45:40.720634] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1782 (0.1880)  time: 0.1697  data: 0.0001  max mem: 15821
[19:45:42.422224] Test:  [100/345]  eta: 0:00:42  loss: 0.1732 (0.1882)  time: 0.1700  data: 0.0001  max mem: 15821
[19:45:44.128030] Test:  [110/345]  eta: 0:00:40  loss: 0.1855 (0.1879)  time: 0.1703  data: 0.0001  max mem: 15821
[19:45:45.835717] Test:  [120/345]  eta: 0:00:38  loss: 0.1777 (0.1870)  time: 0.1706  data: 0.0001  max mem: 15821
[19:45:47.547950] Test:  [130/345]  eta: 0:00:36  loss: 0.1762 (0.1867)  time: 0.1709  data: 0.0001  max mem: 15821
[19:45:49.263843] Test:  [140/345]  eta: 0:00:35  loss: 0.1834 (0.1868)  time: 0.1713  data: 0.0001  max mem: 15821
[19:45:50.981460] Test:  [150/345]  eta: 0:00:33  loss: 0.1822 (0.1869)  time: 0.1716  data: 0.0001  max mem: 15821
[19:45:52.704062] Test:  [160/345]  eta: 0:00:31  loss: 0.1782 (0.1875)  time: 0.1719  data: 0.0001  max mem: 15821
[19:45:54.429983] Test:  [170/345]  eta: 0:00:30  loss: 0.1805 (0.1872)  time: 0.1724  data: 0.0001  max mem: 15821
[19:45:56.159946] Test:  [180/345]  eta: 0:00:28  loss: 0.1807 (0.1875)  time: 0.1727  data: 0.0001  max mem: 15821
[19:45:57.892758] Test:  [190/345]  eta: 0:00:26  loss: 0.1859 (0.1871)  time: 0.1731  data: 0.0001  max mem: 15821
[19:45:59.628970] Test:  [200/345]  eta: 0:00:24  loss: 0.1784 (0.1870)  time: 0.1734  data: 0.0001  max mem: 15821
[19:46:01.369855] Test:  [210/345]  eta: 0:00:23  loss: 0.1829 (0.1873)  time: 0.1738  data: 0.0001  max mem: 15821
[19:46:03.112987] Test:  [220/345]  eta: 0:00:21  loss: 0.1814 (0.1872)  time: 0.1741  data: 0.0001  max mem: 15821
[19:46:04.860461] Test:  [230/345]  eta: 0:00:19  loss: 0.1777 (0.1869)  time: 0.1745  data: 0.0001  max mem: 15821
[19:46:06.609964] Test:  [240/345]  eta: 0:00:18  loss: 0.1744 (0.1872)  time: 0.1748  data: 0.0001  max mem: 15821
[19:46:08.364027] Test:  [250/345]  eta: 0:00:16  loss: 0.1816 (0.1872)  time: 0.1751  data: 0.0001  max mem: 15821
[19:46:10.123266] Test:  [260/345]  eta: 0:00:14  loss: 0.1816 (0.1870)  time: 0.1756  data: 0.0001  max mem: 15821
[19:46:11.883066] Test:  [270/345]  eta: 0:00:12  loss: 0.1843 (0.1870)  time: 0.1758  data: 0.0001  max mem: 15821
[19:46:13.646492] Test:  [280/345]  eta: 0:00:11  loss: 0.1836 (0.1869)  time: 0.1761  data: 0.0001  max mem: 15821
[19:46:15.414825] Test:  [290/345]  eta: 0:00:09  loss: 0.1909 (0.1875)  time: 0.1765  data: 0.0001  max mem: 15821
[19:46:17.185416] Test:  [300/345]  eta: 0:00:07  loss: 0.1917 (0.1877)  time: 0.1769  data: 0.0001  max mem: 15821
[19:46:18.959727] Test:  [310/345]  eta: 0:00:06  loss: 0.1856 (0.1878)  time: 0.1772  data: 0.0001  max mem: 15821
[19:46:20.738109] Test:  [320/345]  eta: 0:00:04  loss: 0.1852 (0.1876)  time: 0.1776  data: 0.0001  max mem: 15821
[19:46:22.520812] Test:  [330/345]  eta: 0:00:02  loss: 0.1773 (0.1873)  time: 0.1780  data: 0.0001  max mem: 15821
[19:46:24.305745] Test:  [340/345]  eta: 0:00:00  loss: 0.1824 (0.1873)  time: 0.1783  data: 0.0001  max mem: 15821
[19:46:25.022153] Test:  [344/345]  eta: 0:00:00  loss: 0.1826 (0.1874)  time: 0.1786  data: 0.0001  max mem: 15821
[19:46:25.080190] Test: Total time: 0:01:00 (0.1739 s / it)
[19:46:35.099458] Test:  [ 0/57]  eta: 0:00:29  loss: 0.4433 (0.4433)  time: 0.5242  data: 0.3613  max mem: 15821
[19:46:36.753804] Test:  [10/57]  eta: 0:00:09  loss: 0.4099 (0.4305)  time: 0.1980  data: 0.0329  max mem: 15821
[19:46:38.412721] Test:  [20/57]  eta: 0:00:06  loss: 0.4077 (0.4120)  time: 0.1656  data: 0.0001  max mem: 15821
[19:46:40.075022] Test:  [30/57]  eta: 0:00:04  loss: 0.2703 (0.3617)  time: 0.1660  data: 0.0001  max mem: 15821
[19:46:41.741294] Test:  [40/57]  eta: 0:00:02  loss: 0.2625 (0.3426)  time: 0.1664  data: 0.0001  max mem: 15821
[19:46:43.411795] Test:  [50/57]  eta: 0:00:01  loss: 0.2828 (0.3413)  time: 0.1668  data: 0.0001  max mem: 15821
[19:46:44.314166] Test:  [56/57]  eta: 0:00:00  loss: 0.3000 (0.3477)  time: 0.1619  data: 0.0001  max mem: 15821
[19:46:44.385921] Test: Total time: 0:00:09 (0.1721 s / it)
[19:46:46.157006] Dice score of the network on the train images: 0.835518, val images: 0.783860
[19:46:46.161405] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:46:47.060553] Epoch: [18]  [  0/345]  eta: 0:05:09  lr: 0.000113  loss: 0.1976 (0.1976)  time: 0.8980  data: 0.2958  max mem: 15821
[19:46:59.086236] Epoch: [18]  [ 20/345]  eta: 0:03:19  lr: 0.000113  loss: 0.1968 (0.2021)  time: 0.6012  data: 0.0001  max mem: 15821
[19:47:11.265858] Epoch: [18]  [ 40/345]  eta: 0:03:06  lr: 0.000113  loss: 0.1948 (0.2006)  time: 0.6089  data: 0.0001  max mem: 15821
[19:47:23.326312] Epoch: [18]  [ 60/345]  eta: 0:02:53  lr: 0.000114  loss: 0.2083 (0.2008)  time: 0.6030  data: 0.0001  max mem: 15821
[19:47:35.389970] Epoch: [18]  [ 80/345]  eta: 0:02:41  lr: 0.000114  loss: 0.1885 (0.1986)  time: 0.6031  data: 0.0001  max mem: 15821
[19:47:47.469033] Epoch: [18]  [100/345]  eta: 0:02:28  lr: 0.000114  loss: 0.1788 (0.1958)  time: 0.6039  data: 0.0001  max mem: 15821
[19:47:59.563204] Epoch: [18]  [120/345]  eta: 0:02:16  lr: 0.000115  loss: 0.1868 (0.1940)  time: 0.6047  data: 0.0001  max mem: 15821
[19:48:11.696526] Epoch: [18]  [140/345]  eta: 0:02:04  lr: 0.000115  loss: 0.1846 (0.1943)  time: 0.6066  data: 0.0001  max mem: 15821
[19:48:23.829688] Epoch: [18]  [160/345]  eta: 0:01:52  lr: 0.000115  loss: 0.1931 (0.1944)  time: 0.6066  data: 0.0001  max mem: 15821
[19:48:35.966466] Epoch: [18]  [180/345]  eta: 0:01:40  lr: 0.000116  loss: 0.1922 (0.1937)  time: 0.6068  data: 0.0001  max mem: 15821
[19:48:48.113139] Epoch: [18]  [200/345]  eta: 0:01:27  lr: 0.000116  loss: 0.1835 (0.1939)  time: 0.6073  data: 0.0001  max mem: 15821
[19:49:00.247307] Epoch: [18]  [220/345]  eta: 0:01:15  lr: 0.000116  loss: 0.1959 (0.1942)  time: 0.6067  data: 0.0001  max mem: 15821
[19:49:12.380405] Epoch: [18]  [240/345]  eta: 0:01:03  lr: 0.000117  loss: 0.1824 (0.1938)  time: 0.6066  data: 0.0001  max mem: 15821
[19:49:24.483895] Epoch: [18]  [260/345]  eta: 0:00:51  lr: 0.000117  loss: 0.1946 (0.1940)  time: 0.6051  data: 0.0001  max mem: 15821
[19:49:36.604134] Epoch: [18]  [280/345]  eta: 0:00:39  lr: 0.000118  loss: 0.2035 (0.1948)  time: 0.6060  data: 0.0001  max mem: 15821

[19:49:48.723865] Epoch: [18]  [300/345]  eta: 0:00:27  lr: 0.000118  loss: 0.1926 (0.1949)  time: 0.6059  data: 0.0001  max mem: 15821
[19:50:00.847602] Epoch: [18]  [320/345]  eta: 0:00:15  lr: 0.000118  loss: 0.1909 (0.1949)  time: 0.6061  data: 0.0001  max mem: 15821
[19:50:12.964136] Epoch: [18]  [340/345]  eta: 0:00:03  lr: 0.000119  loss: 0.1928 (0.1949)  time: 0.6058  data: 0.0001  max mem: 15821
[19:50:15.386257] Epoch: [18]  [344/345]  eta: 0:00:00  lr: 0.000119  loss: 0.1958 (0.1951)  time: 0.6057  data: 0.0001  max mem: 15821
[19:50:15.456432] Epoch: [18] Total time: 0:03:29 (0.6067 s / it)
[19:50:15.456740] Averaged stats: lr: 0.000119  loss: 0.1958 (0.1951)
[19:50:15.971549] Test:  [  0/345]  eta: 0:02:55  loss: 0.1913 (0.1913)  time: 0.5093  data: 0.3453  max mem: 15821
[19:50:17.642184] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1775 (0.1791)  time: 0.1981  data: 0.0315  max mem: 15821
[19:50:19.317303] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1770 (0.1856)  time: 0.1672  data: 0.0001  max mem: 15821
[19:50:20.993974] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1829 (0.1849)  time: 0.1675  data: 0.0001  max mem: 15821
[19:50:22.675311] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1829 (0.1849)  time: 0.1678  data: 0.0001  max mem: 15821
[19:50:24.359607] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1837 (0.1859)  time: 0.1682  data: 0.0001  max mem: 15821
[19:50:26.047536] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1833 (0.1860)  time: 0.1685  data: 0.0001  max mem: 15821
[19:50:27.738055] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1792 (0.1847)  time: 0.1689  data: 0.0001  max mem: 15821
[19:50:29.433026] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1853 (0.1863)  time: 0.1692  data: 0.0001  max mem: 15821
[19:50:31.131993] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1855 (0.1858)  time: 0.1696  data: 0.0001  max mem: 15821
[19:50:32.833114] Test:  [100/345]  eta: 0:00:42  loss: 0.1782 (0.1850)  time: 0.1699  data: 0.0001  max mem: 15821
[19:50:34.537189] Test:  [110/345]  eta: 0:00:40  loss: 0.1784 (0.1851)  time: 0.1702  data: 0.0001  max mem: 15821
[19:50:36.245346] Test:  [120/345]  eta: 0:00:38  loss: 0.1813 (0.1856)  time: 0.1705  data: 0.0001  max mem: 15821
[19:50:37.957500] Test:  [130/345]  eta: 0:00:36  loss: 0.1813 (0.1848)  time: 0.1709  data: 0.0001  max mem: 15821
[19:50:39.673361] Test:  [140/345]  eta: 0:00:35  loss: 0.1769 (0.1851)  time: 0.1713  data: 0.0001  max mem: 15821
[19:50:41.392262] Test:  [150/345]  eta: 0:00:33  loss: 0.1817 (0.1850)  time: 0.1717  data: 0.0001  max mem: 15821
[19:50:43.114351] Test:  [160/345]  eta: 0:00:31  loss: 0.1844 (0.1855)  time: 0.1720  data: 0.0001  max mem: 15821
[19:50:44.839518] Test:  [170/345]  eta: 0:00:30  loss: 0.1807 (0.1856)  time: 0.1723  data: 0.0001  max mem: 15821
[19:50:46.570369] Test:  [180/345]  eta: 0:00:28  loss: 0.1775 (0.1859)  time: 0.1727  data: 0.0001  max mem: 15821
[19:50:48.305099] Test:  [190/345]  eta: 0:00:26  loss: 0.1844 (0.1858)  time: 0.1732  data: 0.0001  max mem: 15821
[19:50:50.041689] Test:  [200/345]  eta: 0:00:24  loss: 0.1884 (0.1862)  time: 0.1735  data: 0.0001  max mem: 15821
[19:50:51.781687] Test:  [210/345]  eta: 0:00:23  loss: 0.1849 (0.1862)  time: 0.1738  data: 0.0001  max mem: 15821
[19:50:53.525289] Test:  [220/345]  eta: 0:00:21  loss: 0.1839 (0.1867)  time: 0.1741  data: 0.0001  max mem: 15821
[19:50:55.271231] Test:  [230/345]  eta: 0:00:19  loss: 0.1857 (0.1869)  time: 0.1744  data: 0.0001  max mem: 15821
[19:50:57.020345] Test:  [240/345]  eta: 0:00:18  loss: 0.1813 (0.1868)  time: 0.1747  data: 0.0001  max mem: 15821
[19:50:58.773720] Test:  [250/345]  eta: 0:00:16  loss: 0.1892 (0.1871)  time: 0.1750  data: 0.0001  max mem: 15821
[19:51:00.530560] Test:  [260/345]  eta: 0:00:14  loss: 0.1981 (0.1872)  time: 0.1754  data: 0.0001  max mem: 15821
[19:51:02.290628] Test:  [270/345]  eta: 0:00:12  loss: 0.1981 (0.1876)  time: 0.1758  data: 0.0001  max mem: 15821
[19:51:04.052989] Test:  [280/345]  eta: 0:00:11  loss: 0.1891 (0.1875)  time: 0.1761  data: 0.0001  max mem: 15821
[19:51:05.819763] Test:  [290/345]  eta: 0:00:09  loss: 0.1825 (0.1876)  time: 0.1764  data: 0.0001  max mem: 15821
[19:51:07.589803] Test:  [300/345]  eta: 0:00:07  loss: 0.1712 (0.1870)  time: 0.1768  data: 0.0001  max mem: 15821
[19:51:09.364252] Test:  [310/345]  eta: 0:00:06  loss: 0.1725 (0.1867)  time: 0.1772  data: 0.0001  max mem: 15821
[19:51:11.142509] Test:  [320/345]  eta: 0:00:04  loss: 0.1739 (0.1868)  time: 0.1776  data: 0.0001  max mem: 15821
[19:51:12.924186] Test:  [330/345]  eta: 0:00:02  loss: 0.1663 (0.1861)  time: 0.1779  data: 0.0001  max mem: 15821
[19:51:14.708240] Test:  [340/345]  eta: 0:00:00  loss: 0.1808 (0.1865)  time: 0.1782  data: 0.0001  max mem: 15821
[19:51:15.424000] Test:  [344/345]  eta: 0:00:00  loss: 0.1808 (0.1865)  time: 0.1784  data: 0.0001  max mem: 15821
[19:51:15.484466] Test: Total time: 0:01:00 (0.1740 s / it)
[19:51:25.473823] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4663 (0.4663)  time: 0.4524  data: 0.2890  max mem: 15821
[19:51:27.127079] Test:  [10/57]  eta: 0:00:08  loss: 0.4271 (0.4433)  time: 0.1913  data: 0.0264  max mem: 15821
[19:51:28.784231] Test:  [20/57]  eta: 0:00:06  loss: 0.4271 (0.4252)  time: 0.1654  data: 0.0001  max mem: 15821
[19:51:30.446270] Test:  [30/57]  eta: 0:00:04  loss: 0.2839 (0.3715)  time: 0.1659  data: 0.0001  max mem: 15821
[19:51:32.110562] Test:  [40/57]  eta: 0:00:02  loss: 0.2684 (0.3497)  time: 0.1662  data: 0.0001  max mem: 15821
[19:51:33.780531] Test:  [50/57]  eta: 0:00:01  loss: 0.2764 (0.3463)  time: 0.1667  data: 0.0001  max mem: 15821
[19:51:34.680722] Test:  [56/57]  eta: 0:00:00  loss: 0.3047 (0.3500)  time: 0.1617  data: 0.0000  max mem: 15821
[19:51:34.754090] Test: Total time: 0:00:09 (0.1708 s / it)
[19:51:36.508613] Dice score of the network on the train images: 0.840985, val images: 0.793331
[19:51:36.512594] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:51:37.431373] Epoch: [19]  [  0/345]  eta: 0:05:16  lr: 0.000119  loss: 0.2022 (0.2022)  time: 0.9176  data: 0.3165  max mem: 15821
[19:51:49.432185] Epoch: [19]  [ 20/345]  eta: 0:03:19  lr: 0.000119  loss: 0.1972 (0.1956)  time: 0.6000  data: 0.0001  max mem: 15821
[19:52:01.492387] Epoch: [19]  [ 40/345]  eta: 0:03:05  lr: 0.000119  loss: 0.1860 (0.1934)  time: 0.6030  data: 0.0001  max mem: 15821
[19:52:13.565007] Epoch: [19]  [ 60/345]  eta: 0:02:53  lr: 0.000120  loss: 0.1833 (0.1915)  time: 0.6036  data: 0.0001  max mem: 15821
[19:52:25.648685] Epoch: [19]  [ 80/345]  eta: 0:02:40  lr: 0.000120  loss: 0.1899 (0.1917)  time: 0.6041  data: 0.0001  max mem: 15821
[19:52:37.748626] Epoch: [19]  [100/345]  eta: 0:02:28  lr: 0.000121  loss: 0.1911 (0.1930)  time: 0.6049  data: 0.0001  max mem: 15821
[19:52:49.854753] Epoch: [19]  [120/345]  eta: 0:02:16  lr: 0.000121  loss: 0.1936 (0.1931)  time: 0.6053  data: 0.0001  max mem: 15821
[19:53:01.964009] Epoch: [19]  [140/345]  eta: 0:02:04  lr: 0.000121  loss: 0.1792 (0.1920)  time: 0.6054  data: 0.0001  max mem: 15821
[19:53:14.093085] Epoch: [19]  [160/345]  eta: 0:01:52  lr: 0.000122  loss: 0.1819 (0.1909)  time: 0.6064  data: 0.0001  max mem: 15821
[19:53:26.222465] Epoch: [19]  [180/345]  eta: 0:01:40  lr: 0.000122  loss: 0.1887 (0.1909)  time: 0.6064  data: 0.0001  max mem: 15821
[19:53:38.355649] Epoch: [19]  [200/345]  eta: 0:01:27  lr: 0.000122  loss: 0.1960 (0.1919)  time: 0.6066  data: 0.0001  max mem: 15821
[19:53:50.483880] Epoch: [19]  [220/345]  eta: 0:01:15  lr: 0.000123  loss: 0.1765 (0.1916)  time: 0.6064  data: 0.0001  max mem: 15821
[19:54:02.608657] Epoch: [19]  [240/345]  eta: 0:01:03  lr: 0.000123  loss: 0.1827 (0.1911)  time: 0.6062  data: 0.0001  max mem: 15821
[19:54:14.719164] Epoch: [19]  [260/345]  eta: 0:00:51  lr: 0.000123  loss: 0.1880 (0.1915)  time: 0.6055  data: 0.0001  max mem: 15821
[19:54:26.823648] Epoch: [19]  [280/345]  eta: 0:00:39  lr: 0.000124  loss: 0.1969 (0.1921)  time: 0.6052  data: 0.0001  max mem: 15821
[19:54:39.007558] Epoch: [19]  [300/345]  eta: 0:00:27  lr: 0.000124  loss: 0.1926 (0.1921)  time: 0.6091  data: 0.0001  max mem: 15821
[19:54:51.106771] Epoch: [19]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.1976 (0.1928)  time: 0.6049  data: 0.0001  max mem: 15821
[19:55:03.189779] Epoch: [19]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.2002 (0.1934)  time: 0.6041  data: 0.0001  max mem: 15821
[19:55:05.609175] Epoch: [19]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.1942 (0.1935)  time: 0.6041  data: 0.0001  max mem: 15821
[19:55:05.683680] Epoch: [19] Total time: 0:03:29 (0.6063 s / it)
[19:55:05.683943] Averaged stats: lr: 0.000125  loss: 0.1942 (0.1935)
[19:55:06.230341] Test:  [  0/345]  eta: 0:03:06  loss: 0.1755 (0.1755)  time: 0.5412  data: 0.3772  max mem: 15821
[19:55:07.900298] Test:  [ 10/345]  eta: 0:01:07  loss: 0.1698 (0.1733)  time: 0.2009  data: 0.0344  max mem: 15821
[19:55:09.573646] Test:  [ 20/345]  eta: 0:01:00  loss: 0.1698 (0.1766)  time: 0.1671  data: 0.0001  max mem: 15821
[19:55:11.250373] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1671 (0.1764)  time: 0.1674  data: 0.0001  max mem: 15821
[19:55:12.930834] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1667 (0.1778)  time: 0.1678  data: 0.0001  max mem: 15821
[19:55:14.613797] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1832 (0.1808)  time: 0.1681  data: 0.0001  max mem: 15821
[19:55:16.300918] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1885 (0.1817)  time: 0.1684  data: 0.0001  max mem: 15821
[19:55:17.991126] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1791 (0.1811)  time: 0.1688  data: 0.0001  max mem: 15821
[19:55:19.685520] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1884 (0.1836)  time: 0.1692  data: 0.0001  max mem: 15821
[19:55:21.383200] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1883 (0.1834)  time: 0.1695  data: 0.0001  max mem: 15821
[19:55:23.083691] Test:  [100/345]  eta: 0:00:42  loss: 0.1804 (0.1836)  time: 0.1698  data: 0.0001  max mem: 15821
[19:55:24.788124] Test:  [110/345]  eta: 0:00:40  loss: 0.1755 (0.1826)  time: 0.1702  data: 0.0001  max mem: 15821
[19:55:26.496277] Test:  [120/345]  eta: 0:00:38  loss: 0.1721 (0.1826)  time: 0.1706  data: 0.0001  max mem: 15821
[19:55:28.207453] Test:  [130/345]  eta: 0:00:36  loss: 0.1730 (0.1816)  time: 0.1709  data: 0.0001  max mem: 15821
[19:55:29.921474] Test:  [140/345]  eta: 0:00:35  loss: 0.1754 (0.1813)  time: 0.1712  data: 0.0001  max mem: 15821
[19:55:31.639167] Test:  [150/345]  eta: 0:00:33  loss: 0.1843 (0.1824)  time: 0.1715  data: 0.0001  max mem: 15821
[19:55:33.361393] Test:  [160/345]  eta: 0:00:31  loss: 0.1900 (0.1821)  time: 0.1719  data: 0.0001  max mem: 15821
[19:55:35.087858] Test:  [170/345]  eta: 0:00:30  loss: 0.1736 (0.1814)  time: 0.1724  data: 0.0001  max mem: 15821
[19:55:36.817298] Test:  [180/345]  eta: 0:00:28  loss: 0.1785 (0.1821)  time: 0.1727  data: 0.0001  max mem: 15821
[19:55:38.550240] Test:  [190/345]  eta: 0:00:26  loss: 0.1922 (0.1825)  time: 0.1731  data: 0.0001  max mem: 15821
[19:55:40.285034] Test:  [200/345]  eta: 0:00:24  loss: 0.1836 (0.1829)  time: 0.1733  data: 0.0001  max mem: 15821
[19:55:42.024237] Test:  [210/345]  eta: 0:00:23  loss: 0.1836 (0.1830)  time: 0.1736  data: 0.0001  max mem: 15821
[19:55:43.766973] Test:  [220/345]  eta: 0:00:21  loss: 0.1748 (0.1826)  time: 0.1740  data: 0.0001  max mem: 15821
[19:55:45.512462] Test:  [230/345]  eta: 0:00:19  loss: 0.1779 (0.1832)  time: 0.1743  data: 0.0001  max mem: 15821
[19:55:47.261126] Test:  [240/345]  eta: 0:00:18  loss: 0.1964 (0.1837)  time: 0.1746  data: 0.0001  max mem: 15821
[19:55:49.015556] Test:  [250/345]  eta: 0:00:16  loss: 0.1680 (0.1831)  time: 0.1751  data: 0.0001  max mem: 15821
[19:55:50.772080] Test:  [260/345]  eta: 0:00:14  loss: 0.1680 (0.1830)  time: 0.1755  data: 0.0001  max mem: 15821
[19:55:52.532901] Test:  [270/345]  eta: 0:00:12  loss: 0.1813 (0.1828)  time: 0.1758  data: 0.0001  max mem: 15821
[19:55:54.296622] Test:  [280/345]  eta: 0:00:11  loss: 0.1777 (0.1826)  time: 0.1762  data: 0.0001  max mem: 15821
[19:55:56.062751] Test:  [290/345]  eta: 0:00:09  loss: 0.1777 (0.1828)  time: 0.1764  data: 0.0001  max mem: 15821
[19:55:57.835137] Test:  [300/345]  eta: 0:00:07  loss: 0.1718 (0.1823)  time: 0.1769  data: 0.0001  max mem: 15821
[19:55:59.609762] Test:  [310/345]  eta: 0:00:06  loss: 0.1687 (0.1820)  time: 0.1773  data: 0.0001  max mem: 15821
[19:56:01.386860] Test:  [320/345]  eta: 0:00:04  loss: 0.1839 (0.1825)  time: 0.1775  data: 0.0001  max mem: 15821
[19:56:03.169053] Test:  [330/345]  eta: 0:00:02  loss: 0.1874 (0.1826)  time: 0.1779  data: 0.0001  max mem: 15821
[19:56:04.952687] Test:  [340/345]  eta: 0:00:00  loss: 0.1859 (0.1827)  time: 0.1782  data: 0.0001  max mem: 15821
[19:56:05.667525] Test:  [344/345]  eta: 0:00:00  loss: 0.1840 (0.1826)  time: 0.1784  data: 0.0001  max mem: 15821
[19:56:05.726816] Test: Total time: 0:01:00 (0.1740 s / it)
[19:56:15.757957] Test:  [ 0/57]  eta: 0:00:30  loss: 0.4297 (0.4297)  time: 0.5343  data: 0.3710  max mem: 15821
[19:56:17.411495] Test:  [10/57]  eta: 0:00:09  loss: 0.4068 (0.4134)  time: 0.1988  data: 0.0338  max mem: 15821
[19:56:19.069724] Test:  [20/57]  eta: 0:00:06  loss: 0.3919 (0.3952)  time: 0.1655  data: 0.0001  max mem: 15821
[19:56:20.732275] Test:  [30/57]  eta: 0:00:04  loss: 0.2778 (0.3428)  time: 0.1660  data: 0.0001  max mem: 15821
[19:56:22.396895] Test:  [40/57]  eta: 0:00:02  loss: 0.2337 (0.3208)  time: 0.1663  data: 0.0001  max mem: 15821
[19:56:24.065565] Test:  [50/57]  eta: 0:00:01  loss: 0.2592 (0.3192)  time: 0.1666  data: 0.0001  max mem: 15821
[19:56:24.967147] Test:  [56/57]  eta: 0:00:00  loss: 0.2811 (0.3243)  time: 0.1618  data: 0.0001  max mem: 15821
[19:56:25.040446] Test: Total time: 0:00:09 (0.1722 s / it)
[19:56:26.781812] Dice score of the network on the train images: 0.836407, val images: 0.801291
[19:56:26.785851] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[19:56:27.684553] Epoch: [20]  [  0/345]  eta: 0:05:09  lr: 0.000125  loss: 0.1893 (0.1893)  time: 0.8975  data: 0.2949  max mem: 15821
[19:56:39.701566] Epoch: [20]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.1759 (0.1828)  time: 0.6008  data: 0.0001  max mem: 15821
[19:56:51.765667] Epoch: [20]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.1923 (0.1881)  time: 0.6032  data: 0.0001  max mem: 15821
[19:57:03.850895] Epoch: [20]  [ 60/345]  eta: 0:02:53  lr: 0.000125  loss: 0.1885 (0.1881)  time: 0.6042  data: 0.0001  max mem: 15821
[19:57:15.939004] Epoch: [20]  [ 80/345]  eta: 0:02:40  lr: 0.000125  loss: 0.1894 (0.1892)  time: 0.6044  data: 0.0001  max mem: 15821
[19:57:28.047671] Epoch: [20]  [100/345]  eta: 0:02:28  lr: 0.000125  loss: 0.2047 (0.1915)  time: 0.6054  data: 0.0001  max mem: 15821
[19:57:40.166497] Epoch: [20]  [120/345]  eta: 0:02:16  lr: 0.000125  loss: 0.1724 (0.1903)  time: 0.6059  data: 0.0001  max mem: 15821
[19:57:52.297144] Epoch: [20]  [140/345]  eta: 0:02:04  lr: 0.000125  loss: 0.2021 (0.1924)  time: 0.6065  data: 0.0001  max mem: 15821
[19:58:04.439256] Epoch: [20]  [160/345]  eta: 0:01:52  lr: 0.000125  loss: 0.1883 (0.1923)  time: 0.6071  data: 0.0001  max mem: 15821
[19:58:16.580797] Epoch: [20]  [180/345]  eta: 0:01:40  lr: 0.000125  loss: 0.1964 (0.1928)  time: 0.6070  data: 0.0001  max mem: 15821
[19:58:28.717998] Epoch: [20]  [200/345]  eta: 0:01:27  lr: 0.000125  loss: 0.1875 (0.1925)  time: 0.6068  data: 0.0001  max mem: 15821
[19:58:40.835395] Epoch: [20]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.1869 (0.1925)  time: 0.6058  data: 0.0001  max mem: 15821
[19:58:52.966747] Epoch: [20]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.1904 (0.1924)  time: 0.6065  data: 0.0001  max mem: 15821
[19:59:05.103323] Epoch: [20]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.1916 (0.1927)  time: 0.6068  data: 0.0001  max mem: 15821
[19:59:17.234116] Epoch: [20]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.1761 (0.1920)  time: 0.6065  data: 0.0001  max mem: 15821
[19:59:29.364210] Epoch: [20]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.1763 (0.1915)  time: 0.6065  data: 0.0001  max mem: 15821
[19:59:41.495370] Epoch: [20]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.1865 (0.1916)  time: 0.6065  data: 0.0001  max mem: 15821
[19:59:53.609858] Epoch: [20]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.1833 (0.1911)  time: 0.6057  data: 0.0001  max mem: 15821
[19:59:56.033070] Epoch: [20]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.1938 (0.1913)  time: 0.6057  data: 0.0001  max mem: 15821
[19:59:56.103354] Epoch: [20] Total time: 0:03:29 (0.6067 s / it)
[19:59:56.104028] Averaged stats: lr: 0.000125  loss: 0.1938 (0.1913)
[19:59:56.584513] Test:  [  0/345]  eta: 0:02:43  loss: 0.1594 (0.1594)  time: 0.4748  data: 0.3104  max mem: 15821
[19:59:58.256368] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1864 (0.1905)  time: 0.1951  data: 0.0283  max mem: 15821
[19:59:59.931323] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1808 (0.1859)  time: 0.1673  data: 0.0001  max mem: 15821
[20:00:01.609443] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1790 (0.1872)  time: 0.1676  data: 0.0001  max mem: 15821
[20:00:03.290239] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1852 (0.1869)  time: 0.1679  data: 0.0001  max mem: 15821
[20:00:04.974142] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1870 (0.1870)  time: 0.1682  data: 0.0001  max mem: 15821
[20:00:06.661633] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1789 (0.1857)  time: 0.1685  data: 0.0001  max mem: 15821
[20:00:08.352578] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1773 (0.1846)  time: 0.1689  data: 0.0001  max mem: 15821
[20:00:10.047349] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1864 (0.1850)  time: 0.1692  data: 0.0001  max mem: 15821
[20:00:11.746113] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1890 (0.1849)  time: 0.1696  data: 0.0001  max mem: 15821
[20:00:13.446670] Test:  [100/345]  eta: 0:00:42  loss: 0.1843 (0.1857)  time: 0.1699  data: 0.0001  max mem: 15821
[20:00:15.151812] Test:  [110/345]  eta: 0:00:40  loss: 0.1826 (0.1862)  time: 0.1702  data: 0.0001  max mem: 15821
[20:00:16.860122] Test:  [120/345]  eta: 0:00:38  loss: 0.1790 (0.1852)  time: 0.1706  data: 0.0001  max mem: 15821
[20:00:18.571327] Test:  [130/345]  eta: 0:00:36  loss: 0.1753 (0.1843)  time: 0.1709  data: 0.0001  max mem: 15821
[20:00:20.287029] Test:  [140/345]  eta: 0:00:35  loss: 0.1694 (0.1840)  time: 0.1713  data: 0.0001  max mem: 15821
[20:00:22.005952] Test:  [150/345]  eta: 0:00:33  loss: 0.1850 (0.1840)  time: 0.1717  data: 0.0001  max mem: 15821
[20:00:23.729380] Test:  [160/345]  eta: 0:00:31  loss: 0.1804 (0.1836)  time: 0.1720  data: 0.0001  max mem: 15821
[20:00:25.455016] Test:  [170/345]  eta: 0:00:30  loss: 0.1799 (0.1835)  time: 0.1724  data: 0.0001  max mem: 15821
[20:00:27.183834] Test:  [180/345]  eta: 0:00:28  loss: 0.1799 (0.1841)  time: 0.1727  data: 0.0001  max mem: 15821
[20:00:28.918460] Test:  [190/345]  eta: 0:00:26  loss: 0.1853 (0.1843)  time: 0.1731  data: 0.0001  max mem: 15821
[20:00:30.654310] Test:  [200/345]  eta: 0:00:24  loss: 0.1924 (0.1850)  time: 0.1735  data: 0.0001  max mem: 15821
[20:00:32.394705] Test:  [210/345]  eta: 0:00:23  loss: 0.1918 (0.1854)  time: 0.1737  data: 0.0001  max mem: 15821
[20:00:34.138414] Test:  [220/345]  eta: 0:00:21  loss: 0.1918 (0.1860)  time: 0.1741  data: 0.0001  max mem: 15821
[20:00:35.884053] Test:  [230/345]  eta: 0:00:19  loss: 0.1888 (0.1858)  time: 0.1744  data: 0.0001  max mem: 15821
[20:00:37.633879] Test:  [240/345]  eta: 0:00:18  loss: 0.1850 (0.1859)  time: 0.1747  data: 0.0001  max mem: 15821
[20:00:39.386979] Test:  [250/345]  eta: 0:00:16  loss: 0.1835 (0.1855)  time: 0.1751  data: 0.0001  max mem: 15821
[20:00:41.144275] Test:  [260/345]  eta: 0:00:14  loss: 0.1852 (0.1860)  time: 0.1755  data: 0.0001  max mem: 15821
[20:00:42.905029] Test:  [270/345]  eta: 0:00:12  loss: 0.1800 (0.1856)  time: 0.1758  data: 0.0001  max mem: 15821
[20:00:44.669210] Test:  [280/345]  eta: 0:00:11  loss: 0.1761 (0.1856)  time: 0.1762  data: 0.0001  max mem: 15821
[20:00:46.438696] Test:  [290/345]  eta: 0:00:09  loss: 0.1893 (0.1861)  time: 0.1766  data: 0.0001  max mem: 15821
[20:00:48.209238] Test:  [300/345]  eta: 0:00:07  loss: 0.1914 (0.1860)  time: 0.1769  data: 0.0001  max mem: 15821
[20:00:49.984313] Test:  [310/345]  eta: 0:00:06  loss: 0.1759 (0.1856)  time: 0.1772  data: 0.0001  max mem: 15821
[20:00:51.762792] Test:  [320/345]  eta: 0:00:04  loss: 0.1796 (0.1856)  time: 0.1776  data: 0.0001  max mem: 15821
[20:00:53.544640] Test:  [330/345]  eta: 0:00:02  loss: 0.1795 (0.1854)  time: 0.1780  data: 0.0001  max mem: 15821
[20:00:55.330059] Test:  [340/345]  eta: 0:00:00  loss: 0.1795 (0.1856)  time: 0.1783  data: 0.0001  max mem: 15821
[20:00:56.044959] Test:  [344/345]  eta: 0:00:00  loss: 0.1831 (0.1857)  time: 0.1784  data: 0.0001  max mem: 15821
[20:00:56.119759] Test: Total time: 0:01:00 (0.1739 s / it)
[20:01:06.022510] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4406 (0.4406)  time: 0.4534  data: 0.2903  max mem: 15821
[20:01:07.675183] Test:  [10/57]  eta: 0:00:08  loss: 0.4199 (0.4479)  time: 0.1914  data: 0.0265  max mem: 15821
[20:01:09.334381] Test:  [20/57]  eta: 0:00:06  loss: 0.4199 (0.4296)  time: 0.1655  data: 0.0001  max mem: 15821
[20:01:10.996392] Test:  [30/57]  eta: 0:00:04  loss: 0.2825 (0.3727)  time: 0.1660  data: 0.0001  max mem: 15821
[20:01:12.662175] Test:  [40/57]  eta: 0:00:02  loss: 0.2718 (0.3527)  time: 0.1663  data: 0.0001  max mem: 15821
[20:01:14.332049] Test:  [50/57]  eta: 0:00:01  loss: 0.2855 (0.3528)  time: 0.1667  data: 0.0001  max mem: 15821
[20:01:15.233056] Test:  [56/57]  eta: 0:00:00  loss: 0.3246 (0.3593)  time: 0.1618  data: 0.0000  max mem: 15821
[20:01:15.302842] Test: Total time: 0:00:09 (0.1708 s / it)
[20:01:17.051416] Dice score of the network on the train images: 0.850577, val images: 0.777653
[20:01:17.051639] saving best_prec_model_0 @ epoch 20
[20:01:18.366733] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[20:01:19.268050] Epoch: [21]  [  0/345]  eta: 0:05:10  lr: 0.000125  loss: 0.1899 (0.1899)  time: 0.9000  data: 0.2977  max mem: 15821
[20:01:31.286142] Epoch: [21]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.1788 (0.1825)  time: 0.6008  data: 0.0001  max mem: 15821
[20:01:43.340052] Epoch: [21]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.1867 (0.1860)  time: 0.6026  data: 0.0001  max mem: 15821
[20:01:55.411460] Epoch: [21]  [ 60/345]  eta: 0:02:53  lr: 0.000125  loss: 0.1817 (0.1854)  time: 0.6035  data: 0.0001  max mem: 15821
[20:02:07.505798] Epoch: [21]  [ 80/345]  eta: 0:02:40  lr: 0.000125  loss: 0.1825 (0.1844)  time: 0.6047  data: 0.0001  max mem: 15821

[20:02:19.614729] Epoch: [21]  [100/345]  eta: 0:02:28  lr: 0.000125  loss: 0.1872 (0.1863)  time: 0.6054  data: 0.0001  max mem: 15821
[20:02:31.735716] Epoch: [21]  [120/345]  eta: 0:02:16  lr: 0.000125  loss: 0.1694 (0.1851)  time: 0.6060  data: 0.0001  max mem: 15821
[20:02:43.869846] Epoch: [21]  [140/345]  eta: 0:02:04  lr: 0.000125  loss: 0.1692 (0.1840)  time: 0.6067  data: 0.0001  max mem: 15821
[20:02:56.017882] Epoch: [21]  [160/345]  eta: 0:01:52  lr: 0.000125  loss: 0.1728 (0.1836)  time: 0.6074  data: 0.0001  max mem: 15821
[20:03:08.164799] Epoch: [21]  [180/345]  eta: 0:01:40  lr: 0.000125  loss: 0.1722 (0.1833)  time: 0.6073  data: 0.0001  max mem: 15821
[20:03:20.318502] Epoch: [21]  [200/345]  eta: 0:01:27  lr: 0.000125  loss: 0.1684 (0.1821)  time: 0.6076  data: 0.0001  max mem: 15821
[20:03:32.444589] Epoch: [21]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.1741 (0.1821)  time: 0.6063  data: 0.0001  max mem: 15821
[20:03:44.685048] Epoch: [21]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.1734 (0.1816)  time: 0.6120  data: 0.0001  max mem: 15821
[20:03:56.806743] Epoch: [21]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.1739 (0.1811)  time: 0.6060  data: 0.0001  max mem: 15821
[20:04:08.922325] Epoch: [21]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.1952 (0.1821)  time: 0.6057  data: 0.0001  max mem: 15821
[20:04:21.028704] Epoch: [21]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.1766 (0.1823)  time: 0.6053  data: 0.0001  max mem: 15821
[20:04:33.136270] Epoch: [21]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.1748 (0.1820)  time: 0.6053  data: 0.0001  max mem: 15821
[20:04:45.256093] Epoch: [21]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.1815 (0.1822)  time: 0.6059  data: 0.0001  max mem: 15821
[20:04:47.679746] Epoch: [21]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.1803 (0.1823)  time: 0.6058  data: 0.0001  max mem: 15821
[20:04:47.756264] Epoch: [21] Total time: 0:03:29 (0.6069 s / it)
[20:04:47.756619] Averaged stats: lr: 0.000125  loss: 0.1803 (0.1823)
[20:04:48.268636] Test:  [  0/345]  eta: 0:02:54  loss: 0.1409 (0.1409)  time: 0.5046  data: 0.3406  max mem: 15821
[20:04:49.940525] Test:  [ 10/345]  eta: 0:01:06  loss: 0.1786 (0.1728)  time: 0.1978  data: 0.0311  max mem: 15821
[20:04:51.614364] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1721 (0.1743)  time: 0.1672  data: 0.0001  max mem: 15821
[20:04:53.292476] Test:  [ 30/345]  eta: 0:00:56  loss: 0.1721 (0.1746)  time: 0.1675  data: 0.0001  max mem: 15821
[20:04:54.973316] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1754 (0.1742)  time: 0.1679  data: 0.0001  max mem: 15821
[20:04:56.657237] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1660 (0.1729)  time: 0.1682  data: 0.0001  max mem: 15821
[20:04:58.345505] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1660 (0.1742)  time: 0.1685  data: 0.0001  max mem: 15821
[20:05:00.036312] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1746 (0.1741)  time: 0.1689  data: 0.0001  max mem: 15821
[20:05:01.730558] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1802 (0.1751)  time: 0.1692  data: 0.0001  max mem: 15821
[20:05:03.429142] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1802 (0.1755)  time: 0.1696  data: 0.0001  max mem: 15821
[20:05:05.130673] Test:  [100/345]  eta: 0:00:42  loss: 0.1736 (0.1758)  time: 0.1699  data: 0.0001  max mem: 15821
[20:05:06.834699] Test:  [110/345]  eta: 0:00:40  loss: 0.1733 (0.1762)  time: 0.1702  data: 0.0001  max mem: 15821
[20:05:08.543753] Test:  [120/345]  eta: 0:00:38  loss: 0.1719 (0.1756)  time: 0.1706  data: 0.0001  max mem: 15821
[20:05:10.255028] Test:  [130/345]  eta: 0:00:36  loss: 0.1641 (0.1750)  time: 0.1710  data: 0.0001  max mem: 15821
[20:05:11.970532] Test:  [140/345]  eta: 0:00:35  loss: 0.1718 (0.1757)  time: 0.1713  data: 0.0001  max mem: 15821
[20:05:13.688686] Test:  [150/345]  eta: 0:00:33  loss: 0.1822 (0.1754)  time: 0.1716  data: 0.0001  max mem: 15821
[20:05:15.412189] Test:  [160/345]  eta: 0:00:31  loss: 0.1725 (0.1758)  time: 0.1720  data: 0.0001  max mem: 15821
[20:05:17.137632] Test:  [170/345]  eta: 0:00:30  loss: 0.1725 (0.1761)  time: 0.1724  data: 0.0001  max mem: 15821
[20:05:18.867871] Test:  [180/345]  eta: 0:00:28  loss: 0.1748 (0.1760)  time: 0.1727  data: 0.0001  max mem: 15821
[20:05:20.599147] Test:  [190/345]  eta: 0:00:26  loss: 0.1748 (0.1761)  time: 0.1730  data: 0.0001  max mem: 15821
[20:05:22.335091] Test:  [200/345]  eta: 0:00:24  loss: 0.1702 (0.1757)  time: 0.1733  data: 0.0001  max mem: 15821
[20:05:24.074474] Test:  [210/345]  eta: 0:00:23  loss: 0.1695 (0.1755)  time: 0.1737  data: 0.0001  max mem: 15821
[20:05:25.817322] Test:  [220/345]  eta: 0:00:21  loss: 0.1778 (0.1759)  time: 0.1740  data: 0.0001  max mem: 15821
[20:05:27.564898] Test:  [230/345]  eta: 0:00:19  loss: 0.1778 (0.1756)  time: 0.1745  data: 0.0001  max mem: 15821
[20:05:29.315510] Test:  [240/345]  eta: 0:00:18  loss: 0.1688 (0.1754)  time: 0.1748  data: 0.0001  max mem: 15821
[20:05:31.068914] Test:  [250/345]  eta: 0:00:16  loss: 0.1621 (0.1749)  time: 0.1751  data: 0.0001  max mem: 15821
[20:05:32.825594] Test:  [260/345]  eta: 0:00:14  loss: 0.1621 (0.1749)  time: 0.1754  data: 0.0001  max mem: 15821
[20:05:34.585183] Test:  [270/345]  eta: 0:00:12  loss: 0.1684 (0.1746)  time: 0.1758  data: 0.0001  max mem: 15821
[20:05:36.347734] Test:  [280/345]  eta: 0:00:11  loss: 0.1756 (0.1754)  time: 0.1760  data: 0.0001  max mem: 15821
[20:05:38.114268] Test:  [290/345]  eta: 0:00:09  loss: 0.1824 (0.1756)  time: 0.1764  data: 0.0001  max mem: 15821
[20:05:39.884369] Test:  [300/345]  eta: 0:00:07  loss: 0.1780 (0.1757)  time: 0.1768  data: 0.0001  max mem: 15821
[20:05:41.658190] Test:  [310/345]  eta: 0:00:06  loss: 0.1780 (0.1757)  time: 0.1771  data: 0.0001  max mem: 15821
[20:05:43.436724] Test:  [320/345]  eta: 0:00:04  loss: 0.1694 (0.1755)  time: 0.1776  data: 0.0001  max mem: 15821
[20:05:45.217496] Test:  [330/345]  eta: 0:00:02  loss: 0.1806 (0.1759)  time: 0.1779  data: 0.0001  max mem: 15821
[20:05:47.002856] Test:  [340/345]  eta: 0:00:00  loss: 0.1848 (0.1759)  time: 0.1783  data: 0.0001  max mem: 15821
[20:05:47.717014] Test:  [344/345]  eta: 0:00:00  loss: 0.1835 (0.1759)  time: 0.1784  data: 0.0001  max mem: 15821
[20:05:47.781376] Test: Total time: 0:01:00 (0.1740 s / it)
[20:05:57.607746] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4265 (0.4265)  time: 0.4522  data: 0.2886  max mem: 15821
[20:05:59.261350] Test:  [10/57]  eta: 0:00:08  loss: 0.3844 (0.4325)  time: 0.1913  data: 0.0263  max mem: 15821
[20:06:00.920202] Test:  [20/57]  eta: 0:00:06  loss: 0.3844 (0.4137)  time: 0.1655  data: 0.0001  max mem: 15821
[20:06:02.582478] Test:  [30/57]  eta: 0:00:04  loss: 0.2759 (0.3575)  time: 0.1660  data: 0.0001  max mem: 15821
[20:06:04.248771] Test:  [40/57]  eta: 0:00:02  loss: 0.2474 (0.3368)  time: 0.1664  data: 0.0001  max mem: 15821
[20:06:05.919323] Test:  [50/57]  eta: 0:00:01  loss: 0.2690 (0.3405)  time: 0.1668  data: 0.0001  max mem: 15821
[20:06:06.820672] Test:  [56/57]  eta: 0:00:00  loss: 0.3340 (0.3557)  time: 0.1619  data: 0.0000  max mem: 15821
[20:06:06.886785] Test: Total time: 0:00:09 (0.1707 s / it)
[20:06:08.656618] Dice score of the network on the train images: 0.843727, val images: 0.769792
[20:06:08.660819] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[20:06:09.564470] Epoch: [22]  [  0/345]  eta: 0:05:11  lr: 0.000125  loss: 0.1700 (0.1700)  time: 0.9026  data: 0.2990  max mem: 15821
[20:06:21.566239] Epoch: [22]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.1681 (0.1691)  time: 0.6000  data: 0.0001  max mem: 15821
[20:06:33.604107] Epoch: [22]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.1670 (0.1709)  time: 0.6018  data: 0.0001  max mem: 15821
[20:06:45.655865] Epoch: [22]  [ 60/345]  eta: 0:02:52  lr: 0.000125  loss: 0.1746 (0.1745)  time: 0.6025  data: 0.0001  max mem: 15821
[20:06:57.721155] Epoch: [22]  [ 80/345]  eta: 0:02:40  lr: 0.000125  loss: 0.1816 (0.1768)  time: 0.6032  data: 0.0001  max mem: 15821
[20:07:09.795819] Epoch: [22]  [100/345]  eta: 0:02:28  lr: 0.000125  loss: 0.1766 (0.1765)  time: 0.6037  data: 0.0001  max mem: 15821
[20:07:21.889732] Epoch: [22]  [120/345]  eta: 0:02:16  lr: 0.000125  loss: 0.1759 (0.1776)  time: 0.6046  data: 0.0001  max mem: 15821
[20:07:33.993570] Epoch: [22]  [140/345]  eta: 0:02:04  lr: 0.000125  loss: 0.1765 (0.1786)  time: 0.6051  data: 0.0001  max mem: 15821
[20:07:46.103377] Epoch: [22]  [160/345]  eta: 0:01:51  lr: 0.000125  loss: 0.1848 (0.1794)  time: 0.6054  data: 0.0001  max mem: 15821
[20:07:58.216715] Epoch: [22]  [180/345]  eta: 0:01:39  lr: 0.000125  loss: 0.1747 (0.1789)  time: 0.6056  data: 0.0001  max mem: 15821
[20:08:10.328521] Epoch: [22]  [200/345]  eta: 0:01:27  lr: 0.000125  loss: 0.1781 (0.1789)  time: 0.6055  data: 0.0001  max mem: 15821
[20:08:22.444736] Epoch: [22]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.1715 (0.1788)  time: 0.6058  data: 0.0001  max mem: 15821
[20:08:34.552657] Epoch: [22]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.1695 (0.1785)  time: 0.6054  data: 0.0001  max mem: 15821
[20:08:46.656022] Epoch: [22]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.1688 (0.1778)  time: 0.6051  data: 0.0001  max mem: 15821
[20:08:58.756792] Epoch: [22]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.1690 (0.1776)  time: 0.6050  data: 0.0001  max mem: 15821
[20:09:10.850872] Epoch: [22]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.1745 (0.1777)  time: 0.6047  data: 0.0001  max mem: 15821
[20:09:22.950834] Epoch: [22]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.1835 (0.1783)  time: 0.6050  data: 0.0001  max mem: 15821
[20:09:35.040584] Epoch: [22]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.1709 (0.1786)  time: 0.6044  data: 0.0001  max mem: 15821
[20:09:37.458009] Epoch: [22]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.1743 (0.1786)  time: 0.6044  data: 0.0001  max mem: 15821
[20:09:37.531610] Epoch: [22] Total time: 0:03:28 (0.6054 s / it)
[20:09:37.532126] Averaged stats: lr: 0.000125  loss: 0.1743 (0.1786)
[20:09:38.018949] Test:  [  0/345]  eta: 0:02:46  loss: 0.1467 (0.1467)  time: 0.4812  data: 0.3166  max mem: 15821
[20:09:39.689990] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1656 (0.1708)  time: 0.1956  data: 0.0289  max mem: 15821
[20:09:41.363718] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1656 (0.1662)  time: 0.1672  data: 0.0001  max mem: 15821
[20:09:43.041261] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1649 (0.1684)  time: 0.1675  data: 0.0001  max mem: 15821
[20:09:44.721591] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1653 (0.1689)  time: 0.1678  data: 0.0001  max mem: 15821
[20:09:46.406142] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1627 (0.1704)  time: 0.1682  data: 0.0001  max mem: 15821
[20:09:48.094894] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1610 (0.1701)  time: 0.1686  data: 0.0001  max mem: 15821
[20:09:49.785724] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1693 (0.1711)  time: 0.1689  data: 0.0001  max mem: 15821
[20:09:51.479994] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1703 (0.1709)  time: 0.1692  data: 0.0001  max mem: 15821
[20:09:53.177700] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1632 (0.1697)  time: 0.1695  data: 0.0001  max mem: 15821
[20:09:54.878380] Test:  [100/345]  eta: 0:00:42  loss: 0.1632 (0.1695)  time: 0.1699  data: 0.0001  max mem: 15821
[20:09:56.582579] Test:  [110/345]  eta: 0:00:40  loss: 0.1619 (0.1693)  time: 0.1702  data: 0.0001  max mem: 15821
[20:09:58.291282] Test:  [120/345]  eta: 0:00:38  loss: 0.1654 (0.1695)  time: 0.1706  data: 0.0001  max mem: 15821
[20:10:00.002648] Test:  [130/345]  eta: 0:00:36  loss: 0.1719 (0.1704)  time: 0.1709  data: 0.0001  max mem: 15821
[20:10:01.718055] Test:  [140/345]  eta: 0:00:35  loss: 0.1725 (0.1701)  time: 0.1713  data: 0.0001  max mem: 15821
[20:10:03.436016] Test:  [150/345]  eta: 0:00:33  loss: 0.1725 (0.1706)  time: 0.1716  data: 0.0001  max mem: 15821
[20:10:05.158142] Test:  [160/345]  eta: 0:00:31  loss: 0.1748 (0.1706)  time: 0.1719  data: 0.0001  max mem: 15821
[20:10:06.884437] Test:  [170/345]  eta: 0:00:30  loss: 0.1649 (0.1705)  time: 0.1724  data: 0.0001  max mem: 15821
[20:10:08.615434] Test:  [180/345]  eta: 0:00:28  loss: 0.1642 (0.1701)  time: 0.1728  data: 0.0001  max mem: 15821
[20:10:10.348490] Test:  [190/345]  eta: 0:00:26  loss: 0.1545 (0.1699)  time: 0.1731  data: 0.0001  max mem: 15821
[20:10:12.084222] Test:  [200/345]  eta: 0:00:24  loss: 0.1633 (0.1699)  time: 0.1734  data: 0.0001  max mem: 15821
[20:10:13.823857] Test:  [210/345]  eta: 0:00:23  loss: 0.1640 (0.1695)  time: 0.1737  data: 0.0001  max mem: 15821
[20:10:15.567168] Test:  [220/345]  eta: 0:00:21  loss: 0.1694 (0.1696)  time: 0.1741  data: 0.0001  max mem: 15821
[20:10:17.314127] Test:  [230/345]  eta: 0:00:19  loss: 0.1703 (0.1698)  time: 0.1745  data: 0.0001  max mem: 15821
[20:10:19.064055] Test:  [240/345]  eta: 0:00:18  loss: 0.1665 (0.1696)  time: 0.1748  data: 0.0001  max mem: 15821
[20:10:20.817639] Test:  [250/345]  eta: 0:00:16  loss: 0.1576 (0.1691)  time: 0.1751  data: 0.0001  max mem: 15821
[20:10:22.575149] Test:  [260/345]  eta: 0:00:14  loss: 0.1621 (0.1692)  time: 0.1755  data: 0.0001  max mem: 15821
[20:10:24.336341] Test:  [270/345]  eta: 0:00:12  loss: 0.1678 (0.1693)  time: 0.1759  data: 0.0001  max mem: 15821
[20:10:26.100843] Test:  [280/345]  eta: 0:00:11  loss: 0.1724 (0.1698)  time: 0.1762  data: 0.0001  max mem: 15821
[20:10:27.867289] Test:  [290/345]  eta: 0:00:09  loss: 0.1707 (0.1698)  time: 0.1765  data: 0.0001  max mem: 15821
[20:10:29.636441] Test:  [300/345]  eta: 0:00:07  loss: 0.1627 (0.1699)  time: 0.1767  data: 0.0001  max mem: 15821
[20:10:31.411467] Test:  [310/345]  eta: 0:00:06  loss: 0.1692 (0.1703)  time: 0.1771  data: 0.0001  max mem: 15821
[20:10:33.190159] Test:  [320/345]  eta: 0:00:04  loss: 0.1692 (0.1700)  time: 0.1776  data: 0.0001  max mem: 15821
[20:10:34.971803] Test:  [330/345]  eta: 0:00:02  loss: 0.1566 (0.1698)  time: 0.1780  data: 0.0001  max mem: 15821
[20:10:36.756976] Test:  [340/345]  eta: 0:00:00  loss: 0.1622 (0.1700)  time: 0.1783  data: 0.0001  max mem: 15821
[20:10:37.472315] Test:  [344/345]  eta: 0:00:00  loss: 0.1622 (0.1699)  time: 0.1785  data: 0.0001  max mem: 15821
[20:10:37.538259] Test: Total time: 0:01:00 (0.1739 s / it)
[20:10:47.531444] Test:  [ 0/57]  eta: 0:00:27  loss: 0.4820 (0.4820)  time: 0.4841  data: 0.3211  max mem: 15821
[20:10:49.184079] Test:  [10/57]  eta: 0:00:09  loss: 0.4149 (0.4300)  time: 0.1941  data: 0.0293  max mem: 15821
[20:10:50.842314] Test:  [20/57]  eta: 0:00:06  loss: 0.4149 (0.4234)  time: 0.1655  data: 0.0001  max mem: 15821
[20:10:52.505424] Test:  [30/57]  eta: 0:00:04  loss: 0.3234 (0.3722)  time: 0.1660  data: 0.0001  max mem: 15821
[20:10:54.171008] Test:  [40/57]  eta: 0:00:02  loss: 0.2681 (0.3538)  time: 0.1664  data: 0.0001  max mem: 15821
[20:10:55.841779] Test:  [50/57]  eta: 0:00:01  loss: 0.2970 (0.3521)  time: 0.1668  data: 0.0001  max mem: 15821
[20:10:56.744148] Test:  [56/57]  eta: 0:00:00  loss: 0.3313 (0.3595)  time: 0.1619  data: 0.0001  max mem: 15821
[20:10:56.821646] Test: Total time: 0:00:09 (0.1715 s / it)
[20:10:58.558622] Dice score of the network on the train images: 0.848517, val images: 0.788839
[20:10:58.562640] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[20:10:59.458715] Epoch: [23]  [  0/345]  eta: 0:05:08  lr: 0.000125  loss: 0.1901 (0.1901)  time: 0.8950  data: 0.2942  max mem: 15821
[20:11:11.447312] Epoch: [23]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.1711 (0.1765)  time: 0.5994  data: 0.0001  max mem: 15821
[20:11:23.473334] Epoch: [23]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.1745 (0.1761)  time: 0.6013  data: 0.0001  max mem: 15821
[20:11:35.526554] Epoch: [23]  [ 60/345]  eta: 0:02:52  lr: 0.000125  loss: 0.1779 (0.1773)  time: 0.6026  data: 0.0001  max mem: 15821
[20:11:47.595227] Epoch: [23]  [ 80/345]  eta: 0:02:40  lr: 0.000125  loss: 0.1637 (0.1746)  time: 0.6034  data: 0.0001  max mem: 15821
[20:11:59.673093] Epoch: [23]  [100/345]  eta: 0:02:28  lr: 0.000125  loss: 0.1702 (0.1744)  time: 0.6038  data: 0.0001  max mem: 15821
[20:12:11.891503] Epoch: [23]  [120/345]  eta: 0:02:16  lr: 0.000125  loss: 0.1637 (0.1725)  time: 0.6109  data: 0.0001  max mem: 15821
[20:12:24.023403] Epoch: [23]  [140/345]  eta: 0:02:04  lr: 0.000125  loss: 0.1596 (0.1721)  time: 0.6066  data: 0.0001  max mem: 15821
[20:12:36.159080] Epoch: [23]  [160/345]  eta: 0:01:52  lr: 0.000125  loss: 0.1632 (0.1719)  time: 0.6067  data: 0.0001  max mem: 15821
[20:12:48.283322] Epoch: [23]  [180/345]  eta: 0:01:40  lr: 0.000125  loss: 0.1636 (0.1717)  time: 0.6062  data: 0.0001  max mem: 15821
[20:13:00.419662] Epoch: [23]  [200/345]  eta: 0:01:27  lr: 0.000125  loss: 0.1613 (0.1714)  time: 0.6068  data: 0.0001  max mem: 15821
[20:13:12.565960] Epoch: [23]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.1733 (0.1720)  time: 0.6073  data: 0.0001  max mem: 15821
[20:13:24.699858] Epoch: [23]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.1743 (0.1723)  time: 0.6066  data: 0.0001  max mem: 15821
[20:13:36.834843] Epoch: [23]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.1766 (0.1727)  time: 0.6067  data: 0.0001  max mem: 15821
[20:13:48.962287] Epoch: [23]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.1691 (0.1724)  time: 0.6063  data: 0.0001  max mem: 15821
[20:14:01.082183] Epoch: [23]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.1719 (0.1725)  time: 0.6060  data: 0.0001  max mem: 15821
[20:14:13.201813] Epoch: [23]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.1778 (0.1732)  time: 0.6059  data: 0.0001  max mem: 15821
[20:14:25.313501] Epoch: [23]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.1682 (0.1731)  time: 0.6055  data: 0.0001  max mem: 15821
[20:14:27.738088] Epoch: [23]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.1638 (0.1731)  time: 0.6055  data: 0.0001  max mem: 15821
[20:14:27.806278] Epoch: [23] Total time: 0:03:29 (0.6065 s / it)
[20:14:27.806561] Averaged stats: lr: 0.000125  loss: 0.1638 (0.1731)
[20:14:28.300657] Test:  [  0/345]  eta: 0:02:48  loss: 0.1446 (0.1446)  time: 0.4872  data: 0.3234  max mem: 15821
[20:14:29.971777] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1648 (0.1672)  time: 0.1961  data: 0.0295  max mem: 15821
[20:14:31.644975] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1589 (0.1597)  time: 0.1671  data: 0.0001  max mem: 15821
[20:14:33.322236] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1626 (0.1640)  time: 0.1675  data: 0.0001  max mem: 15821
[20:14:35.003293] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1660 (0.1632)  time: 0.1679  data: 0.0001  max mem: 15821
[20:14:36.687053] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1593 (0.1640)  time: 0.1682  data: 0.0001  max mem: 15821
[20:14:38.374104] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1586 (0.1631)  time: 0.1685  data: 0.0001  max mem: 15821
[20:14:40.065077] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1517 (0.1625)  time: 0.1688  data: 0.0001  max mem: 15821
[20:14:41.759809] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1529 (0.1631)  time: 0.1692  data: 0.0001  max mem: 15821
[20:14:43.457120] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1629 (0.1630)  time: 0.1695  data: 0.0001  max mem: 15821
[20:14:45.157879] Test:  [100/345]  eta: 0:00:42  loss: 0.1618 (0.1633)  time: 0.1698  data: 0.0001  max mem: 15821
[20:14:46.861714] Test:  [110/345]  eta: 0:00:40  loss: 0.1533 (0.1626)  time: 0.1702  data: 0.0001  max mem: 15821
[20:14:48.568873] Test:  [120/345]  eta: 0:00:38  loss: 0.1533 (0.1623)  time: 0.1705  data: 0.0001  max mem: 15821
[20:14:50.280473] Test:  [130/345]  eta: 0:00:36  loss: 0.1577 (0.1622)  time: 0.1709  data: 0.0001  max mem: 15821
[20:14:51.995584] Test:  [140/345]  eta: 0:00:35  loss: 0.1587 (0.1627)  time: 0.1713  data: 0.0001  max mem: 15821
[20:14:53.714120] Test:  [150/345]  eta: 0:00:33  loss: 0.1603 (0.1626)  time: 0.1716  data: 0.0001  max mem: 15821
[20:14:55.437259] Test:  [160/345]  eta: 0:00:31  loss: 0.1632 (0.1629)  time: 0.1720  data: 0.0001  max mem: 15821
[20:14:57.162256] Test:  [170/345]  eta: 0:00:30  loss: 0.1640 (0.1629)  time: 0.1723  data: 0.0001  max mem: 15821
[20:14:58.891655] Test:  [180/345]  eta: 0:00:28  loss: 0.1574 (0.1632)  time: 0.1727  data: 0.0001  max mem: 15821
[20:15:00.624370] Test:  [190/345]  eta: 0:00:26  loss: 0.1621 (0.1642)  time: 0.1730  data: 0.0001  max mem: 15821
[20:15:02.361070] Test:  [200/345]  eta: 0:00:24  loss: 0.1792 (0.1646)  time: 0.1734  data: 0.0001  max mem: 15821
[20:15:04.100782] Test:  [210/345]  eta: 0:00:23  loss: 0.1807 (0.1649)  time: 0.1738  data: 0.0001  max mem: 15821
[20:15:05.843028] Test:  [220/345]  eta: 0:00:21  loss: 0.1618 (0.1649)  time: 0.1740  data: 0.0001  max mem: 15821
[20:15:07.587998] Test:  [230/345]  eta: 0:00:19  loss: 0.1602 (0.1649)  time: 0.1743  data: 0.0001  max mem: 15821
[20:15:09.338425] Test:  [240/345]  eta: 0:00:18  loss: 0.1602 (0.1648)  time: 0.1747  data: 0.0001  max mem: 15821
[20:15:11.091306] Test:  [250/345]  eta: 0:00:16  loss: 0.1543 (0.1646)  time: 0.1751  data: 0.0001  max mem: 15821
[20:15:12.846907] Test:  [260/345]  eta: 0:00:14  loss: 0.1618 (0.1647)  time: 0.1754  data: 0.0001  max mem: 15821
[20:15:14.608088] Test:  [270/345]  eta: 0:00:12  loss: 0.1647 (0.1649)  time: 0.1758  data: 0.0001  max mem: 15821
[20:15:16.372801] Test:  [280/345]  eta: 0:00:11  loss: 0.1713 (0.1650)  time: 0.1762  data: 0.0001  max mem: 15821
[20:15:18.139439] Test:  [290/345]  eta: 0:00:09  loss: 0.1713 (0.1654)  time: 0.1765  data: 0.0001  max mem: 15821
[20:15:19.909258] Test:  [300/345]  eta: 0:00:07  loss: 0.1650 (0.1653)  time: 0.1768  data: 0.0001  max mem: 15821
[20:15:21.683659] Test:  [310/345]  eta: 0:00:06  loss: 0.1636 (0.1653)  time: 0.1772  data: 0.0001  max mem: 15821
[20:15:23.461649] Test:  [320/345]  eta: 0:00:04  loss: 0.1577 (0.1650)  time: 0.1776  data: 0.0001  max mem: 15821
[20:15:25.242926] Test:  [330/345]  eta: 0:00:02  loss: 0.1547 (0.1650)  time: 0.1779  data: 0.0001  max mem: 15821
[20:15:27.027329] Test:  [340/345]  eta: 0:00:00  loss: 0.1648 (0.1650)  time: 0.1782  data: 0.0001  max mem: 15821
[20:15:27.742319] Test:  [344/345]  eta: 0:00:00  loss: 0.1698 (0.1653)  time: 0.1783  data: 0.0001  max mem: 15821
[20:15:27.808576] Test: Total time: 0:00:59 (0.1739 s / it)
[20:15:37.694437] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4361 (0.4361)  time: 0.4475  data: 0.2844  max mem: 15821
[20:15:39.348429] Test:  [10/57]  eta: 0:00:08  loss: 0.3915 (0.4009)  time: 0.1909  data: 0.0259  max mem: 15821
[20:15:41.007276] Test:  [20/57]  eta: 0:00:06  loss: 0.3821 (0.3925)  time: 0.1656  data: 0.0001  max mem: 15821
[20:15:42.669156] Test:  [30/57]  eta: 0:00:04  loss: 0.2593 (0.3386)  time: 0.1660  data: 0.0001  max mem: 15821
[20:15:44.335681] Test:  [40/57]  eta: 0:00:02  loss: 0.2338 (0.3163)  time: 0.1664  data: 0.0001  max mem: 15821
[20:15:46.005345] Test:  [50/57]  eta: 0:00:01  loss: 0.2564 (0.3162)  time: 0.1667  data: 0.0001  max mem: 15821
[20:15:46.907180] Test:  [56/57]  eta: 0:00:00  loss: 0.2767 (0.3256)  time: 0.1619  data: 0.0001  max mem: 15821
[20:15:46.964847] Test: Total time: 0:00:09 (0.1705 s / it)
[20:15:48.701071] Dice score of the network on the train images: 0.839593, val images: 0.799014
[20:15:48.705064] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[20:15:49.607541] Epoch: [24]  [  0/345]  eta: 0:05:10  lr: 0.000125  loss: 0.1802 (0.1802)  time: 0.9014  data: 0.2948  max mem: 15821
[20:16:01.635764] Epoch: [24]  [ 20/345]  eta: 0:03:20  lr: 0.000125  loss: 0.1553 (0.1658)  time: 0.6014  data: 0.0001  max mem: 15821
[20:16:13.701814] Epoch: [24]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.1651 (0.1688)  time: 0.6033  data: 0.0001  max mem: 15821
[20:16:25.769401] Epoch: [24]  [ 60/345]  eta: 0:02:53  lr: 0.000125  loss: 0.1608 (0.1663)  time: 0.6033  data: 0.0001  max mem: 15821
[20:16:37.851454] Epoch: [24]  [ 80/345]  eta: 0:02:40  lr: 0.000125  loss: 0.1814 (0.1701)  time: 0.6041  data: 0.0001  max mem: 15821
[20:16:49.956335] Epoch: [24]  [100/345]  eta: 0:02:28  lr: 0.000125  loss: 0.1709 (0.1718)  time: 0.6052  data: 0.0001  max mem: 15821
[20:17:02.064554] Epoch: [24]  [120/345]  eta: 0:02:16  lr: 0.000125  loss: 0.1745 (0.1726)  time: 0.6054  data: 0.0001  max mem: 15821
[20:17:14.179098] Epoch: [24]  [140/345]  eta: 0:02:04  lr: 0.000125  loss: 0.1593 (0.1713)  time: 0.6057  data: 0.0001  max mem: 15821
[20:17:26.292400] Epoch: [24]  [160/345]  eta: 0:01:52  lr: 0.000125  loss: 0.1654 (0.1712)  time: 0.6056  data: 0.0001  max mem: 15821
[20:17:38.414796] Epoch: [24]  [180/345]  eta: 0:01:40  lr: 0.000125  loss: 0.1615 (0.1708)  time: 0.6061  data: 0.0001  max mem: 15821
[20:17:50.524142] Epoch: [24]  [200/345]  eta: 0:01:27  lr: 0.000125  loss: 0.1705 (0.1708)  time: 0.6054  data: 0.0001  max mem: 15821
[20:18:02.643591] Epoch: [24]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.1588 (0.1703)  time: 0.6059  data: 0.0001  max mem: 15821
[20:18:14.759054] Epoch: [24]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.1645 (0.1705)  time: 0.6057  data: 0.0001  max mem: 15821
[20:18:26.869306] Epoch: [24]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.1617 (0.1700)  time: 0.6055  data: 0.0001  max mem: 15821
[20:18:38.975085] Epoch: [24]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.1768 (0.1707)  time: 0.6052  data: 0.0001  max mem: 15821
[20:18:51.080141] Epoch: [24]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.1582 (0.1700)  time: 0.6052  data: 0.0001  max mem: 15821
[20:19:03.181754] Epoch: [24]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.1638 (0.1700)  time: 0.6050  data: 0.0001  max mem: 15821
[20:19:15.279277] Epoch: [24]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.1731 (0.1701)  time: 0.6048  data: 0.0001  max mem: 15821
[20:19:17.698190] Epoch: [24]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.1731 (0.1701)  time: 0.6047  data: 0.0001  max mem: 15821
[20:19:17.773469] Epoch: [24] Total time: 0:03:29 (0.6060 s / it)
[20:19:17.773778] Averaged stats: lr: 0.000125  loss: 0.1731 (0.1701)
[20:19:18.264445] Test:  [  0/345]  eta: 0:02:47  loss: 0.1562 (0.1562)  time: 0.4853  data: 0.3209  max mem: 15821
[20:19:19.935567] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1717 (0.1617)  time: 0.1960  data: 0.0293  max mem: 15821
[20:19:21.610255] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1614 (0.1605)  time: 0.1672  data: 0.0001  max mem: 15821
[20:19:23.288292] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1548 (0.1592)  time: 0.1676  data: 0.0001  max mem: 15821
[20:19:24.969866] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1622 (0.1597)  time: 0.1679  data: 0.0001  max mem: 15821
[20:19:26.655167] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1648 (0.1606)  time: 0.1683  data: 0.0001  max mem: 15821
[20:19:28.343191] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1557 (0.1592)  time: 0.1686  data: 0.0001  max mem: 15821
[20:19:30.033692] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1511 (0.1591)  time: 0.1689  data: 0.0001  max mem: 15821
[20:19:31.729149] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1539 (0.1595)  time: 0.1692  data: 0.0001  max mem: 15821
[20:19:33.427938] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1455 (0.1589)  time: 0.1696  data: 0.0001  max mem: 15821
[20:19:35.128886] Test:  [100/345]  eta: 0:00:42  loss: 0.1484 (0.1581)  time: 0.1699  data: 0.0001  max mem: 15821
[20:19:36.833215] Test:  [110/345]  eta: 0:00:40  loss: 0.1505 (0.1580)  time: 0.1702  data: 0.0001  max mem: 15821
[20:19:38.541515] Test:  [120/345]  eta: 0:00:38  loss: 0.1612 (0.1592)  time: 0.1706  data: 0.0001  max mem: 15821
[20:19:40.254141] Test:  [130/345]  eta: 0:00:36  loss: 0.1628 (0.1596)  time: 0.1710  data: 0.0001  max mem: 15821
[20:19:41.970569] Test:  [140/345]  eta: 0:00:35  loss: 0.1601 (0.1588)  time: 0.1714  data: 0.0001  max mem: 15821
[20:19:43.689633] Test:  [150/345]  eta: 0:00:33  loss: 0.1459 (0.1582)  time: 0.1717  data: 0.0001  max mem: 15821
[20:19:45.412189] Test:  [160/345]  eta: 0:00:31  loss: 0.1490 (0.1583)  time: 0.1720  data: 0.0001  max mem: 15821
[20:19:47.138500] Test:  [170/345]  eta: 0:00:30  loss: 0.1584 (0.1591)  time: 0.1724  data: 0.0001  max mem: 15821
[20:19:48.867889] Test:  [180/345]  eta: 0:00:28  loss: 0.1670 (0.1591)  time: 0.1727  data: 0.0001  max mem: 15821
[20:19:50.601342] Test:  [190/345]  eta: 0:00:26  loss: 0.1508 (0.1590)  time: 0.1731  data: 0.0001  max mem: 15821
[20:19:52.335631] Test:  [200/345]  eta: 0:00:24  loss: 0.1508 (0.1588)  time: 0.1733  data: 0.0001  max mem: 15821
[20:19:54.074280] Test:  [210/345]  eta: 0:00:23  loss: 0.1458 (0.1582)  time: 0.1736  data: 0.0001  max mem: 15821
[20:19:55.816498] Test:  [220/345]  eta: 0:00:21  loss: 0.1536 (0.1585)  time: 0.1740  data: 0.0001  max mem: 15821
[20:19:57.563199] Test:  [230/345]  eta: 0:00:19  loss: 0.1585 (0.1585)  time: 0.1744  data: 0.0001  max mem: 15821
[20:19:59.312909] Test:  [240/345]  eta: 0:00:18  loss: 0.1565 (0.1583)  time: 0.1748  data: 0.0001  max mem: 15821
[20:20:01.067262] Test:  [250/345]  eta: 0:00:16  loss: 0.1602 (0.1585)  time: 0.1751  data: 0.0001  max mem: 15821
[20:20:02.823807] Test:  [260/345]  eta: 0:00:14  loss: 0.1674 (0.1588)  time: 0.1755  data: 0.0001  max mem: 15821
[20:20:04.583583] Test:  [270/345]  eta: 0:00:12  loss: 0.1725 (0.1591)  time: 0.1758  data: 0.0001  max mem: 15821
[20:20:06.347547] Test:  [280/345]  eta: 0:00:11  loss: 0.1606 (0.1590)  time: 0.1761  data: 0.0001  max mem: 15821
[20:20:08.115285] Test:  [290/345]  eta: 0:00:09  loss: 0.1622 (0.1592)  time: 0.1765  data: 0.0001  max mem: 15821
[20:20:09.885527] Test:  [300/345]  eta: 0:00:07  loss: 0.1702 (0.1595)  time: 0.1768  data: 0.0001  max mem: 15821
[20:20:11.660528] Test:  [310/345]  eta: 0:00:06  loss: 0.1582 (0.1595)  time: 0.1772  data: 0.0001  max mem: 15821
[20:20:13.438117] Test:  [320/345]  eta: 0:00:04  loss: 0.1530 (0.1594)  time: 0.1776  data: 0.0001  max mem: 15821
[20:20:15.220471] Test:  [330/345]  eta: 0:00:02  loss: 0.1481 (0.1591)  time: 0.1779  data: 0.0001  max mem: 15821
[20:20:17.004597] Test:  [340/345]  eta: 0:00:00  loss: 0.1483 (0.1589)  time: 0.1783  data: 0.0001  max mem: 15821
[20:20:17.719812] Test:  [344/345]  eta: 0:00:00  loss: 0.1483 (0.1588)  time: 0.1785  data: 0.0001  max mem: 15821
[20:20:17.786751] Test: Total time: 0:01:00 (0.1739 s / it)
[20:20:27.853952] Test:  [ 0/57]  eta: 0:00:25  loss: 0.4463 (0.4463)  time: 0.4474  data: 0.2844  max mem: 15821
[20:20:29.508058] Test:  [10/57]  eta: 0:00:08  loss: 0.4078 (0.4171)  time: 0.1909  data: 0.0259  max mem: 15821
[20:20:31.166804] Test:  [20/57]  eta: 0:00:06  loss: 0.4014 (0.3950)  time: 0.1656  data: 0.0001  max mem: 15821
[20:20:32.829463] Test:  [30/57]  eta: 0:00:04  loss: 0.2584 (0.3364)  time: 0.1660  data: 0.0001  max mem: 15821
[20:20:34.494789] Test:  [40/57]  eta: 0:00:02  loss: 0.2280 (0.3126)  time: 0.1663  data: 0.0001  max mem: 15821
[20:20:36.165510] Test:  [50/57]  eta: 0:00:01  loss: 0.2523 (0.3127)  time: 0.1667  data: 0.0001  max mem: 15821
[20:20:37.066382] Test:  [56/57]  eta: 0:00:00  loss: 0.2724 (0.3250)  time: 0.1618  data: 0.0000  max mem: 15821
[20:20:37.131578] Test: Total time: 0:00:09 (0.1706 s / it)
[20:20:38.885431] Dice score of the network on the train images: 0.846460, val images: 0.801914
[20:20:38.889443] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[20:20:39.791324] Epoch: [25]  [  0/345]  eta: 0:05:10  lr: 0.000125  loss: 0.1462 (0.1462)  time: 0.9007  data: 0.2966  max mem: 15821
[20:20:51.812247] Epoch: [25]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.1577 (0.1624)  time: 0.6010  data: 0.0001  max mem: 15821
[20:21:03.864851] Epoch: [25]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.1549 (0.1605)  time: 0.6026  data: 0.0001  max mem: 15821
[20:21:15.936669] Epoch: [25]  [ 60/345]  eta: 0:02:53  lr: 0.000125  loss: 0.1541 (0.1597)  time: 0.6035  data: 0.0001  max mem: 15821
[20:21:28.026997] Epoch: [25]  [ 80/345]  eta: 0:02:40  lr: 0.000125  loss: 0.1568 (0.1609)  time: 0.6045  data: 0.0001  max mem: 15821
[20:21:40.131071] Epoch: [25]  [100/345]  eta: 0:02:28  lr: 0.000125  loss: 0.1636 (0.1628)  time: 0.6052  data: 0.0001  max mem: 15821
[20:21:52.241209] Epoch: [25]  [120/345]  eta: 0:02:16  lr: 0.000125  loss: 0.1610 (0.1628)  time: 0.6055  data: 0.0001  max mem: 15821
[20:22:04.357490] Epoch: [25]  [140/345]  eta: 0:02:04  lr: 0.000125  loss: 0.1647 (0.1636)  time: 0.6058  data: 0.0001  max mem: 15821
[20:22:16.488080] Epoch: [25]  [160/345]  eta: 0:01:52  lr: 0.000125  loss: 0.1584 (0.1636)  time: 0.6065  data: 0.0001  max mem: 15821
[20:22:28.625732] Epoch: [25]  [180/345]  eta: 0:01:40  lr: 0.000125  loss: 0.1659 (0.1646)  time: 0.6068  data: 0.0001  max mem: 15821
[20:22:40.769814] Epoch: [25]  [200/345]  eta: 0:01:27  lr: 0.000125  loss: 0.1600 (0.1644)  time: 0.6072  data: 0.0001  max mem: 15821
[20:22:52.895622] Epoch: [25]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.1679 (0.1644)  time: 0.6062  data: 0.0001  max mem: 15821
[20:23:05.005003] Epoch: [25]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.1630 (0.1646)  time: 0.6054  data: 0.0001  max mem: 15821
[20:23:17.122760] Epoch: [25]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.1625 (0.1647)  time: 0.6058  data: 0.0001  max mem: 15821
[20:23:29.228322] Epoch: [25]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.1572 (0.1644)  time: 0.6052  data: 0.0001  max mem: 15821
[20:23:41.334119] Epoch: [25]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.1549 (0.1642)  time: 0.6052  data: 0.0001  max mem: 15821
[20:23:53.442872] Epoch: [25]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.1597 (0.1642)  time: 0.6054  data: 0.0001  max mem: 15821
[20:24:05.561937] Epoch: [25]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.1631 (0.1643)  time: 0.6059  data: 0.0001  max mem: 15821
[20:24:07.985148] Epoch: [25]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.1652 (0.1646)  time: 0.6059  data: 0.0001  max mem: 15821
[20:24:08.058154] Epoch: [25] Total time: 0:03:29 (0.6063 s / it)
[20:24:08.058453] Averaged stats: lr: 0.000125  loss: 0.1652 (0.1646)
[20:24:08.544180] Test:  [  0/345]  eta: 0:02:45  loss: 0.1540 (0.1540)  time: 0.4797  data: 0.3150  max mem: 15821
[20:24:10.217458] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1525 (0.1516)  time: 0.1956  data: 0.0288  max mem: 15821
[20:24:11.891487] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1522 (0.1550)  time: 0.1673  data: 0.0001  max mem: 15821
[20:24:13.569380] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1603 (0.1583)  time: 0.1675  data: 0.0001  max mem: 15821
[20:24:15.249939] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1615 (0.1618)  time: 0.1679  data: 0.0001  max mem: 15821
[20:24:16.933681] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1545 (0.1604)  time: 0.1681  data: 0.0001  max mem: 15821
[20:24:18.621030] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1526 (0.1605)  time: 0.1685  data: 0.0001  max mem: 15821
[20:24:20.312922] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1624 (0.1626)  time: 0.1689  data: 0.0001  max mem: 15821
[20:24:22.008030] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1558 (0.1617)  time: 0.1693  data: 0.0001  max mem: 15821
[20:24:23.706669] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1558 (0.1624)  time: 0.1696  data: 0.0001  max mem: 15821
[20:24:25.407765] Test:  [100/345]  eta: 0:00:42  loss: 0.1577 (0.1626)  time: 0.1699  data: 0.0001  max mem: 15821
[20:24:27.113575] Test:  [110/345]  eta: 0:00:40  loss: 0.1603 (0.1628)  time: 0.1703  data: 0.0001  max mem: 15821
[20:24:28.822134] Test:  [120/345]  eta: 0:00:38  loss: 0.1591 (0.1631)  time: 0.1706  data: 0.0001  max mem: 15821
[20:24:30.533790] Test:  [130/345]  eta: 0:00:36  loss: 0.1580 (0.1628)  time: 0.1709  data: 0.0001  max mem: 15821
[20:24:32.250709] Test:  [140/345]  eta: 0:00:35  loss: 0.1580 (0.1626)  time: 0.1714  data: 0.0001  max mem: 15821
[20:24:33.970384] Test:  [150/345]  eta: 0:00:33  loss: 0.1486 (0.1621)  time: 0.1718  data: 0.0001  max mem: 15821
[20:24:35.692225] Test:  [160/345]  eta: 0:00:31  loss: 0.1595 (0.1624)  time: 0.1720  data: 0.0001  max mem: 15821
[20:24:37.418589] Test:  [170/345]  eta: 0:00:30  loss: 0.1640 (0.1624)  time: 0.1723  data: 0.0001  max mem: 15821
[20:24:39.148101] Test:  [180/345]  eta: 0:00:28  loss: 0.1688 (0.1628)  time: 0.1727  data: 0.0001  max mem: 15821
[20:24:40.880756] Test:  [190/345]  eta: 0:00:26  loss: 0.1688 (0.1631)  time: 0.1730  data: 0.0001  max mem: 15821
[20:24:42.616729] Test:  [200/345]  eta: 0:00:24  loss: 0.1625 (0.1633)  time: 0.1734  data: 0.0001  max mem: 15821
[20:24:44.356311] Test:  [210/345]  eta: 0:00:23  loss: 0.1610 (0.1634)  time: 0.1737  data: 0.0001  max mem: 15821
[20:24:46.098817] Test:  [220/345]  eta: 0:00:21  loss: 0.1579 (0.1631)  time: 0.1740  data: 0.0001  max mem: 15821
[20:24:47.845669] Test:  [230/345]  eta: 0:00:19  loss: 0.1617 (0.1636)  time: 0.1744  data: 0.0001  max mem: 15821
[20:24:49.594608] Test:  [240/345]  eta: 0:00:18  loss: 0.1655 (0.1638)  time: 0.1747  data: 0.0001  max mem: 15821
[20:24:51.348572] Test:  [250/345]  eta: 0:00:16  loss: 0.1616 (0.1634)  time: 0.1751  data: 0.0001  max mem: 15821
[20:24:53.105125] Test:  [260/345]  eta: 0:00:14  loss: 0.1570 (0.1635)  time: 0.1755  data: 0.0001  max mem: 15821
[20:24:54.864366] Test:  [270/345]  eta: 0:00:12  loss: 0.1540 (0.1637)  time: 0.1757  data: 0.0001  max mem: 15821
[20:24:56.628878] Test:  [280/345]  eta: 0:00:11  loss: 0.1517 (0.1631)  time: 0.1761  data: 0.0001  max mem: 15821
[20:24:58.395208] Test:  [290/345]  eta: 0:00:09  loss: 0.1517 (0.1632)  time: 0.1765  data: 0.0001  max mem: 15821
[20:25:00.166457] Test:  [300/345]  eta: 0:00:07  loss: 0.1607 (0.1632)  time: 0.1768  data: 0.0001  max mem: 15821
[20:25:01.939712] Test:  [310/345]  eta: 0:00:06  loss: 0.1560 (0.1632)  time: 0.1772  data: 0.0001  max mem: 15821
[20:25:03.717840] Test:  [320/345]  eta: 0:00:04  loss: 0.1587 (0.1635)  time: 0.1775  data: 0.0001  max mem: 15821
[20:25:05.500209] Test:  [330/345]  eta: 0:00:02  loss: 0.1635 (0.1636)  time: 0.1780  data: 0.0001  max mem: 15821
[20:25:07.287692] Test:  [340/345]  eta: 0:00:00  loss: 0.1629 (0.1637)  time: 0.1784  data: 0.0001  max mem: 15821
[20:25:08.002488] Test:  [344/345]  eta: 0:00:00  loss: 0.1613 (0.1636)  time: 0.1785  data: 0.0001  max mem: 15821
[20:25:08.072961] Test: Total time: 0:01:00 (0.1739 s / it)
[20:25:18.015957] Test:  [ 0/57]  eta: 0:00:26  loss: 0.4587 (0.4587)  time: 0.4687  data: 0.3056  max mem: 15821
[20:25:19.669906] Test:  [10/57]  eta: 0:00:09  loss: 0.4289 (0.4342)  time: 0.1929  data: 0.0279  max mem: 15821
[20:25:21.329383] Test:  [20/57]  eta: 0:00:06  loss: 0.4289 (0.4193)  time: 0.1656  data: 0.0001  max mem: 15821
[20:25:22.993180] Test:  [30/57]  eta: 0:00:04  loss: 0.2696 (0.3583)  time: 0.1661  data: 0.0001  max mem: 15821
[20:25:24.659741] Test:  [40/57]  eta: 0:00:02  loss: 0.2382 (0.3339)  time: 0.1664  data: 0.0001  max mem: 15821
[20:25:26.330167] Test:  [50/57]  eta: 0:00:01  loss: 0.2715 (0.3343)  time: 0.1668  data: 0.0001  max mem: 15821
[20:25:27.230839] Test:  [56/57]  eta: 0:00:00  loss: 0.3098 (0.3454)  time: 0.1618  data: 0.0001  max mem: 15821
[20:25:27.293683] Test: Total time: 0:00:09 (0.1710 s / it)
[20:25:29.034755] Dice score of the network on the train images: 0.852086, val images: 0.798971
[20:25:29.038822] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[20:25:29.944663] Epoch: [26]  [  0/345]  eta: 0:05:12  lr: 0.000125  loss: 0.1519 (0.1519)  time: 0.9046  data: 0.3012  max mem: 15821
[20:25:41.975878] Epoch: [26]  [ 20/345]  eta: 0:03:20  lr: 0.000125  loss: 0.1667 (0.1737)  time: 0.6015  data: 0.0001  max mem: 15821
[20:25:54.164407] Epoch: [26]  [ 40/345]  eta: 0:03:06  lr: 0.000125  loss: 0.1646 (0.1677)  time: 0.6094  data: 0.0001  max mem: 15821
[20:26:06.244930] Epoch: [26]  [ 60/345]  eta: 0:02:53  lr: 0.000125  loss: 0.1536 (0.1655)  time: 0.6040  data: 0.0001  max mem: 15821
[20:26:18.334632] Epoch: [26]  [ 80/345]  eta: 0:02:41  lr: 0.000125  loss: 0.1594 (0.1652)  time: 0.6044  data: 0.0001  max mem: 15821
[20:26:30.436909] Epoch: [26]  [100/345]  eta: 0:02:28  lr: 0.000125  loss: 0.1512 (0.1644)  time: 0.6051  data: 0.0001  max mem: 15821
[20:26:42.550303] Epoch: [26]  [120/345]  eta: 0:02:16  lr: 0.000125  loss: 0.1591 (0.1654)  time: 0.6056  data: 0.0001  max mem: 15821
[20:26:54.678217] Epoch: [26]  [140/345]  eta: 0:02:04  lr: 0.000125  loss: 0.1580 (0.1648)  time: 0.6064  data: 0.0001  max mem: 15821
[20:27:06.814795] Epoch: [26]  [160/345]  eta: 0:01:52  lr: 0.000125  loss: 0.1571 (0.1643)  time: 0.6068  data: 0.0001  max mem: 15821
[20:27:18.960963] Epoch: [26]  [180/345]  eta: 0:01:40  lr: 0.000125  loss: 0.1551 (0.1639)  time: 0.6073  data: 0.0001  max mem: 15821
[20:27:31.097450] Epoch: [26]  [200/345]  eta: 0:01:28  lr: 0.000125  loss: 0.1662 (0.1645)  time: 0.6068  data: 0.0001  max mem: 15821
[20:27:43.231922] Epoch: [26]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.1657 (0.1648)  time: 0.6067  data: 0.0001  max mem: 15821
[20:27:55.343339] Epoch: [26]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.1676 (0.1655)  time: 0.6055  data: 0.0001  max mem: 15821
[20:28:07.444662] Epoch: [26]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.1562 (0.1653)  time: 0.6050  data: 0.0001  max mem: 15821
[20:28:19.556544] Epoch: [26]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.1699 (0.1657)  time: 0.6055  data: 0.0001  max mem: 15821
[20:28:31.657565] Epoch: [26]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.1608 (0.1653)  time: 0.6050  data: 0.0001  max mem: 15821
[20:28:43.778720] Epoch: [26]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.1597 (0.1655)  time: 0.6060  data: 0.0001  max mem: 15821
[20:28:55.888835] Epoch: [26]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.1761 (0.1663)  time: 0.6055  data: 0.0001  max mem: 15821
[20:28:58.310218] Epoch: [26]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.1761 (0.1663)  time: 0.6053  data: 0.0001  max mem: 15821
[20:28:58.381746] Epoch: [26] Total time: 0:03:29 (0.6068 s / it)
[20:28:58.382072] Averaged stats: lr: 0.000125  loss: 0.1761 (0.1663)
[20:28:58.870961] Test:  [  0/345]  eta: 0:02:46  loss: 0.1509 (0.1509)  time: 0.4830  data: 0.3185  max mem: 15821
[20:29:00.542498] Test:  [ 10/345]  eta: 0:01:05  loss: 0.1545 (0.1562)  time: 0.1958  data: 0.0290  max mem: 15821
[20:29:02.217354] Test:  [ 20/345]  eta: 0:00:59  loss: 0.1567 (0.1582)  time: 0.1672  data: 0.0001  max mem: 15821
[20:29:03.895500] Test:  [ 30/345]  eta: 0:00:55  loss: 0.1628 (0.1613)  time: 0.1676  data: 0.0001  max mem: 15821
[20:29:05.575286] Test:  [ 40/345]  eta: 0:00:53  loss: 0.1594 (0.1609)  time: 0.1678  data: 0.0001  max mem: 15821
[20:29:07.259295] Test:  [ 50/345]  eta: 0:00:51  loss: 0.1577 (0.1597)  time: 0.1681  data: 0.0001  max mem: 15821
[20:29:08.946600] Test:  [ 60/345]  eta: 0:00:49  loss: 0.1527 (0.1590)  time: 0.1685  data: 0.0001  max mem: 15821
[20:29:10.637835] Test:  [ 70/345]  eta: 0:00:47  loss: 0.1530 (0.1594)  time: 0.1689  data: 0.0001  max mem: 15821
[20:29:12.333354] Test:  [ 80/345]  eta: 0:00:45  loss: 0.1607 (0.1599)  time: 0.1693  data: 0.0001  max mem: 15821
[20:29:14.031824] Test:  [ 90/345]  eta: 0:00:43  loss: 0.1606 (0.1599)  time: 0.1696  data: 0.0001  max mem: 15821
[20:29:15.733306] Test:  [100/345]  eta: 0:00:42  loss: 0.1536 (0.1593)  time: 0.1699  data: 0.0001  max mem: 15821
[20:29:17.437909] Test:  [110/345]  eta: 0:00:40  loss: 0.1477 (0.1586)  time: 0.1702  data: 0.0001  max mem: 15821
[20:29:19.144988] Test:  [120/345]  eta: 0:00:38  loss: 0.1555 (0.1583)  time: 0.1705  data: 0.0001  max mem: 15821
[20:29:20.856500] Test:  [130/345]  eta: 0:00:36  loss: 0.1610 (0.1590)  time: 0.1709  data: 0.0001  max mem: 15821
[20:29:22.571672] Test:  [140/345]  eta: 0:00:35  loss: 0.1653 (0.1598)  time: 0.1713  data: 0.0001  max mem: 15821
[20:29:24.291275] Test:  [150/345]  eta: 0:00:33  loss: 0.1590 (0.1601)  time: 0.1717  data: 0.0001  max mem: 15821
[20:29:26.013661] Test:  [160/345]  eta: 0:00:31  loss: 0.1616 (0.1602)  time: 0.1720  data: 0.0001  max mem: 15821
[20:29:27.739599] Test:  [170/345]  eta: 0:00:30  loss: 0.1629 (0.1604)  time: 0.1723  data: 0.0001  max mem: 15821
[20:29:29.468819] Test:  [180/345]  eta: 0:00:28  loss: 0.1657 (0.1606)  time: 0.1727  data: 0.0001  max mem: 15821
[20:29:31.200821] Test:  [190/345]  eta: 0:00:26  loss: 0.1639 (0.1606)  time: 0.1730  data: 0.0001  max mem: 15821
[20:29:32.936762] Test:  [200/345]  eta: 0:00:24  loss: 0.1586 (0.1607)  time: 0.1733  data: 0.0001  max mem: 15821
[20:29:34.675124] Test:  [210/345]  eta: 0:00:23  loss: 0.1586 (0.1607)  time: 0.1736  data: 0.0001  max mem: 15821
[20:29:36.418186] Test:  [220/345]  eta: 0:00:21  loss: 0.1559 (0.1606)  time: 0.1740  data: 0.0001  max mem: 15821
[20:29:38.163891] Test:  [230/345]  eta: 0:00:19  loss: 0.1519 (0.1602)  time: 0.1744  data: 0.0001  max mem: 15821
[20:29:39.913998] Test:  [240/345]  eta: 0:00:18  loss: 0.1519 (0.1600)  time: 0.1747  data: 0.0001  max mem: 15821
[20:29:41.667142] Test:  [250/345]  eta: 0:00:16  loss: 0.1539 (0.1597)  time: 0.1751  data: 0.0001  max mem: 15821
[20:29:43.422616] Test:  [260/345]  eta: 0:00:14  loss: 0.1557 (0.1601)  time: 0.1754  data: 0.0001  max mem: 15821
[20:29:45.182491] Test:  [270/345]  eta: 0:00:12  loss: 0.1597 (0.1600)  time: 0.1757  data: 0.0001  max mem: 15821
[20:29:46.945553] Test:  [280/345]  eta: 0:00:11  loss: 0.1627 (0.1603)  time: 0.1761  data: 0.0001  max mem: 15821
[20:29:48.713338] Test:  [290/345]  eta: 0:00:09  loss: 0.1617 (0.1601)  time: 0.1765  data: 0.0001  max mem: 15821
[20:29:50.483491] Test:  [300/345]  eta: 0:00:07  loss: 0.1504 (0.1600)  time: 0.1768  data: 0.0001  max mem: 15821
[20:29:52.258872] Test:  [310/345]  eta: 0:00:06  loss: 0.1528 (0.1600)  time: 0.1772  data: 0.0001  max mem: 15821
[20:29:54.037037] Test:  [320/345]  eta: 0:00:04  loss: 0.1562 (0.1599)  time: 0.1776  data: 0.0001  max mem: 15821
[20:29:55.818942] Test:  [330/345]  eta: 0:00:02  loss: 0.1572 (0.1600)  time: 0.1779  data: 0.0001  max mem: 15821
[20:29:57.603735] Test:  [340/345]  eta: 0:00:00  loss: 0.1528 (0.1600)  time: 0.1783  data: 0.0001  max mem: 15821
[20:29:58.319039] Test:  [344/345]  eta: 0:00:00  loss: 0.1508 (0.1598)  time: 0.1784  data: 0.0001  max mem: 15821
[20:29:58.394111] Test: Total time: 0:01:00 (0.1739 s / it)
[20:30:08.812955] Test:  [ 0/57]  eta: 0:00:26  loss: 0.4531 (0.4531)  time: 0.4620  data: 0.2991  max mem: 15821
[20:30:10.465923] Test:  [10/57]  eta: 0:00:09  loss: 0.4191 (0.4363)  time: 0.1922  data: 0.0273  max mem: 15821
[20:30:12.122505] Test:  [20/57]  eta: 0:00:06  loss: 0.4191 (0.4184)  time: 0.1654  data: 0.0001  max mem: 15821
[20:30:13.783344] Test:  [30/57]  eta: 0:00:04  loss: 0.2719 (0.3558)  time: 0.1658  data: 0.0001  max mem: 15821
[20:30:15.448367] Test:  [40/57]  eta: 0:00:02  loss: 0.2258 (0.3326)  time: 0.1662  data: 0.0001  max mem: 15821
[20:30:17.117211] Test:  [50/57]  eta: 0:00:01  loss: 0.2637 (0.3341)  time: 0.1666  data: 0.0001  max mem: 15821
[20:30:18.017919] Test:  [56/57]  eta: 0:00:00  loss: 0.3319 (0.3478)  time: 0.1617  data: 0.0000  max mem: 15821
[20:30:18.089658] Test: Total time: 0:00:09 (0.1709 s / it)
[20:30:20.042247] Dice score of the network on the train images: 0.862779, val images: 0.798440
[20:30:20.042476] saving best_prec_model_0 @ epoch 26
[20:30:21.320024] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[20:30:22.223110] Epoch: [27]  [  0/345]  eta: 0:05:11  lr: 0.000125  loss: 0.1446 (0.1446)  time: 0.9023  data: 0.3006  max mem: 15821
[20:30:34.233307] Epoch: [27]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.1583 (0.1587)  time: 0.6005  data: 0.0001  max mem: 15821
[20:30:46.278879] Epoch: [27]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.1551 (0.1594)  time: 0.6022  data: 0.0001  max mem: 15821
[20:30:58.347835] Epoch: [27]  [ 60/345]  eta: 0:02:52  lr: 0.000125  loss: 0.1536 (0.1590)  time: 0.6034  data: 0.0001  max mem: 15821
[20:31:10.438814] Epoch: [27]  [ 80/345]  eta: 0:02:40  lr: 0.000125  loss: 0.1541 (0.1582)  time: 0.6045  data: 0.0001  max mem: 15821
[20:31:22.667397] Epoch: [27]  [100/345]  eta: 0:02:28  lr: 0.000125  loss: 0.1576 (0.1589)  time: 0.6114  data: 0.0001  max mem: 15821
[20:31:34.782870] Epoch: [27]  [120/345]  eta: 0:02:16  lr: 0.000125  loss: 0.1712 (0.1607)  time: 0.6057  data: 0.0001  max mem: 15821
[20:31:46.921624] Epoch: [27]  [140/345]  eta: 0:02:04  lr: 0.000125  loss: 0.1576 (0.1607)  time: 0.6069  data: 0.0001  max mem: 15821
[20:31:59.078697] Epoch: [27]  [160/345]  eta: 0:01:52  lr: 0.000125  loss: 0.1576 (0.1605)  time: 0.6078  data: 0.0001  max mem: 15821
[20:32:11.229836] Epoch: [27]  [180/345]  eta: 0:01:40  lr: 0.000125  loss: 0.1605 (0.1607)  time: 0.6075  data: 0.0001  max mem: 15821
[20:32:23.364640] Epoch: [27]  [200/345]  eta: 0:01:28  lr: 0.000125  loss: 0.1682 (0.1618)  time: 0.6067  data: 0.0001  max mem: 15821
[20:32:35.494125] Epoch: [27]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.1445 (0.1612)  time: 0.6064  data: 0.0001  max mem: 15821
[20:32:47.617384] Epoch: [27]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.1525 (0.1608)  time: 0.6061  data: 0.0001  max mem: 15821

[20:32:59.738192] Epoch: [27]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.1590 (0.1610)  time: 0.6060  data: 0.0001  max mem: 15821
[20:33:11.852679] Epoch: [27]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.1550 (0.1612)  time: 0.6057  data: 0.0001  max mem: 15821
[20:33:21.081836] [20:33:21.082092] [20:33:21.082165] [20:33:21.082227] [20:33:21.082285] [20:33:21.082341] [20:33:21.082405] [20:33:21.082461] [20:33:21.082517] [20:33:21.082573] [20:33:21.082628] [20:33:21.082682] [20:33:21.082735] [20:33:21.082790] [20:33:21.082844] [20:33:21.082898] [20:33:21.082953] [20:33:21.083008] [20:33:21.083062] [20:33:21.083117] [20:33:21.083171] [20:33:21.083225] [20:33:21.083278] [20:33:21.083335] [20:33:21.083391] [20:33:21.083453]
Traceback (most recent call last):
  File "/root/anaconda3/envs/vivim/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/anaconda3/envs/vivim/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/seg_framework/MS-Mamba/run_scripts/K_fold_mslesseg_25D.py", line 515, in <module>
  File "/root/seg_framework/MS-Mamba/run_scripts/K_fold_mslesseg_25D.py", line 415, in main
    print("model:", model)
  File "/root/seg_framework/MS-Mamba/run_scripts/K_fold_mslesseg_25D.py", line 119, in train_one_epoch
    outputs = model(samples)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/seg_framework/MS-Mamba/model/Vivim2D.py", line 324, in forward
    outs = self.encoder(x_in)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/seg_framework/MS-Mamba/model/Vivim2D.py", line 232, in forward
    x = self.forward_features(x)
  File "/root/seg_framework/MS-Mamba/model/Vivim2D.py", line 211, in forward_features
    layer_outputs = blk(hs, height, width, False)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/transformers/models/segformer/modeling_segformer.py", line 334, in forward
    self_attention_outputs = self.attention(
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/transformers/models/segformer/modeling_segformer.py", line 272, in forward
    self_outputs = self.self(hidden_states, height, width, output_attentions)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/transformers/models/segformer/modeling_segformer.py", line 204, in forward
    value_layer = self.transpose_for_scores(self.value(hidden_states))
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
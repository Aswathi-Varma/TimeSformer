Not using distributed mode
[07:41:33.648115] job dir: /root/seg_framework/MS-Mamba/run_scripts
[07:41:33.648262] Namespace(accum_iter=1,
model='SegFormer3D',
in_channels=1,
lr=None,
blr=0.001,
min_lr=0,
dist_on_itp=False,
warmup_epochs=20,
device='cuda:0',
seed=42,
layer_decay=0.75,
clip_grad=None,
num_workers=1,
pin_mem=True,
resume='',
mask_mode='concatenate to image',
world_size=1,
embed_dim=1,
local_rank=-1,
dist_url='env://',
nb_classes=2,
data_dir='/root/MSLesSeg24/data',
datalist=None,
preprocess=False,
dim=2,
loss='tripleloss',
distributed=False)
[07:41:33.648375] device  cuda:0
[07:41:33.649016] Random seed set as 42
[07:41:33.649345] Starting for fold 0
[07:41:33.838676] Elements in data_dir_paths: 11052
[07:41:33.874125] Elements in data_dir_paths: 1803
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
[07:41:35.515883] number of params: 59617327
[07:41:35.516119] model: Vivim2D(
  (encoder): mamba_block(
    (downsample_layers): SegformerEncoder(
      (patch_embeddings): ModuleList(
        (0): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(2, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (3): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (block): ModuleList(
        (0): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): Identity()
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.003703703870996833)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.007407407741993666)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.011111111380159855)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.014814815483987331)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.018518518656492233)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.02222222276031971)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.025925926864147186)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.029629630967974663)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03333333507180214)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03703703731298447)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04074074327945709)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04444444552063942)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.048148151487112045)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.051851850003004074)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0555555559694767)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.05925925821065903)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06296296417713165)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06666667014360428)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07037036865949631)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07407407462596893)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07777778059244156)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08148147910833359)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08518518507480621)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08888889104127884)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.09259259700775146)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0962962955236435)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.10000000149011612)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (layer_norm): ModuleList(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (2): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (3): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (decoder): SegformerDecodeHead(
    (linear_c): ModuleList(
      (0): SegformerMLP(
        (proj): Linear(in_features=64, out_features=768, bias=True)
      )
      (1): SegformerMLP(
        (proj): Linear(in_features=128, out_features=768, bias=True)
      )
      (2): SegformerMLP(
        (proj): Linear(in_features=320, out_features=768, bias=True)
      )
      (3): SegformerMLP(
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
    )
    (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Conv2d(768, 150, kernel_size=(1, 1), stride=(1, 1))
  )
  (out): Conv2d(768, 1, kernel_size=(1, 1), stride=(1, 1))
)
[07:41:35.519267] base lr: 1.00e-03
[07:41:35.519335] actual lr: 1.25e-04
[07:41:35.519398] accumulate grad iterations: 1
[07:41:35.519445] effective batch size: 32
[07:41:35.521086] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.000125
    maximize: False
    weight_decay: 0.01
)
[07:41:35.522991] Start training for 50 epochs
[07:41:35.524838] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[07:41:37.105932] Epoch: [0]  [  0/345]  eta: 0:09:05  lr: 0.000000  loss: 1.6965 (1.6965)  time: 1.5800  data: 0.1926  max mem: 15231
[07:41:51.847314] Epoch: [0]  [ 20/345]  eta: 0:04:12  lr: 0.000000  loss: 1.6955 (1.6956)  time: 0.7370  data: 0.0001  max mem: 15689
[07:42:06.730443] Epoch: [0]  [ 40/345]  eta: 0:03:52  lr: 0.000001  loss: 1.6917 (1.6936)  time: 0.7441  data: 0.0001  max mem: 15689
[07:42:21.698372] Epoch: [0]  [ 60/345]  eta: 0:03:35  lr: 0.000001  loss: 1.6874 (1.6916)  time: 0.7483  data: 0.0001  max mem: 15689
[07:42:36.746233] Epoch: [0]  [ 80/345]  eta: 0:03:20  lr: 0.000001  loss: 1.6850 (1.6901)  time: 0.7523  data: 0.0001  max mem: 15689
[07:42:51.815830] Epoch: [0]  [100/345]  eta: 0:03:05  lr: 0.000002  loss: 1.6822 (1.6885)  time: 0.7534  data: 0.0001  max mem: 15689
[07:43:06.910263] Epoch: [0]  [120/345]  eta: 0:02:49  lr: 0.000002  loss: 1.6764 (1.6865)  time: 0.7547  data: 0.0001  max mem: 15689
[07:43:22.016999] Epoch: [0]  [140/345]  eta: 0:02:34  lr: 0.000003  loss: 1.6697 (1.6843)  time: 0.7553  data: 0.0001  max mem: 15689
[07:43:37.127609] Epoch: [0]  [160/345]  eta: 0:02:19  lr: 0.000003  loss: 1.6670 (1.6822)  time: 0.7555  data: 0.0001  max mem: 15689
[07:43:52.260705] Epoch: [0]  [180/345]  eta: 0:02:04  lr: 0.000003  loss: 1.6606 (1.6798)  time: 0.7566  data: 0.0001  max mem: 15689
[07:44:07.394646] Epoch: [0]  [200/345]  eta: 0:01:49  lr: 0.000004  loss: 1.6525 (1.6772)  time: 0.7567  data: 0.0001  max mem: 15689
[07:44:22.520476] Epoch: [0]  [220/345]  eta: 0:01:34  lr: 0.000004  loss: 1.6446 (1.6743)  time: 0.7563  data: 0.0001  max mem: 15689
[07:44:37.646965] Epoch: [0]  [240/345]  eta: 0:01:19  lr: 0.000004  loss: 1.6343 (1.6711)  time: 0.7563  data: 0.0001  max mem: 15689
[07:44:52.768617] Epoch: [0]  [260/345]  eta: 0:01:04  lr: 0.000005  loss: 1.6247 (1.6675)  time: 0.7560  data: 0.0001  max mem: 15689
[07:45:07.884207] Epoch: [0]  [280/345]  eta: 0:00:49  lr: 0.000005  loss: 1.6127 (1.6636)  time: 0.7557  data: 0.0001  max mem: 15689
[07:45:23.007538] Epoch: [0]  [300/345]  eta: 0:00:34  lr: 0.000005  loss: 1.5994 (1.6594)  time: 0.7561  data: 0.0001  max mem: 15689
[07:45:38.201330] Epoch: [0]  [320/345]  eta: 0:00:18  lr: 0.000006  loss: 1.5854 (1.6548)  time: 0.7597  data: 0.0001  max mem: 15689
[07:45:53.302808] Epoch: [0]  [340/345]  eta: 0:00:03  lr: 0.000006  loss: 1.5717 (1.6499)  time: 0.7550  data: 0.0001  max mem: 15689
[07:45:56.327440] Epoch: [0]  [344/345]  eta: 0:00:00  lr: 0.000006  loss: 1.5658 (1.6489)  time: 0.7553  data: 0.0001  max mem: 15689
[07:45:56.364782] Epoch: [0] Total time: 0:04:20 (0.7561 s / it)
[07:45:56.365028] Averaged stats: lr: 0.000006  loss: 1.5658 (1.6489)
[07:45:56.711567] Test:  [  0/345]  eta: 0:01:58  loss: 1.5938 (1.5938)  time: 0.3432  data: 0.1593  max mem: 15689
[07:45:58.568549] Test:  [ 10/345]  eta: 0:01:06  loss: 1.5938 (1.5936)  time: 0.2000  data: 0.0145  max mem: 15689
[07:46:00.430142] Test:  [ 20/345]  eta: 0:01:02  loss: 1.5941 (1.5938)  time: 0.1859  data: 0.0001  max mem: 15689
[07:46:02.295780] Test:  [ 30/345]  eta: 0:01:00  loss: 1.5925 (1.5932)  time: 0.1863  data: 0.0001  max mem: 15689
[07:46:04.163908] Test:  [ 40/345]  eta: 0:00:57  loss: 1.5925 (1.5931)  time: 0.1866  data: 0.0001  max mem: 15689
[07:46:06.035073] Test:  [ 50/345]  eta: 0:00:55  loss: 1.5936 (1.5932)  time: 0.1869  data: 0.0001  max mem: 15689
[07:46:07.910703] Test:  [ 60/345]  eta: 0:00:53  loss: 1.5936 (1.5931)  time: 0.1873  data: 0.0001  max mem: 15689
[07:46:09.790494] Test:  [ 70/345]  eta: 0:00:51  loss: 1.5913 (1.5928)  time: 0.1877  data: 0.0001  max mem: 15689
[07:46:11.671337] Test:  [ 80/345]  eta: 0:00:50  loss: 1.5913 (1.5928)  time: 0.1880  data: 0.0001  max mem: 15689
[07:46:13.555946] Test:  [ 90/345]  eta: 0:00:48  loss: 1.5924 (1.5927)  time: 0.1882  data: 0.0001  max mem: 15689
[07:46:15.443590] Test:  [100/345]  eta: 0:00:46  loss: 1.5928 (1.5926)  time: 0.1886  data: 0.0001  max mem: 15689
[07:46:17.336182] Test:  [110/345]  eta: 0:00:44  loss: 1.5931 (1.5927)  time: 0.1890  data: 0.0001  max mem: 15689
[07:46:19.230374] Test:  [120/345]  eta: 0:00:42  loss: 1.5936 (1.5927)  time: 0.1893  data: 0.0001  max mem: 15689
[07:46:21.128161] Test:  [130/345]  eta: 0:00:40  loss: 1.5921 (1.5926)  time: 0.1895  data: 0.0001  max mem: 15689
[07:46:23.031151] Test:  [140/345]  eta: 0:00:38  loss: 1.5925 (1.5925)  time: 0.1900  data: 0.0001  max mem: 15689
[07:46:24.937076] Test:  [150/345]  eta: 0:00:36  loss: 1.5931 (1.5925)  time: 0.1904  data: 0.0001  max mem: 15689
[07:46:26.846806] Test:  [160/345]  eta: 0:00:35  loss: 1.5937 (1.5926)  time: 0.1907  data: 0.0001  max mem: 15689
[07:46:28.760025] Test:  [170/345]  eta: 0:00:33  loss: 1.5926 (1.5926)  time: 0.1911  data: 0.0001  max mem: 15689
[07:46:30.676404] Test:  [180/345]  eta: 0:00:31  loss: 1.5930 (1.5926)  time: 0.1914  data: 0.0001  max mem: 15689
[07:46:32.597856] Test:  [190/345]  eta: 0:00:29  loss: 1.5935 (1.5926)  time: 0.1918  data: 0.0001  max mem: 15689
[07:46:34.521595] Test:  [200/345]  eta: 0:00:27  loss: 1.5923 (1.5926)  time: 0.1922  data: 0.0001  max mem: 15689
[07:46:36.855997] Test:  [210/345]  eta: 0:00:25  loss: 1.5923 (1.5926)  time: 0.2129  data: 0.0001  max mem: 15689
[07:46:38.792454] Test:  [220/345]  eta: 0:00:23  loss: 1.5940 (1.5927)  time: 0.2135  data: 0.0001  max mem: 15689
[07:46:40.870199] Test:  [230/345]  eta: 0:00:22  loss: 1.5944 (1.5927)  time: 0.2007  data: 0.0001  max mem: 15689
[07:46:42.955748] Test:  [240/345]  eta: 0:00:20  loss: 1.5926 (1.5927)  time: 0.2081  data: 0.0001  max mem: 15689
[07:46:44.926989] Test:  [250/345]  eta: 0:00:18  loss: 1.5927 (1.5927)  time: 0.2028  data: 0.0001  max mem: 15689
[07:46:47.055714] Test:  [260/345]  eta: 0:00:16  loss: 1.5921 (1.5927)  time: 0.2049  data: 0.0001  max mem: 15689
[07:46:49.167279] Test:  [270/345]  eta: 0:00:14  loss: 1.5934 (1.5927)  time: 0.2120  data: 0.0001  max mem: 15689
[07:46:51.143272] Test:  [280/345]  eta: 0:00:12  loss: 1.5934 (1.5927)  time: 0.2043  data: 0.0001  max mem: 15689
[07:46:53.284879] Test:  [290/345]  eta: 0:00:10  loss: 1.5910 (1.5926)  time: 0.2058  data: 0.0001  max mem: 15689
[07:46:55.503294] Test:  [300/345]  eta: 0:00:08  loss: 1.5934 (1.5927)  time: 0.2179  data: 0.0001  max mem: 15689
[07:46:57.646181] Test:  [310/345]  eta: 0:00:06  loss: 1.5937 (1.5927)  time: 0.2180  data: 0.0001  max mem: 15689

[07:46:59.853175] Test:  [320/345]  eta: 0:00:04  loss: 1.5932 (1.5927)  time: 0.2174  data: 0.0001  max mem: 15689
[07:47:02.055104] Test:  [330/345]  eta: 0:00:02  loss: 1.5928 (1.5927)  time: 0.2204  data: 0.0001  max mem: 15689
[07:47:04.308052] Test:  [340/345]  eta: 0:00:00  loss: 1.5917 (1.5927)  time: 0.2227  data: 0.0001  max mem: 15689
[07:47:05.119428] Test:  [344/345]  eta: 0:00:00  loss: 1.5918 (1.5927)  time: 0.2233  data: 0.0001  max mem: 15689
[07:47:05.179980] Test: Total time: 0:01:08 (0.1995 s / it)
[07:47:15.958088] Test:  [ 0/57]  eta: 0:00:18  loss: 1.6031 (1.6031)  time: 0.3273  data: 0.1450  max mem: 15689
[07:47:17.791232] Test:  [10/57]  eta: 0:00:09  loss: 1.5973 (1.5980)  time: 0.1963  data: 0.0132  max mem: 15689
[07:47:19.632156] Test:  [20/57]  eta: 0:00:07  loss: 1.5978 (1.5966)  time: 0.1836  data: 0.0001  max mem: 15689
[07:47:21.477319] Test:  [30/57]  eta: 0:00:05  loss: 1.5949 (1.5923)  time: 0.1843  data: 0.0001  max mem: 15689
[07:47:23.325800] Test:  [40/57]  eta: 0:00:03  loss: 1.5840 (1.5894)  time: 0.1846  data: 0.0001  max mem: 15689
[07:47:25.179304] Test:  [50/57]  eta: 0:00:01  loss: 1.5840 (1.5885)  time: 0.1851  data: 0.0001  max mem: 15689
[07:47:26.203421] Test:  [56/57]  eta: 0:00:00  loss: 1.5873 (1.5884)  time: 0.1808  data: 0.0001  max mem: 15689
[07:47:26.260962] Test: Total time: 0:00:10 (0.1865 s / it)
[07:47:28.208072] Dice score of the network on the train images: 0.000000, val images: 0.000000
[07:47:28.208320] saving best_dice_model_0 @ epoch 0
[07:47:29.281647] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[07:47:30.178271] Epoch: [1]  [  0/345]  eta: 0:05:08  lr: 0.000006  loss: 1.5589 (1.5589)  time: 0.8953  data: 0.1411  max mem: 15689
[07:47:45.303972] Epoch: [1]  [ 20/345]  eta: 0:04:07  lr: 0.000007  loss: 1.5513 (1.5526)  time: 0.7562  data: 0.0001  max mem: 15689
[07:48:00.503176] Epoch: [1]  [ 40/345]  eta: 0:03:52  lr: 0.000007  loss: 1.5420 (1.5472)  time: 0.7599  data: 0.0001  max mem: 15689
[07:48:15.718434] Epoch: [1]  [ 60/345]  eta: 0:03:36  lr: 0.000007  loss: 1.5235 (1.5400)  time: 0.7607  data: 0.0001  max mem: 15689
[07:48:30.950628] Epoch: [1]  [ 80/345]  eta: 0:03:21  lr: 0.000008  loss: 1.5102 (1.5334)  time: 0.7616  data: 0.0001  max mem: 15689
[07:48:46.210797] Epoch: [1]  [100/345]  eta: 0:03:06  lr: 0.000008  loss: 1.4967 (1.5266)  time: 0.7630  data: 0.0001  max mem: 15689
[07:49:01.472632] Epoch: [1]  [120/345]  eta: 0:02:51  lr: 0.000008  loss: 1.4868 (1.5198)  time: 0.7630  data: 0.0001  max mem: 15689
[07:49:16.700322] Epoch: [1]  [140/345]  eta: 0:02:36  lr: 0.000009  loss: 1.4719 (1.5131)  time: 0.7613  data: 0.0001  max mem: 15689
[07:49:31.922513] Epoch: [1]  [160/345]  eta: 0:02:20  lr: 0.000009  loss: 1.4666 (1.5074)  time: 0.7611  data: 0.0001  max mem: 15689
[07:49:47.131740] Epoch: [1]  [180/345]  eta: 0:02:05  lr: 0.000010  loss: 1.4543 (1.5017)  time: 0.7604  data: 0.0001  max mem: 15689
[07:50:02.345026] Epoch: [1]  [200/345]  eta: 0:01:50  lr: 0.000010  loss: 1.4464 (1.4965)  time: 0.7606  data: 0.0001  max mem: 15689
[07:50:17.555333] Epoch: [1]  [220/345]  eta: 0:01:35  lr: 0.000010  loss: 1.4327 (1.4908)  time: 0.7605  data: 0.0001  max mem: 15689
[07:50:32.754258] Epoch: [1]  [240/345]  eta: 0:01:19  lr: 0.000011  loss: 1.4221 (1.4853)  time: 0.7599  data: 0.0001  max mem: 15689
[07:50:47.948122] Epoch: [1]  [260/345]  eta: 0:01:04  lr: 0.000011  loss: 1.4219 (1.4805)  time: 0.7597  data: 0.0001  max mem: 15689
[07:51:03.144658] Epoch: [1]  [280/345]  eta: 0:00:49  lr: 0.000011  loss: 1.4123 (1.4760)  time: 0.7598  data: 0.0001  max mem: 15689
[07:51:18.339171] Epoch: [1]  [300/345]  eta: 0:00:34  lr: 0.000012  loss: 1.4043 (1.4714)  time: 0.7597  data: 0.0001  max mem: 15689
[07:51:33.533885] Epoch: [1]  [320/345]  eta: 0:00:19  lr: 0.000012  loss: 1.4005 (1.4672)  time: 0.7597  data: 0.0001  max mem: 15689
[07:51:48.729142] Epoch: [1]  [340/345]  eta: 0:00:03  lr: 0.000012  loss: 1.3953 (1.4632)  time: 0.7597  data: 0.0001  max mem: 15689
[07:51:51.768817] Epoch: [1]  [344/345]  eta: 0:00:00  lr: 0.000012  loss: 1.3953 (1.4624)  time: 0.7597  data: 0.0001  max mem: 15689
[07:51:51.835708] Epoch: [1] Total time: 0:04:22 (0.7610 s / it)
[07:51:51.836196] Averaged stats: lr: 0.000012  loss: 1.3953 (1.4624)
[07:51:52.173393] Test:  [  0/345]  eta: 0:01:55  loss: 1.3930 (1.3930)  time: 0.3339  data: 0.1505  max mem: 15689
[07:51:54.029533] Test:  [ 10/345]  eta: 0:01:06  loss: 1.3928 (1.3932)  time: 0.1990  data: 0.0137  max mem: 15689
[07:51:55.889601] Test:  [ 20/345]  eta: 0:01:02  loss: 1.3928 (1.3934)  time: 0.1857  data: 0.0001  max mem: 15689
[07:51:57.753125] Test:  [ 30/345]  eta: 0:01:00  loss: 1.3928 (1.3932)  time: 0.1861  data: 0.0001  max mem: 15689
[07:51:59.619621] Test:  [ 40/345]  eta: 0:00:57  loss: 1.3927 (1.3930)  time: 0.1865  data: 0.0001  max mem: 15689
[07:52:01.489915] Test:  [ 50/345]  eta: 0:00:55  loss: 1.3925 (1.3928)  time: 0.1868  data: 0.0001  max mem: 15689
[07:52:03.363270] Test:  [ 60/345]  eta: 0:00:53  loss: 1.3926 (1.3929)  time: 0.1871  data: 0.0001  max mem: 15689
[07:52:05.240467] Test:  [ 70/345]  eta: 0:00:51  loss: 1.3935 (1.3931)  time: 0.1875  data: 0.0001  max mem: 15689
[07:52:07.122564] Test:  [ 80/345]  eta: 0:00:49  loss: 1.3934 (1.3931)  time: 0.1879  data: 0.0001  max mem: 15689
[07:52:09.007727] Test:  [ 90/345]  eta: 0:00:48  loss: 1.3921 (1.3930)  time: 0.1883  data: 0.0001  max mem: 15689
[07:52:10.895915] Test:  [100/345]  eta: 0:00:46  loss: 1.3919 (1.3929)  time: 0.1886  data: 0.0001  max mem: 15689
[07:52:12.789384] Test:  [110/345]  eta: 0:00:44  loss: 1.3929 (1.3929)  time: 0.1890  data: 0.0001  max mem: 15689
[07:52:14.686128] Test:  [120/345]  eta: 0:00:42  loss: 1.3929 (1.3929)  time: 0.1895  data: 0.0001  max mem: 15689
[07:52:16.586378] Test:  [130/345]  eta: 0:00:40  loss: 1.3929 (1.3929)  time: 0.1898  data: 0.0001  max mem: 15689
[07:52:18.488181] Test:  [140/345]  eta: 0:00:38  loss: 1.3933 (1.3929)  time: 0.1901  data: 0.0001  max mem: 15689
[07:52:20.394184] Test:  [150/345]  eta: 0:00:36  loss: 1.3927 (1.3929)  time: 0.1903  data: 0.0001  max mem: 15689
[07:52:22.304482] Test:  [160/345]  eta: 0:00:34  loss: 1.3926 (1.3929)  time: 0.1908  data: 0.0001  max mem: 15689
[07:52:24.217019] Test:  [170/345]  eta: 0:00:33  loss: 1.3929 (1.3929)  time: 0.1911  data: 0.0001  max mem: 15689
[07:52:26.134755] Test:  [180/345]  eta: 0:00:31  loss: 1.3933 (1.3929)  time: 0.1915  data: 0.0001  max mem: 15689
[07:52:28.054858] Test:  [190/345]  eta: 0:00:29  loss: 1.3930 (1.3929)  time: 0.1918  data: 0.0001  max mem: 15689
[07:52:29.978138] Test:  [200/345]  eta: 0:00:27  loss: 1.3932 (1.3929)  time: 0.1921  data: 0.0001  max mem: 15689
[07:52:31.906805] Test:  [210/345]  eta: 0:00:25  loss: 1.3935 (1.3929)  time: 0.1925  data: 0.0001  max mem: 15689
[07:52:33.837056] Test:  [220/345]  eta: 0:00:23  loss: 1.3933 (1.3929)  time: 0.1929  data: 0.0001  max mem: 15689
[07:52:35.771245] Test:  [230/345]  eta: 0:00:21  loss: 1.3929 (1.3930)  time: 0.1932  data: 0.0001  max mem: 15689
[07:52:37.709653] Test:  [240/345]  eta: 0:00:19  loss: 1.3929 (1.3929)  time: 0.1936  data: 0.0001  max mem: 15689
[07:52:39.650666] Test:  [250/345]  eta: 0:00:18  loss: 1.3929 (1.3929)  time: 0.1939  data: 0.0001  max mem: 15689
[07:52:41.595956] Test:  [260/345]  eta: 0:00:16  loss: 1.3924 (1.3929)  time: 0.1943  data: 0.0001  max mem: 15689
[07:52:43.545101] Test:  [270/345]  eta: 0:00:14  loss: 1.3924 (1.3929)  time: 0.1947  data: 0.0001  max mem: 15689
[07:52:45.496604] Test:  [280/345]  eta: 0:00:12  loss: 1.3926 (1.3929)  time: 0.1950  data: 0.0001  max mem: 15689
[07:52:47.452180] Test:  [290/345]  eta: 0:00:10  loss: 1.3927 (1.3929)  time: 0.1953  data: 0.0001  max mem: 15689
[07:52:49.412541] Test:  [300/345]  eta: 0:00:08  loss: 1.3926 (1.3929)  time: 0.1957  data: 0.0001  max mem: 15689
[07:52:51.372872] Test:  [310/345]  eta: 0:00:06  loss: 1.3926 (1.3929)  time: 0.1960  data: 0.0001  max mem: 15689
[07:52:53.339161] Test:  [320/345]  eta: 0:00:04  loss: 1.3924 (1.3929)  time: 0.1963  data: 0.0001  max mem: 15689
[07:52:55.309779] Test:  [330/345]  eta: 0:00:02  loss: 1.3924 (1.3929)  time: 0.1968  data: 0.0001  max mem: 15689
[07:52:57.283717] Test:  [340/345]  eta: 0:00:00  loss: 1.3931 (1.3929)  time: 0.1972  data: 0.0001  max mem: 15689
[07:52:58.073770] Test:  [344/345]  eta: 0:00:00  loss: 1.3933 (1.3929)  time: 0.1973  data: 0.0001  max mem: 15689
[07:52:58.134457] Test: Total time: 0:01:06 (0.1922 s / it)
[07:53:08.872399] Test:  [ 0/57]  eta: 0:00:18  loss: 1.4002 (1.4002)  time: 0.3235  data: 0.1421  max mem: 15689
[07:53:10.710088] Test:  [10/57]  eta: 0:00:09  loss: 1.3979 (1.3969)  time: 0.1964  data: 0.0130  max mem: 15689
[07:53:12.551766] Test:  [20/57]  eta: 0:00:07  loss: 1.3979 (1.3960)  time: 0.1839  data: 0.0001  max mem: 15689
[07:53:14.397727] Test:  [30/57]  eta: 0:00:05  loss: 1.3925 (1.3927)  time: 0.1843  data: 0.0001  max mem: 15689
[07:53:16.246722] Test:  [40/57]  eta: 0:00:03  loss: 1.3870 (1.3906)  time: 0.1847  data: 0.0001  max mem: 15689
[07:53:18.101318] Test:  [50/57]  eta: 0:00:01  loss: 1.3870 (1.3899)  time: 0.1851  data: 0.0001  max mem: 15689
[07:53:19.100816] Test:  [56/57]  eta: 0:00:00  loss: 1.3898 (1.3898)  time: 0.1796  data: 0.0001  max mem: 15689
[07:53:19.160670] Test: Total time: 0:00:10 (0.1862 s / it)
[07:53:21.165410] Dice score of the network on the train images: 0.000000, val images: 0.000000
[07:53:21.170222] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[07:53:22.072078] Epoch: [2]  [  0/345]  eta: 0:05:10  lr: 0.000013  loss: 1.3903 (1.3903)  time: 0.9010  data: 0.1452  max mem: 15689
[07:53:37.187106] Epoch: [2]  [ 20/345]  eta: 0:04:07  lr: 0.000013  loss: 1.3872 (1.3882)  time: 0.7557  data: 0.0001  max mem: 15689

[07:53:52.364368] Epoch: [2]  [ 40/345]  eta: 0:03:52  lr: 0.000013  loss: 1.3834 (1.3868)  time: 0.7588  data: 0.0001  max mem: 15689
[07:54:07.564908] Epoch: [2]  [ 60/345]  eta: 0:03:36  lr: 0.000014  loss: 1.3799 (1.3847)  time: 0.7600  data: 0.0001  max mem: 15689
[07:54:22.776937] Epoch: [2]  [ 80/345]  eta: 0:03:21  lr: 0.000014  loss: 1.3744 (1.3833)  time: 0.7606  data: 0.0001  max mem: 15689
[07:54:38.009267] Epoch: [2]  [100/345]  eta: 0:03:06  lr: 0.000014  loss: 1.3708 (1.3813)  time: 0.7616  data: 0.0001  max mem: 15689
[07:54:53.245722] Epoch: [2]  [120/345]  eta: 0:02:51  lr: 0.000015  loss: 1.3642 (1.3789)  time: 0.7618  data: 0.0001  max mem: 15689
[07:55:08.469298] Epoch: [2]  [140/345]  eta: 0:02:35  lr: 0.000015  loss: 1.3576 (1.3760)  time: 0.7611  data: 0.0001  max mem: 15689
[07:55:23.693311] Epoch: [2]  [160/345]  eta: 0:02:20  lr: 0.000015  loss: 1.3560 (1.3740)  time: 0.7612  data: 0.0001  max mem: 15689
[07:55:38.913127] Epoch: [2]  [180/345]  eta: 0:02:05  lr: 0.000016  loss: 1.3545 (1.3721)  time: 0.7609  data: 0.0001  max mem: 15689
[07:55:54.135671] Epoch: [2]  [200/345]  eta: 0:01:50  lr: 0.000016  loss: 1.3490 (1.3699)  time: 0.7611  data: 0.0001  max mem: 15689
[07:56:09.353277] Epoch: [2]  [220/345]  eta: 0:01:35  lr: 0.000016  loss: 1.3484 (1.3679)  time: 0.7608  data: 0.0001  max mem: 15689
[07:56:24.562724] Epoch: [2]  [240/345]  eta: 0:01:19  lr: 0.000017  loss: 1.3415 (1.3660)  time: 0.7604  data: 0.0001  max mem: 15689
[07:56:39.775523] Epoch: [2]  [260/345]  eta: 0:01:04  lr: 0.000017  loss: 1.3398 (1.3645)  time: 0.7606  data: 0.0001  max mem: 15689
[07:56:54.969449] Epoch: [2]  [280/345]  eta: 0:00:49  lr: 0.000018  loss: 1.3390 (1.3629)  time: 0.7597  data: 0.0001  max mem: 15689
[07:57:10.164517] Epoch: [2]  [300/345]  eta: 0:00:34  lr: 0.000018  loss: 1.3289 (1.3609)  time: 0.7597  data: 0.0001  max mem: 15689
[07:57:25.359291] Epoch: [2]  [320/345]  eta: 0:00:19  lr: 0.000018  loss: 1.3324 (1.3595)  time: 0.7597  data: 0.0001  max mem: 15689
[07:57:40.552047] Epoch: [2]  [340/345]  eta: 0:00:03  lr: 0.000019  loss: 1.3339 (1.3581)  time: 0.7596  data: 0.0001  max mem: 15689
[07:57:43.591502] Epoch: [2]  [344/345]  eta: 0:00:00  lr: 0.000019  loss: 1.3339 (1.3579)  time: 0.7597  data: 0.0001  max mem: 15689
[07:57:43.656441] Epoch: [2] Total time: 0:04:22 (0.7608 s / it)
[07:57:43.656961] Averaged stats: lr: 0.000019  loss: 1.3339 (1.3579)
[07:57:43.999786] Test:  [  0/345]  eta: 0:01:57  loss: 1.3400 (1.3400)  time: 0.3392  data: 0.1553  max mem: 15689
[07:57:45.856073] Test:  [ 10/345]  eta: 0:01:06  loss: 1.3419 (1.3417)  time: 0.1995  data: 0.0142  max mem: 15689
[07:57:47.715107] Test:  [ 20/345]  eta: 0:01:02  loss: 1.3419 (1.3414)  time: 0.1857  data: 0.0001  max mem: 15689
[07:57:49.577696] Test:  [ 30/345]  eta: 0:01:00  loss: 1.3428 (1.3417)  time: 0.1860  data: 0.0001  max mem: 15689
[07:57:51.444282] Test:  [ 40/345]  eta: 0:00:57  loss: 1.3432 (1.3422)  time: 0.1864  data: 0.0001  max mem: 15689
[07:57:53.315816] Test:  [ 50/345]  eta: 0:00:55  loss: 1.3432 (1.3424)  time: 0.1869  data: 0.0001  max mem: 15689
[07:57:55.190063] Test:  [ 60/345]  eta: 0:00:53  loss: 1.3425 (1.3426)  time: 0.1872  data: 0.0001  max mem: 15689
[07:57:57.067471] Test:  [ 70/345]  eta: 0:00:51  loss: 1.3422 (1.3425)  time: 0.1875  data: 0.0001  max mem: 15689
[07:57:58.950803] Test:  [ 80/345]  eta: 0:00:50  loss: 1.3431 (1.3426)  time: 0.1880  data: 0.0001  max mem: 15689
[07:58:00.834009] Test:  [ 90/345]  eta: 0:00:48  loss: 1.3438 (1.3427)  time: 0.1883  data: 0.0001  max mem: 15689
[07:58:02.722306] Test:  [100/345]  eta: 0:00:46  loss: 1.3435 (1.3427)  time: 0.1885  data: 0.0001  max mem: 15689
[07:58:04.615235] Test:  [110/345]  eta: 0:00:44  loss: 1.3428 (1.3427)  time: 0.1890  data: 0.0001  max mem: 15689
[07:58:06.511985] Test:  [120/345]  eta: 0:00:42  loss: 1.3422 (1.3426)  time: 0.1894  data: 0.0001  max mem: 15689
[07:58:08.414368] Test:  [130/345]  eta: 0:00:40  loss: 1.3426 (1.3428)  time: 0.1899  data: 0.0001  max mem: 15689
[07:58:10.316863] Test:  [140/345]  eta: 0:00:38  loss: 1.3430 (1.3427)  time: 0.1902  data: 0.0001  max mem: 15689
[07:58:12.223197] Test:  [150/345]  eta: 0:00:36  loss: 1.3427 (1.3427)  time: 0.1904  data: 0.0001  max mem: 15689
[07:58:14.132979] Test:  [160/345]  eta: 0:00:35  loss: 1.3428 (1.3428)  time: 0.1908  data: 0.0001  max mem: 15689
[07:58:16.048190] Test:  [170/345]  eta: 0:00:33  loss: 1.3425 (1.3427)  time: 0.1912  data: 0.0001  max mem: 15689
[07:58:17.966278] Test:  [180/345]  eta: 0:00:31  loss: 1.3411 (1.3427)  time: 0.1916  data: 0.0001  max mem: 15689
[07:58:19.887464] Test:  [190/345]  eta: 0:00:29  loss: 1.3422 (1.3426)  time: 0.1919  data: 0.0001  max mem: 15689
[07:58:21.811704] Test:  [200/345]  eta: 0:00:27  loss: 1.3427 (1.3427)  time: 0.1922  data: 0.0001  max mem: 15689
[07:58:23.738719] Test:  [210/345]  eta: 0:00:25  loss: 1.3426 (1.3426)  time: 0.1925  data: 0.0001  max mem: 15689
[07:58:25.671323] Test:  [220/345]  eta: 0:00:23  loss: 1.3418 (1.3426)  time: 0.1929  data: 0.0001  max mem: 15689
[07:58:27.606989] Test:  [230/345]  eta: 0:00:21  loss: 1.3439 (1.3427)  time: 0.1934  data: 0.0001  max mem: 15689
[07:58:29.547261] Test:  [240/345]  eta: 0:00:19  loss: 1.3440 (1.3427)  time: 0.1937  data: 0.0001  max mem: 15689
[07:58:31.489985] Test:  [250/345]  eta: 0:00:18  loss: 1.3431 (1.3427)  time: 0.1941  data: 0.0001  max mem: 15689
[07:58:33.435673] Test:  [260/345]  eta: 0:00:16  loss: 1.3429 (1.3427)  time: 0.1944  data: 0.0001  max mem: 15689
[07:58:35.383977] Test:  [270/345]  eta: 0:00:14  loss: 1.3436 (1.3428)  time: 0.1946  data: 0.0001  max mem: 15689
[07:58:37.334117] Test:  [280/345]  eta: 0:00:12  loss: 1.3447 (1.3429)  time: 0.1949  data: 0.0001  max mem: 15689
[07:58:39.289544] Test:  [290/345]  eta: 0:00:10  loss: 1.3435 (1.3428)  time: 0.1952  data: 0.0001  max mem: 15689
[07:58:41.247263] Test:  [300/345]  eta: 0:00:08  loss: 1.3426 (1.3428)  time: 0.1956  data: 0.0001  max mem: 15689
[07:58:43.208697] Test:  [310/345]  eta: 0:00:06  loss: 1.3420 (1.3428)  time: 0.1959  data: 0.0001  max mem: 15689
[07:58:45.172553] Test:  [320/345]  eta: 0:00:04  loss: 1.3427 (1.3428)  time: 0.1962  data: 0.0001  max mem: 15689
[07:58:47.139905] Test:  [330/345]  eta: 0:00:02  loss: 1.3439 (1.3428)  time: 0.1965  data: 0.0001  max mem: 15689
[07:58:49.112154] Test:  [340/345]  eta: 0:00:00  loss: 1.3431 (1.3428)  time: 0.1969  data: 0.0001  max mem: 15689
[07:58:49.903296] Test:  [344/345]  eta: 0:00:00  loss: 1.3434 (1.3428)  time: 0.1972  data: 0.0001  max mem: 15689
[07:58:49.963876] Test: Total time: 0:01:06 (0.1922 s / it)
[07:59:00.642059] Test:  [ 0/57]  eta: 0:00:18  loss: 1.3513 (1.3513)  time: 0.3237  data: 0.1426  max mem: 15689
[07:59:02.477203] Test:  [10/57]  eta: 0:00:09  loss: 1.3483 (1.3474)  time: 0.1962  data: 0.0130  max mem: 15689
[07:59:04.316800] Test:  [20/57]  eta: 0:00:07  loss: 1.3483 (1.3466)  time: 0.1837  data: 0.0001  max mem: 15689
[07:59:06.161332] Test:  [30/57]  eta: 0:00:05  loss: 1.3410 (1.3424)  time: 0.1842  data: 0.0001  max mem: 15689
[07:59:08.009551] Test:  [40/57]  eta: 0:00:03  loss: 1.3345 (1.3397)  time: 0.1846  data: 0.0001  max mem: 15689
[07:59:09.863879] Test:  [50/57]  eta: 0:00:01  loss: 1.3345 (1.3388)  time: 0.1851  data: 0.0001  max mem: 15689
[07:59:10.864336] Test:  [56/57]  eta: 0:00:00  loss: 1.3384 (1.3386)  time: 0.1797  data: 0.0001  max mem: 15689
[07:59:10.923226] Test: Total time: 0:00:10 (0.1861 s / it)
[07:59:12.840675] Dice score of the network on the train images: 0.000000, val images: 0.000000
[07:59:12.844926] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[07:59:13.743455] Epoch: [3]  [  0/345]  eta: 0:05:09  lr: 0.000019  loss: 1.3233 (1.3233)  time: 0.8978  data: 0.1444  max mem: 15689
[07:59:28.859978] Epoch: [3]  [ 20/345]  eta: 0:04:07  lr: 0.000019  loss: 1.3268 (1.3270)  time: 0.7558  data: 0.0001  max mem: 15689
[07:59:44.026683] Epoch: [3]  [ 40/345]  eta: 0:03:51  lr: 0.000019  loss: 1.3227 (1.3284)  time: 0.7583  data: 0.0001  max mem: 15689
[07:59:59.217333] Epoch: [3]  [ 60/345]  eta: 0:03:36  lr: 0.000020  loss: 1.3264 (1.3277)  time: 0.7595  data: 0.0001  max mem: 15689
[08:00:14.421922] Epoch: [3]  [ 80/345]  eta: 0:03:21  lr: 0.000020  loss: 1.3173 (1.3259)  time: 0.7602  data: 0.0001  max mem: 15689
[08:00:29.739698] Epoch: [3]  [100/345]  eta: 0:03:06  lr: 0.000021  loss: 1.3134 (1.3249)  time: 0.7659  data: 0.0001  max mem: 15689
[08:00:44.962556] Epoch: [3]  [120/345]  eta: 0:02:51  lr: 0.000021  loss: 1.3152 (1.3246)  time: 0.7611  data: 0.0001  max mem: 15689
[08:01:00.178933] Epoch: [3]  [140/345]  eta: 0:02:36  lr: 0.000021  loss: 1.3107 (1.3230)  time: 0.7608  data: 0.0001  max mem: 15689
[08:01:15.398403] Epoch: [3]  [160/345]  eta: 0:02:20  lr: 0.000022  loss: 1.3077 (1.3214)  time: 0.7609  data: 0.0001  max mem: 15689
[08:01:30.613026] Epoch: [3]  [180/345]  eta: 0:02:05  lr: 0.000022  loss: 1.3088 (1.3203)  time: 0.7607  data: 0.0001  max mem: 15689
[08:01:45.821008] Epoch: [3]  [200/345]  eta: 0:01:50  lr: 0.000022  loss: 1.3048 (1.3189)  time: 0.7604  data: 0.0001  max mem: 15689
[08:02:01.030960] Epoch: [3]  [220/345]  eta: 0:01:35  lr: 0.000023  loss: 1.3027 (1.3180)  time: 0.7605  data: 0.0001  max mem: 15689
[08:02:16.229598] Epoch: [3]  [240/345]  eta: 0:01:19  lr: 0.000023  loss: 1.2995 (1.3170)  time: 0.7599  data: 0.0001  max mem: 15689
[08:02:31.421386] Epoch: [3]  [260/345]  eta: 0:01:04  lr: 0.000023  loss: 1.2995 (1.3157)  time: 0.7596  data: 0.0001  max mem: 15689
[08:02:46.617276] Epoch: [3]  [280/345]  eta: 0:00:49  lr: 0.000024  loss: 1.2979 (1.3145)  time: 0.7598  data: 0.0001  max mem: 15689
[08:03:01.835998] Epoch: [3]  [300/345]  eta: 0:00:34  lr: 0.000024  loss: 1.2990 (1.3137)  time: 0.7609  data: 0.0001  max mem: 15689
[08:03:17.068048] Epoch: [3]  [320/345]  eta: 0:00:19  lr: 0.000025  loss: 1.2966 (1.3128)  time: 0.7616  data: 0.0001  max mem: 15689
[08:03:32.262532] Epoch: [3]  [340/345]  eta: 0:00:03  lr: 0.000025  loss: 1.2925 (1.3118)  time: 0.7597  data: 0.0001  max mem: 15689
[08:03:35.301090] Epoch: [3]  [344/345]  eta: 0:00:00  lr: 0.000025  loss: 1.2922 (1.3117)  time: 0.7597  data: 0.0001  max mem: 15689
[08:03:35.367521] Epoch: [3] Total time: 0:04:22 (0.7609 s / it)
[08:03:35.368030] Averaged stats: lr: 0.000025  loss: 1.2922 (1.3117)
[08:03:35.704891] Test:  [  0/345]  eta: 0:01:55  loss: 1.3005 (1.3005)  time: 0.3335  data: 0.1501  max mem: 15689
[08:03:37.560479] Test:  [ 10/345]  eta: 0:01:06  loss: 1.2931 (1.2937)  time: 0.1989  data: 0.0137  max mem: 15689
[08:03:39.418096] Test:  [ 20/345]  eta: 0:01:02  loss: 1.2920 (1.2927)  time: 0.1856  data: 0.0001  max mem: 15689
[08:03:41.282368] Test:  [ 30/345]  eta: 0:01:00  loss: 1.2913 (1.2927)  time: 0.1860  data: 0.0001  max mem: 15689
[08:03:43.149473] Test:  [ 40/345]  eta: 0:00:57  loss: 1.2926 (1.2926)  time: 0.1865  data: 0.0001  max mem: 15689
[08:03:45.018575] Test:  [ 50/345]  eta: 0:00:55  loss: 1.2928 (1.2926)  time: 0.1868  data: 0.0001  max mem: 15689
[08:03:46.893350] Test:  [ 60/345]  eta: 0:00:53  loss: 1.2915 (1.2924)  time: 0.1871  data: 0.0001  max mem: 15689
[08:03:48.771647] Test:  [ 70/345]  eta: 0:00:51  loss: 1.2927 (1.2925)  time: 0.1876  data: 0.0001  max mem: 15689
[08:03:50.654778] Test:  [ 80/345]  eta: 0:00:49  loss: 1.2927 (1.2923)  time: 0.1880  data: 0.0001  max mem: 15689
[08:03:52.541391] Test:  [ 90/345]  eta: 0:00:48  loss: 1.2901 (1.2921)  time: 0.1884  data: 0.0001  max mem: 15689
[08:03:54.431583] Test:  [100/345]  eta: 0:00:46  loss: 1.2898 (1.2920)  time: 0.1888  data: 0.0001  max mem: 15689
[08:03:56.325496] Test:  [110/345]  eta: 0:00:44  loss: 1.2900 (1.2920)  time: 0.1892  data: 0.0001  max mem: 15689
[08:03:58.222678] Test:  [120/345]  eta: 0:00:42  loss: 1.2923 (1.2920)  time: 0.1895  data: 0.0001  max mem: 15689
[08:04:00.122297] Test:  [130/345]  eta: 0:00:40  loss: 1.2908 (1.2918)  time: 0.1898  data: 0.0001  max mem: 15689
[08:04:02.024012] Test:  [140/345]  eta: 0:00:38  loss: 1.2908 (1.2918)  time: 0.1900  data: 0.0001  max mem: 15689
[08:04:03.930426] Test:  [150/345]  eta: 0:00:36  loss: 1.2926 (1.2918)  time: 0.1904  data: 0.0001  max mem: 15689
[08:04:05.840172] Test:  [160/345]  eta: 0:00:35  loss: 1.2921 (1.2919)  time: 0.1908  data: 0.0001  max mem: 15689
[08:04:07.753215] Test:  [170/345]  eta: 0:00:33  loss: 1.2912 (1.2918)  time: 0.1911  data: 0.0001  max mem: 15689
[08:04:09.669903] Test:  [180/345]  eta: 0:00:31  loss: 1.2906 (1.2917)  time: 0.1914  data: 0.0001  max mem: 15689
[08:04:11.591685] Test:  [190/345]  eta: 0:00:29  loss: 1.2915 (1.2917)  time: 0.1919  data: 0.0001  max mem: 15689
[08:04:13.518020] Test:  [200/345]  eta: 0:00:27  loss: 1.2928 (1.2918)  time: 0.1924  data: 0.0001  max mem: 15689
[08:04:15.444684] Test:  [210/345]  eta: 0:00:25  loss: 1.2928 (1.2918)  time: 0.1926  data: 0.0001  max mem: 15689
[08:04:17.375110] Test:  [220/345]  eta: 0:00:23  loss: 1.2904 (1.2917)  time: 0.1928  data: 0.0001  max mem: 15689
[08:04:19.308619] Test:  [230/345]  eta: 0:00:21  loss: 1.2903 (1.2916)  time: 0.1931  data: 0.0001  max mem: 15689
[08:04:21.245517] Test:  [240/345]  eta: 0:00:19  loss: 1.2877 (1.2916)  time: 0.1935  data: 0.0001  max mem: 15689
[08:04:23.187289] Test:  [250/345]  eta: 0:00:18  loss: 1.2923 (1.2917)  time: 0.1939  data: 0.0001  max mem: 15689
[08:04:25.132902] Test:  [260/345]  eta: 0:00:16  loss: 1.2925 (1.2917)  time: 0.1943  data: 0.0001  max mem: 15689
[08:04:27.081415] Test:  [270/345]  eta: 0:00:14  loss: 1.2918 (1.2917)  time: 0.1947  data: 0.0001  max mem: 15689
[08:04:29.034045] Test:  [280/345]  eta: 0:00:12  loss: 1.2908 (1.2917)  time: 0.1950  data: 0.0001  max mem: 15689
[08:04:30.989971] Test:  [290/345]  eta: 0:00:10  loss: 1.2924 (1.2916)  time: 0.1954  data: 0.0001  max mem: 15689
[08:04:32.950377] Test:  [300/345]  eta: 0:00:08  loss: 1.2937 (1.2917)  time: 0.1958  data: 0.0001  max mem: 15689
[08:04:34.911763] Test:  [310/345]  eta: 0:00:06  loss: 1.2937 (1.2917)  time: 0.1960  data: 0.0001  max mem: 15689
[08:04:36.878355] Test:  [320/345]  eta: 0:00:04  loss: 1.2904 (1.2917)  time: 0.1963  data: 0.0001  max mem: 15689
[08:04:38.847283] Test:  [330/345]  eta: 0:00:02  loss: 1.2901 (1.2917)  time: 0.1967  data: 0.0001  max mem: 15689
[08:04:40.819808] Test:  [340/345]  eta: 0:00:00  loss: 1.2914 (1.2917)  time: 0.1970  data: 0.0001  max mem: 15689
[08:04:41.610285] Test:  [344/345]  eta: 0:00:00  loss: 1.2902 (1.2917)  time: 0.1972  data: 0.0001  max mem: 15689
[08:04:41.668589] Test: Total time: 0:01:06 (0.1922 s / it)
[08:04:52.352519] Test:  [ 0/57]  eta: 0:00:18  loss: 1.3054 (1.3054)  time: 0.3263  data: 0.1447  max mem: 15689
[08:04:54.190094] Test:  [10/57]  eta: 0:00:09  loss: 1.2999 (1.2989)  time: 0.1966  data: 0.0132  max mem: 15689
[08:04:56.032549] Test:  [20/57]  eta: 0:00:07  loss: 1.2999 (1.2983)  time: 0.1839  data: 0.0001  max mem: 15689
[08:04:57.877660] Test:  [30/57]  eta: 0:00:05  loss: 1.2842 (1.2905)  time: 0.1843  data: 0.0001  max mem: 15689
[08:04:59.728629] Test:  [40/57]  eta: 0:00:03  loss: 1.2728 (1.2854)  time: 0.1848  data: 0.0001  max mem: 15689
[08:05:01.581705] Test:  [50/57]  eta: 0:00:01  loss: 1.2728 (1.2835)  time: 0.1852  data: 0.0001  max mem: 15689
[08:05:02.582332] Test:  [56/57]  eta: 0:00:00  loss: 1.2804 (1.2833)  time: 0.1797  data: 0.0001  max mem: 15689
[08:05:02.640883] Test: Total time: 0:00:10 (0.1862 s / it)
[08:05:04.627217] Dice score of the network on the train images: 0.000000, val images: 0.000000
[08:05:04.631453] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:05:05.535390] Epoch: [4]  [  0/345]  eta: 0:05:11  lr: 0.000025  loss: 1.2985 (1.2985)  time: 0.9031  data: 0.1481  max mem: 15689
[08:05:20.644750] Epoch: [4]  [ 20/345]  eta: 0:04:07  lr: 0.000025  loss: 1.2862 (1.2910)  time: 0.7554  data: 0.0001  max mem: 15689
[08:05:35.831764] Epoch: [4]  [ 40/345]  eta: 0:03:52  lr: 0.000026  loss: 1.2880 (1.2895)  time: 0.7593  data: 0.0001  max mem: 15689
[08:05:51.038588] Epoch: [4]  [ 60/345]  eta: 0:03:36  lr: 0.000026  loss: 1.2772 (1.2875)  time: 0.7603  data: 0.0001  max mem: 15689
[08:06:06.257307] Epoch: [4]  [ 80/345]  eta: 0:03:21  lr: 0.000026  loss: 1.2739 (1.2849)  time: 0.7609  data: 0.0001  max mem: 15689
[08:06:21.483103] Epoch: [4]  [100/345]  eta: 0:03:06  lr: 0.000027  loss: 1.2696 (1.2826)  time: 0.7613  data: 0.0001  max mem: 15689
[08:06:36.717601] Epoch: [4]  [120/345]  eta: 0:02:51  lr: 0.000027  loss: 1.2637 (1.2795)  time: 0.7617  data: 0.0001  max mem: 15689
[08:06:51.948536] Epoch: [4]  [140/345]  eta: 0:02:36  lr: 0.000028  loss: 1.2583 (1.2768)  time: 0.7615  data: 0.0001  max mem: 15689
[08:07:07.166428] Epoch: [4]  [160/345]  eta: 0:02:20  lr: 0.000028  loss: 1.2646 (1.2751)  time: 0.7609  data: 0.0001  max mem: 15689
[08:07:22.390569] Epoch: [4]  [180/345]  eta: 0:02:05  lr: 0.000028  loss: 1.2414 (1.2719)  time: 0.7612  data: 0.0001  max mem: 15689
[08:07:37.726276] Epoch: [4]  [200/345]  eta: 0:01:50  lr: 0.000029  loss: 1.2341 (1.2682)  time: 0.7667  data: 0.0001  max mem: 15689
[08:07:52.927275] Epoch: [4]  [220/345]  eta: 0:01:35  lr: 0.000029  loss: 1.2287 (1.2650)  time: 0.7600  data: 0.0001  max mem: 15689
[08:08:08.134635] Epoch: [4]  [240/345]  eta: 0:01:19  lr: 0.000029  loss: 1.2205 (1.2614)  time: 0.7603  data: 0.0001  max mem: 15689
[08:08:23.329467] Epoch: [4]  [260/345]  eta: 0:01:04  lr: 0.000030  loss: 1.2116 (1.2577)  time: 0.7597  data: 0.0001  max mem: 15689
[08:08:38.522816] Epoch: [4]  [280/345]  eta: 0:00:49  lr: 0.000030  loss: 1.2045 (1.2540)  time: 0.7596  data: 0.0001  max mem: 15689
[08:08:53.720449] Epoch: [4]  [300/345]  eta: 0:00:34  lr: 0.000030  loss: 1.1858 (1.2498)  time: 0.7598  data: 0.0001  max mem: 15689
[08:09:08.914002] Epoch: [4]  [320/345]  eta: 0:00:19  lr: 0.000031  loss: 1.1773 (1.2456)  time: 0.7596  data: 0.0001  max mem: 15689
[08:09:24.110265] Epoch: [4]  [340/345]  eta: 0:00:03  lr: 0.000031  loss: 1.1822 (1.2419)  time: 0.7598  data: 0.0001  max mem: 15689
[08:09:27.149434] Epoch: [4]  [344/345]  eta: 0:00:00  lr: 0.000031  loss: 1.1823 (1.2414)  time: 0.7598  data: 0.0001  max mem: 15689
[08:09:27.216405] Epoch: [4] Total time: 0:04:22 (0.7611 s / it)
[08:09:27.216920] Averaged stats: lr: 0.000031  loss: 1.1823 (1.2414)
[08:09:27.554072] Test:  [  0/345]  eta: 0:01:55  loss: 1.1812 (1.1812)  time: 0.3335  data: 0.1495  max mem: 15689
[08:09:29.410464] Test:  [ 10/345]  eta: 0:01:06  loss: 1.1656 (1.1612)  time: 0.1990  data: 0.0137  max mem: 15689
[08:09:31.270771] Test:  [ 20/345]  eta: 0:01:02  loss: 1.1555 (1.1597)  time: 0.1858  data: 0.0001  max mem: 15689
[08:09:33.132877] Test:  [ 30/345]  eta: 0:01:00  loss: 1.1528 (1.1537)  time: 0.1861  data: 0.0001  max mem: 15689
[08:09:34.999699] Test:  [ 40/345]  eta: 0:00:57  loss: 1.1528 (1.1559)  time: 0.1864  data: 0.0001  max mem: 15689
[08:09:36.869880] Test:  [ 50/345]  eta: 0:00:55  loss: 1.1542 (1.1540)  time: 0.1868  data: 0.0001  max mem: 15689
[08:09:38.742398] Test:  [ 60/345]  eta: 0:00:53  loss: 1.1507 (1.1542)  time: 0.1871  data: 0.0001  max mem: 15689
[08:09:40.618409] Test:  [ 70/345]  eta: 0:00:51  loss: 1.1503 (1.1536)  time: 0.1874  data: 0.0001  max mem: 15689
[08:09:42.501608] Test:  [ 80/345]  eta: 0:00:49  loss: 1.1457 (1.1528)  time: 0.1879  data: 0.0001  max mem: 15689
[08:09:44.385810] Test:  [ 90/345]  eta: 0:00:48  loss: 1.1501 (1.1530)  time: 0.1883  data: 0.0001  max mem: 15689
[08:09:46.273043] Test:  [100/345]  eta: 0:00:46  loss: 1.1521 (1.1524)  time: 0.1885  data: 0.0001  max mem: 15689
[08:09:48.164707] Test:  [110/345]  eta: 0:00:44  loss: 1.1521 (1.1529)  time: 0.1889  data: 0.0001  max mem: 15689
[08:09:50.060810] Test:  [120/345]  eta: 0:00:42  loss: 1.1480 (1.1523)  time: 0.1893  data: 0.0001  max mem: 15689
[08:09:51.962416] Test:  [130/345]  eta: 0:00:40  loss: 1.1436 (1.1511)  time: 0.1898  data: 0.0001  max mem: 15689
[08:09:53.865420] Test:  [140/345]  eta: 0:00:38  loss: 1.1494 (1.1515)  time: 0.1902  data: 0.0001  max mem: 15689
[08:09:55.771688] Test:  [150/345]  eta: 0:00:36  loss: 1.1554 (1.1518)  time: 0.1904  data: 0.0001  max mem: 15689
[08:09:57.681004] Test:  [160/345]  eta: 0:00:34  loss: 1.1497 (1.1505)  time: 0.1907  data: 0.0001  max mem: 15689
[08:09:59.592919] Test:  [170/345]  eta: 0:00:33  loss: 1.1459 (1.1510)  time: 0.1910  data: 0.0001  max mem: 15689
[08:10:01.511328] Test:  [180/345]  eta: 0:00:31  loss: 1.1463 (1.1507)  time: 0.1915  data: 0.0001  max mem: 15689
[08:10:03.431961] Test:  [190/345]  eta: 0:00:29  loss: 1.1450 (1.1504)  time: 0.1919  data: 0.0001  max mem: 15689
[08:10:05.356176] Test:  [200/345]  eta: 0:00:27  loss: 1.1540 (1.1507)  time: 0.1922  data: 0.0001  max mem: 15689
[08:10:07.283024] Test:  [210/345]  eta: 0:00:25  loss: 1.1623 (1.1514)  time: 0.1925  data: 0.0001  max mem: 15689
[08:10:09.213358] Test:  [220/345]  eta: 0:00:23  loss: 1.1646 (1.1519)  time: 0.1928  data: 0.0001  max mem: 15689
[08:10:11.149333] Test:  [230/345]  eta: 0:00:21  loss: 1.1529 (1.1520)  time: 0.1933  data: 0.0001  max mem: 15689
[08:10:13.086673] Test:  [240/345]  eta: 0:00:19  loss: 1.1599 (1.1524)  time: 0.1936  data: 0.0001  max mem: 15689
[08:10:15.028727] Test:  [250/345]  eta: 0:00:18  loss: 1.1624 (1.1525)  time: 0.1939  data: 0.0001  max mem: 15689
[08:10:16.974448] Test:  [260/345]  eta: 0:00:16  loss: 1.1586 (1.1526)  time: 0.1943  data: 0.0001  max mem: 15689
[08:10:18.922223] Test:  [270/345]  eta: 0:00:14  loss: 1.1551 (1.1527)  time: 0.1946  data: 0.0001  max mem: 15689
[08:10:20.874674] Test:  [280/345]  eta: 0:00:12  loss: 1.1567 (1.1533)  time: 0.1950  data: 0.0001  max mem: 15689
[08:10:22.829532] Test:  [290/345]  eta: 0:00:10  loss: 1.1696 (1.1539)  time: 0.1953  data: 0.0001  max mem: 15689
[08:10:24.786940] Test:  [300/345]  eta: 0:00:08  loss: 1.1539 (1.1539)  time: 0.1956  data: 0.0001  max mem: 15689
[08:10:26.749141] Test:  [310/345]  eta: 0:00:06  loss: 1.1517 (1.1541)  time: 0.1959  data: 0.0001  max mem: 15689
[08:10:28.714103] Test:  [320/345]  eta: 0:00:04  loss: 1.1517 (1.1538)  time: 0.1963  data: 0.0001  max mem: 15689
[08:10:30.684066] Test:  [330/345]  eta: 0:00:02  loss: 1.1553 (1.1540)  time: 0.1967  data: 0.0001  max mem: 15689
[08:10:32.657538] Test:  [340/345]  eta: 0:00:00  loss: 1.1595 (1.1539)  time: 0.1971  data: 0.0001  max mem: 15689
[08:10:33.447128] Test:  [344/345]  eta: 0:00:00  loss: 1.1553 (1.1537)  time: 0.1972  data: 0.0001  max mem: 15689
[08:10:33.507976] Test: Total time: 0:01:06 (0.1921 s / it)
[08:10:44.315668] Test:  [ 0/57]  eta: 0:00:18  loss: 1.2304 (1.2304)  time: 0.3259  data: 0.1451  max mem: 15689
[08:10:46.151903] Test:  [10/57]  eta: 0:00:09  loss: 1.2051 (1.1942)  time: 0.1965  data: 0.0132  max mem: 15689
[08:10:47.992905] Test:  [20/57]  eta: 0:00:07  loss: 1.2155 (1.1957)  time: 0.1838  data: 0.0001  max mem: 15689
[08:10:49.838764] Test:  [30/57]  eta: 0:00:05  loss: 1.1276 (1.1485)  time: 0.1843  data: 0.0001  max mem: 15689
[08:10:51.687835] Test:  [40/57]  eta: 0:00:03  loss: 1.0376 (1.1187)  time: 0.1847  data: 0.0001  max mem: 15689
[08:10:53.542320] Test:  [50/57]  eta: 0:00:01  loss: 1.0407 (1.1103)  time: 0.1851  data: 0.0001  max mem: 15689
[08:10:54.541962] Test:  [56/57]  eta: 0:00:00  loss: 1.0977 (1.1143)  time: 0.1796  data: 0.0001  max mem: 15689
[08:10:54.599996] Test: Total time: 0:00:10 (0.1862 s / it)
[08:10:56.535588] Dice score of the network on the train images: 0.522772, val images: 0.622236
[08:10:56.535823] saving best_prec_model_0 @ epoch 4
[08:10:57.631022] saving best_rec_model_0 @ epoch 4
[08:10:58.772156] saving best_dice_model_0 @ epoch 4
[08:10:59.823274] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:11:00.724471] Epoch: [5]  [  0/345]  eta: 0:05:10  lr: 0.000031  loss: 1.2070 (1.2070)  time: 0.9002  data: 0.1461  max mem: 15689
[08:11:15.805938] Epoch: [5]  [ 20/345]  eta: 0:04:07  lr: 0.000032  loss: 1.1559 (1.1634)  time: 0.7540  data: 0.0001  max mem: 15689
[08:11:30.947987] Epoch: [5]  [ 40/345]  eta: 0:03:51  lr: 0.000032  loss: 1.1517 (1.1597)  time: 0.7571  data: 0.0001  max mem: 15689
[08:11:46.140755] Epoch: [5]  [ 60/345]  eta: 0:03:36  lr: 0.000032  loss: 1.1607 (1.1605)  time: 0.7596  data: 0.0001  max mem: 15689
[08:12:01.351396] Epoch: [5]  [ 80/345]  eta: 0:03:21  lr: 0.000033  loss: 1.1420 (1.1573)  time: 0.7605  data: 0.0001  max mem: 15689
[08:12:16.592449] Epoch: [5]  [100/345]  eta: 0:03:06  lr: 0.000033  loss: 1.1394 (1.1539)  time: 0.7620  data: 0.0001  max mem: 15689
[08:12:31.825649] Epoch: [5]  [120/345]  eta: 0:02:51  lr: 0.000033  loss: 1.1146 (1.1489)  time: 0.7616  data: 0.0001  max mem: 15689
[08:12:47.059135] Epoch: [5]  [140/345]  eta: 0:02:35  lr: 0.000034  loss: 1.1219 (1.1451)  time: 0.7616  data: 0.0001  max mem: 15689

[08:13:02.284862] Epoch: [5]  [160/345]  eta: 0:02:20  lr: 0.000034  loss: 1.1241 (1.1422)  time: 0.7612  data: 0.0001  max mem: 15689
[08:13:17.500650] Epoch: [5]  [180/345]  eta: 0:02:05  lr: 0.000035  loss: 1.1245 (1.1399)  time: 0.7608  data: 0.0001  max mem: 15689
[08:13:32.718073] Epoch: [5]  [200/345]  eta: 0:01:50  lr: 0.000035  loss: 1.0985 (1.1365)  time: 0.7608  data: 0.0001  max mem: 15689
[08:13:47.926519] Epoch: [5]  [220/345]  eta: 0:01:35  lr: 0.000035  loss: 1.0902 (1.1323)  time: 0.7604  data: 0.0001  max mem: 15689
[08:14:03.126970] Epoch: [5]  [240/345]  eta: 0:01:19  lr: 0.000036  loss: 1.0830 (1.1286)  time: 0.7600  data: 0.0001  max mem: 15689
[08:14:18.329998] Epoch: [5]  [260/345]  eta: 0:01:04  lr: 0.000036  loss: 1.0778 (1.1252)  time: 0.7601  data: 0.0001  max mem: 15689
[08:14:33.529995] Epoch: [5]  [280/345]  eta: 0:00:49  lr: 0.000036  loss: 1.0884 (1.1231)  time: 0.7600  data: 0.0001  max mem: 15689
[08:14:48.725251] Epoch: [5]  [300/345]  eta: 0:00:34  lr: 0.000037  loss: 1.0671 (1.1192)  time: 0.7597  data: 0.0001  max mem: 15689
[08:15:03.920650] Epoch: [5]  [320/345]  eta: 0:00:19  lr: 0.000037  loss: 1.0668 (1.1166)  time: 0.7597  data: 0.0001  max mem: 15689
[08:15:19.117615] Epoch: [5]  [340/345]  eta: 0:00:03  lr: 0.000037  loss: 1.0684 (1.1137)  time: 0.7598  data: 0.0001  max mem: 15689
[08:15:22.156739] Epoch: [5]  [344/345]  eta: 0:00:00  lr: 0.000037  loss: 1.0684 (1.1130)  time: 0.7598  data: 0.0001  max mem: 15689
[08:15:22.222384] Epoch: [5] Total time: 0:04:22 (0.7606 s / it)
[08:15:22.222875] Averaged stats: lr: 0.000037  loss: 1.0684 (1.1130)
[08:15:22.562621] Test:  [  0/345]  eta: 0:01:55  loss: 0.9757 (0.9757)  time: 0.3361  data: 0.1526  max mem: 15689
[08:15:24.416190] Test:  [ 10/345]  eta: 0:01:06  loss: 1.0177 (1.0135)  time: 0.1990  data: 0.0139  max mem: 15689
[08:15:26.275393] Test:  [ 20/345]  eta: 0:01:02  loss: 1.0189 (1.0174)  time: 0.1856  data: 0.0001  max mem: 15689
[08:15:28.138001] Test:  [ 30/345]  eta: 0:01:00  loss: 1.0222 (1.0220)  time: 0.1860  data: 0.0001  max mem: 15689
[08:15:30.005570] Test:  [ 40/345]  eta: 0:00:57  loss: 1.0192 (1.0206)  time: 0.1865  data: 0.0001  max mem: 15689
[08:15:31.875758] Test:  [ 50/345]  eta: 0:00:55  loss: 1.0159 (1.0212)  time: 0.1868  data: 0.0001  max mem: 15689
[08:15:33.747328] Test:  [ 60/345]  eta: 0:00:53  loss: 1.0106 (1.0196)  time: 0.1870  data: 0.0001  max mem: 15689
[08:15:35.621825] Test:  [ 70/345]  eta: 0:00:51  loss: 1.0134 (1.0209)  time: 0.1873  data: 0.0001  max mem: 15689
[08:15:37.504804] Test:  [ 80/345]  eta: 0:00:49  loss: 1.0211 (1.0210)  time: 0.1878  data: 0.0001  max mem: 15689
[08:15:39.388276] Test:  [ 90/345]  eta: 0:00:48  loss: 1.0127 (1.0201)  time: 0.1883  data: 0.0001  max mem: 15689
[08:15:41.275951] Test:  [100/345]  eta: 0:00:46  loss: 1.0131 (1.0194)  time: 0.1885  data: 0.0001  max mem: 15689
[08:15:43.167552] Test:  [110/345]  eta: 0:00:44  loss: 1.0092 (1.0190)  time: 0.1889  data: 0.0001  max mem: 15689
[08:15:45.065354] Test:  [120/345]  eta: 0:00:42  loss: 1.0169 (1.0189)  time: 0.1894  data: 0.0001  max mem: 15689
[08:15:46.965110] Test:  [130/345]  eta: 0:00:40  loss: 1.0229 (1.0197)  time: 0.1898  data: 0.0001  max mem: 15689
[08:15:48.868618] Test:  [140/345]  eta: 0:00:38  loss: 1.0298 (1.0205)  time: 0.1901  data: 0.0001  max mem: 15689
[08:15:50.774988] Test:  [150/345]  eta: 0:00:36  loss: 1.0298 (1.0208)  time: 0.1904  data: 0.0001  max mem: 15689
[08:15:52.684536] Test:  [160/345]  eta: 0:00:34  loss: 1.0264 (1.0204)  time: 0.1907  data: 0.0001  max mem: 15689
[08:15:54.596870] Test:  [170/345]  eta: 0:00:33  loss: 1.0161 (1.0203)  time: 0.1910  data: 0.0001  max mem: 15689
[08:15:56.515878] Test:  [180/345]  eta: 0:00:31  loss: 1.0190 (1.0204)  time: 0.1915  data: 0.0001  max mem: 15689
[08:15:58.438080] Test:  [190/345]  eta: 0:00:29  loss: 1.0176 (1.0200)  time: 0.1920  data: 0.0001  max mem: 15689
[08:16:00.360975] Test:  [200/345]  eta: 0:00:27  loss: 1.0269 (1.0207)  time: 0.1922  data: 0.0001  max mem: 15689
[08:16:02.286849] Test:  [210/345]  eta: 0:00:25  loss: 1.0291 (1.0208)  time: 0.1924  data: 0.0001  max mem: 15689
[08:16:04.216335] Test:  [220/345]  eta: 0:00:23  loss: 1.0055 (1.0198)  time: 0.1927  data: 0.0001  max mem: 15689
[08:16:06.152373] Test:  [230/345]  eta: 0:00:21  loss: 1.0057 (1.0194)  time: 0.1932  data: 0.0001  max mem: 15689
[08:16:08.093078] Test:  [240/345]  eta: 0:00:19  loss: 1.0194 (1.0200)  time: 0.1938  data: 0.0001  max mem: 15689
[08:16:10.035965] Test:  [250/345]  eta: 0:00:18  loss: 1.0291 (1.0200)  time: 0.1941  data: 0.0001  max mem: 15689
[08:16:11.980985] Test:  [260/345]  eta: 0:00:16  loss: 1.0284 (1.0205)  time: 0.1943  data: 0.0001  max mem: 15689
[08:16:13.930180] Test:  [270/345]  eta: 0:00:14  loss: 1.0295 (1.0206)  time: 0.1947  data: 0.0001  max mem: 15689
[08:16:15.880788] Test:  [280/345]  eta: 0:00:12  loss: 1.0295 (1.0210)  time: 0.1949  data: 0.0001  max mem: 15689
[08:16:17.837188] Test:  [290/345]  eta: 0:00:10  loss: 1.0135 (1.0212)  time: 0.1953  data: 0.0001  max mem: 15689
[08:16:19.798447] Test:  [300/345]  eta: 0:00:08  loss: 1.0270 (1.0218)  time: 0.1958  data: 0.0001  max mem: 15689
[08:16:21.760908] Test:  [310/345]  eta: 0:00:06  loss: 1.0224 (1.0216)  time: 0.1961  data: 0.0001  max mem: 15689
[08:16:23.727597] Test:  [320/345]  eta: 0:00:04  loss: 1.0136 (1.0216)  time: 0.1964  data: 0.0001  max mem: 15689
[08:16:25.697530] Test:  [330/345]  eta: 0:00:02  loss: 1.0152 (1.0216)  time: 0.1968  data: 0.0001  max mem: 15689
[08:16:27.669187] Test:  [340/345]  eta: 0:00:00  loss: 1.0158 (1.0212)  time: 0.1970  data: 0.0001  max mem: 15689
[08:16:28.459586] Test:  [344/345]  eta: 0:00:00  loss: 1.0158 (1.0213)  time: 0.1972  data: 0.0001  max mem: 15689
[08:16:28.521640] Test: Total time: 0:01:06 (0.1922 s / it)
[08:16:39.149786] Test:  [ 0/57]  eta: 0:00:18  loss: 1.1082 (1.1082)  time: 0.3256  data: 0.1438  max mem: 15689
[08:16:40.984952] Test:  [10/57]  eta: 0:00:09  loss: 1.0800 (1.0736)  time: 0.1964  data: 0.0131  max mem: 15689
[08:16:42.826463] Test:  [20/57]  eta: 0:00:07  loss: 1.1031 (1.0796)  time: 0.1838  data: 0.0001  max mem: 15689
[08:16:44.669963] Test:  [30/57]  eta: 0:00:05  loss: 0.9594 (1.0229)  time: 0.1842  data: 0.0001  max mem: 15689
[08:16:46.520378] Test:  [40/57]  eta: 0:00:03  loss: 0.8888 (0.9886)  time: 0.1846  data: 0.0001  max mem: 15689
[08:16:48.374471] Test:  [50/57]  eta: 0:00:01  loss: 0.8888 (0.9790)  time: 0.1852  data: 0.0001  max mem: 15689
[08:16:49.375805] Test:  [56/57]  eta: 0:00:00  loss: 0.9676 (0.9842)  time: 0.1797  data: 0.0001  max mem: 15689
[08:16:49.428832] Test: Total time: 0:00:10 (0.1861 s / it)
[08:16:51.319545] Dice score of the network on the train images: 0.668583, val images: 0.730781
[08:16:51.319781] saving best_prec_model_0 @ epoch 5
[08:16:52.524258] saving best_dice_model_0 @ epoch 5
[08:16:53.567553] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:16:54.474755] Epoch: [6]  [  0/345]  eta: 0:05:12  lr: 0.000038  loss: 1.0378 (1.0378)  time: 0.9063  data: 0.1512  max mem: 15689
[08:17:09.559115] Epoch: [6]  [ 20/345]  eta: 0:04:07  lr: 0.000038  loss: 1.0485 (1.0482)  time: 0.7542  data: 0.0001  max mem: 15689
[08:17:24.716407] Epoch: [6]  [ 40/345]  eta: 0:03:51  lr: 0.000038  loss: 1.0447 (1.0504)  time: 0.7578  data: 0.0001  max mem: 15689
[08:17:39.909893] Epoch: [6]  [ 60/345]  eta: 0:03:36  lr: 0.000039  loss: 1.0404 (1.0456)  time: 0.7596  data: 0.0001  max mem: 15689
[08:17:55.112975] Epoch: [6]  [ 80/345]  eta: 0:03:21  lr: 0.000039  loss: 1.0368 (1.0426)  time: 0.7601  data: 0.0001  max mem: 15689
[08:18:10.347868] Epoch: [6]  [100/345]  eta: 0:03:06  lr: 0.000039  loss: 1.0308 (1.0420)  time: 0.7617  data: 0.0001  max mem: 15689
[08:18:25.581729] Epoch: [6]  [120/345]  eta: 0:02:51  lr: 0.000040  loss: 1.0210 (1.0394)  time: 0.7617  data: 0.0001  max mem: 15689
[08:18:40.819193] Epoch: [6]  [140/345]  eta: 0:02:35  lr: 0.000040  loss: 1.0418 (1.0395)  time: 0.7618  data: 0.0001  max mem: 15689
[08:18:56.046478] Epoch: [6]  [160/345]  eta: 0:02:20  lr: 0.000040  loss: 1.0092 (1.0357)  time: 0.7613  data: 0.0001  max mem: 15689
[08:19:11.262622] Epoch: [6]  [180/345]  eta: 0:02:05  lr: 0.000041  loss: 1.0117 (1.0331)  time: 0.7608  data: 0.0001  max mem: 15689
[08:19:26.482338] Epoch: [6]  [200/345]  eta: 0:01:50  lr: 0.000041  loss: 1.0075 (1.0311)  time: 0.7610  data: 0.0001  max mem: 15689
[08:19:41.670541] Epoch: [6]  [220/345]  eta: 0:01:35  lr: 0.000041  loss: 1.0133 (1.0299)  time: 0.7594  data: 0.0001  max mem: 15689
[08:19:56.869939] Epoch: [6]  [240/345]  eta: 0:01:19  lr: 0.000042  loss: 1.0002 (1.0272)  time: 0.7599  data: 0.0001  max mem: 15689
[08:20:12.063648] Epoch: [6]  [260/345]  eta: 0:01:04  lr: 0.000042  loss: 0.9863 (1.0252)  time: 0.7596  data: 0.0001  max mem: 15689
[08:20:27.252849] Epoch: [6]  [280/345]  eta: 0:00:49  lr: 0.000043  loss: 0.9720 (1.0217)  time: 0.7594  data: 0.0001  max mem: 15689
[08:20:42.449688] Epoch: [6]  [300/345]  eta: 0:00:34  lr: 0.000043  loss: 0.9807 (1.0196)  time: 0.7598  data: 0.0001  max mem: 15689
[08:20:57.643893] Epoch: [6]  [320/345]  eta: 0:00:19  lr: 0.000043  loss: 0.9729 (1.0174)  time: 0.7597  data: 0.0001  max mem: 15689

[08:21:12.839108] Epoch: [6]  [340/345]  eta: 0:00:03  lr: 0.000044  loss: 0.9723 (1.0154)  time: 0.7597  data: 0.0001  max mem: 15689
[08:21:15.879265] Epoch: [6]  [344/345]  eta: 0:00:00  lr: 0.000044  loss: 0.9902 (1.0153)  time: 0.7598  data: 0.0001  max mem: 15689
[08:21:15.945156] Epoch: [6] Total time: 0:04:22 (0.7605 s / it)
[08:21:15.945625] Averaged stats: lr: 0.000044  loss: 0.9902 (1.0153)
[08:21:16.283937] Test:  [  0/345]  eta: 0:01:55  loss: 0.9547 (0.9547)  time: 0.3348  data: 0.1509  max mem: 15689
[08:21:18.140288] Test:  [ 10/345]  eta: 0:01:06  loss: 0.9477 (0.9449)  time: 0.1991  data: 0.0138  max mem: 15689
[08:21:19.999986] Test:  [ 20/345]  eta: 0:01:02  loss: 0.9492 (0.9556)  time: 0.1857  data: 0.0001  max mem: 15689
[08:21:21.863497] Test:  [ 30/345]  eta: 0:01:00  loss: 0.9607 (0.9544)  time: 0.1861  data: 0.0001  max mem: 15689
[08:21:23.731539] Test:  [ 40/345]  eta: 0:00:57  loss: 0.9612 (0.9583)  time: 0.1865  data: 0.0001  max mem: 15689
[08:21:25.602557] Test:  [ 50/345]  eta: 0:00:55  loss: 0.9555 (0.9554)  time: 0.1869  data: 0.0001  max mem: 15689
[08:21:27.477079] Test:  [ 60/345]  eta: 0:00:53  loss: 0.9428 (0.9541)  time: 0.1872  data: 0.0001  max mem: 15689
[08:21:29.356736] Test:  [ 70/345]  eta: 0:00:51  loss: 0.9645 (0.9574)  time: 0.1877  data: 0.0001  max mem: 15689
[08:21:31.240540] Test:  [ 80/345]  eta: 0:00:50  loss: 0.9747 (0.9597)  time: 0.1881  data: 0.0001  max mem: 15689
[08:21:33.125591] Test:  [ 90/345]  eta: 0:00:48  loss: 0.9645 (0.9597)  time: 0.1884  data: 0.0001  max mem: 15689
[08:21:35.015464] Test:  [100/345]  eta: 0:00:46  loss: 0.9445 (0.9581)  time: 0.1887  data: 0.0001  max mem: 15689
[08:21:36.905066] Test:  [110/345]  eta: 0:00:44  loss: 0.9435 (0.9573)  time: 0.1889  data: 0.0001  max mem: 15689
[08:21:38.802017] Test:  [120/345]  eta: 0:00:42  loss: 0.9442 (0.9569)  time: 0.1893  data: 0.0001  max mem: 15689
[08:21:40.703125] Test:  [130/345]  eta: 0:00:40  loss: 0.9364 (0.9563)  time: 0.1898  data: 0.0001  max mem: 15689
[08:21:42.606664] Test:  [140/345]  eta: 0:00:38  loss: 0.9364 (0.9552)  time: 0.1902  data: 0.0001  max mem: 15689
[08:21:44.513144] Test:  [150/345]  eta: 0:00:36  loss: 0.9496 (0.9557)  time: 0.1905  data: 0.0001  max mem: 15689
[08:21:46.422880] Test:  [160/345]  eta: 0:00:35  loss: 0.9419 (0.9552)  time: 0.1908  data: 0.0001  max mem: 15689
[08:21:48.335719] Test:  [170/345]  eta: 0:00:33  loss: 0.9384 (0.9559)  time: 0.1911  data: 0.0001  max mem: 15689
[08:21:50.252629] Test:  [180/345]  eta: 0:00:31  loss: 0.9653 (0.9553)  time: 0.1914  data: 0.0001  max mem: 15689
[08:21:52.174913] Test:  [190/345]  eta: 0:00:29  loss: 0.9398 (0.9543)  time: 0.1919  data: 0.0001  max mem: 15689
[08:21:54.098721] Test:  [200/345]  eta: 0:00:27  loss: 0.9396 (0.9540)  time: 0.1923  data: 0.0001  max mem: 15689
[08:21:56.025716] Test:  [210/345]  eta: 0:00:25  loss: 0.9534 (0.9548)  time: 0.1925  data: 0.0001  max mem: 15689
[08:21:57.958896] Test:  [220/345]  eta: 0:00:23  loss: 0.9552 (0.9543)  time: 0.1929  data: 0.0001  max mem: 15689
[08:21:59.895323] Test:  [230/345]  eta: 0:00:21  loss: 0.9364 (0.9539)  time: 0.1934  data: 0.0001  max mem: 15689
[08:22:01.835300] Test:  [240/345]  eta: 0:00:19  loss: 0.9525 (0.9539)  time: 0.1938  data: 0.0001  max mem: 15689
[08:22:03.777002] Test:  [250/345]  eta: 0:00:18  loss: 0.9525 (0.9536)  time: 0.1940  data: 0.0001  max mem: 15689
[08:22:05.722989] Test:  [260/345]  eta: 0:00:16  loss: 0.9304 (0.9529)  time: 0.1943  data: 0.0001  max mem: 15689
[08:22:07.672042] Test:  [270/345]  eta: 0:00:14  loss: 0.9422 (0.9531)  time: 0.1947  data: 0.0001  max mem: 15689
[08:22:09.625290] Test:  [280/345]  eta: 0:00:12  loss: 0.9639 (0.9532)  time: 0.1951  data: 0.0001  max mem: 15689
[08:22:11.581668] Test:  [290/345]  eta: 0:00:10  loss: 0.9495 (0.9526)  time: 0.1954  data: 0.0001  max mem: 15689
[08:22:13.542469] Test:  [300/345]  eta: 0:00:08  loss: 0.9382 (0.9525)  time: 0.1958  data: 0.0001  max mem: 15689
[08:22:15.504027] Test:  [310/345]  eta: 0:00:06  loss: 0.9503 (0.9522)  time: 0.1961  data: 0.0001  max mem: 15689
[08:22:17.469453] Test:  [320/345]  eta: 0:00:04  loss: 0.9518 (0.9524)  time: 0.1963  data: 0.0001  max mem: 15689
[08:22:19.438912] Test:  [330/345]  eta: 0:00:02  loss: 0.9459 (0.9522)  time: 0.1967  data: 0.0001  max mem: 15689
[08:22:21.411229] Test:  [340/345]  eta: 0:00:00  loss: 0.9459 (0.9519)  time: 0.1970  data: 0.0001  max mem: 15689
[08:22:22.200251] Test:  [344/345]  eta: 0:00:00  loss: 0.9474 (0.9517)  time: 0.1971  data: 0.0001  max mem: 15689
[08:22:22.261489] Test: Total time: 0:01:06 (0.1922 s / it)
[08:22:33.008292] Test:  [ 0/57]  eta: 0:00:18  loss: 1.0382 (1.0382)  time: 0.3216  data: 0.1401  max mem: 15689
[08:22:34.844206] Test:  [10/57]  eta: 0:00:09  loss: 0.9997 (1.0051)  time: 0.1961  data: 0.0128  max mem: 15689
[08:22:36.684614] Test:  [20/57]  eta: 0:00:07  loss: 0.9951 (0.9991)  time: 0.1838  data: 0.0001  max mem: 15689
[08:22:38.531178] Test:  [30/57]  eta: 0:00:05  loss: 0.9072 (0.9508)  time: 0.1843  data: 0.0001  max mem: 15689
[08:22:40.380969] Test:  [40/57]  eta: 0:00:03  loss: 0.8310 (0.9213)  time: 0.1848  data: 0.0001  max mem: 15689
[08:22:42.236207] Test:  [50/57]  eta: 0:00:01  loss: 0.8310 (0.9122)  time: 0.1852  data: 0.0001  max mem: 15689
[08:22:43.235580] Test:  [56/57]  eta: 0:00:00  loss: 0.8899 (0.9170)  time: 0.1797  data: 0.0001  max mem: 15689
[08:22:43.287978] Test: Total time: 0:00:10 (0.1860 s / it)
[08:22:45.195703] Dice score of the network on the train images: 0.681405, val images: 0.756410
[08:22:45.195941] saving best_prec_model_0 @ epoch 6
[08:22:46.291084] saving best_rec_model_0 @ epoch 6
[08:22:47.465971] saving best_dice_model_0 @ epoch 6
[08:22:48.501808] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:22:49.406313] Epoch: [7]  [  0/345]  eta: 0:05:11  lr: 0.000044  loss: 0.9426 (0.9426)  time: 0.9032  data: 0.1481  max mem: 15689
[08:23:04.507655] Epoch: [7]  [ 20/345]  eta: 0:04:07  lr: 0.000044  loss: 0.9616 (0.9691)  time: 0.7550  data: 0.0001  max mem: 15689
[08:23:19.677648] Epoch: [7]  [ 40/345]  eta: 0:03:51  lr: 0.000044  loss: 0.9729 (0.9727)  time: 0.7585  data: 0.0001  max mem: 15689
[08:23:34.888350] Epoch: [7]  [ 60/345]  eta: 0:03:36  lr: 0.000045  loss: 0.9591 (0.9685)  time: 0.7605  data: 0.0001  max mem: 15689
[08:23:50.126430] Epoch: [7]  [ 80/345]  eta: 0:03:21  lr: 0.000045  loss: 0.9590 (0.9657)  time: 0.7619  data: 0.0001  max mem: 15689
[08:24:05.382643] Epoch: [7]  [100/345]  eta: 0:03:06  lr: 0.000046  loss: 0.9665 (0.9684)  time: 0.7628  data: 0.0001  max mem: 15689
[08:24:20.639887] Epoch: [7]  [120/345]  eta: 0:02:51  lr: 0.000046  loss: 0.9654 (0.9678)  time: 0.7628  data: 0.0001  max mem: 15689
[08:24:35.885611] Epoch: [7]  [140/345]  eta: 0:02:36  lr: 0.000046  loss: 0.9612 (0.9663)  time: 0.7622  data: 0.0001  max mem: 15689

[08:24:51.138100] Epoch: [7]  [160/345]  eta: 0:02:20  lr: 0.000047  loss: 0.9501 (0.9648)  time: 0.7626  data: 0.0001  max mem: 15689
[08:25:06.383207] Epoch: [7]  [180/345]  eta: 0:02:05  lr: 0.000047  loss: 0.9488 (0.9645)  time: 0.7622  data: 0.0001  max mem: 15689
[08:25:21.702062] Epoch: [7]  [200/345]  eta: 0:01:50  lr: 0.000047  loss: 0.9502 (0.9627)  time: 0.7659  data: 0.0001  max mem: 15689
[08:25:36.932729] Epoch: [7]  [220/345]  eta: 0:01:35  lr: 0.000048  loss: 0.9446 (0.9615)  time: 0.7615  data: 0.0001  max mem: 15689
[08:25:52.154126] Epoch: [7]  [240/345]  eta: 0:01:20  lr: 0.000048  loss: 0.9388 (0.9595)  time: 0.7610  data: 0.0001  max mem: 15689
[08:26:07.379768] Epoch: [7]  [260/345]  eta: 0:01:04  lr: 0.000048  loss: 0.9238 (0.9571)  time: 0.7612  data: 0.0001  max mem: 15689
[08:26:22.607791] Epoch: [7]  [280/345]  eta: 0:00:49  lr: 0.000049  loss: 0.9259 (0.9555)  time: 0.7614  data: 0.0001  max mem: 15689
[08:26:37.832053] Epoch: [7]  [300/345]  eta: 0:00:34  lr: 0.000049  loss: 0.9246 (0.9536)  time: 0.7612  data: 0.0001  max mem: 15689
[08:26:53.053734] Epoch: [7]  [320/345]  eta: 0:00:19  lr: 0.000050  loss: 0.9269 (0.9525)  time: 0.7610  data: 0.0001  max mem: 15689
[08:27:08.283214] Epoch: [7]  [340/345]  eta: 0:00:03  lr: 0.000050  loss: 0.9231 (0.9513)  time: 0.7614  data: 0.0001  max mem: 15689
[08:27:11.324351] Epoch: [7]  [344/345]  eta: 0:00:00  lr: 0.000050  loss: 0.9231 (0.9508)  time: 0.7614  data: 0.0001  max mem: 15689
[08:27:11.386956] Epoch: [7] Total time: 0:04:22 (0.7620 s / it)
[08:27:11.387499] Averaged stats: lr: 0.000050  loss: 0.9231 (0.9508)
[08:27:11.727879] Test:  [  0/345]  eta: 0:01:55  loss: 0.8656 (0.8656)  time: 0.3357  data: 0.1514  max mem: 15689
[08:27:13.585205] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8656 (0.8644)  time: 0.1993  data: 0.0138  max mem: 15689
[08:27:15.444737] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8688 (0.8738)  time: 0.1858  data: 0.0001  max mem: 15689
[08:27:17.308228] Test:  [ 30/345]  eta: 0:01:00  loss: 0.8690 (0.8720)  time: 0.1861  data: 0.0001  max mem: 15689
[08:27:19.177696] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8512 (0.8651)  time: 0.1866  data: 0.0001  max mem: 15689
[08:27:21.048266] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8662 (0.8669)  time: 0.1870  data: 0.0001  max mem: 15689
[08:27:22.921809] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8678 (0.8683)  time: 0.1872  data: 0.0001  max mem: 15689
[08:27:24.799493] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8587 (0.8671)  time: 0.1875  data: 0.0001  max mem: 15689
[08:27:26.682502] Test:  [ 80/345]  eta: 0:00:50  loss: 0.8621 (0.8674)  time: 0.1880  data: 0.0001  max mem: 15689
[08:27:28.567593] Test:  [ 90/345]  eta: 0:00:48  loss: 0.8621 (0.8679)  time: 0.1884  data: 0.0001  max mem: 15689
[08:27:30.455934] Test:  [100/345]  eta: 0:00:46  loss: 0.8703 (0.8686)  time: 0.1886  data: 0.0001  max mem: 15689
[08:27:32.348531] Test:  [110/345]  eta: 0:00:44  loss: 0.8761 (0.8694)  time: 0.1890  data: 0.0001  max mem: 15689
[08:27:34.247648] Test:  [120/345]  eta: 0:00:42  loss: 0.8777 (0.8698)  time: 0.1895  data: 0.0001  max mem: 15689
[08:27:36.148902] Test:  [130/345]  eta: 0:00:40  loss: 0.8722 (0.8700)  time: 0.1900  data: 0.0001  max mem: 15689
[08:27:38.051590] Test:  [140/345]  eta: 0:00:38  loss: 0.8595 (0.8684)  time: 0.1901  data: 0.0001  max mem: 15689
[08:27:39.958836] Test:  [150/345]  eta: 0:00:36  loss: 0.8688 (0.8698)  time: 0.1904  data: 0.0001  max mem: 15689
[08:27:41.868154] Test:  [160/345]  eta: 0:00:35  loss: 0.8824 (0.8700)  time: 0.1908  data: 0.0001  max mem: 15689
[08:27:43.779594] Test:  [170/345]  eta: 0:00:33  loss: 0.8744 (0.8701)  time: 0.1910  data: 0.0001  max mem: 15689
[08:27:45.696485] Test:  [180/345]  eta: 0:00:31  loss: 0.8744 (0.8706)  time: 0.1914  data: 0.0001  max mem: 15689
[08:27:47.617038] Test:  [190/345]  eta: 0:00:29  loss: 0.8703 (0.8699)  time: 0.1918  data: 0.0001  max mem: 15689
[08:27:49.539328] Test:  [200/345]  eta: 0:00:27  loss: 0.8594 (0.8696)  time: 0.1921  data: 0.0001  max mem: 15689
[08:27:51.467588] Test:  [210/345]  eta: 0:00:25  loss: 0.8658 (0.8696)  time: 0.1925  data: 0.0001  max mem: 15689
[08:27:53.399609] Test:  [220/345]  eta: 0:00:23  loss: 0.8660 (0.8697)  time: 0.1930  data: 0.0001  max mem: 15689
[08:27:55.333693] Test:  [230/345]  eta: 0:00:21  loss: 0.8711 (0.8700)  time: 0.1933  data: 0.0001  max mem: 15689
[08:27:57.272177] Test:  [240/345]  eta: 0:00:19  loss: 0.8763 (0.8704)  time: 0.1936  data: 0.0001  max mem: 15689
[08:27:59.215736] Test:  [250/345]  eta: 0:00:18  loss: 0.8665 (0.8698)  time: 0.1940  data: 0.0001  max mem: 15689
[08:28:01.161185] Test:  [260/345]  eta: 0:00:16  loss: 0.8592 (0.8697)  time: 0.1944  data: 0.0001  max mem: 15689
[08:28:03.111316] Test:  [270/345]  eta: 0:00:14  loss: 0.8592 (0.8700)  time: 0.1947  data: 0.0001  max mem: 15689
[08:28:05.063581] Test:  [280/345]  eta: 0:00:12  loss: 0.8701 (0.8701)  time: 0.1951  data: 0.0001  max mem: 15689
[08:28:07.021239] Test:  [290/345]  eta: 0:00:10  loss: 0.8701 (0.8703)  time: 0.1954  data: 0.0001  max mem: 15689
[08:28:08.983118] Test:  [300/345]  eta: 0:00:08  loss: 0.8741 (0.8703)  time: 0.1959  data: 0.0001  max mem: 15689
[08:28:10.944971] Test:  [310/345]  eta: 0:00:06  loss: 0.8741 (0.8708)  time: 0.1961  data: 0.0001  max mem: 15689
[08:28:12.910073] Test:  [320/345]  eta: 0:00:04  loss: 0.8773 (0.8710)  time: 0.1963  data: 0.0001  max mem: 15689
[08:28:14.878400] Test:  [330/345]  eta: 0:00:02  loss: 0.8672 (0.8709)  time: 0.1966  data: 0.0001  max mem: 15689
[08:28:16.850138] Test:  [340/345]  eta: 0:00:00  loss: 0.8684 (0.8709)  time: 0.1970  data: 0.0001  max mem: 15689
[08:28:17.641083] Test:  [344/345]  eta: 0:00:00  loss: 0.8751 (0.8711)  time: 0.1972  data: 0.0001  max mem: 15689
[08:28:17.699481] Test: Total time: 0:01:06 (0.1922 s / it)
[08:28:28.219476] Test:  [ 0/57]  eta: 0:00:18  loss: 0.9774 (0.9774)  time: 0.3245  data: 0.1421  max mem: 15689
[08:28:30.056303] Test:  [10/57]  eta: 0:00:09  loss: 0.9292 (0.9343)  time: 0.1964  data: 0.0130  max mem: 15689
[08:28:31.898613] Test:  [20/57]  eta: 0:00:07  loss: 0.9292 (0.9291)  time: 0.1839  data: 0.0001  max mem: 15689
[08:28:33.743830] Test:  [30/57]  eta: 0:00:05  loss: 0.8266 (0.8863)  time: 0.1843  data: 0.0001  max mem: 15689
[08:28:35.595166] Test:  [40/57]  eta: 0:00:03  loss: 0.7950 (0.8608)  time: 0.1848  data: 0.0001  max mem: 15689
[08:28:37.450482] Test:  [50/57]  eta: 0:00:01  loss: 0.7950 (0.8531)  time: 0.1853  data: 0.0001  max mem: 15689
[08:28:38.450043] Test:  [56/57]  eta: 0:00:00  loss: 0.8252 (0.8578)  time: 0.1797  data: 0.0001  max mem: 15689
[08:28:38.509703] Test: Total time: 0:00:10 (0.1862 s / it)
[08:28:40.352793] Dice score of the network on the train images: 0.729330, val images: 0.788022
[08:28:40.353032] saving best_prec_model_0 @ epoch 7
[08:28:41.444403] saving best_dice_model_0 @ epoch 7
[08:28:42.499407] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft

[08:28:43.399345] Epoch: [8]  [  0/345]  eta: 0:05:10  lr: 0.000050  loss: 0.9412 (0.9412)  time: 0.8991  data: 0.1446  max mem: 15689
[08:28:58.508556] Epoch: [8]  [ 20/345]  eta: 0:04:07  lr: 0.000050  loss: 0.9133 (0.9233)  time: 0.7554  data: 0.0001  max mem: 15689
[08:29:13.694299] Epoch: [8]  [ 40/345]  eta: 0:03:52  lr: 0.000051  loss: 0.9090 (0.9209)  time: 0.7592  data: 0.0001  max mem: 15689
[08:29:28.898178] Epoch: [8]  [ 60/345]  eta: 0:03:36  lr: 0.000051  loss: 0.9217 (0.9234)  time: 0.7602  data: 0.0001  max mem: 15689
[08:29:44.111580] Epoch: [8]  [ 80/345]  eta: 0:03:21  lr: 0.000051  loss: 0.9160 (0.9213)  time: 0.7606  data: 0.0001  max mem: 15689
[08:29:59.343326] Epoch: [8]  [100/345]  eta: 0:03:06  lr: 0.000052  loss: 0.9122 (0.9207)  time: 0.7616  data: 0.0001  max mem: 15689
[08:30:14.582572] Epoch: [8]  [120/345]  eta: 0:02:51  lr: 0.000052  loss: 0.9220 (0.9201)  time: 0.7619  data: 0.0001  max mem: 15689
[08:30:29.815475] Epoch: [8]  [140/345]  eta: 0:02:36  lr: 0.000053  loss: 0.9269 (0.9214)  time: 0.7616  data: 0.0001  max mem: 15689
[08:30:45.036306] Epoch: [8]  [160/345]  eta: 0:02:20  lr: 0.000053  loss: 0.9119 (0.9205)  time: 0.7610  data: 0.0001  max mem: 15689
[08:31:00.255101] Epoch: [8]  [180/345]  eta: 0:02:05  lr: 0.000053  loss: 0.9009 (0.9196)  time: 0.7609  data: 0.0001  max mem: 15689
[08:31:15.469226] Epoch: [8]  [200/345]  eta: 0:01:50  lr: 0.000054  loss: 0.8875 (0.9170)  time: 0.7607  data: 0.0001  max mem: 15689
[08:31:30.723996] Epoch: [8]  [220/345]  eta: 0:01:35  lr: 0.000054  loss: 0.8934 (0.9148)  time: 0.7627  data: 0.0001  max mem: 15689
[08:31:45.937860] Epoch: [8]  [240/345]  eta: 0:01:19  lr: 0.000054  loss: 0.8994 (0.9138)  time: 0.7607  data: 0.0001  max mem: 15689
[08:32:01.149588] Epoch: [8]  [260/345]  eta: 0:01:04  lr: 0.000055  loss: 0.9063 (0.9134)  time: 0.7605  data: 0.0001  max mem: 15689
[08:32:16.369176] Epoch: [8]  [280/345]  eta: 0:00:49  lr: 0.000055  loss: 0.8981 (0.9123)  time: 0.7609  data: 0.0001  max mem: 15689
[08:32:31.598990] Epoch: [8]  [300/345]  eta: 0:00:34  lr: 0.000055  loss: 0.8865 (0.9109)  time: 0.7614  data: 0.0001  max mem: 15689
[08:32:46.821464] Epoch: [8]  [320/345]  eta: 0:00:19  lr: 0.000056  loss: 0.8810 (0.9098)  time: 0.7611  data: 0.0001  max mem: 15689
[08:33:02.044364] Epoch: [8]  [340/345]  eta: 0:00:03  lr: 0.000056  loss: 0.8815 (0.9082)  time: 0.7611  data: 0.0001  max mem: 15689
[08:33:05.088870] Epoch: [8]  [344/345]  eta: 0:00:00  lr: 0.000056  loss: 0.8824 (0.9079)  time: 0.7609  data: 0.0001  max mem: 15689
[08:33:05.156054] Epoch: [8] Total time: 0:04:22 (0.7613 s / it)
[08:33:05.156500] Averaged stats: lr: 0.000056  loss: 0.8824 (0.9079)
[08:33:05.501413] Test:  [  0/345]  eta: 0:01:57  loss: 0.8719 (0.8719)  time: 0.3408  data: 0.1572  max mem: 15689
[08:33:07.357266] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8431 (0.8451)  time: 0.1996  data: 0.0144  max mem: 15689
[08:33:09.216524] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8318 (0.8361)  time: 0.1857  data: 0.0001  max mem: 15689
[08:33:11.081140] Test:  [ 30/345]  eta: 0:01:00  loss: 0.8407 (0.8400)  time: 0.1861  data: 0.0001  max mem: 15689
[08:33:12.949436] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8495 (0.8429)  time: 0.1866  data: 0.0001  max mem: 15689
[08:33:14.820989] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8419 (0.8419)  time: 0.1869  data: 0.0001  max mem: 15689
[08:33:16.696546] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8398 (0.8436)  time: 0.1873  data: 0.0001  max mem: 15689
[08:33:18.574098] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8430 (0.8448)  time: 0.1876  data: 0.0001  max mem: 15689
[08:33:20.456339] Test:  [ 80/345]  eta: 0:00:50  loss: 0.8520 (0.8449)  time: 0.1879  data: 0.0001  max mem: 15689
[08:33:22.341585] Test:  [ 90/345]  eta: 0:00:48  loss: 0.8520 (0.8465)  time: 0.1883  data: 0.0001  max mem: 15689
[08:33:24.230298] Test:  [100/345]  eta: 0:00:46  loss: 0.8472 (0.8461)  time: 0.1886  data: 0.0001  max mem: 15689
[08:33:26.124356] Test:  [110/345]  eta: 0:00:44  loss: 0.8472 (0.8467)  time: 0.1891  data: 0.0001  max mem: 15689
[08:33:28.022309] Test:  [120/345]  eta: 0:00:42  loss: 0.8581 (0.8468)  time: 0.1895  data: 0.0001  max mem: 15689
[08:33:29.924245] Test:  [130/345]  eta: 0:00:40  loss: 0.8327 (0.8461)  time: 0.1899  data: 0.0001  max mem: 15689
[08:33:31.826929] Test:  [140/345]  eta: 0:00:38  loss: 0.8437 (0.8466)  time: 0.1902  data: 0.0001  max mem: 15689
[08:33:33.735408] Test:  [150/345]  eta: 0:00:36  loss: 0.8437 (0.8464)  time: 0.1905  data: 0.0001  max mem: 15689
[08:33:35.647754] Test:  [160/345]  eta: 0:00:35  loss: 0.8390 (0.8466)  time: 0.1910  data: 0.0001  max mem: 15689
[08:33:37.561782] Test:  [170/345]  eta: 0:00:33  loss: 0.8601 (0.8476)  time: 0.1913  data: 0.0001  max mem: 15689
[08:33:39.479115] Test:  [180/345]  eta: 0:00:31  loss: 0.8567 (0.8471)  time: 0.1915  data: 0.0001  max mem: 15689
[08:33:41.400586] Test:  [190/345]  eta: 0:00:29  loss: 0.8487 (0.8475)  time: 0.1919  data: 0.0001  max mem: 15689
[08:33:43.327025] Test:  [200/345]  eta: 0:00:27  loss: 0.8466 (0.8470)  time: 0.1923  data: 0.0001  max mem: 15689
[08:33:45.254262] Test:  [210/345]  eta: 0:00:25  loss: 0.8460 (0.8472)  time: 0.1926  data: 0.0001  max mem: 15689
[08:33:47.187606] Test:  [220/345]  eta: 0:00:23  loss: 0.8460 (0.8471)  time: 0.1930  data: 0.0001  max mem: 15689
[08:33:49.123651] Test:  [230/345]  eta: 0:00:21  loss: 0.8436 (0.8478)  time: 0.1934  data: 0.0001  max mem: 15689
[08:33:51.064631] Test:  [240/345]  eta: 0:00:19  loss: 0.8528 (0.8477)  time: 0.1938  data: 0.0001  max mem: 15689
[08:33:53.008120] Test:  [250/345]  eta: 0:00:18  loss: 0.8441 (0.8472)  time: 0.1942  data: 0.0001  max mem: 15689
[08:33:54.954134] Test:  [260/345]  eta: 0:00:16  loss: 0.8527 (0.8479)  time: 0.1944  data: 0.0001  max mem: 15689
[08:33:56.903151] Test:  [270/345]  eta: 0:00:14  loss: 0.8553 (0.8478)  time: 0.1947  data: 0.0001  max mem: 15689
[08:33:58.854917] Test:  [280/345]  eta: 0:00:12  loss: 0.8570 (0.8486)  time: 0.1950  data: 0.0001  max mem: 15689
[08:34:00.813487] Test:  [290/345]  eta: 0:00:10  loss: 0.8574 (0.8490)  time: 0.1955  data: 0.0001  max mem: 15689
[08:34:02.773531] Test:  [300/345]  eta: 0:00:08  loss: 0.8548 (0.8492)  time: 0.1959  data: 0.0001  max mem: 15689
[08:34:04.734951] Test:  [310/345]  eta: 0:00:06  loss: 0.8548 (0.8494)  time: 0.1960  data: 0.0001  max mem: 15689
[08:34:06.701361] Test:  [320/345]  eta: 0:00:04  loss: 0.8325 (0.8487)  time: 0.1963  data: 0.0001  max mem: 15689
[08:34:08.672718] Test:  [330/345]  eta: 0:00:02  loss: 0.8389 (0.8487)  time: 0.1968  data: 0.0001  max mem: 15689
[08:34:10.645594] Test:  [340/345]  eta: 0:00:00  loss: 0.8490 (0.8493)  time: 0.1972  data: 0.0001  max mem: 15689
[08:34:11.436904] Test:  [344/345]  eta: 0:00:00  loss: 0.8653 (0.8494)  time: 0.1974  data: 0.0001  max mem: 15689
[08:34:11.495790] Test: Total time: 0:01:06 (0.1923 s / it)
[08:34:22.287176] Test:  [ 0/57]  eta: 0:00:18  loss: 0.9296 (0.9296)  time: 0.3241  data: 0.1415  max mem: 15689
[08:34:24.125182] Test:  [10/57]  eta: 0:00:09  loss: 0.8990 (0.9122)  time: 0.1965  data: 0.0129  max mem: 15689
[08:34:25.966106] Test:  [20/57]  eta: 0:00:07  loss: 0.8976 (0.9061)  time: 0.1839  data: 0.0001  max mem: 15689
[08:34:27.811652] Test:  [30/57]  eta: 0:00:05  loss: 0.8267 (0.8677)  time: 0.1843  data: 0.0001  max mem: 15689
[08:34:29.662513] Test:  [40/57]  eta: 0:00:03  loss: 0.7752 (0.8446)  time: 0.1848  data: 0.0001  max mem: 15689
[08:34:31.517444] Test:  [50/57]  eta: 0:00:01  loss: 0.7752 (0.8375)  time: 0.1852  data: 0.0001  max mem: 15689
[08:34:32.517551] Test:  [56/57]  eta: 0:00:00  loss: 0.8187 (0.8432)  time: 0.1797  data: 0.0001  max mem: 15689
[08:34:32.576253] Test: Total time: 0:00:10 (0.1862 s / it)
[08:34:34.425438] Dice score of the network on the train images: 0.730497, val images: 0.796035
[08:34:34.425677] saving best_rec_model_0 @ epoch 8
[08:34:35.522306] saving best_dice_model_0 @ epoch 8
[08:34:36.673274] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:34:37.577921] Epoch: [9]  [  0/345]  eta: 0:05:11  lr: 0.000056  loss: 0.8763 (0.8763)  time: 0.9036  data: 0.1492  max mem: 15689
[08:34:52.688516] Epoch: [9]  [ 20/345]  eta: 0:04:07  lr: 0.000057  loss: 0.8655 (0.8770)  time: 0.7555  data: 0.0001  max mem: 15689

[08:35:07.868255] Epoch: [9]  [ 40/345]  eta: 0:03:52  lr: 0.000057  loss: 0.9120 (0.8887)  time: 0.7589  data: 0.0001  max mem: 15689
[08:35:23.082973] Epoch: [9]  [ 60/345]  eta: 0:03:36  lr: 0.000057  loss: 0.8856 (0.8913)  time: 0.7607  data: 0.0001  max mem: 15689
[08:35:38.334498] Epoch: [9]  [ 80/345]  eta: 0:03:21  lr: 0.000058  loss: 0.8776 (0.8887)  time: 0.7625  data: 0.0001  max mem: 15689
[08:35:53.599602] Epoch: [9]  [100/345]  eta: 0:03:06  lr: 0.000058  loss: 0.8855 (0.8877)  time: 0.7632  data: 0.0001  max mem: 15689
[08:36:08.865801] Epoch: [9]  [120/345]  eta: 0:02:51  lr: 0.000058  loss: 0.8789 (0.8875)  time: 0.7633  data: 0.0001  max mem: 15689
[08:36:24.123358] Epoch: [9]  [140/345]  eta: 0:02:36  lr: 0.000059  loss: 0.8781 (0.8860)  time: 0.7628  data: 0.0001  max mem: 15689
[08:36:39.370356] Epoch: [9]  [160/345]  eta: 0:02:20  lr: 0.000059  loss: 0.8810 (0.8848)  time: 0.7623  data: 0.0001  max mem: 15689
[08:36:54.615656] Epoch: [9]  [180/345]  eta: 0:02:05  lr: 0.000060  loss: 0.8845 (0.8853)  time: 0.7622  data: 0.0001  max mem: 15689
[08:37:09.843965] Epoch: [9]  [200/345]  eta: 0:01:50  lr: 0.000060  loss: 0.8720 (0.8838)  time: 0.7614  data: 0.0001  max mem: 15689
[08:37:25.071901] Epoch: [9]  [220/345]  eta: 0:01:35  lr: 0.000060  loss: 0.8813 (0.8836)  time: 0.7614  data: 0.0001  max mem: 15689
[08:37:40.284786] Epoch: [9]  [240/345]  eta: 0:01:19  lr: 0.000061  loss: 0.8562 (0.8817)  time: 0.7606  data: 0.0001  max mem: 15689
[08:37:55.495495] Epoch: [9]  [260/345]  eta: 0:01:04  lr: 0.000061  loss: 0.8867 (0.8823)  time: 0.7605  data: 0.0001  max mem: 15689
[08:38:10.695410] Epoch: [9]  [280/345]  eta: 0:00:49  lr: 0.000061  loss: 0.8789 (0.8822)  time: 0.7599  data: 0.0001  max mem: 15689
[08:38:25.905086] Epoch: [9]  [300/345]  eta: 0:00:34  lr: 0.000062  loss: 0.8809 (0.8817)  time: 0.7604  data: 0.0001  max mem: 15689
[08:38:41.104260] Epoch: [9]  [320/345]  eta: 0:00:19  lr: 0.000062  loss: 0.8755 (0.8815)  time: 0.7599  data: 0.0001  max mem: 15689
[08:38:56.295047] Epoch: [9]  [340/345]  eta: 0:00:03  lr: 0.000062  loss: 0.8718 (0.8809)  time: 0.7595  data: 0.0001  max mem: 15689
[08:38:59.333547] Epoch: [9]  [344/345]  eta: 0:00:00  lr: 0.000062  loss: 0.8546 (0.8808)  time: 0.7594  data: 0.0001  max mem: 15689
[08:38:59.400247] Epoch: [9] Total time: 0:04:22 (0.7615 s / it)
[08:38:59.400819] Averaged stats: lr: 0.000062  loss: 0.8546 (0.8808)
[08:38:59.745503] Test:  [  0/345]  eta: 0:01:57  loss: 0.8384 (0.8384)  time: 0.3413  data: 0.1576  max mem: 15689
[08:39:01.604753] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8384 (0.8361)  time: 0.2000  data: 0.0144  max mem: 15689
[08:39:03.466323] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8334 (0.8329)  time: 0.1860  data: 0.0001  max mem: 15689
[08:39:05.330584] Test:  [ 30/345]  eta: 0:01:00  loss: 0.8180 (0.8288)  time: 0.1862  data: 0.0001  max mem: 15689
[08:39:07.198824] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8260 (0.8313)  time: 0.1866  data: 0.0001  max mem: 15689
[08:39:09.070881] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8430 (0.8340)  time: 0.1870  data: 0.0001  max mem: 15689
[08:39:10.946275] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8267 (0.8314)  time: 0.1873  data: 0.0001  max mem: 15689
[08:39:12.824623] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8121 (0.8313)  time: 0.1876  data: 0.0001  max mem: 15689
[08:39:14.707473] Test:  [ 80/345]  eta: 0:00:50  loss: 0.8121 (0.8293)  time: 0.1880  data: 0.0001  max mem: 15689
[08:39:16.594356] Test:  [ 90/345]  eta: 0:00:48  loss: 0.8212 (0.8297)  time: 0.1884  data: 0.0001  max mem: 15689
[08:39:18.484774] Test:  [100/345]  eta: 0:00:46  loss: 0.8238 (0.8296)  time: 0.1888  data: 0.0001  max mem: 15689
[08:39:20.378787] Test:  [110/345]  eta: 0:00:44  loss: 0.8178 (0.8292)  time: 0.1892  data: 0.0001  max mem: 15689
[08:39:22.276546] Test:  [120/345]  eta: 0:00:42  loss: 0.8217 (0.8298)  time: 0.1895  data: 0.0001  max mem: 15689
[08:39:24.177827] Test:  [130/345]  eta: 0:00:40  loss: 0.8244 (0.8297)  time: 0.1899  data: 0.0001  max mem: 15689
[08:39:26.080434] Test:  [140/345]  eta: 0:00:38  loss: 0.8197 (0.8287)  time: 0.1901  data: 0.0001  max mem: 15689
[08:39:27.988327] Test:  [150/345]  eta: 0:00:36  loss: 0.8193 (0.8287)  time: 0.1905  data: 0.0001  max mem: 15689
[08:39:29.899582] Test:  [160/345]  eta: 0:00:35  loss: 0.8195 (0.8284)  time: 0.1909  data: 0.0001  max mem: 15689
[08:39:31.814505] Test:  [170/345]  eta: 0:00:33  loss: 0.8298 (0.8290)  time: 0.1913  data: 0.0001  max mem: 15689
[08:39:33.733017] Test:  [180/345]  eta: 0:00:31  loss: 0.8392 (0.8298)  time: 0.1916  data: 0.0001  max mem: 15689
[08:39:35.654112] Test:  [190/345]  eta: 0:00:29  loss: 0.8294 (0.8296)  time: 0.1919  data: 0.0001  max mem: 15689
[08:39:37.576779] Test:  [200/345]  eta: 0:00:27  loss: 0.8244 (0.8293)  time: 0.1921  data: 0.0001  max mem: 15689
[08:39:39.504117] Test:  [210/345]  eta: 0:00:25  loss: 0.8319 (0.8293)  time: 0.1924  data: 0.0001  max mem: 15689
[08:39:41.435946] Test:  [220/345]  eta: 0:00:23  loss: 0.8319 (0.8296)  time: 0.1929  data: 0.0001  max mem: 15689
[08:39:43.371573] Test:  [230/345]  eta: 0:00:21  loss: 0.8423 (0.8303)  time: 0.1933  data: 0.0001  max mem: 15689
[08:39:45.310860] Test:  [240/345]  eta: 0:00:19  loss: 0.8423 (0.8305)  time: 0.1937  data: 0.0001  max mem: 15689
[08:39:47.253200] Test:  [250/345]  eta: 0:00:18  loss: 0.8265 (0.8305)  time: 0.1940  data: 0.0001  max mem: 15689
[08:39:49.198688] Test:  [260/345]  eta: 0:00:16  loss: 0.8265 (0.8304)  time: 0.1943  data: 0.0001  max mem: 15689
[08:39:51.148370] Test:  [270/345]  eta: 0:00:14  loss: 0.8217 (0.8300)  time: 0.1947  data: 0.0001  max mem: 15689
[08:39:53.101196] Test:  [280/345]  eta: 0:00:12  loss: 0.8226 (0.8301)  time: 0.1951  data: 0.0001  max mem: 15689
[08:39:55.059247] Test:  [290/345]  eta: 0:00:10  loss: 0.8231 (0.8298)  time: 0.1955  data: 0.0001  max mem: 15689
[08:39:57.019075] Test:  [300/345]  eta: 0:00:08  loss: 0.8200 (0.8296)  time: 0.1958  data: 0.0001  max mem: 15689
[08:39:58.982066] Test:  [310/345]  eta: 0:00:06  loss: 0.8215 (0.8298)  time: 0.1961  data: 0.0001  max mem: 15689
[08:40:00.947570] Test:  [320/345]  eta: 0:00:04  loss: 0.8341 (0.8300)  time: 0.1964  data: 0.0001  max mem: 15689
[08:40:02.917643] Test:  [330/345]  eta: 0:00:02  loss: 0.8226 (0.8297)  time: 0.1967  data: 0.0001  max mem: 15689
[08:40:04.891679] Test:  [340/345]  eta: 0:00:00  loss: 0.8226 (0.8299)  time: 0.1972  data: 0.0001  max mem: 15689
[08:40:05.682788] Test:  [344/345]  eta: 0:00:00  loss: 0.8226 (0.8298)  time: 0.1973  data: 0.0001  max mem: 15689
[08:40:05.743087] Test: Total time: 0:01:06 (0.1923 s / it)
[08:40:16.319416] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8844 (0.8844)  time: 0.3277  data: 0.1457  max mem: 15689
[08:40:18.155304] Test:  [10/57]  eta: 0:00:09  loss: 0.8844 (0.8994)  time: 0.1966  data: 0.0133  max mem: 15689
[08:40:19.997175] Test:  [20/57]  eta: 0:00:07  loss: 0.8972 (0.8954)  time: 0.1838  data: 0.0001  max mem: 15689
[08:40:21.843093] Test:  [30/57]  eta: 0:00:05  loss: 0.8073 (0.8581)  time: 0.1843  data: 0.0001  max mem: 15689
[08:40:23.692672] Test:  [40/57]  eta: 0:00:03  loss: 0.7724 (0.8363)  time: 0.1847  data: 0.0001  max mem: 15689
[08:40:25.548286] Test:  [50/57]  eta: 0:00:01  loss: 0.7724 (0.8297)  time: 0.1852  data: 0.0001  max mem: 15689
[08:40:26.547859] Test:  [56/57]  eta: 0:00:00  loss: 0.8096 (0.8354)  time: 0.1797  data: 0.0001  max mem: 15689
[08:40:26.608409] Test: Total time: 0:00:10 (0.1863 s / it)
[08:40:28.474955] Dice score of the network on the train images: 0.742932, val images: 0.803741
[08:40:28.475196] saving best_prec_model_0 @ epoch 9
[08:40:29.514293] saving best_dice_model_0 @ epoch 9
[08:40:30.618028] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:40:31.519515] Epoch: [10]  [  0/345]  eta: 0:05:10  lr: 0.000063  loss: 0.8903 (0.8903)  time: 0.9004  data: 0.1463  max mem: 15689
[08:40:46.614656] Epoch: [10]  [ 20/345]  eta: 0:04:07  lr: 0.000063  loss: 0.8676 (0.8706)  time: 0.7547  data: 0.0001  max mem: 15689
[08:41:01.772819] Epoch: [10]  [ 40/345]  eta: 0:03:51  lr: 0.000063  loss: 0.8545 (0.8648)  time: 0.7579  data: 0.0001  max mem: 15689
[08:41:16.973926] Epoch: [10]  [ 60/345]  eta: 0:03:36  lr: 0.000064  loss: 0.8624 (0.8650)  time: 0.7600  data: 0.0001  max mem: 15689
[08:41:32.189884] Epoch: [10]  [ 80/345]  eta: 0:03:21  lr: 0.000064  loss: 0.8647 (0.8652)  time: 0.7608  data: 0.0001  max mem: 15689
[08:41:47.434040] Epoch: [10]  [100/345]  eta: 0:03:06  lr: 0.000064  loss: 0.8617 (0.8653)  time: 0.7622  data: 0.0001  max mem: 15689
[08:42:02.695099] Epoch: [10]  [120/345]  eta: 0:02:51  lr: 0.000065  loss: 0.8544 (0.8640)  time: 0.7630  data: 0.0001  max mem: 15689
[08:42:17.931592] Epoch: [10]  [140/345]  eta: 0:02:36  lr: 0.000065  loss: 0.8419 (0.8619)  time: 0.7618  data: 0.0001  max mem: 15689
[08:42:33.190174] Epoch: [10]  [160/345]  eta: 0:02:20  lr: 0.000065  loss: 0.8594 (0.8620)  time: 0.7629  data: 0.0001  max mem: 15689
[08:42:48.440071] Epoch: [10]  [180/345]  eta: 0:02:05  lr: 0.000066  loss: 0.8473 (0.8619)  time: 0.7624  data: 0.0001  max mem: 15689
[08:43:03.679448] Epoch: [10]  [200/345]  eta: 0:01:50  lr: 0.000066  loss: 0.8408 (0.8610)  time: 0.7619  data: 0.0001  max mem: 15689
[08:43:19.001391] Epoch: [10]  [220/345]  eta: 0:01:35  lr: 0.000066  loss: 0.8421 (0.8595)  time: 0.7660  data: 0.0001  max mem: 15689
[08:43:34.236646] Epoch: [10]  [240/345]  eta: 0:01:19  lr: 0.000067  loss: 0.8482 (0.8592)  time: 0.7617  data: 0.0001  max mem: 15689
[08:43:49.463348] Epoch: [10]  [260/345]  eta: 0:01:04  lr: 0.000067  loss: 0.8569 (0.8590)  time: 0.7613  data: 0.0001  max mem: 15689
[08:44:04.677900] Epoch: [10]  [280/345]  eta: 0:00:49  lr: 0.000068  loss: 0.8410 (0.8581)  time: 0.7607  data: 0.0001  max mem: 15689
[08:44:19.881846] Epoch: [10]  [300/345]  eta: 0:00:34  lr: 0.000068  loss: 0.8325 (0.8568)  time: 0.7602  data: 0.0001  max mem: 15689
[08:44:35.088873] Epoch: [10]  [320/345]  eta: 0:00:19  lr: 0.000068  loss: 0.8382 (0.8562)  time: 0.7603  data: 0.0001  max mem: 15689
[08:44:50.284597] Epoch: [10]  [340/345]  eta: 0:00:03  lr: 0.000069  loss: 0.8357 (0.8551)  time: 0.7597  data: 0.0001  max mem: 15689
[08:44:53.324602] Epoch: [10]  [344/345]  eta: 0:00:00  lr: 0.000069  loss: 0.8331 (0.8548)  time: 0.7597  data: 0.0001  max mem: 15689
[08:44:53.385695] Epoch: [10] Total time: 0:04:22 (0.7616 s / it)
[08:44:53.386151] Averaged stats: lr: 0.000069  loss: 0.8331 (0.8548)
[08:44:53.725272] Test:  [  0/345]  eta: 0:01:55  loss: 0.7754 (0.7754)  time: 0.3353  data: 0.1513  max mem: 15689
[08:44:55.583112] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8110 (0.8060)  time: 0.1993  data: 0.0138  max mem: 15689
[08:44:57.444434] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8087 (0.8052)  time: 0.1859  data: 0.0001  max mem: 15689
[08:44:59.307655] Test:  [ 30/345]  eta: 0:01:00  loss: 0.8058 (0.8070)  time: 0.1862  data: 0.0001  max mem: 15689
[08:45:01.176451] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8174 (0.8100)  time: 0.1866  data: 0.0001  max mem: 15689
[08:45:03.047018] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8050 (0.8080)  time: 0.1869  data: 0.0001  max mem: 15689
[08:45:04.922100] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7983 (0.8095)  time: 0.1872  data: 0.0001  max mem: 15689
[08:45:06.800831] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8030 (0.8096)  time: 0.1876  data: 0.0001  max mem: 15689
[08:45:08.684138] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7973 (0.8081)  time: 0.1881  data: 0.0001  max mem: 15689
[08:45:10.569348] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7939 (0.8080)  time: 0.1884  data: 0.0001  max mem: 15689
[08:45:12.459318] Test:  [100/345]  eta: 0:00:46  loss: 0.8170 (0.8090)  time: 0.1887  data: 0.0001  max mem: 15689
[08:45:14.353236] Test:  [110/345]  eta: 0:00:44  loss: 0.8135 (0.8092)  time: 0.1891  data: 0.0001  max mem: 15689
[08:45:16.252262] Test:  [120/345]  eta: 0:00:42  loss: 0.8085 (0.8087)  time: 0.1896  data: 0.0001  max mem: 15689
[08:45:18.154512] Test:  [130/345]  eta: 0:00:40  loss: 0.8038 (0.8083)  time: 0.1900  data: 0.0001  max mem: 15689
[08:45:20.058393] Test:  [140/345]  eta: 0:00:38  loss: 0.7985 (0.8077)  time: 0.1903  data: 0.0001  max mem: 15689
[08:45:21.965414] Test:  [150/345]  eta: 0:00:36  loss: 0.8001 (0.8075)  time: 0.1905  data: 0.0001  max mem: 15689
[08:45:23.876304] Test:  [160/345]  eta: 0:00:35  loss: 0.8126 (0.8081)  time: 0.1908  data: 0.0001  max mem: 15689
[08:45:25.790711] Test:  [170/345]  eta: 0:00:33  loss: 0.8120 (0.8080)  time: 0.1912  data: 0.0001  max mem: 15689
[08:45:27.708196] Test:  [180/345]  eta: 0:00:31  loss: 0.8105 (0.8081)  time: 0.1915  data: 0.0001  max mem: 15689
[08:45:29.631994] Test:  [190/345]  eta: 0:00:29  loss: 0.8143 (0.8090)  time: 0.1920  data: 0.0001  max mem: 15689
[08:45:31.557680] Test:  [200/345]  eta: 0:00:27  loss: 0.8204 (0.8097)  time: 0.1924  data: 0.0001  max mem: 15689
[08:45:33.486986] Test:  [210/345]  eta: 0:00:25  loss: 0.8134 (0.8098)  time: 0.1927  data: 0.0001  max mem: 15689
[08:45:35.417750] Test:  [220/345]  eta: 0:00:23  loss: 0.8092 (0.8102)  time: 0.1930  data: 0.0001  max mem: 15689
[08:45:37.352978] Test:  [230/345]  eta: 0:00:21  loss: 0.8094 (0.8109)  time: 0.1932  data: 0.0001  max mem: 15689
[08:45:39.291409] Test:  [240/345]  eta: 0:00:19  loss: 0.7989 (0.8105)  time: 0.1936  data: 0.0001  max mem: 15689
[08:45:41.234257] Test:  [250/345]  eta: 0:00:18  loss: 0.8129 (0.8106)  time: 0.1940  data: 0.0001  max mem: 15689
[08:45:43.182062] Test:  [260/345]  eta: 0:00:16  loss: 0.8088 (0.8106)  time: 0.1945  data: 0.0001  max mem: 15689
[08:45:45.132904] Test:  [270/345]  eta: 0:00:14  loss: 0.8061 (0.8104)  time: 0.1949  data: 0.0001  max mem: 15689
[08:45:47.086009] Test:  [280/345]  eta: 0:00:12  loss: 0.8053 (0.8103)  time: 0.1951  data: 0.0001  max mem: 15689
[08:45:49.044525] Test:  [290/345]  eta: 0:00:10  loss: 0.8088 (0.8105)  time: 0.1955  data: 0.0001  max mem: 15689
[08:45:51.004907] Test:  [300/345]  eta: 0:00:08  loss: 0.7941 (0.8103)  time: 0.1959  data: 0.0001  max mem: 15689
[08:45:52.966388] Test:  [310/345]  eta: 0:00:06  loss: 0.7866 (0.8096)  time: 0.1960  data: 0.0001  max mem: 15689
[08:45:54.932288] Test:  [320/345]  eta: 0:00:04  loss: 0.8023 (0.8098)  time: 0.1963  data: 0.0001  max mem: 15689
[08:45:56.903284] Test:  [330/345]  eta: 0:00:02  loss: 0.8096 (0.8097)  time: 0.1968  data: 0.0001  max mem: 15689
[08:45:58.877028] Test:  [340/345]  eta: 0:00:00  loss: 0.8088 (0.8100)  time: 0.1972  data: 0.0001  max mem: 15689
[08:45:59.667104] Test:  [344/345]  eta: 0:00:00  loss: 0.8150 (0.8100)  time: 0.1973  data: 0.0001  max mem: 15689
[08:45:59.727751] Test: Total time: 0:01:06 (0.1923 s / it)
[08:46:10.349415] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8876 (0.8876)  time: 0.3258  data: 0.1441  max mem: 15689
[08:46:12.186058] Test:  [10/57]  eta: 0:00:09  loss: 0.8856 (0.8962)  time: 0.1965  data: 0.0132  max mem: 15689
[08:46:14.028300] Test:  [20/57]  eta: 0:00:07  loss: 0.8856 (0.8899)  time: 0.1839  data: 0.0001  max mem: 15689
[08:46:15.874699] Test:  [30/57]  eta: 0:00:05  loss: 0.7968 (0.8506)  time: 0.1844  data: 0.0001  max mem: 15689
[08:46:17.725889] Test:  [40/57]  eta: 0:00:03  loss: 0.7571 (0.8272)  time: 0.1848  data: 0.0001  max mem: 15689
[08:46:19.582983] Test:  [50/57]  eta: 0:00:01  loss: 0.7571 (0.8199)  time: 0.1854  data: 0.0001  max mem: 15689
[08:46:20.583344] Test:  [56/57]  eta: 0:00:00  loss: 0.7877 (0.8250)  time: 0.1799  data: 0.0001  max mem: 15689
[08:46:20.638186] Test: Total time: 0:00:10 (0.1862 s / it)
[08:46:22.519450] Dice score of the network on the train images: 0.763837, val images: 0.804447
[08:46:22.519684] saving best_prec_model_0 @ epoch 10
[08:46:23.625018] saving best_dice_model_0 @ epoch 10
[08:46:24.777582] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:46:25.683155] Epoch: [11]  [  0/345]  eta: 0:05:12  lr: 0.000069  loss: 0.8785 (0.8785)  time: 0.9045  data: 0.1486  max mem: 15689
[08:46:40.776637] Epoch: [11]  [ 20/345]  eta: 0:04:07  lr: 0.000069  loss: 0.8468 (0.8461)  time: 0.7546  data: 0.0001  max mem: 15689
[08:46:55.931202] Epoch: [11]  [ 40/345]  eta: 0:03:51  lr: 0.000069  loss: 0.8282 (0.8413)  time: 0.7577  data: 0.0001  max mem: 15689
[08:47:11.121235] Epoch: [11]  [ 60/345]  eta: 0:03:36  lr: 0.000070  loss: 0.8275 (0.8385)  time: 0.7595  data: 0.0001  max mem: 15689
[08:47:26.343921] Epoch: [11]  [ 80/345]  eta: 0:03:21  lr: 0.000070  loss: 0.8359 (0.8385)  time: 0.7611  data: 0.0001  max mem: 15689
[08:47:41.586107] Epoch: [11]  [100/345]  eta: 0:03:06  lr: 0.000071  loss: 0.8355 (0.8392)  time: 0.7621  data: 0.0001  max mem: 15689
[08:47:56.838787] Epoch: [11]  [120/345]  eta: 0:02:51  lr: 0.000071  loss: 0.8360 (0.8400)  time: 0.7626  data: 0.0001  max mem: 15689
[08:48:12.083108] Epoch: [11]  [140/345]  eta: 0:02:36  lr: 0.000071  loss: 0.8330 (0.8398)  time: 0.7622  data: 0.0001  max mem: 15689
[08:48:27.332128] Epoch: [11]  [160/345]  eta: 0:02:20  lr: 0.000072  loss: 0.8362 (0.8390)  time: 0.7624  data: 0.0001  max mem: 15689
[08:48:42.579829] Epoch: [11]  [180/345]  eta: 0:02:05  lr: 0.000072  loss: 0.8244 (0.8382)  time: 0.7623  data: 0.0001  max mem: 15689
[08:48:57.818715] Epoch: [11]  [200/345]  eta: 0:01:50  lr: 0.000072  loss: 0.8252 (0.8374)  time: 0.7619  data: 0.0001  max mem: 15689
[08:49:13.045790] Epoch: [11]  [220/345]  eta: 0:01:35  lr: 0.000073  loss: 0.8355 (0.8378)  time: 0.7613  data: 0.0001  max mem: 15689
[08:49:28.410223] Epoch: [11]  [240/345]  eta: 0:01:20  lr: 0.000073  loss: 0.8241 (0.8370)  time: 0.7682  data: 0.0001  max mem: 15689
[08:49:43.637767] Epoch: [11]  [260/345]  eta: 0:01:04  lr: 0.000073  loss: 0.8255 (0.8368)  time: 0.7613  data: 0.0001  max mem: 15689
[08:49:58.868198] Epoch: [11]  [280/345]  eta: 0:00:49  lr: 0.000074  loss: 0.8305 (0.8371)  time: 0.7615  data: 0.0001  max mem: 15689
[08:50:14.101547] Epoch: [11]  [300/345]  eta: 0:00:34  lr: 0.000074  loss: 0.8291 (0.8366)  time: 0.7616  data: 0.0001  max mem: 15689
[08:50:29.332754] Epoch: [11]  [320/345]  eta: 0:00:19  lr: 0.000075  loss: 0.8345 (0.8366)  time: 0.7615  data: 0.0001  max mem: 15689
[08:50:44.558286] Epoch: [11]  [340/345]  eta: 0:00:03  lr: 0.000075  loss: 0.8279 (0.8361)  time: 0.7612  data: 0.0001  max mem: 15689
[08:50:47.607151] Epoch: [11]  [344/345]  eta: 0:00:00  lr: 0.000075  loss: 0.8278 (0.8361)  time: 0.7615  data: 0.0001  max mem: 15689
[08:50:47.673579] Epoch: [11] Total time: 0:04:22 (0.7620 s / it)
[08:50:47.674093] Averaged stats: lr: 0.000075  loss: 0.8278 (0.8361)
[08:50:48.019072] Test:  [  0/345]  eta: 0:01:57  loss: 0.8192 (0.8192)  time: 0.3411  data: 0.1571  max mem: 15689
[08:50:49.879641] Test:  [ 10/345]  eta: 0:01:07  loss: 0.8011 (0.8029)  time: 0.2001  data: 0.0143  max mem: 15689
[08:50:51.741581] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7950 (0.7980)  time: 0.1861  data: 0.0001  max mem: 15689
[08:50:53.608265] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7842 (0.7965)  time: 0.1864  data: 0.0001  max mem: 15689
[08:50:55.475555] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8046 (0.8034)  time: 0.1866  data: 0.0001  max mem: 15689
[08:50:57.348809] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8208 (0.8047)  time: 0.1870  data: 0.0001  max mem: 15689
[08:50:59.225100] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8091 (0.8042)  time: 0.1874  data: 0.0001  max mem: 15689
[08:51:01.105160] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8043 (0.8039)  time: 0.1878  data: 0.0001  max mem: 15689
[08:51:02.990174] Test:  [ 80/345]  eta: 0:00:50  loss: 0.8043 (0.8043)  time: 0.1882  data: 0.0001  max mem: 15689
[08:51:04.877548] Test:  [ 90/345]  eta: 0:00:48  loss: 0.8061 (0.8054)  time: 0.1886  data: 0.0001  max mem: 15689
[08:51:06.769551] Test:  [100/345]  eta: 0:00:46  loss: 0.8062 (0.8053)  time: 0.1889  data: 0.0001  max mem: 15689
[08:51:08.667467] Test:  [110/345]  eta: 0:00:44  loss: 0.8057 (0.8056)  time: 0.1894  data: 0.0001  max mem: 15689
[08:51:10.568604] Test:  [120/345]  eta: 0:00:42  loss: 0.8057 (0.8050)  time: 0.1899  data: 0.0001  max mem: 15689
[08:51:12.471167] Test:  [130/345]  eta: 0:00:40  loss: 0.8021 (0.8050)  time: 0.1901  data: 0.0001  max mem: 15689
[08:51:14.376384] Test:  [140/345]  eta: 0:00:38  loss: 0.8021 (0.8053)  time: 0.1903  data: 0.0001  max mem: 15689
[08:51:16.286170] Test:  [150/345]  eta: 0:00:36  loss: 0.8048 (0.8055)  time: 0.1907  data: 0.0001  max mem: 15689
[08:51:18.197908] Test:  [160/345]  eta: 0:00:35  loss: 0.8046 (0.8049)  time: 0.1910  data: 0.0001  max mem: 15689
[08:51:20.113164] Test:  [170/345]  eta: 0:00:33  loss: 0.7891 (0.8044)  time: 0.1913  data: 0.0001  max mem: 15689
[08:51:22.033623] Test:  [180/345]  eta: 0:00:31  loss: 0.8060 (0.8051)  time: 0.1917  data: 0.0001  max mem: 15689
[08:51:23.954944] Test:  [190/345]  eta: 0:00:29  loss: 0.8113 (0.8054)  time: 0.1920  data: 0.0001  max mem: 15689
[08:51:25.879246] Test:  [200/345]  eta: 0:00:27  loss: 0.8050 (0.8049)  time: 0.1922  data: 0.0001  max mem: 15689
[08:51:27.809421] Test:  [210/345]  eta: 0:00:25  loss: 0.8001 (0.8048)  time: 0.1927  data: 0.0001  max mem: 15689
[08:51:29.742374] Test:  [220/345]  eta: 0:00:23  loss: 0.8029 (0.8046)  time: 0.1931  data: 0.0001  max mem: 15689
[08:51:31.679048] Test:  [230/345]  eta: 0:00:21  loss: 0.8006 (0.8044)  time: 0.1934  data: 0.0001  max mem: 15689
[08:51:33.619331] Test:  [240/345]  eta: 0:00:20  loss: 0.8053 (0.8044)  time: 0.1938  data: 0.0001  max mem: 15689
[08:51:35.563429] Test:  [250/345]  eta: 0:00:18  loss: 0.7959 (0.8038)  time: 0.1942  data: 0.0001  max mem: 15689
[08:51:37.509274] Test:  [260/345]  eta: 0:00:16  loss: 0.7919 (0.8036)  time: 0.1944  data: 0.0001  max mem: 15689
[08:51:39.460003] Test:  [270/345]  eta: 0:00:14  loss: 0.7988 (0.8035)  time: 0.1948  data: 0.0001  max mem: 15689
[08:51:41.413238] Test:  [280/345]  eta: 0:00:12  loss: 0.8074 (0.8038)  time: 0.1951  data: 0.0001  max mem: 15689
[08:51:43.370878] Test:  [290/345]  eta: 0:00:10  loss: 0.8149 (0.8041)  time: 0.1955  data: 0.0001  max mem: 15689
[08:51:45.330524] Test:  [300/345]  eta: 0:00:08  loss: 0.8092 (0.8040)  time: 0.1958  data: 0.0001  max mem: 15689
[08:51:47.294128] Test:  [310/345]  eta: 0:00:06  loss: 0.7975 (0.8039)  time: 0.1961  data: 0.0001  max mem: 15689
[08:51:49.263214] Test:  [320/345]  eta: 0:00:04  loss: 0.8014 (0.8041)  time: 0.1966  data: 0.0001  max mem: 15689
[08:51:51.238503] Test:  [330/345]  eta: 0:00:02  loss: 0.7983 (0.8039)  time: 0.1972  data: 0.0001  max mem: 15689
[08:51:53.214459] Test:  [340/345]  eta: 0:00:00  loss: 0.7983 (0.8038)  time: 0.1975  data: 0.0001  max mem: 15689
[08:51:54.005705] Test:  [344/345]  eta: 0:00:00  loss: 0.7983 (0.8038)  time: 0.1976  data: 0.0001  max mem: 15689
[08:51:54.067466] Test: Total time: 0:01:06 (0.1924 s / it)
[08:52:04.748884] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8531 (0.8531)  time: 0.3252  data: 0.1432  max mem: 15689
[08:52:06.585737] Test:  [10/57]  eta: 0:00:09  loss: 0.8586 (0.8805)  time: 0.1965  data: 0.0131  max mem: 15689
[08:52:08.428097] Test:  [20/57]  eta: 0:00:07  loss: 0.8608 (0.8715)  time: 0.1839  data: 0.0001  max mem: 15689
[08:52:10.275308] Test:  [30/57]  eta: 0:00:05  loss: 0.7746 (0.8379)  time: 0.1844  data: 0.0001  max mem: 15689
[08:52:12.125323] Test:  [40/57]  eta: 0:00:03  loss: 0.7703 (0.8195)  time: 0.1848  data: 0.0001  max mem: 15689
[08:52:13.982495] Test:  [50/57]  eta: 0:00:01  loss: 0.7634 (0.8143)  time: 0.1853  data: 0.0001  max mem: 15689
[08:52:14.982681] Test:  [56/57]  eta: 0:00:00  loss: 0.7920 (0.8198)  time: 0.1799  data: 0.0001  max mem: 15689
[08:52:15.042494] Test: Total time: 0:00:10 (0.1863 s / it)
[08:52:16.922191] Dice score of the network on the train images: 0.740240, val images: 0.802335
[08:52:16.922432] saving best_rec_model_0 @ epoch 11
[08:52:17.980245] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:52:18.879991] Epoch: [12]  [  0/345]  eta: 0:05:10  lr: 0.000075  loss: 0.8906 (0.8906)  time: 0.8989  data: 0.1456  max mem: 15689
[08:52:33.976218] Epoch: [12]  [ 20/345]  eta: 0:04:07  lr: 0.000075  loss: 0.8230 (0.8341)  time: 0.7548  data: 0.0001  max mem: 15689

[08:52:49.141747] Epoch: [12]  [ 40/345]  eta: 0:03:51  lr: 0.000076  loss: 0.8226 (0.8290)  time: 0.7582  data: 0.0001  max mem: 15689
[08:53:04.440687] Epoch: [12]  [ 60/345]  eta: 0:03:37  lr: 0.000076  loss: 0.8190 (0.8279)  time: 0.7649  data: 0.0001  max mem: 15689
[08:53:19.661396] Epoch: [12]  [ 80/345]  eta: 0:03:21  lr: 0.000076  loss: 0.8232 (0.8269)  time: 0.7610  data: 0.0001  max mem: 15689
[08:53:34.909731] Epoch: [12]  [100/345]  eta: 0:03:06  lr: 0.000077  loss: 0.7999 (0.8232)  time: 0.7624  data: 0.0001  max mem: 15689
[08:53:50.158125] Epoch: [12]  [120/345]  eta: 0:02:51  lr: 0.000077  loss: 0.8272 (0.8234)  time: 0.7624  data: 0.0001  max mem: 15689
[08:54:05.399542] Epoch: [12]  [140/345]  eta: 0:02:36  lr: 0.000078  loss: 0.8252 (0.8227)  time: 0.7620  data: 0.0001  max mem: 15689
[08:54:20.635403] Epoch: [12]  [160/345]  eta: 0:02:20  lr: 0.000078  loss: 0.8343 (0.8243)  time: 0.7618  data: 0.0001  max mem: 15689
[08:54:35.866634] Epoch: [12]  [180/345]  eta: 0:02:05  lr: 0.000078  loss: 0.8166 (0.8238)  time: 0.7615  data: 0.0001  max mem: 15689
[08:54:51.094461] Epoch: [12]  [200/345]  eta: 0:01:50  lr: 0.000079  loss: 0.8097 (0.8224)  time: 0.7614  data: 0.0001  max mem: 15689
[08:55:06.302078] Epoch: [12]  [220/345]  eta: 0:01:35  lr: 0.000079  loss: 0.8209 (0.8218)  time: 0.7603  data: 0.0001  max mem: 15689
[08:55:21.513939] Epoch: [12]  [240/345]  eta: 0:01:19  lr: 0.000079  loss: 0.8021 (0.8207)  time: 0.7606  data: 0.0001  max mem: 15689
[08:55:36.769233] Epoch: [12]  [260/345]  eta: 0:01:04  lr: 0.000080  loss: 0.8147 (0.8206)  time: 0.7627  data: 0.0001  max mem: 15689
[08:55:51.973414] Epoch: [12]  [280/345]  eta: 0:00:49  lr: 0.000080  loss: 0.8280 (0.8208)  time: 0.7602  data: 0.0001  max mem: 15689
[08:56:07.185206] Epoch: [12]  [300/345]  eta: 0:00:34  lr: 0.000080  loss: 0.8135 (0.8208)  time: 0.7606  data: 0.0001  max mem: 15689
[08:56:22.380910] Epoch: [12]  [320/345]  eta: 0:00:19  lr: 0.000081  loss: 0.8140 (0.8207)  time: 0.7597  data: 0.0001  max mem: 15689
[08:56:37.577792] Epoch: [12]  [340/345]  eta: 0:00:03  lr: 0.000081  loss: 0.8250 (0.8209)  time: 0.7598  data: 0.0001  max mem: 15689
[08:56:40.616482] Epoch: [12]  [344/345]  eta: 0:00:00  lr: 0.000081  loss: 0.8277 (0.8210)  time: 0.7597  data: 0.0001  max mem: 15689
[08:56:40.681294] Epoch: [12] Total time: 0:04:22 (0.7615 s / it)
[08:56:40.681501] Averaged stats: lr: 0.000081  loss: 0.8277 (0.8210)
[08:56:41.023826] Test:  [  0/345]  eta: 0:01:56  loss: 0.8481 (0.8481)  time: 0.3389  data: 0.1551  max mem: 15689
[08:56:42.885070] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8146 (0.8144)  time: 0.1999  data: 0.0142  max mem: 15689
[08:56:44.747171] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8031 (0.8073)  time: 0.1861  data: 0.0001  max mem: 15689
[08:56:46.613580] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7960 (0.8055)  time: 0.1864  data: 0.0001  max mem: 15689
[08:56:48.482543] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7989 (0.8046)  time: 0.1867  data: 0.0001  max mem: 15689
[08:56:50.355960] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7921 (0.8026)  time: 0.1871  data: 0.0001  max mem: 15689
[08:56:52.232498] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8036 (0.8036)  time: 0.1874  data: 0.0001  max mem: 15689
[08:56:54.110850] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8036 (0.8033)  time: 0.1877  data: 0.0001  max mem: 15689
[08:56:55.996059] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7979 (0.8037)  time: 0.1881  data: 0.0001  max mem: 15689
[08:56:57.883850] Test:  [ 90/345]  eta: 0:00:48  loss: 0.8024 (0.8033)  time: 0.1886  data: 0.0001  max mem: 15689
[08:56:59.775202] Test:  [100/345]  eta: 0:00:46  loss: 0.7992 (0.8023)  time: 0.1889  data: 0.0001  max mem: 15689
[08:57:01.670954] Test:  [110/345]  eta: 0:00:44  loss: 0.7951 (0.8022)  time: 0.1893  data: 0.0001  max mem: 15689
[08:57:03.569993] Test:  [120/345]  eta: 0:00:42  loss: 0.8028 (0.8026)  time: 0.1897  data: 0.0001  max mem: 15689
[08:57:05.473232] Test:  [130/345]  eta: 0:00:40  loss: 0.8041 (0.8026)  time: 0.1901  data: 0.0001  max mem: 15689
[08:57:07.377905] Test:  [140/345]  eta: 0:00:38  loss: 0.7991 (0.8026)  time: 0.1903  data: 0.0001  max mem: 15689
[08:57:09.286936] Test:  [150/345]  eta: 0:00:36  loss: 0.7963 (0.8022)  time: 0.1906  data: 0.0001  max mem: 15689
[08:57:11.198546] Test:  [160/345]  eta: 0:00:35  loss: 0.7963 (0.8021)  time: 0.1910  data: 0.0001  max mem: 15689
[08:57:13.114375] Test:  [170/345]  eta: 0:00:33  loss: 0.7981 (0.8019)  time: 0.1913  data: 0.0001  max mem: 15689
[08:57:15.034349] Test:  [180/345]  eta: 0:00:31  loss: 0.7936 (0.8015)  time: 0.1917  data: 0.0001  max mem: 15689
[08:57:16.957165] Test:  [190/345]  eta: 0:00:29  loss: 0.7936 (0.8010)  time: 0.1921  data: 0.0001  max mem: 15689
[08:57:18.883715] Test:  [200/345]  eta: 0:00:27  loss: 0.8010 (0.8012)  time: 0.1924  data: 0.0001  max mem: 15689
[08:57:20.813273] Test:  [210/345]  eta: 0:00:25  loss: 0.8025 (0.8010)  time: 0.1928  data: 0.0001  max mem: 15689
[08:57:22.747302] Test:  [220/345]  eta: 0:00:23  loss: 0.7994 (0.8012)  time: 0.1931  data: 0.0001  max mem: 15689
[08:57:24.683952] Test:  [230/345]  eta: 0:00:21  loss: 0.7985 (0.8012)  time: 0.1935  data: 0.0001  max mem: 15689
[08:57:26.624988] Test:  [240/345]  eta: 0:00:20  loss: 0.7985 (0.8010)  time: 0.1938  data: 0.0001  max mem: 15689
[08:57:28.569492] Test:  [250/345]  eta: 0:00:18  loss: 0.8022 (0.8015)  time: 0.1942  data: 0.0001  max mem: 15689
[08:57:30.516907] Test:  [260/345]  eta: 0:00:16  loss: 0.8009 (0.8011)  time: 0.1945  data: 0.0001  max mem: 15689
[08:57:32.467034] Test:  [270/345]  eta: 0:00:14  loss: 0.7947 (0.8012)  time: 0.1948  data: 0.0001  max mem: 15689
[08:57:34.422093] Test:  [280/345]  eta: 0:00:12  loss: 0.8062 (0.8016)  time: 0.1952  data: 0.0001  max mem: 15689
[08:57:36.379407] Test:  [290/345]  eta: 0:00:10  loss: 0.8092 (0.8018)  time: 0.1956  data: 0.0001  max mem: 15689
[08:57:38.341640] Test:  [300/345]  eta: 0:00:08  loss: 0.7970 (0.8018)  time: 0.1959  data: 0.0001  max mem: 15689
[08:57:40.306714] Test:  [310/345]  eta: 0:00:06  loss: 0.7970 (0.8017)  time: 0.1963  data: 0.0001  max mem: 15689
[08:57:42.275083] Test:  [320/345]  eta: 0:00:04  loss: 0.7960 (0.8015)  time: 0.1966  data: 0.0001  max mem: 15689
[08:57:44.246803] Test:  [330/345]  eta: 0:00:02  loss: 0.7906 (0.8012)  time: 0.1970  data: 0.0001  max mem: 15689
[08:57:46.219188] Test:  [340/345]  eta: 0:00:00  loss: 0.7969 (0.8012)  time: 0.1972  data: 0.0001  max mem: 15689
[08:57:47.010535] Test:  [344/345]  eta: 0:00:00  loss: 0.7980 (0.8013)  time: 0.1973  data: 0.0001  max mem: 15689
[08:57:47.069011] Test: Total time: 0:01:06 (0.1924 s / it)
[08:57:57.733122] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8418 (0.8418)  time: 0.3279  data: 0.1462  max mem: 15689
[08:57:59.570496] Test:  [10/57]  eta: 0:00:09  loss: 0.8663 (0.8806)  time: 0.1968  data: 0.0133  max mem: 15689
[08:58:01.413477] Test:  [20/57]  eta: 0:00:07  loss: 0.8782 (0.8754)  time: 0.1840  data: 0.0001  max mem: 15689
[08:58:03.260824] Test:  [30/57]  eta: 0:00:05  loss: 0.7857 (0.8395)  time: 0.1845  data: 0.0001  max mem: 15689
[08:58:05.112780] Test:  [40/57]  eta: 0:00:03  loss: 0.7645 (0.8193)  time: 0.1849  data: 0.0001  max mem: 15689
[08:58:06.969996] Test:  [50/57]  eta: 0:00:01  loss: 0.7496 (0.8124)  time: 0.1854  data: 0.0001  max mem: 15689
[08:58:07.971355] Test:  [56/57]  eta: 0:00:00  loss: 0.8028 (0.8187)  time: 0.1800  data: 0.0001  max mem: 15689
[08:58:08.030021] Test: Total time: 0:00:10 (0.1864 s / it)
[08:58:09.875339] Dice score of the network on the train images: 0.721125, val images: 0.797650
[08:58:09.875577] saving best_rec_model_0 @ epoch 12
[08:58:10.965739] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[08:58:11.871293] Epoch: [13]  [  0/345]  eta: 0:05:12  lr: 0.000081  loss: 0.8197 (0.8197)  time: 0.9046  data: 0.1497  max mem: 15689
[08:58:26.998280] Epoch: [13]  [ 20/345]  eta: 0:04:08  lr: 0.000082  loss: 0.8105 (0.8109)  time: 0.7563  data: 0.0001  max mem: 15689
[08:58:42.192596] Epoch: [13]  [ 40/345]  eta: 0:03:52  lr: 0.000082  loss: 0.8028 (0.8099)  time: 0.7597  data: 0.0001  max mem: 15689
[08:58:57.410776] Epoch: [13]  [ 60/345]  eta: 0:03:36  lr: 0.000082  loss: 0.8176 (0.8130)  time: 0.7609  data: 0.0001  max mem: 15689
[08:59:12.651602] Epoch: [13]  [ 80/345]  eta: 0:03:21  lr: 0.000083  loss: 0.8137 (0.8133)  time: 0.7620  data: 0.0001  max mem: 15689
[08:59:28.054443] Epoch: [13]  [100/345]  eta: 0:03:06  lr: 0.000083  loss: 0.8118 (0.8139)  time: 0.7701  data: 0.0001  max mem: 15689
[08:59:43.328291] Epoch: [13]  [120/345]  eta: 0:02:51  lr: 0.000083  loss: 0.8034 (0.8122)  time: 0.7636  data: 0.0001  max mem: 15689
[08:59:58.586724] Epoch: [13]  [140/345]  eta: 0:02:36  lr: 0.000084  loss: 0.8120 (0.8120)  time: 0.7629  data: 0.0001  max mem: 15689
[09:00:13.853315] Epoch: [13]  [160/345]  eta: 0:02:21  lr: 0.000084  loss: 0.8189 (0.8133)  time: 0.7633  data: 0.0001  max mem: 15689
[09:00:29.126846] Epoch: [13]  [180/345]  eta: 0:02:05  lr: 0.000085  loss: 0.8120 (0.8134)  time: 0.7636  data: 0.0001  max mem: 15689
[09:00:44.381116] Epoch: [13]  [200/345]  eta: 0:01:50  lr: 0.000085  loss: 0.8096 (0.8134)  time: 0.7627  data: 0.0001  max mem: 15689
[09:00:59.630031] Epoch: [13]  [220/345]  eta: 0:01:35  lr: 0.000085  loss: 0.8110 (0.8134)  time: 0.7624  data: 0.0001  max mem: 15689
[09:01:14.872635] Epoch: [13]  [240/345]  eta: 0:01:20  lr: 0.000086  loss: 0.8167 (0.8137)  time: 0.7621  data: 0.0001  max mem: 15689
[09:01:30.104805] Epoch: [13]  [260/345]  eta: 0:01:04  lr: 0.000086  loss: 0.8018 (0.8130)  time: 0.7616  data: 0.0001  max mem: 15689
[09:01:45.345668] Epoch: [13]  [280/345]  eta: 0:00:49  lr: 0.000086  loss: 0.7964 (0.8120)  time: 0.7620  data: 0.0001  max mem: 15689
[09:02:00.575471] Epoch: [13]  [300/345]  eta: 0:00:34  lr: 0.000087  loss: 0.8108 (0.8118)  time: 0.7614  data: 0.0001  max mem: 15689

[09:02:15.807836] Epoch: [13]  [320/345]  eta: 0:00:19  lr: 0.000087  loss: 0.8067 (0.8115)  time: 0.7616  data: 0.0001  max mem: 15689
[09:02:31.122330] Epoch: [13]  [340/345]  eta: 0:00:03  lr: 0.000087  loss: 0.7987 (0.8109)  time: 0.7657  data: 0.0001  max mem: 15689
[09:02:34.163800] Epoch: [13]  [344/345]  eta: 0:00:00  lr: 0.000087  loss: 0.8040 (0.8108)  time: 0.7653  data: 0.0001  max mem: 15689
[09:02:34.228984] Epoch: [13] Total time: 0:04:23 (0.7631 s / it)
[09:02:34.229417] Averaged stats: lr: 0.000087  loss: 0.8040 (0.8108)
[09:02:34.571088] Test:  [  0/345]  eta: 0:01:56  loss: 0.7673 (0.7673)  time: 0.3369  data: 0.1535  max mem: 15689
[09:02:36.429057] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7688 (0.7737)  time: 0.1995  data: 0.0140  max mem: 15689
[09:02:38.291441] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7748 (0.7767)  time: 0.1860  data: 0.0001  max mem: 15689
[09:02:40.156533] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7762 (0.7786)  time: 0.1863  data: 0.0001  max mem: 15689
[09:02:42.025919] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7696 (0.7766)  time: 0.1867  data: 0.0001  max mem: 15689
[09:02:43.899488] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7696 (0.7785)  time: 0.1871  data: 0.0001  max mem: 15689
[09:02:45.775944] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7728 (0.7774)  time: 0.1875  data: 0.0001  max mem: 15689
[09:02:47.655666] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7703 (0.7772)  time: 0.1878  data: 0.0001  max mem: 15689
[09:02:49.539939] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7821 (0.7794)  time: 0.1882  data: 0.0001  max mem: 15689
[09:02:51.427044] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7739 (0.7774)  time: 0.1885  data: 0.0001  max mem: 15689
[09:02:53.318149] Test:  [100/345]  eta: 0:00:46  loss: 0.7713 (0.7772)  time: 0.1889  data: 0.0001  max mem: 15689
[09:02:55.212066] Test:  [110/345]  eta: 0:00:44  loss: 0.7732 (0.7771)  time: 0.1892  data: 0.0001  max mem: 15689
[09:02:57.111000] Test:  [120/345]  eta: 0:00:42  loss: 0.7769 (0.7775)  time: 0.1896  data: 0.0001  max mem: 15689
[09:02:59.013677] Test:  [130/345]  eta: 0:00:40  loss: 0.7829 (0.7782)  time: 0.1900  data: 0.0001  max mem: 15689
[09:03:00.917317] Test:  [140/345]  eta: 0:00:38  loss: 0.7838 (0.7776)  time: 0.1903  data: 0.0001  max mem: 15689
[09:03:02.824702] Test:  [150/345]  eta: 0:00:36  loss: 0.7838 (0.7780)  time: 0.1905  data: 0.0001  max mem: 15689
[09:03:04.737951] Test:  [160/345]  eta: 0:00:35  loss: 0.7746 (0.7776)  time: 0.1910  data: 0.0001  max mem: 15689
[09:03:06.654193] Test:  [170/345]  eta: 0:00:33  loss: 0.7729 (0.7775)  time: 0.1914  data: 0.0001  max mem: 15689
[09:03:08.574230] Test:  [180/345]  eta: 0:00:31  loss: 0.7721 (0.7773)  time: 0.1918  data: 0.0001  max mem: 15689
[09:03:10.496973] Test:  [190/345]  eta: 0:00:29  loss: 0.7779 (0.7777)  time: 0.1921  data: 0.0001  max mem: 15689
[09:03:12.423410] Test:  [200/345]  eta: 0:00:27  loss: 0.7790 (0.7777)  time: 0.1924  data: 0.0001  max mem: 15689
[09:03:14.351987] Test:  [210/345]  eta: 0:00:25  loss: 0.7715 (0.7774)  time: 0.1927  data: 0.0001  max mem: 15689
[09:03:16.285645] Test:  [220/345]  eta: 0:00:23  loss: 0.7711 (0.7775)  time: 0.1931  data: 0.0001  max mem: 15689
[09:03:18.223137] Test:  [230/345]  eta: 0:00:21  loss: 0.7762 (0.7774)  time: 0.1935  data: 0.0001  max mem: 15689
[09:03:20.163175] Test:  [240/345]  eta: 0:00:20  loss: 0.7716 (0.7774)  time: 0.1938  data: 0.0001  max mem: 15689
[09:03:22.107852] Test:  [250/345]  eta: 0:00:18  loss: 0.7699 (0.7770)  time: 0.1942  data: 0.0001  max mem: 15689
[09:03:24.055119] Test:  [260/345]  eta: 0:00:16  loss: 0.7681 (0.7773)  time: 0.1945  data: 0.0001  max mem: 15689
[09:03:26.004622] Test:  [270/345]  eta: 0:00:14  loss: 0.7792 (0.7773)  time: 0.1948  data: 0.0001  max mem: 15689
[09:03:27.958670] Test:  [280/345]  eta: 0:00:12  loss: 0.7723 (0.7772)  time: 0.1951  data: 0.0001  max mem: 15689
[09:03:29.915898] Test:  [290/345]  eta: 0:00:10  loss: 0.7725 (0.7769)  time: 0.1955  data: 0.0001  max mem: 15689
[09:03:31.877993] Test:  [300/345]  eta: 0:00:08  loss: 0.7725 (0.7769)  time: 0.1959  data: 0.0001  max mem: 15689
[09:03:33.843135] Test:  [310/345]  eta: 0:00:06  loss: 0.7683 (0.7768)  time: 0.1963  data: 0.0001  max mem: 15689
[09:03:35.811376] Test:  [320/345]  eta: 0:00:04  loss: 0.7774 (0.7769)  time: 0.1966  data: 0.0001  max mem: 15689
[09:03:37.782143] Test:  [330/345]  eta: 0:00:02  loss: 0.7757 (0.7768)  time: 0.1969  data: 0.0001  max mem: 15689
[09:03:39.756726] Test:  [340/345]  eta: 0:00:00  loss: 0.7730 (0.7765)  time: 0.1972  data: 0.0001  max mem: 15689
[09:03:40.547804] Test:  [344/345]  eta: 0:00:00  loss: 0.7627 (0.7764)  time: 0.1974  data: 0.0001  max mem: 15689
[09:03:40.609150] Test: Total time: 0:01:06 (0.1924 s / it)
[09:03:51.313017] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8599 (0.8599)  time: 0.3231  data: 0.1411  max mem: 15689
[09:03:53.151056] Test:  [10/57]  eta: 0:00:09  loss: 0.8642 (0.8748)  time: 0.1964  data: 0.0129  max mem: 15689
[09:03:54.994229] Test:  [20/57]  eta: 0:00:07  loss: 0.8701 (0.8673)  time: 0.1840  data: 0.0001  max mem: 15689
[09:03:56.841370] Test:  [30/57]  eta: 0:00:05  loss: 0.7711 (0.8303)  time: 0.1845  data: 0.0001  max mem: 15689
[09:03:58.692654] Test:  [40/57]  eta: 0:00:03  loss: 0.7472 (0.8095)  time: 0.1849  data: 0.0001  max mem: 15689
[09:04:00.549225] Test:  [50/57]  eta: 0:00:01  loss: 0.7416 (0.8015)  time: 0.1853  data: 0.0001  max mem: 15689
[09:04:01.549455] Test:  [56/57]  eta: 0:00:00  loss: 0.7882 (0.8069)  time: 0.1799  data: 0.0001  max mem: 15689
[09:04:01.610636] Test: Total time: 0:00:10 (0.1863 s / it)
[09:04:03.378829] Dice score of the network on the train images: 0.747966, val images: 0.803799
[09:04:03.383657] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:04:04.284237] Epoch: [14]  [  0/345]  eta: 0:05:10  lr: 0.000087  loss: 0.8218 (0.8218)  time: 0.8995  data: 0.1439  max mem: 15689
[09:04:19.401942] Epoch: [14]  [ 20/345]  eta: 0:04:07  lr: 0.000088  loss: 0.7923 (0.7994)  time: 0.7558  data: 0.0001  max mem: 15689
[09:04:34.569961] Epoch: [14]  [ 40/345]  eta: 0:03:51  lr: 0.000088  loss: 0.8016 (0.8025)  time: 0.7584  data: 0.0001  max mem: 15689
[09:04:49.774628] Epoch: [14]  [ 60/345]  eta: 0:03:36  lr: 0.000089  loss: 0.8076 (0.8036)  time: 0.7602  data: 0.0001  max mem: 15689
[09:05:04.991740] Epoch: [14]  [ 80/345]  eta: 0:03:21  lr: 0.000089  loss: 0.8016 (0.8037)  time: 0.7608  data: 0.0001  max mem: 15689
[09:05:20.226744] Epoch: [14]  [100/345]  eta: 0:03:06  lr: 0.000089  loss: 0.8196 (0.8064)  time: 0.7617  data: 0.0001  max mem: 15689
[09:05:35.464703] Epoch: [14]  [120/345]  eta: 0:02:51  lr: 0.000090  loss: 0.8178 (0.8087)  time: 0.7619  data: 0.0001  max mem: 15689
[09:05:50.696179] Epoch: [14]  [140/345]  eta: 0:02:36  lr: 0.000090  loss: 0.8088 (0.8094)  time: 0.7615  data: 0.0001  max mem: 15689
[09:06:05.917451] Epoch: [14]  [160/345]  eta: 0:02:20  lr: 0.000090  loss: 0.8088 (0.8099)  time: 0.7610  data: 0.0001  max mem: 15689
[09:06:21.140808] Epoch: [14]  [180/345]  eta: 0:02:05  lr: 0.000091  loss: 0.8053 (0.8097)  time: 0.7611  data: 0.0001  max mem: 15689
[09:06:36.356870] Epoch: [14]  [200/345]  eta: 0:01:50  lr: 0.000091  loss: 0.7993 (0.8091)  time: 0.7608  data: 0.0001  max mem: 15689
[09:06:51.574601] Epoch: [14]  [220/345]  eta: 0:01:35  lr: 0.000091  loss: 0.8092 (0.8089)  time: 0.7608  data: 0.0001  max mem: 15689
[09:07:06.792493] Epoch: [14]  [240/345]  eta: 0:01:19  lr: 0.000092  loss: 0.8072 (0.8092)  time: 0.7609  data: 0.0001  max mem: 15689
[09:07:21.996256] Epoch: [14]  [260/345]  eta: 0:01:04  lr: 0.000092  loss: 0.7887 (0.8082)  time: 0.7602  data: 0.0001  max mem: 15689
[09:07:37.196426] Epoch: [14]  [280/345]  eta: 0:00:49  lr: 0.000093  loss: 0.7959 (0.8075)  time: 0.7600  data: 0.0001  max mem: 15689
[09:07:52.392381] Epoch: [14]  [300/345]  eta: 0:00:34  lr: 0.000093  loss: 0.8125 (0.8077)  time: 0.7598  data: 0.0001  max mem: 15689
[09:08:07.590102] Epoch: [14]  [320/345]  eta: 0:00:19  lr: 0.000093  loss: 0.8065 (0.8078)  time: 0.7598  data: 0.0001  max mem: 15689
[09:08:22.786705] Epoch: [14]  [340/345]  eta: 0:00:03  lr: 0.000094  loss: 0.7862 (0.8069)  time: 0.7598  data: 0.0001  max mem: 15689
[09:08:25.826392] Epoch: [14]  [344/345]  eta: 0:00:00  lr: 0.000094  loss: 0.7869 (0.8067)  time: 0.7599  data: 0.0001  max mem: 15689
[09:08:25.889165] Epoch: [14] Total time: 0:04:22 (0.7609 s / it)
[09:08:25.889617] Averaged stats: lr: 0.000094  loss: 0.7869 (0.8067)
[09:08:26.225897] Test:  [  0/345]  eta: 0:01:54  loss: 0.7684 (0.7684)  time: 0.3327  data: 0.1487  max mem: 15689
[09:08:28.083994] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7595 (0.7637)  time: 0.1991  data: 0.0136  max mem: 15689
[09:08:29.943900] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7606 (0.7651)  time: 0.1858  data: 0.0001  max mem: 15689
[09:08:31.810270] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7611 (0.7628)  time: 0.1863  data: 0.0001  max mem: 15689
[09:08:33.679482] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7576 (0.7629)  time: 0.1867  data: 0.0001  max mem: 15689
[09:08:35.552749] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7561 (0.7626)  time: 0.1871  data: 0.0001  max mem: 15689
[09:08:37.428857] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7589 (0.7640)  time: 0.1874  data: 0.0001  max mem: 15689
[09:08:39.309829] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7656 (0.7637)  time: 0.1878  data: 0.0001  max mem: 15689
[09:08:41.195922] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7659 (0.7651)  time: 0.1883  data: 0.0001  max mem: 15689
[09:08:43.084375] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7649 (0.7651)  time: 0.1887  data: 0.0001  max mem: 15689
[09:08:44.975340] Test:  [100/345]  eta: 0:00:46  loss: 0.7630 (0.7661)  time: 0.1889  data: 0.0001  max mem: 15689
[09:08:46.869538] Test:  [110/345]  eta: 0:00:44  loss: 0.7647 (0.7661)  time: 0.1892  data: 0.0001  max mem: 15689
[09:08:48.768047] Test:  [120/345]  eta: 0:00:42  loss: 0.7611 (0.7658)  time: 0.1896  data: 0.0001  max mem: 15689
[09:08:50.670777] Test:  [130/345]  eta: 0:00:40  loss: 0.7584 (0.7658)  time: 0.1900  data: 0.0001  max mem: 15689
[09:08:52.575010] Test:  [140/345]  eta: 0:00:38  loss: 0.7692 (0.7658)  time: 0.1903  data: 0.0001  max mem: 15689
[09:08:54.484183] Test:  [150/345]  eta: 0:00:36  loss: 0.7599 (0.7652)  time: 0.1906  data: 0.0001  max mem: 15689
[09:08:56.395347] Test:  [160/345]  eta: 0:00:35  loss: 0.7588 (0.7655)  time: 0.1910  data: 0.0001  max mem: 15689
[09:08:58.310166] Test:  [170/345]  eta: 0:00:33  loss: 0.7650 (0.7651)  time: 0.1912  data: 0.0001  max mem: 15689
[09:09:00.228170] Test:  [180/345]  eta: 0:00:31  loss: 0.7628 (0.7649)  time: 0.1916  data: 0.0001  max mem: 15689
[09:09:02.151123] Test:  [190/345]  eta: 0:00:29  loss: 0.7653 (0.7650)  time: 0.1920  data: 0.0001  max mem: 15689
[09:09:04.077396] Test:  [200/345]  eta: 0:00:27  loss: 0.7727 (0.7649)  time: 0.1924  data: 0.0001  max mem: 15689
[09:09:06.005814] Test:  [210/345]  eta: 0:00:25  loss: 0.7594 (0.7648)  time: 0.1927  data: 0.0001  max mem: 15689
[09:09:07.938848] Test:  [220/345]  eta: 0:00:23  loss: 0.7596 (0.7648)  time: 0.1930  data: 0.0001  max mem: 15689
[09:09:09.874478] Test:  [230/345]  eta: 0:00:21  loss: 0.7571 (0.7643)  time: 0.1934  data: 0.0001  max mem: 15689
[09:09:11.815666] Test:  [240/345]  eta: 0:00:20  loss: 0.7608 (0.7648)  time: 0.1938  data: 0.0001  max mem: 15689
[09:09:13.758005] Test:  [250/345]  eta: 0:00:18  loss: 0.7701 (0.7648)  time: 0.1941  data: 0.0001  max mem: 15689
[09:09:15.704868] Test:  [260/345]  eta: 0:00:16  loss: 0.7705 (0.7651)  time: 0.1944  data: 0.0001  max mem: 15689
[09:09:17.655061] Test:  [270/345]  eta: 0:00:14  loss: 0.7667 (0.7652)  time: 0.1948  data: 0.0001  max mem: 15689
[09:09:19.608238] Test:  [280/345]  eta: 0:00:12  loss: 0.7607 (0.7653)  time: 0.1951  data: 0.0001  max mem: 15689
[09:09:21.565621] Test:  [290/345]  eta: 0:00:10  loss: 0.7547 (0.7651)  time: 0.1955  data: 0.0001  max mem: 15689
[09:09:23.527012] Test:  [300/345]  eta: 0:00:08  loss: 0.7634 (0.7652)  time: 0.1959  data: 0.0001  max mem: 15689
[09:09:25.492736] Test:  [310/345]  eta: 0:00:06  loss: 0.7657 (0.7651)  time: 0.1963  data: 0.0001  max mem: 15689
[09:09:27.461593] Test:  [320/345]  eta: 0:00:04  loss: 0.7637 (0.7651)  time: 0.1967  data: 0.0001  max mem: 15689
[09:09:29.433462] Test:  [330/345]  eta: 0:00:02  loss: 0.7691 (0.7654)  time: 0.1970  data: 0.0001  max mem: 15689
[09:09:31.407421] Test:  [340/345]  eta: 0:00:00  loss: 0.7614 (0.7651)  time: 0.1972  data: 0.0001  max mem: 15689
[09:09:32.198783] Test:  [344/345]  eta: 0:00:00  loss: 0.7653 (0.7652)  time: 0.1974  data: 0.0001  max mem: 15689
[09:09:32.258801] Test: Total time: 0:01:06 (0.1924 s / it)
[09:09:42.877704] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8366 (0.8366)  time: 0.3248  data: 0.1428  max mem: 15689
[09:09:44.716169] Test:  [10/57]  eta: 0:00:09  loss: 0.8434 (0.8629)  time: 0.1966  data: 0.0130  max mem: 15689
[09:09:46.560072] Test:  [20/57]  eta: 0:00:07  loss: 0.8545 (0.8537)  time: 0.1841  data: 0.0001  max mem: 15689
[09:09:48.406307] Test:  [30/57]  eta: 0:00:05  loss: 0.7666 (0.8205)  time: 0.1845  data: 0.0001  max mem: 15689
[09:09:50.257546] Test:  [40/57]  eta: 0:00:03  loss: 0.7449 (0.8025)  time: 0.1848  data: 0.0001  max mem: 15689
[09:09:52.114571] Test:  [50/57]  eta: 0:00:01  loss: 0.7345 (0.7954)  time: 0.1854  data: 0.0001  max mem: 15689
[09:09:53.115044] Test:  [56/57]  eta: 0:00:00  loss: 0.7723 (0.8014)  time: 0.1799  data: 0.0001  max mem: 15689
[09:09:53.171360] Test: Total time: 0:00:10 (0.1863 s / it)
[09:09:55.052680] Dice score of the network on the train images: 0.752484, val images: 0.805983
[09:09:55.052920] saving best_dice_model_0 @ epoch 14
[09:09:56.154580] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:09:57.056872] Epoch: [15]  [  0/345]  eta: 0:05:10  lr: 0.000094  loss: 0.7808 (0.7808)  time: 0.9011  data: 0.1458  max mem: 15689
[09:10:12.166096] Epoch: [15]  [ 20/345]  eta: 0:04:07  lr: 0.000094  loss: 0.7872 (0.7861)  time: 0.7554  data: 0.0001  max mem: 15689
[09:10:27.338851] Epoch: [15]  [ 40/345]  eta: 0:03:51  lr: 0.000094  loss: 0.7843 (0.7910)  time: 0.7586  data: 0.0001  max mem: 15689
[09:10:42.543149] Epoch: [15]  [ 60/345]  eta: 0:03:36  lr: 0.000095  loss: 0.7987 (0.7952)  time: 0.7602  data: 0.0001  max mem: 15689
[09:10:57.875782] Epoch: [15]  [ 80/345]  eta: 0:03:21  lr: 0.000095  loss: 0.8158 (0.8012)  time: 0.7666  data: 0.0001  max mem: 15689
[09:11:13.108821] Epoch: [15]  [100/345]  eta: 0:03:06  lr: 0.000096  loss: 0.7970 (0.8008)  time: 0.7616  data: 0.0001  max mem: 15689
[09:11:28.342352] Epoch: [15]  [120/345]  eta: 0:02:51  lr: 0.000096  loss: 0.8057 (0.8018)  time: 0.7616  data: 0.0001  max mem: 15689
[09:11:43.572850] Epoch: [15]  [140/345]  eta: 0:02:36  lr: 0.000096  loss: 0.8178 (0.8063)  time: 0.7615  data: 0.0001  max mem: 15689
[09:11:58.802515] Epoch: [15]  [160/345]  eta: 0:02:20  lr: 0.000097  loss: 0.8283 (0.8095)  time: 0.7614  data: 0.0001  max mem: 15689
[09:12:14.028842] Epoch: [15]  [180/345]  eta: 0:02:05  lr: 0.000097  loss: 0.8222 (0.8117)  time: 0.7613  data: 0.0001  max mem: 15689
[09:12:29.242537] Epoch: [15]  [200/345]  eta: 0:01:50  lr: 0.000097  loss: 0.8104 (0.8109)  time: 0.7606  data: 0.0001  max mem: 15689
[09:12:44.447320] Epoch: [15]  [220/345]  eta: 0:01:35  lr: 0.000098  loss: 0.7938 (0.8096)  time: 0.7602  data: 0.0001  max mem: 15689
[09:12:59.657606] Epoch: [15]  [240/345]  eta: 0:01:19  lr: 0.000098  loss: 0.8058 (0.8090)  time: 0.7605  data: 0.0001  max mem: 15689
[09:13:14.846586] Epoch: [15]  [260/345]  eta: 0:01:04  lr: 0.000098  loss: 0.7939 (0.8084)  time: 0.7594  data: 0.0001  max mem: 15689
[09:13:30.044921] Epoch: [15]  [280/345]  eta: 0:00:49  lr: 0.000099  loss: 0.7987 (0.8078)  time: 0.7599  data: 0.0001  max mem: 15689
[09:13:45.246038] Epoch: [15]  [300/345]  eta: 0:00:34  lr: 0.000099  loss: 0.7960 (0.8073)  time: 0.7600  data: 0.0001  max mem: 15689
[09:14:00.433628] Epoch: [15]  [320/345]  eta: 0:00:19  lr: 0.000100  loss: 0.7892 (0.8067)  time: 0.7593  data: 0.0001  max mem: 15689
[09:14:15.626927] Epoch: [15]  [340/345]  eta: 0:00:03  lr: 0.000100  loss: 0.7939 (0.8061)  time: 0.7596  data: 0.0001  max mem: 15689
[09:14:18.668885] Epoch: [15]  [344/345]  eta: 0:00:00  lr: 0.000100  loss: 0.7939 (0.8059)  time: 0.7597  data: 0.0001  max mem: 15689
[09:14:18.734133] Epoch: [15] Total time: 0:04:22 (0.7611 s / it)
[09:14:18.734518] Averaged stats: lr: 0.000100  loss: 0.7939 (0.8059)
[09:14:19.078288] Test:  [  0/345]  eta: 0:01:57  loss: 0.7550 (0.7550)  time: 0.3404  data: 0.1565  max mem: 15689
[09:14:20.937053] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7703 (0.7700)  time: 0.1999  data: 0.0143  max mem: 15689
[09:14:22.799743] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7663 (0.7647)  time: 0.1860  data: 0.0001  max mem: 15689
[09:14:24.666349] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7606 (0.7641)  time: 0.1864  data: 0.0001  max mem: 15689
[09:14:26.534684] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7644 (0.7642)  time: 0.1867  data: 0.0001  max mem: 15689
[09:14:28.407179] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7644 (0.7656)  time: 0.1870  data: 0.0001  max mem: 15689
[09:14:30.284542] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7637 (0.7661)  time: 0.1874  data: 0.0001  max mem: 15689
[09:14:32.163171] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7661 (0.7662)  time: 0.1877  data: 0.0001  max mem: 15689
[09:14:34.049040] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7631 (0.7654)  time: 0.1882  data: 0.0001  max mem: 15689
[09:14:35.936308] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7569 (0.7652)  time: 0.1886  data: 0.0001  max mem: 15689
[09:14:37.827872] Test:  [100/345]  eta: 0:00:46  loss: 0.7629 (0.7658)  time: 0.1889  data: 0.0001  max mem: 15689
[09:14:39.722493] Test:  [110/345]  eta: 0:00:44  loss: 0.7787 (0.7666)  time: 0.1893  data: 0.0001  max mem: 15689
[09:14:41.621005] Test:  [120/345]  eta: 0:00:42  loss: 0.7732 (0.7666)  time: 0.1896  data: 0.0001  max mem: 15689
[09:14:43.524783] Test:  [130/345]  eta: 0:00:40  loss: 0.7557 (0.7663)  time: 0.1901  data: 0.0001  max mem: 15689
[09:14:45.429655] Test:  [140/345]  eta: 0:00:38  loss: 0.7647 (0.7669)  time: 0.1904  data: 0.0001  max mem: 15689
[09:14:47.337197] Test:  [150/345]  eta: 0:00:36  loss: 0.7717 (0.7668)  time: 0.1906  data: 0.0001  max mem: 15689
[09:14:49.250374] Test:  [160/345]  eta: 0:00:35  loss: 0.7707 (0.7674)  time: 0.1910  data: 0.0001  max mem: 15689
[09:14:51.164455] Test:  [170/345]  eta: 0:00:33  loss: 0.7709 (0.7671)  time: 0.1913  data: 0.0001  max mem: 15689
[09:14:53.084425] Test:  [180/345]  eta: 0:00:31  loss: 0.7618 (0.7670)  time: 0.1917  data: 0.0001  max mem: 15689
[09:14:55.007807] Test:  [190/345]  eta: 0:00:29  loss: 0.7723 (0.7674)  time: 0.1921  data: 0.0001  max mem: 15689
[09:14:56.935126] Test:  [200/345]  eta: 0:00:27  loss: 0.7700 (0.7672)  time: 0.1925  data: 0.0001  max mem: 15689
[09:14:58.864634] Test:  [210/345]  eta: 0:00:25  loss: 0.7553 (0.7666)  time: 0.1928  data: 0.0001  max mem: 15689
[09:15:00.799170] Test:  [220/345]  eta: 0:00:23  loss: 0.7569 (0.7666)  time: 0.1932  data: 0.0001  max mem: 15689
[09:15:02.736536] Test:  [230/345]  eta: 0:00:21  loss: 0.7629 (0.7666)  time: 0.1935  data: 0.0001  max mem: 15689
[09:15:04.677185] Test:  [240/345]  eta: 0:00:20  loss: 0.7657 (0.7668)  time: 0.1939  data: 0.0001  max mem: 15689
[09:15:06.621503] Test:  [250/345]  eta: 0:00:18  loss: 0.7705 (0.7669)  time: 0.1942  data: 0.0001  max mem: 15689
[09:15:08.567998] Test:  [260/345]  eta: 0:00:16  loss: 0.7565 (0.7665)  time: 0.1945  data: 0.0001  max mem: 15689
[09:15:10.517924] Test:  [270/345]  eta: 0:00:14  loss: 0.7565 (0.7665)  time: 0.1948  data: 0.0001  max mem: 15689
[09:15:12.471615] Test:  [280/345]  eta: 0:00:12  loss: 0.7746 (0.7668)  time: 0.1951  data: 0.0001  max mem: 15689
[09:15:14.428915] Test:  [290/345]  eta: 0:00:10  loss: 0.7746 (0.7673)  time: 0.1955  data: 0.0001  max mem: 15689
[09:15:16.389760] Test:  [300/345]  eta: 0:00:08  loss: 0.7639 (0.7671)  time: 0.1959  data: 0.0001  max mem: 15689
[09:15:18.353401] Test:  [310/345]  eta: 0:00:06  loss: 0.7621 (0.7669)  time: 0.1962  data: 0.0001  max mem: 15689
[09:15:20.322844] Test:  [320/345]  eta: 0:00:04  loss: 0.7678 (0.7668)  time: 0.1966  data: 0.0001  max mem: 15689
[09:15:22.295089] Test:  [330/345]  eta: 0:00:02  loss: 0.7708 (0.7668)  time: 0.1970  data: 0.0001  max mem: 15689
[09:15:24.268753] Test:  [340/345]  eta: 0:00:00  loss: 0.7742 (0.7670)  time: 0.1972  data: 0.0001  max mem: 15689
[09:15:25.059756] Test:  [344/345]  eta: 0:00:00  loss: 0.7742 (0.7670)  time: 0.1974  data: 0.0001  max mem: 15689
[09:15:25.096712] Test: Total time: 0:01:06 (0.1923 s / it)
[09:15:35.625152] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8514 (0.8514)  time: 0.3229  data: 0.1408  max mem: 15689
[09:15:37.463608] Test:  [10/57]  eta: 0:00:09  loss: 0.8787 (0.8759)  time: 0.1964  data: 0.0129  max mem: 15689
[09:15:39.306116] Test:  [20/57]  eta: 0:00:07  loss: 0.8787 (0.8689)  time: 0.1840  data: 0.0001  max mem: 15689
[09:15:41.152672] Test:  [30/57]  eta: 0:00:05  loss: 0.7639 (0.8289)  time: 0.1844  data: 0.0001  max mem: 15689
[09:15:43.004876] Test:  [40/57]  eta: 0:00:03  loss: 0.7484 (0.8074)  time: 0.1849  data: 0.0001  max mem: 15689
[09:15:44.862232] Test:  [50/57]  eta: 0:00:01  loss: 0.7425 (0.8014)  time: 0.1854  data: 0.0001  max mem: 15689
[09:15:45.864618] Test:  [56/57]  eta: 0:00:00  loss: 0.7696 (0.8065)  time: 0.1800  data: 0.0001  max mem: 15689
[09:15:45.926346] Test: Total time: 0:00:10 (0.1864 s / it)
[09:15:47.744742] Dice score of the network on the train images: 0.753555, val images: 0.806288
[09:15:47.744976] saving best_dice_model_0 @ epoch 15
[09:15:48.817155] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:15:49.719763] Epoch: [16]  [  0/345]  eta: 0:05:11  lr: 0.000100  loss: 0.7739 (0.7739)  time: 0.9016  data: 0.1485  max mem: 15689

[09:16:04.808269] Epoch: [16]  [ 20/345]  eta: 0:04:07  lr: 0.000100  loss: 0.7907 (0.7920)  time: 0.7544  data: 0.0001  max mem: 15689
[09:16:19.965644] Epoch: [16]  [ 40/345]  eta: 0:03:51  lr: 0.000101  loss: 0.7993 (0.7967)  time: 0.7578  data: 0.0001  max mem: 15689
[09:16:35.168703] Epoch: [16]  [ 60/345]  eta: 0:03:36  lr: 0.000101  loss: 0.7992 (0.7978)  time: 0.7601  data: 0.0001  max mem: 15689
[09:16:50.404937] Epoch: [16]  [ 80/345]  eta: 0:03:21  lr: 0.000101  loss: 0.7910 (0.7951)  time: 0.7618  data: 0.0001  max mem: 15689
[09:17:05.633925] Epoch: [16]  [100/345]  eta: 0:03:06  lr: 0.000102  loss: 0.7844 (0.7933)  time: 0.7614  data: 0.0001  max mem: 15689
[09:17:20.870673] Epoch: [16]  [120/345]  eta: 0:02:51  lr: 0.000102  loss: 0.7994 (0.7963)  time: 0.7618  data: 0.0001  max mem: 15689
[09:17:36.107405] Epoch: [16]  [140/345]  eta: 0:02:35  lr: 0.000103  loss: 0.8124 (0.7987)  time: 0.7618  data: 0.0001  max mem: 15689
[09:17:51.331313] Epoch: [16]  [160/345]  eta: 0:02:20  lr: 0.000103  loss: 0.8121 (0.8012)  time: 0.7612  data: 0.0001  max mem: 15689
[09:18:06.554937] Epoch: [16]  [180/345]  eta: 0:02:05  lr: 0.000103  loss: 0.8049 (0.8019)  time: 0.7611  data: 0.0001  max mem: 15689
[09:18:21.755650] Epoch: [16]  [200/345]  eta: 0:01:50  lr: 0.000104  loss: 0.7926 (0.8018)  time: 0.7600  data: 0.0001  max mem: 15689
[09:18:36.968501] Epoch: [16]  [220/345]  eta: 0:01:35  lr: 0.000104  loss: 0.8039 (0.8014)  time: 0.7606  data: 0.0001  max mem: 15689
[09:18:52.172323] Epoch: [16]  [240/345]  eta: 0:01:19  lr: 0.000104  loss: 0.7880 (0.8003)  time: 0.7602  data: 0.0001  max mem: 15689
[09:19:07.375347] Epoch: [16]  [260/345]  eta: 0:01:04  lr: 0.000105  loss: 0.7922 (0.7999)  time: 0.7601  data: 0.0001  max mem: 15689
[09:19:22.569794] Epoch: [16]  [280/345]  eta: 0:00:49  lr: 0.000105  loss: 0.7992 (0.7999)  time: 0.7597  data: 0.0001  max mem: 15689
[09:19:37.763619] Epoch: [16]  [300/345]  eta: 0:00:34  lr: 0.000105  loss: 0.7972 (0.7998)  time: 0.7597  data: 0.0001  max mem: 15689
[09:19:52.959074] Epoch: [16]  [320/345]  eta: 0:00:19  lr: 0.000106  loss: 0.7858 (0.7989)  time: 0.7597  data: 0.0001  max mem: 15689
[09:20:08.156264] Epoch: [16]  [340/345]  eta: 0:00:03  lr: 0.000106  loss: 0.7974 (0.7990)  time: 0.7598  data: 0.0001  max mem: 15689
[09:20:11.195422] Epoch: [16]  [344/345]  eta: 0:00:00  lr: 0.000106  loss: 0.7902 (0.7988)  time: 0.7598  data: 0.0001  max mem: 15689
[09:20:11.262278] Epoch: [16] Total time: 0:04:22 (0.7607 s / it)
[09:20:11.262787] Averaged stats: lr: 0.000106  loss: 0.7902 (0.7988)
[09:20:11.602092] Test:  [  0/345]  eta: 0:01:55  loss: 0.7601 (0.7601)  time: 0.3357  data: 0.1517  max mem: 15689
[09:20:13.462162] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7674 (0.7713)  time: 0.1996  data: 0.0138  max mem: 15689
[09:20:15.324315] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7674 (0.7683)  time: 0.1860  data: 0.0001  max mem: 15689
[09:20:17.189639] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7650 (0.7677)  time: 0.1863  data: 0.0001  max mem: 15689
[09:20:19.056563] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7650 (0.7687)  time: 0.1866  data: 0.0001  max mem: 15689
[09:20:20.929524] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7655 (0.7673)  time: 0.1869  data: 0.0001  max mem: 15689
[09:20:22.805806] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7612 (0.7663)  time: 0.1874  data: 0.0001  max mem: 15689
[09:20:24.684446] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7580 (0.7650)  time: 0.1877  data: 0.0001  max mem: 15689
[09:20:26.570252] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7624 (0.7653)  time: 0.1882  data: 0.0001  max mem: 15689
[09:20:28.457581] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7564 (0.7648)  time: 0.1886  data: 0.0001  max mem: 15689
[09:20:30.350214] Test:  [100/345]  eta: 0:00:46  loss: 0.7555 (0.7643)  time: 0.1889  data: 0.0001  max mem: 15689
[09:20:32.244708] Test:  [110/345]  eta: 0:00:44  loss: 0.7610 (0.7640)  time: 0.1893  data: 0.0001  max mem: 15689
[09:20:34.146223] Test:  [120/345]  eta: 0:00:42  loss: 0.7610 (0.7637)  time: 0.1897  data: 0.0001  max mem: 15689
[09:20:36.050259] Test:  [130/345]  eta: 0:00:40  loss: 0.7571 (0.7634)  time: 0.1902  data: 0.0001  max mem: 15689
[09:20:37.956516] Test:  [140/345]  eta: 0:00:38  loss: 0.7615 (0.7637)  time: 0.1905  data: 0.0001  max mem: 15689
[09:20:39.865729] Test:  [150/345]  eta: 0:00:36  loss: 0.7721 (0.7645)  time: 0.1907  data: 0.0001  max mem: 15689
[09:20:41.777628] Test:  [160/345]  eta: 0:00:35  loss: 0.7727 (0.7648)  time: 0.1910  data: 0.0001  max mem: 15689
[09:20:43.692058] Test:  [170/345]  eta: 0:00:33  loss: 0.7602 (0.7645)  time: 0.1913  data: 0.0001  max mem: 15689
[09:20:45.612034] Test:  [180/345]  eta: 0:00:31  loss: 0.7602 (0.7642)  time: 0.1917  data: 0.0001  max mem: 15689
[09:20:47.534001] Test:  [190/345]  eta: 0:00:29  loss: 0.7613 (0.7636)  time: 0.1920  data: 0.0001  max mem: 15689
[09:20:49.459328] Test:  [200/345]  eta: 0:00:27  loss: 0.7522 (0.7634)  time: 0.1923  data: 0.0001  max mem: 15689
[09:20:51.388568] Test:  [210/345]  eta: 0:00:25  loss: 0.7524 (0.7635)  time: 0.1927  data: 0.0001  max mem: 15689
[09:20:53.321925] Test:  [220/345]  eta: 0:00:23  loss: 0.7670 (0.7637)  time: 0.1931  data: 0.0001  max mem: 15689
[09:20:55.258735] Test:  [230/345]  eta: 0:00:21  loss: 0.7662 (0.7638)  time: 0.1935  data: 0.0001  max mem: 15689
[09:20:57.199755] Test:  [240/345]  eta: 0:00:20  loss: 0.7563 (0.7636)  time: 0.1938  data: 0.0001  max mem: 15689
[09:20:59.142835] Test:  [250/345]  eta: 0:00:18  loss: 0.7611 (0.7637)  time: 0.1942  data: 0.0001  max mem: 15689
[09:21:01.090546] Test:  [260/345]  eta: 0:00:16  loss: 0.7611 (0.7635)  time: 0.1945  data: 0.0001  max mem: 15689
[09:21:03.041787] Test:  [270/345]  eta: 0:00:14  loss: 0.7539 (0.7631)  time: 0.1949  data: 0.0001  max mem: 15689
[09:21:04.995713] Test:  [280/345]  eta: 0:00:12  loss: 0.7511 (0.7627)  time: 0.1952  data: 0.0001  max mem: 15689
[09:21:06.953605] Test:  [290/345]  eta: 0:00:10  loss: 0.7611 (0.7630)  time: 0.1955  data: 0.0001  max mem: 15689
[09:21:08.914668] Test:  [300/345]  eta: 0:00:08  loss: 0.7629 (0.7630)  time: 0.1959  data: 0.0001  max mem: 15689
[09:21:10.878807] Test:  [310/345]  eta: 0:00:06  loss: 0.7630 (0.7631)  time: 0.1962  data: 0.0001  max mem: 15689
[09:21:12.846719] Test:  [320/345]  eta: 0:00:04  loss: 0.7638 (0.7631)  time: 0.1966  data: 0.0001  max mem: 15689
[09:21:14.817664] Test:  [330/345]  eta: 0:00:02  loss: 0.7669 (0.7633)  time: 0.1969  data: 0.0001  max mem: 15689
[09:21:16.791917] Test:  [340/345]  eta: 0:00:00  loss: 0.7610 (0.7632)  time: 0.1972  data: 0.0001  max mem: 15689
[09:21:17.583483] Test:  [344/345]  eta: 0:00:00  loss: 0.7610 (0.7633)  time: 0.1974  data: 0.0001  max mem: 15689
[09:21:17.644502] Test: Total time: 0:01:06 (0.1924 s / it)
[09:21:28.260202] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8504 (0.8504)  time: 0.3313  data: 0.1494  max mem: 15689
[09:21:30.099140] Test:  [10/57]  eta: 0:00:09  loss: 0.8610 (0.8698)  time: 0.1972  data: 0.0136  max mem: 15689
[09:21:31.943261] Test:  [20/57]  eta: 0:00:07  loss: 0.8744 (0.8645)  time: 0.1841  data: 0.0001  max mem: 15689
[09:21:33.791741] Test:  [30/57]  eta: 0:00:05  loss: 0.7655 (0.8298)  time: 0.1846  data: 0.0001  max mem: 15689
[09:21:35.642801] Test:  [40/57]  eta: 0:00:03  loss: 0.7599 (0.8135)  time: 0.1849  data: 0.0001  max mem: 15689
[09:21:37.499816] Test:  [50/57]  eta: 0:00:01  loss: 0.7550 (0.8077)  time: 0.1854  data: 0.0001  max mem: 15689
[09:21:38.501837] Test:  [56/57]  eta: 0:00:00  loss: 0.7769 (0.8126)  time: 0.1799  data: 0.0001  max mem: 15689
[09:21:38.561067] Test: Total time: 0:00:10 (0.1865 s / it)
[09:21:40.451063] Dice score of the network on the train images: 0.777788, val images: 0.807731
[09:21:40.451297] saving best_prec_model_0 @ epoch 16
[09:21:41.650163] saving best_dice_model_0 @ epoch 16
[09:21:42.711797] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:21:43.616041] Epoch: [17]  [  0/345]  eta: 0:05:11  lr: 0.000106  loss: 0.7715 (0.7715)  time: 0.9030  data: 0.1455  max mem: 15689
[09:21:58.720095] Epoch: [17]  [ 20/345]  eta: 0:04:07  lr: 0.000107  loss: 0.7878 (0.7905)  time: 0.7551  data: 0.0001  max mem: 15689
[09:22:13.889863] Epoch: [17]  [ 40/345]  eta: 0:03:51  lr: 0.000107  loss: 0.7959 (0.7954)  time: 0.7584  data: 0.0001  max mem: 15689
[09:22:29.091742] Epoch: [17]  [ 60/345]  eta: 0:03:36  lr: 0.000107  loss: 0.7912 (0.7946)  time: 0.7600  data: 0.0001  max mem: 15689
[09:22:44.327550] Epoch: [17]  [ 80/345]  eta: 0:03:21  lr: 0.000108  loss: 0.7859 (0.7936)  time: 0.7617  data: 0.0001  max mem: 15689
[09:22:59.581410] Epoch: [17]  [100/345]  eta: 0:03:06  lr: 0.000108  loss: 0.7926 (0.7929)  time: 0.7627  data: 0.0001  max mem: 15689
[09:23:14.825160] Epoch: [17]  [120/345]  eta: 0:02:51  lr: 0.000108  loss: 0.7953 (0.7927)  time: 0.7621  data: 0.0001  max mem: 15689
[09:23:30.076537] Epoch: [17]  [140/345]  eta: 0:02:36  lr: 0.000109  loss: 0.8080 (0.7949)  time: 0.7625  data: 0.0001  max mem: 15689
[09:23:45.317204] Epoch: [17]  [160/345]  eta: 0:02:20  lr: 0.000109  loss: 0.7958 (0.7960)  time: 0.7620  data: 0.0001  max mem: 15689
[09:24:00.547802] Epoch: [17]  [180/345]  eta: 0:02:05  lr: 0.000110  loss: 0.8320 (0.8000)  time: 0.7615  data: 0.0001  max mem: 15689
[09:24:15.779206] Epoch: [17]  [200/345]  eta: 0:01:50  lr: 0.000110  loss: 0.8019 (0.8003)  time: 0.7615  data: 0.0001  max mem: 15689
[09:24:31.011320] Epoch: [17]  [220/345]  eta: 0:01:35  lr: 0.000110  loss: 0.7872 (0.7997)  time: 0.7616  data: 0.0001  max mem: 15689
[09:24:46.240185] Epoch: [17]  [240/345]  eta: 0:01:19  lr: 0.000111  loss: 0.7859 (0.7989)  time: 0.7614  data: 0.0001  max mem: 15689
[09:25:01.457621] Epoch: [17]  [260/345]  eta: 0:01:04  lr: 0.000111  loss: 0.8008 (0.7990)  time: 0.7608  data: 0.0001  max mem: 15689
[09:25:16.662778] Epoch: [17]  [280/345]  eta: 0:00:49  lr: 0.000111  loss: 0.7916 (0.7990)  time: 0.7602  data: 0.0001  max mem: 15689
[09:25:31.879042] Epoch: [17]  [300/345]  eta: 0:00:34  lr: 0.000112  loss: 0.8196 (0.8009)  time: 0.7608  data: 0.0001  max mem: 15689
[09:25:47.087394] Epoch: [17]  [320/345]  eta: 0:00:19  lr: 0.000112  loss: 0.7999 (0.8012)  time: 0.7604  data: 0.0001  max mem: 15689
[09:26:02.287438] Epoch: [17]  [340/345]  eta: 0:00:03  lr: 0.000112  loss: 0.8056 (0.8019)  time: 0.7600  data: 0.0001  max mem: 15689
[09:26:05.331414] Epoch: [17]  [344/345]  eta: 0:00:00  lr: 0.000112  loss: 0.8148 (0.8023)  time: 0.7601  data: 0.0001  max mem: 15689
[09:26:05.393087] Epoch: [17] Total time: 0:04:22 (0.7614 s / it)
[09:26:05.393582] Averaged stats: lr: 0.000112  loss: 0.8148 (0.8023)
[09:26:05.738462] Test:  [  0/345]  eta: 0:01:57  loss: 0.7917 (0.7917)  time: 0.3413  data: 0.1574  max mem: 15689
[09:26:07.598712] Test:  [ 10/345]  eta: 0:01:07  loss: 0.7882 (0.7835)  time: 0.2001  data: 0.0144  max mem: 15689
[09:26:09.461597] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7721 (0.7735)  time: 0.1861  data: 0.0001  max mem: 15689
[09:26:11.326967] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7721 (0.7805)  time: 0.1864  data: 0.0001  max mem: 15689
[09:26:13.196143] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7841 (0.7807)  time: 0.1867  data: 0.0001  max mem: 15689
[09:26:15.068904] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7891 (0.7838)  time: 0.1870  data: 0.0001  max mem: 15689
[09:26:16.945332] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7839 (0.7828)  time: 0.1874  data: 0.0001  max mem: 15689
[09:26:18.826813] Test:  [ 70/345]  eta: 0:00:52  loss: 0.7757 (0.7817)  time: 0.1878  data: 0.0001  max mem: 15689
[09:26:20.714294] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7761 (0.7814)  time: 0.1884  data: 0.0001  max mem: 15689
[09:26:22.601322] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7789 (0.7815)  time: 0.1887  data: 0.0001  max mem: 15689
[09:26:24.493089] Test:  [100/345]  eta: 0:00:46  loss: 0.7789 (0.7814)  time: 0.1889  data: 0.0001  max mem: 15689
[09:26:26.388011] Test:  [110/345]  eta: 0:00:44  loss: 0.7738 (0.7810)  time: 0.1893  data: 0.0001  max mem: 15689
[09:26:28.287472] Test:  [120/345]  eta: 0:00:42  loss: 0.7800 (0.7811)  time: 0.1897  data: 0.0001  max mem: 15689
[09:26:30.190927] Test:  [130/345]  eta: 0:00:40  loss: 0.7836 (0.7815)  time: 0.1901  data: 0.0001  max mem: 15689
[09:26:32.096430] Test:  [140/345]  eta: 0:00:38  loss: 0.7787 (0.7813)  time: 0.1904  data: 0.0001  max mem: 15689
[09:26:34.005406] Test:  [150/345]  eta: 0:00:36  loss: 0.7734 (0.7811)  time: 0.1907  data: 0.0001  max mem: 15689
[09:26:35.916546] Test:  [160/345]  eta: 0:00:35  loss: 0.7734 (0.7811)  time: 0.1910  data: 0.0001  max mem: 15689
[09:26:37.831431] Test:  [170/345]  eta: 0:00:33  loss: 0.7768 (0.7808)  time: 0.1912  data: 0.0001  max mem: 15689
[09:26:39.750684] Test:  [180/345]  eta: 0:00:31  loss: 0.7815 (0.7813)  time: 0.1917  data: 0.0001  max mem: 15689
[09:26:41.673509] Test:  [190/345]  eta: 0:00:29  loss: 0.7916 (0.7820)  time: 0.1921  data: 0.0001  max mem: 15689
[09:26:43.600012] Test:  [200/345]  eta: 0:00:27  loss: 0.7923 (0.7821)  time: 0.1924  data: 0.0001  max mem: 15689
[09:26:45.531164] Test:  [210/345]  eta: 0:00:25  loss: 0.7820 (0.7824)  time: 0.1928  data: 0.0001  max mem: 15689
[09:26:47.464927] Test:  [220/345]  eta: 0:00:23  loss: 0.7793 (0.7820)  time: 0.1932  data: 0.0001  max mem: 15689
[09:26:49.401847] Test:  [230/345]  eta: 0:00:21  loss: 0.7721 (0.7817)  time: 0.1935  data: 0.0001  max mem: 15689
[09:26:51.342241] Test:  [240/345]  eta: 0:00:20  loss: 0.7721 (0.7816)  time: 0.1938  data: 0.0001  max mem: 15689
[09:26:53.286656] Test:  [250/345]  eta: 0:00:18  loss: 0.7785 (0.7817)  time: 0.1942  data: 0.0001  max mem: 15689
[09:26:55.235295] Test:  [260/345]  eta: 0:00:16  loss: 0.7785 (0.7816)  time: 0.1946  data: 0.0001  max mem: 15689
[09:26:57.186834] Test:  [270/345]  eta: 0:00:14  loss: 0.7878 (0.7820)  time: 0.1950  data: 0.0001  max mem: 15689
[09:26:59.140368] Test:  [280/345]  eta: 0:00:12  loss: 0.7859 (0.7819)  time: 0.1952  data: 0.0001  max mem: 15689
[09:27:01.099749] Test:  [290/345]  eta: 0:00:10  loss: 0.7630 (0.7813)  time: 0.1956  data: 0.0001  max mem: 15689
[09:27:03.061600] Test:  [300/345]  eta: 0:00:08  loss: 0.7755 (0.7816)  time: 0.1960  data: 0.0001  max mem: 15689
[09:27:05.026253] Test:  [310/345]  eta: 0:00:06  loss: 0.7924 (0.7820)  time: 0.1963  data: 0.0001  max mem: 15689
[09:27:06.993222] Test:  [320/345]  eta: 0:00:04  loss: 0.7827 (0.7819)  time: 0.1965  data: 0.0001  max mem: 15689
[09:27:08.963518] Test:  [330/345]  eta: 0:00:02  loss: 0.7770 (0.7817)  time: 0.1968  data: 0.0001  max mem: 15689
[09:27:10.937598] Test:  [340/345]  eta: 0:00:00  loss: 0.7766 (0.7814)  time: 0.1972  data: 0.0001  max mem: 15689
[09:27:11.728965] Test:  [344/345]  eta: 0:00:00  loss: 0.7750 (0.7812)  time: 0.1973  data: 0.0001  max mem: 15689
[09:27:11.788006] Test: Total time: 0:01:06 (0.1924 s / it)
[09:27:22.450748] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8839 (0.8839)  time: 0.3224  data: 0.1402  max mem: 15689
[09:27:24.288837] Test:  [10/57]  eta: 0:00:09  loss: 0.9081 (0.9106)  time: 0.1963  data: 0.0128  max mem: 15689
[09:27:26.132913] Test:  [20/57]  eta: 0:00:07  loss: 0.9097 (0.9001)  time: 0.1840  data: 0.0001  max mem: 15689
[09:27:27.980963] Test:  [30/57]  eta: 0:00:05  loss: 0.7758 (0.8550)  time: 0.1846  data: 0.0001  max mem: 15689
[09:27:29.834327] Test:  [40/57]  eta: 0:00:03  loss: 0.7625 (0.8307)  time: 0.1850  data: 0.0001  max mem: 15689
[09:27:31.692137] Test:  [50/57]  eta: 0:00:01  loss: 0.7532 (0.8212)  time: 0.1855  data: 0.0001  max mem: 15689
[09:27:32.695437] Test:  [56/57]  eta: 0:00:00  loss: 0.7776 (0.8265)  time: 0.1801  data: 0.0001  max mem: 15689
[09:27:32.754154] Test: Total time: 0:00:10 (0.1864 s / it)
[09:27:34.671269] Dice score of the network on the train images: 0.773659, val images: 0.809370
[09:27:34.671525] saving best_prec_model_0 @ epoch 17
[09:27:35.865529] saving best_dice_model_0 @ epoch 17
[09:27:36.897564] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:27:37.801738] Epoch: [18]  [  0/345]  eta: 0:05:11  lr: 0.000113  loss: 0.7875 (0.7875)  time: 0.9032  data: 0.1485  max mem: 15689
[09:27:52.890237] Epoch: [18]  [ 20/345]  eta: 0:04:07  lr: 0.000113  loss: 0.7939 (0.7959)  time: 0.7544  data: 0.0001  max mem: 15689
[09:28:08.030057] Epoch: [18]  [ 40/345]  eta: 0:03:51  lr: 0.000113  loss: 0.7922 (0.7967)  time: 0.7569  data: 0.0001  max mem: 15689
[09:28:23.217980] Epoch: [18]  [ 60/345]  eta: 0:03:36  lr: 0.000114  loss: 0.7877 (0.7951)  time: 0.7594  data: 0.0001  max mem: 15689
[09:28:38.428565] Epoch: [18]  [ 80/345]  eta: 0:03:21  lr: 0.000114  loss: 0.7904 (0.7946)  time: 0.7605  data: 0.0001  max mem: 15689
[09:28:53.661762] Epoch: [18]  [100/345]  eta: 0:03:06  lr: 0.000114  loss: 0.7913 (0.7947)  time: 0.7616  data: 0.0001  max mem: 15689
[09:29:08.903264] Epoch: [18]  [120/345]  eta: 0:02:51  lr: 0.000115  loss: 0.7928 (0.7943)  time: 0.7620  data: 0.0001  max mem: 15689
[09:29:24.218593] Epoch: [18]  [140/345]  eta: 0:02:36  lr: 0.000115  loss: 0.7894 (0.7936)  time: 0.7657  data: 0.0001  max mem: 15689
[09:29:39.462707] Epoch: [18]  [160/345]  eta: 0:02:20  lr: 0.000115  loss: 0.7829 (0.7930)  time: 0.7622  data: 0.0001  max mem: 15689
[09:29:54.699234] Epoch: [18]  [180/345]  eta: 0:02:05  lr: 0.000116  loss: 0.7893 (0.7930)  time: 0.7618  data: 0.0001  max mem: 15689
[09:30:09.929129] Epoch: [18]  [200/345]  eta: 0:01:50  lr: 0.000116  loss: 0.7871 (0.7922)  time: 0.7615  data: 0.0001  max mem: 15689
[09:30:25.161788] Epoch: [18]  [220/345]  eta: 0:01:35  lr: 0.000116  loss: 0.7875 (0.7921)  time: 0.7616  data: 0.0001  max mem: 15689

[09:30:40.389801] Epoch: [18]  [240/345]  eta: 0:01:19  lr: 0.000117  loss: 0.7954 (0.7928)  time: 0.7614  data: 0.0001  max mem: 15689
[09:30:55.614684] Epoch: [18]  [260/345]  eta: 0:01:04  lr: 0.000117  loss: 0.7835 (0.7923)  time: 0.7612  data: 0.0001  max mem: 15689
[09:31:10.839822] Epoch: [18]  [280/345]  eta: 0:00:49  lr: 0.000118  loss: 0.7795 (0.7915)  time: 0.7612  data: 0.0001  max mem: 15689
[09:31:26.057140] Epoch: [18]  [300/345]  eta: 0:00:34  lr: 0.000118  loss: 0.7823 (0.7908)  time: 0.7608  data: 0.0001  max mem: 15689
[09:31:41.277046] Epoch: [18]  [320/345]  eta: 0:00:19  lr: 0.000118  loss: 0.7809 (0.7904)  time: 0.7610  data: 0.0001  max mem: 15689
[09:31:56.495518] Epoch: [18]  [340/345]  eta: 0:00:03  lr: 0.000119  loss: 0.7810 (0.7900)  time: 0.7609  data: 0.0001  max mem: 15689
[09:31:59.542617] Epoch: [18]  [344/345]  eta: 0:00:00  lr: 0.000119  loss: 0.7810 (0.7899)  time: 0.7609  data: 0.0001  max mem: 15689
[09:31:59.607486] Epoch: [18] Total time: 0:04:22 (0.7615 s / it)
[09:31:59.607945] Averaged stats: lr: 0.000119  loss: 0.7810 (0.7899)
[09:31:59.944865] Test:  [  0/345]  eta: 0:01:54  loss: 0.7488 (0.7488)  time: 0.3324  data: 0.1499  max mem: 15689
[09:32:01.805459] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7413 (0.7417)  time: 0.1993  data: 0.0137  max mem: 15689
[09:32:03.668646] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7459 (0.7485)  time: 0.1861  data: 0.0001  max mem: 15689
[09:32:05.535673] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7509 (0.7503)  time: 0.1864  data: 0.0001  max mem: 15689
[09:32:07.405742] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7489 (0.7488)  time: 0.1868  data: 0.0001  max mem: 15689
[09:32:09.279758] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7505 (0.7507)  time: 0.1872  data: 0.0001  max mem: 15689
[09:32:11.157278] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7516 (0.7500)  time: 0.1875  data: 0.0001  max mem: 15689
[09:32:13.037418] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7390 (0.7480)  time: 0.1878  data: 0.0001  max mem: 15689
[09:32:14.921881] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7409 (0.7481)  time: 0.1882  data: 0.0001  max mem: 15689
[09:32:16.809084] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7425 (0.7481)  time: 0.1885  data: 0.0001  max mem: 15689
[09:32:18.701851] Test:  [100/345]  eta: 0:00:46  loss: 0.7423 (0.7475)  time: 0.1889  data: 0.0001  max mem: 15689
[09:32:20.596302] Test:  [110/345]  eta: 0:00:44  loss: 0.7465 (0.7476)  time: 0.1893  data: 0.0001  max mem: 15689
[09:32:22.496938] Test:  [120/345]  eta: 0:00:42  loss: 0.7474 (0.7476)  time: 0.1897  data: 0.0001  max mem: 15689
[09:32:24.401243] Test:  [130/345]  eta: 0:00:40  loss: 0.7457 (0.7476)  time: 0.1902  data: 0.0001  max mem: 15689
[09:32:26.307945] Test:  [140/345]  eta: 0:00:38  loss: 0.7452 (0.7473)  time: 0.1905  data: 0.0001  max mem: 15689
[09:32:28.215905] Test:  [150/345]  eta: 0:00:36  loss: 0.7374 (0.7471)  time: 0.1907  data: 0.0001  max mem: 15689
[09:32:30.127454] Test:  [160/345]  eta: 0:00:35  loss: 0.7426 (0.7475)  time: 0.1909  data: 0.0001  max mem: 15689
[09:32:32.043224] Test:  [170/345]  eta: 0:00:33  loss: 0.7559 (0.7480)  time: 0.1913  data: 0.0001  max mem: 15689
[09:32:33.962928] Test:  [180/345]  eta: 0:00:31  loss: 0.7591 (0.7483)  time: 0.1917  data: 0.0001  max mem: 15689
[09:32:35.885866] Test:  [190/345]  eta: 0:00:29  loss: 0.7517 (0.7486)  time: 0.1921  data: 0.0001  max mem: 15689
[09:32:37.812217] Test:  [200/345]  eta: 0:00:27  loss: 0.7499 (0.7486)  time: 0.1924  data: 0.0001  max mem: 15689
[09:32:39.742104] Test:  [210/345]  eta: 0:00:25  loss: 0.7423 (0.7484)  time: 0.1928  data: 0.0001  max mem: 15689
[09:32:41.675128] Test:  [220/345]  eta: 0:00:23  loss: 0.7448 (0.7483)  time: 0.1931  data: 0.0001  max mem: 15689
[09:32:43.611296] Test:  [230/345]  eta: 0:00:21  loss: 0.7462 (0.7483)  time: 0.1934  data: 0.0001  max mem: 15689
[09:32:45.551163] Test:  [240/345]  eta: 0:00:20  loss: 0.7488 (0.7483)  time: 0.1937  data: 0.0001  max mem: 15689
[09:32:47.495630] Test:  [250/345]  eta: 0:00:18  loss: 0.7383 (0.7481)  time: 0.1942  data: 0.0001  max mem: 15689
[09:32:49.442183] Test:  [260/345]  eta: 0:00:16  loss: 0.7554 (0.7488)  time: 0.1945  data: 0.0001  max mem: 15689
[09:32:51.392703] Test:  [270/345]  eta: 0:00:14  loss: 0.7494 (0.7487)  time: 0.1948  data: 0.0001  max mem: 15689
[09:32:53.346734] Test:  [280/345]  eta: 0:00:12  loss: 0.7476 (0.7489)  time: 0.1952  data: 0.0001  max mem: 15689
[09:32:55.304485] Test:  [290/345]  eta: 0:00:10  loss: 0.7539 (0.7492)  time: 0.1955  data: 0.0001  max mem: 15689
[09:32:57.264335] Test:  [300/345]  eta: 0:00:08  loss: 0.7462 (0.7492)  time: 0.1958  data: 0.0001  max mem: 15689
[09:32:59.227246] Test:  [310/345]  eta: 0:00:06  loss: 0.7443 (0.7490)  time: 0.1961  data: 0.0001  max mem: 15689
[09:33:01.195183] Test:  [320/345]  eta: 0:00:04  loss: 0.7502 (0.7489)  time: 0.1965  data: 0.0001  max mem: 15689
[09:33:03.166536] Test:  [330/345]  eta: 0:00:02  loss: 0.7524 (0.7490)  time: 0.1969  data: 0.0001  max mem: 15689
[09:33:05.139669] Test:  [340/345]  eta: 0:00:00  loss: 0.7425 (0.7488)  time: 0.1972  data: 0.0001  max mem: 15689
[09:33:05.929683] Test:  [344/345]  eta: 0:00:00  loss: 0.7427 (0.7488)  time: 0.1973  data: 0.0001  max mem: 15689
[09:33:05.985626] Test: Total time: 0:01:06 (0.1924 s / it)
[09:33:16.542207] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8439 (0.8439)  time: 0.3268  data: 0.1442  max mem: 15689
[09:33:18.380696] Test:  [10/57]  eta: 0:00:09  loss: 0.8750 (0.8753)  time: 0.1968  data: 0.0132  max mem: 15689
[09:33:20.223870] Test:  [20/57]  eta: 0:00:07  loss: 0.8750 (0.8647)  time: 0.1840  data: 0.0001  max mem: 15689
[09:33:22.070102] Test:  [30/57]  eta: 0:00:05  loss: 0.7595 (0.8246)  time: 0.1844  data: 0.0001  max mem: 15689
[09:33:23.922298] Test:  [40/57]  eta: 0:00:03  loss: 0.7460 (0.8036)  time: 0.1849  data: 0.0001  max mem: 15689
[09:33:25.779113] Test:  [50/57]  eta: 0:00:01  loss: 0.7376 (0.7973)  time: 0.1854  data: 0.0001  max mem: 15689
[09:33:26.780622] Test:  [56/57]  eta: 0:00:00  loss: 0.7695 (0.8031)  time: 0.1799  data: 0.0001  max mem: 15689
[09:33:26.838839] Test: Total time: 0:00:10 (0.1864 s / it)
[09:33:28.718153] Dice score of the network on the train images: 0.777735, val images: 0.812712
[09:33:28.718394] saving best_dice_model_0 @ epoch 18
[09:33:29.912056] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:33:30.819051] Epoch: [19]  [  0/345]  eta: 0:05:12  lr: 0.000119  loss: 0.7769 (0.7769)  time: 0.9057  data: 0.1498  max mem: 15689
[09:33:45.935538] Epoch: [19]  [ 20/345]  eta: 0:04:07  lr: 0.000119  loss: 0.7727 (0.7738)  time: 0.7558  data: 0.0001  max mem: 15689
[09:34:01.107254] Epoch: [19]  [ 40/345]  eta: 0:03:52  lr: 0.000119  loss: 0.7770 (0.7765)  time: 0.7585  data: 0.0001  max mem: 15689
[09:34:16.303491] Epoch: [19]  [ 60/345]  eta: 0:03:36  lr: 0.000120  loss: 0.7779 (0.7776)  time: 0.7598  data: 0.0001  max mem: 15689
[09:34:31.537413] Epoch: [19]  [ 80/345]  eta: 0:03:21  lr: 0.000120  loss: 0.7809 (0.7789)  time: 0.7617  data: 0.0001  max mem: 15689
[09:34:46.790475] Epoch: [19]  [100/345]  eta: 0:03:06  lr: 0.000121  loss: 0.7799 (0.7795)  time: 0.7626  data: 0.0001  max mem: 15689
[09:35:02.056071] Epoch: [19]  [120/345]  eta: 0:02:51  lr: 0.000121  loss: 0.7783 (0.7788)  time: 0.7632  data: 0.0001  max mem: 15689
[09:35:17.313409] Epoch: [19]  [140/345]  eta: 0:02:36  lr: 0.000121  loss: 0.7753 (0.7794)  time: 0.7628  data: 0.0001  max mem: 15689
[09:35:32.560436] Epoch: [19]  [160/345]  eta: 0:02:20  lr: 0.000122  loss: 0.7816 (0.7804)  time: 0.7623  data: 0.0001  max mem: 15689
[09:35:47.804792] Epoch: [19]  [180/345]  eta: 0:02:05  lr: 0.000122  loss: 0.7933 (0.7815)  time: 0.7622  data: 0.0001  max mem: 15689
[09:36:03.049625] Epoch: [19]  [200/345]  eta: 0:01:50  lr: 0.000122  loss: 0.7752 (0.7812)  time: 0.7622  data: 0.0001  max mem: 15689
[09:36:18.285300] Epoch: [19]  [220/345]  eta: 0:01:35  lr: 0.000123  loss: 0.7669 (0.7801)  time: 0.7617  data: 0.0001  max mem: 15689
[09:36:33.520453] Epoch: [19]  [240/345]  eta: 0:01:19  lr: 0.000123  loss: 0.7981 (0.7816)  time: 0.7617  data: 0.0001  max mem: 15689
[09:36:48.745860] Epoch: [19]  [260/345]  eta: 0:01:04  lr: 0.000123  loss: 0.7784 (0.7820)  time: 0.7612  data: 0.0001  max mem: 15689
[09:37:03.947238] Epoch: [19]  [280/345]  eta: 0:00:49  lr: 0.000124  loss: 0.7815 (0.7822)  time: 0.7600  data: 0.0001  max mem: 15689
[09:37:19.147571] Epoch: [19]  [300/345]  eta: 0:00:34  lr: 0.000124  loss: 0.7847 (0.7824)  time: 0.7600  data: 0.0001  max mem: 15689
[09:37:34.360473] Epoch: [19]  [320/345]  eta: 0:00:19  lr: 0.000125  loss: 0.7682 (0.7818)  time: 0.7606  data: 0.0001  max mem: 15689
[09:37:49.572122] Epoch: [19]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.7754 (0.7815)  time: 0.7605  data: 0.0001  max mem: 15689
[09:37:52.612844] Epoch: [19]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.7752 (0.7815)  time: 0.7602  data: 0.0001  max mem: 15689
[09:37:52.674821] Epoch: [19] Total time: 0:04:22 (0.7616 s / it)
[09:37:52.675291] Averaged stats: lr: 0.000125  loss: 0.7752 (0.7815)
[09:37:53.015464] Test:  [  0/345]  eta: 0:01:56  loss: 0.7398 (0.7398)  time: 0.3365  data: 0.1522  max mem: 15689
[09:37:54.876002] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7379 (0.7398)  time: 0.1996  data: 0.0139  max mem: 15689
[09:37:56.737306] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7397 (0.7438)  time: 0.1860  data: 0.0001  max mem: 15689
[09:37:58.604011] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7408 (0.7420)  time: 0.1864  data: 0.0001  max mem: 15689
[09:38:00.474327] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7449 (0.7452)  time: 0.1868  data: 0.0001  max mem: 15689
[09:38:02.348305] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7477 (0.7450)  time: 0.1872  data: 0.0001  max mem: 15689
[09:38:04.225319] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7471 (0.7454)  time: 0.1875  data: 0.0001  max mem: 15689
[09:38:06.107288] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7442 (0.7443)  time: 0.1879  data: 0.0001  max mem: 15689
[09:38:07.992364] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7442 (0.7448)  time: 0.1883  data: 0.0001  max mem: 15689
[09:38:09.881156] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7510 (0.7458)  time: 0.1886  data: 0.0001  max mem: 15689
[09:38:11.773966] Test:  [100/345]  eta: 0:00:46  loss: 0.7470 (0.7457)  time: 0.1890  data: 0.0001  max mem: 15689
[09:38:13.670523] Test:  [110/345]  eta: 0:00:44  loss: 0.7434 (0.7455)  time: 0.1894  data: 0.0001  max mem: 15689
[09:38:15.571448] Test:  [120/345]  eta: 0:00:42  loss: 0.7416 (0.7456)  time: 0.1898  data: 0.0001  max mem: 15689
[09:38:17.474775] Test:  [130/345]  eta: 0:00:40  loss: 0.7416 (0.7455)  time: 0.1902  data: 0.0001  max mem: 15689
[09:38:19.379269] Test:  [140/345]  eta: 0:00:38  loss: 0.7463 (0.7462)  time: 0.1903  data: 0.0001  max mem: 15689
[09:38:21.287832] Test:  [150/345]  eta: 0:00:36  loss: 0.7547 (0.7467)  time: 0.1906  data: 0.0001  max mem: 15689
[09:38:23.199915] Test:  [160/345]  eta: 0:00:35  loss: 0.7540 (0.7467)  time: 0.1910  data: 0.0001  max mem: 15689
[09:38:25.115356] Test:  [170/345]  eta: 0:00:33  loss: 0.7396 (0.7467)  time: 0.1913  data: 0.0001  max mem: 15689
[09:38:27.035459] Test:  [180/345]  eta: 0:00:31  loss: 0.7381 (0.7464)  time: 0.1917  data: 0.0001  max mem: 15689
[09:38:28.957701] Test:  [190/345]  eta: 0:00:29  loss: 0.7440 (0.7465)  time: 0.1921  data: 0.0001  max mem: 15689
[09:38:30.884719] Test:  [200/345]  eta: 0:00:27  loss: 0.7575 (0.7470)  time: 0.1924  data: 0.0001  max mem: 15689
[09:38:32.815403] Test:  [210/345]  eta: 0:00:25  loss: 0.7510 (0.7467)  time: 0.1928  data: 0.0001  max mem: 15689
[09:38:34.748847] Test:  [220/345]  eta: 0:00:23  loss: 0.7491 (0.7470)  time: 0.1932  data: 0.0001  max mem: 15689
[09:38:36.685711] Test:  [230/345]  eta: 0:00:21  loss: 0.7491 (0.7470)  time: 0.1935  data: 0.0001  max mem: 15689
[09:38:38.627200] Test:  [240/345]  eta: 0:00:20  loss: 0.7479 (0.7472)  time: 0.1939  data: 0.0001  max mem: 15689
[09:38:40.571273] Test:  [250/345]  eta: 0:00:18  loss: 0.7488 (0.7473)  time: 0.1942  data: 0.0001  max mem: 15689
[09:38:42.518570] Test:  [260/345]  eta: 0:00:16  loss: 0.7504 (0.7474)  time: 0.1945  data: 0.0001  max mem: 15689
[09:38:44.469712] Test:  [270/345]  eta: 0:00:14  loss: 0.7533 (0.7474)  time: 0.1949  data: 0.0001  max mem: 15689
[09:38:46.424977] Test:  [280/345]  eta: 0:00:12  loss: 0.7553 (0.7480)  time: 0.1953  data: 0.0001  max mem: 15689
[09:38:48.382924] Test:  [290/345]  eta: 0:00:10  loss: 0.7510 (0.7480)  time: 0.1956  data: 0.0001  max mem: 15689
[09:38:50.345511] Test:  [300/345]  eta: 0:00:08  loss: 0.7464 (0.7481)  time: 0.1960  data: 0.0001  max mem: 15689
[09:38:52.308151] Test:  [310/345]  eta: 0:00:06  loss: 0.7508 (0.7483)  time: 0.1962  data: 0.0001  max mem: 15689
[09:38:54.275114] Test:  [320/345]  eta: 0:00:04  loss: 0.7435 (0.7482)  time: 0.1964  data: 0.0001  max mem: 15689
[09:38:56.247992] Test:  [330/345]  eta: 0:00:02  loss: 0.7345 (0.7478)  time: 0.1969  data: 0.0001  max mem: 15689
[09:38:58.224853] Test:  [340/345]  eta: 0:00:00  loss: 0.7416 (0.7479)  time: 0.1974  data: 0.0001  max mem: 15689
[09:38:59.015808] Test:  [344/345]  eta: 0:00:00  loss: 0.7430 (0.7478)  time: 0.1975  data: 0.0001  max mem: 15689
[09:38:59.071694] Test: Total time: 0:01:06 (0.1924 s / it)
[09:39:09.675818] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8624 (0.8624)  time: 0.3285  data: 0.1468  max mem: 15689
[09:39:11.513929] Test:  [10/57]  eta: 0:00:09  loss: 0.8769 (0.8715)  time: 0.1969  data: 0.0134  max mem: 15689
[09:39:13.358341] Test:  [20/57]  eta: 0:00:07  loss: 0.8748 (0.8594)  time: 0.1841  data: 0.0001  max mem: 15689
[09:39:15.207229] Test:  [30/57]  eta: 0:00:05  loss: 0.7648 (0.8224)  time: 0.1846  data: 0.0001  max mem: 15689
[09:39:17.060903] Test:  [40/57]  eta: 0:00:03  loss: 0.7462 (0.8026)  time: 0.1851  data: 0.0001  max mem: 15689
[09:39:18.918060] Test:  [50/57]  eta: 0:00:01  loss: 0.7489 (0.7962)  time: 0.1855  data: 0.0001  max mem: 15689
[09:39:19.920831] Test:  [56/57]  eta: 0:00:00  loss: 0.7611 (0.8009)  time: 0.1800  data: 0.0001  max mem: 15689
[09:39:19.979905] Test: Total time: 0:00:10 (0.1866 s / it)
[09:39:21.815757] Dice score of the network on the train images: 0.776387, val images: 0.816377
[09:39:21.815996] saving best_dice_model_0 @ epoch 19
[09:39:22.879146] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:39:23.780990] Epoch: [20]  [  0/345]  eta: 0:05:10  lr: 0.000125  loss: 0.7589 (0.7589)  time: 0.9008  data: 0.1447  max mem: 15689
[09:39:38.890221] Epoch: [20]  [ 20/345]  eta: 0:04:07  lr: 0.000125  loss: 0.7585 (0.7679)  time: 0.7554  data: 0.0001  max mem: 15689
[09:39:54.161184] Epoch: [20]  [ 40/345]  eta: 0:03:52  lr: 0.000125  loss: 0.7780 (0.7748)  time: 0.7635  data: 0.0001  max mem: 15689
[09:40:09.370586] Epoch: [20]  [ 60/345]  eta: 0:03:37  lr: 0.000125  loss: 0.7715 (0.7755)  time: 0.7604  data: 0.0001  max mem: 15689
[09:40:24.597042] Epoch: [20]  [ 80/345]  eta: 0:03:21  lr: 0.000125  loss: 0.7786 (0.7756)  time: 0.7613  data: 0.0001  max mem: 15689
[09:40:39.850188] Epoch: [20]  [100/345]  eta: 0:03:06  lr: 0.000125  loss: 0.7848 (0.7784)  time: 0.7626  data: 0.0001  max mem: 15689
[09:40:55.111674] Epoch: [20]  [120/345]  eta: 0:02:51  lr: 0.000125  loss: 0.7908 (0.7805)  time: 0.7630  data: 0.0001  max mem: 15689
[09:41:10.360518] Epoch: [20]  [140/345]  eta: 0:02:36  lr: 0.000125  loss: 0.7854 (0.7813)  time: 0.7624  data: 0.0001  max mem: 15689
[09:41:25.606161] Epoch: [20]  [160/345]  eta: 0:02:21  lr: 0.000125  loss: 0.7853 (0.7818)  time: 0.7622  data: 0.0001  max mem: 15689
[09:41:40.837246] Epoch: [20]  [180/345]  eta: 0:02:05  lr: 0.000125  loss: 0.7837 (0.7824)  time: 0.7615  data: 0.0001  max mem: 15689
[09:41:56.074027] Epoch: [20]  [200/345]  eta: 0:01:50  lr: 0.000125  loss: 0.7796 (0.7826)  time: 0.7618  data: 0.0001  max mem: 15689
[09:42:11.315880] Epoch: [20]  [220/345]  eta: 0:01:35  lr: 0.000125  loss: 0.7808 (0.7824)  time: 0.7620  data: 0.0001  max mem: 15689
[09:42:26.549353] Epoch: [20]  [240/345]  eta: 0:01:20  lr: 0.000125  loss: 0.7800 (0.7822)  time: 0.7616  data: 0.0001  max mem: 15689
[09:42:41.860942] Epoch: [20]  [260/345]  eta: 0:01:04  lr: 0.000125  loss: 0.7957 (0.7836)  time: 0.7655  data: 0.0001  max mem: 15689
[09:42:57.081504] Epoch: [20]  [280/345]  eta: 0:00:49  lr: 0.000125  loss: 0.8128 (0.7861)  time: 0.7610  data: 0.0001  max mem: 15689
[09:43:12.309614] Epoch: [20]  [300/345]  eta: 0:00:34  lr: 0.000125  loss: 0.7929 (0.7867)  time: 0.7614  data: 0.0001  max mem: 15689
[09:43:27.534911] Epoch: [20]  [320/345]  eta: 0:00:19  lr: 0.000125  loss: 0.7842 (0.7864)  time: 0.7612  data: 0.0001  max mem: 15689
[09:43:42.757503] Epoch: [20]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.7824 (0.7863)  time: 0.7611  data: 0.0001  max mem: 15689
[09:43:45.801648] Epoch: [20]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.7833 (0.7863)  time: 0.7609  data: 0.0001  max mem: 15689
[09:43:45.842986] Epoch: [20] Total time: 0:04:22 (0.7622 s / it)
[09:43:45.843432] Averaged stats: lr: 0.000125  loss: 0.7833 (0.7863)
[09:43:46.184700] Test:  [  0/345]  eta: 0:01:56  loss: 0.7662 (0.7662)  time: 0.3364  data: 0.1522  max mem: 15689
[09:43:48.043887] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7432 (0.7491)  time: 0.1995  data: 0.0139  max mem: 15689
[09:43:49.907542] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7463 (0.7484)  time: 0.1861  data: 0.0001  max mem: 15689
[09:43:51.773562] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7501 (0.7507)  time: 0.1864  data: 0.0001  max mem: 15689
[09:43:53.644247] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7510 (0.7508)  time: 0.1868  data: 0.0001  max mem: 15689
[09:43:55.518104] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7510 (0.7512)  time: 0.1872  data: 0.0001  max mem: 15689
[09:43:57.396597] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7540 (0.7528)  time: 0.1876  data: 0.0001  max mem: 15689
[09:43:59.277483] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7540 (0.7529)  time: 0.1879  data: 0.0001  max mem: 15689
[09:44:01.163657] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7514 (0.7521)  time: 0.1883  data: 0.0001  max mem: 15689
[09:44:03.052407] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7514 (0.7531)  time: 0.1887  data: 0.0001  max mem: 15689
[09:44:04.944376] Test:  [100/345]  eta: 0:00:46  loss: 0.7497 (0.7528)  time: 0.1890  data: 0.0001  max mem: 15689
[09:44:06.839913] Test:  [110/345]  eta: 0:00:44  loss: 0.7495 (0.7528)  time: 0.1893  data: 0.0001  max mem: 15689
[09:44:08.741071] Test:  [120/345]  eta: 0:00:42  loss: 0.7524 (0.7527)  time: 0.1898  data: 0.0001  max mem: 15689
[09:44:10.643534] Test:  [130/345]  eta: 0:00:40  loss: 0.7524 (0.7532)  time: 0.1901  data: 0.0001  max mem: 15689
[09:44:12.550422] Test:  [140/345]  eta: 0:00:38  loss: 0.7500 (0.7530)  time: 0.1904  data: 0.0001  max mem: 15689
[09:44:14.459517] Test:  [150/345]  eta: 0:00:36  loss: 0.7547 (0.7536)  time: 0.1907  data: 0.0001  max mem: 15689
[09:44:16.372162] Test:  [160/345]  eta: 0:00:35  loss: 0.7514 (0.7534)  time: 0.1910  data: 0.0001  max mem: 15689
[09:44:18.289026] Test:  [170/345]  eta: 0:00:33  loss: 0.7494 (0.7534)  time: 0.1914  data: 0.0001  max mem: 15689
[09:44:20.209216] Test:  [180/345]  eta: 0:00:31  loss: 0.7440 (0.7529)  time: 0.1918  data: 0.0001  max mem: 15689
[09:44:22.132102] Test:  [190/345]  eta: 0:00:29  loss: 0.7434 (0.7524)  time: 0.1921  data: 0.0001  max mem: 15689
[09:44:24.059142] Test:  [200/345]  eta: 0:00:27  loss: 0.7438 (0.7520)  time: 0.1924  data: 0.0001  max mem: 15689
[09:44:25.990155] Test:  [210/345]  eta: 0:00:25  loss: 0.7472 (0.7527)  time: 0.1929  data: 0.0001  max mem: 15689
[09:44:27.924001] Test:  [220/345]  eta: 0:00:23  loss: 0.7482 (0.7524)  time: 0.1932  data: 0.0001  max mem: 15689
[09:44:29.860194] Test:  [230/345]  eta: 0:00:21  loss: 0.7483 (0.7525)  time: 0.1935  data: 0.0001  max mem: 15689
[09:44:31.803122] Test:  [240/345]  eta: 0:00:20  loss: 0.7507 (0.7525)  time: 0.1939  data: 0.0001  max mem: 15689
[09:44:33.747515] Test:  [250/345]  eta: 0:00:18  loss: 0.7551 (0.7529)  time: 0.1943  data: 0.0001  max mem: 15689
[09:44:35.694065] Test:  [260/345]  eta: 0:00:16  loss: 0.7519 (0.7527)  time: 0.1945  data: 0.0001  max mem: 15689
[09:44:37.645451] Test:  [270/345]  eta: 0:00:14  loss: 0.7525 (0.7530)  time: 0.1948  data: 0.0001  max mem: 15689
[09:44:39.598757] Test:  [280/345]  eta: 0:00:12  loss: 0.7593 (0.7533)  time: 0.1952  data: 0.0001  max mem: 15689
[09:44:41.557262] Test:  [290/345]  eta: 0:00:10  loss: 0.7557 (0.7534)  time: 0.1955  data: 0.0001  max mem: 15689
[09:44:43.519065] Test:  [300/345]  eta: 0:00:08  loss: 0.7512 (0.7534)  time: 0.1960  data: 0.0001  max mem: 15689
[09:44:45.483769] Test:  [310/345]  eta: 0:00:06  loss: 0.7502 (0.7532)  time: 0.1963  data: 0.0001  max mem: 15689
[09:44:47.452006] Test:  [320/345]  eta: 0:00:04  loss: 0.7581 (0.7535)  time: 0.1966  data: 0.0001  max mem: 15689
[09:44:49.424004] Test:  [330/345]  eta: 0:00:02  loss: 0.7545 (0.7533)  time: 0.1970  data: 0.0001  max mem: 15689
[09:44:51.397763] Test:  [340/345]  eta: 0:00:00  loss: 0.7521 (0.7535)  time: 0.1972  data: 0.0001  max mem: 15689
[09:44:52.189241] Test:  [344/345]  eta: 0:00:00  loss: 0.7538 (0.7537)  time: 0.1974  data: 0.0001  max mem: 15689
[09:44:52.250102] Test: Total time: 0:01:06 (0.1925 s / it)
[09:45:02.824766] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8208 (0.8208)  time: 0.3274  data: 0.1451  max mem: 15689
[09:45:04.662856] Test:  [10/57]  eta: 0:00:09  loss: 0.8590 (0.8641)  time: 0.1968  data: 0.0132  max mem: 15689
[09:45:06.507979] Test:  [20/57]  eta: 0:00:07  loss: 0.8590 (0.8561)  time: 0.1841  data: 0.0001  max mem: 15689
[09:45:08.353682] Test:  [30/57]  eta: 0:00:05  loss: 0.7566 (0.8207)  time: 0.1845  data: 0.0001  max mem: 15689
[09:45:10.206239] Test:  [40/57]  eta: 0:00:03  loss: 0.7406 (0.8022)  time: 0.1849  data: 0.0001  max mem: 15689
[09:45:12.063054] Test:  [50/57]  eta: 0:00:01  loss: 0.7376 (0.7963)  time: 0.1854  data: 0.0001  max mem: 15689
[09:45:13.064579] Test:  [56/57]  eta: 0:00:00  loss: 0.7813 (0.8032)  time: 0.1800  data: 0.0001  max mem: 15689
[09:45:13.120493] Test: Total time: 0:00:10 (0.1864 s / it)
[09:45:14.989274] Dice score of the network on the train images: 0.752070, val images: 0.810097
[09:45:14.993799] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:45:15.897050] Epoch: [21]  [  0/345]  eta: 0:05:11  lr: 0.000125  loss: 0.7889 (0.7889)  time: 0.9021  data: 0.1459  max mem: 15689
[09:45:31.026791] Epoch: [21]  [ 20/345]  eta: 0:04:08  lr: 0.000125  loss: 0.7713 (0.7719)  time: 0.7564  data: 0.0001  max mem: 15689
[09:45:46.220830] Epoch: [21]  [ 40/345]  eta: 0:03:52  lr: 0.000125  loss: 0.7802 (0.7758)  time: 0.7597  data: 0.0001  max mem: 15689
[09:46:01.410195] Epoch: [21]  [ 60/345]  eta: 0:03:36  lr: 0.000125  loss: 0.7776 (0.7768)  time: 0.7594  data: 0.0001  max mem: 15689
[09:46:16.646257] Epoch: [21]  [ 80/345]  eta: 0:03:21  lr: 0.000124  loss: 0.7725 (0.7767)  time: 0.7618  data: 0.0001  max mem: 15689
[09:46:31.880059] Epoch: [21]  [100/345]  eta: 0:03:06  lr: 0.000124  loss: 0.7696 (0.7759)  time: 0.7617  data: 0.0001  max mem: 15689
[09:46:47.127304] Epoch: [21]  [120/345]  eta: 0:02:51  lr: 0.000124  loss: 0.7708 (0.7748)  time: 0.7623  data: 0.0001  max mem: 15689
[09:47:02.373133] Epoch: [21]  [140/345]  eta: 0:02:36  lr: 0.000124  loss: 0.7651 (0.7735)  time: 0.7623  data: 0.0001  max mem: 15689
[09:47:17.738588] Epoch: [21]  [160/345]  eta: 0:02:21  lr: 0.000124  loss: 0.7656 (0.7725)  time: 0.7682  data: 0.0001  max mem: 15689
[09:47:32.974064] Epoch: [21]  [180/345]  eta: 0:02:05  lr: 0.000124  loss: 0.7661 (0.7719)  time: 0.7617  data: 0.0001  max mem: 15689
[09:47:48.217356] Epoch: [21]  [200/345]  eta: 0:01:50  lr: 0.000124  loss: 0.7564 (0.7709)  time: 0.7621  data: 0.0001  max mem: 15689
[09:48:03.473441] Epoch: [21]  [220/345]  eta: 0:01:35  lr: 0.000124  loss: 0.7655 (0.7707)  time: 0.7628  data: 0.0001  max mem: 15689
[09:48:18.719580] Epoch: [21]  [240/345]  eta: 0:01:20  lr: 0.000124  loss: 0.7733 (0.7711)  time: 0.7623  data: 0.0001  max mem: 15689
[09:48:33.955318] Epoch: [21]  [260/345]  eta: 0:01:04  lr: 0.000124  loss: 0.7704 (0.7712)  time: 0.7617  data: 0.0001  max mem: 15689
[09:48:49.202966] Epoch: [21]  [280/345]  eta: 0:00:49  lr: 0.000124  loss: 0.7641 (0.7710)  time: 0.7623  data: 0.0001  max mem: 15689
[09:49:04.441412] Epoch: [21]  [300/345]  eta: 0:00:34  lr: 0.000124  loss: 0.7656 (0.7707)  time: 0.7619  data: 0.0001  max mem: 15689
[09:49:19.685447] Epoch: [21]  [320/345]  eta: 0:00:19  lr: 0.000124  loss: 0.7659 (0.7707)  time: 0.7622  data: 0.0001  max mem: 15689
[09:49:34.901167] Epoch: [21]  [340/345]  eta: 0:00:03  lr: 0.000124  loss: 0.7793 (0.7713)  time: 0.7607  data: 0.0001  max mem: 15689
[09:49:37.946505] Epoch: [21]  [344/345]  eta: 0:00:00  lr: 0.000124  loss: 0.7829 (0.7714)  time: 0.7607  data: 0.0001  max mem: 15689
[09:49:38.006826] Epoch: [21] Total time: 0:04:23 (0.7624 s / it)
[09:49:38.007089] Averaged stats: lr: 0.000124  loss: 0.7829 (0.7714)
[09:49:38.344979] Test:  [  0/345]  eta: 0:01:54  loss: 0.7508 (0.7508)  time: 0.3316  data: 0.1474  max mem: 15689
[09:49:40.204926] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7508 (0.7512)  time: 0.1992  data: 0.0135  max mem: 15689
[09:49:42.066541] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7490 (0.7484)  time: 0.1860  data: 0.0001  max mem: 15689
[09:49:43.933509] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7490 (0.7508)  time: 0.1863  data: 0.0001  max mem: 15689
[09:49:45.804404] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7549 (0.7519)  time: 0.1868  data: 0.0001  max mem: 15689
[09:49:47.679259] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7525 (0.7521)  time: 0.1872  data: 0.0001  max mem: 15689
[09:49:49.555568] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7484 (0.7512)  time: 0.1875  data: 0.0001  max mem: 15689
[09:49:51.436536] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7484 (0.7512)  time: 0.1878  data: 0.0001  max mem: 15689
[09:49:53.322120] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7519 (0.7518)  time: 0.1883  data: 0.0001  max mem: 15689
[09:49:55.210412] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7595 (0.7525)  time: 0.1886  data: 0.0001  max mem: 15689
[09:49:57.102959] Test:  [100/345]  eta: 0:00:46  loss: 0.7512 (0.7521)  time: 0.1890  data: 0.0001  max mem: 15689
[09:49:58.998483] Test:  [110/345]  eta: 0:00:44  loss: 0.7546 (0.7531)  time: 0.1894  data: 0.0001  max mem: 15689
[09:50:00.898085] Test:  [120/345]  eta: 0:00:42  loss: 0.7627 (0.7537)  time: 0.1897  data: 0.0001  max mem: 15689
[09:50:02.801156] Test:  [130/345]  eta: 0:00:40  loss: 0.7535 (0.7538)  time: 0.1901  data: 0.0001  max mem: 15689
[09:50:04.707059] Test:  [140/345]  eta: 0:00:38  loss: 0.7488 (0.7540)  time: 0.1904  data: 0.0001  max mem: 15689
[09:50:06.616524] Test:  [150/345]  eta: 0:00:36  loss: 0.7536 (0.7544)  time: 0.1907  data: 0.0001  max mem: 15689
[09:50:08.528739] Test:  [160/345]  eta: 0:00:35  loss: 0.7543 (0.7542)  time: 0.1910  data: 0.0001  max mem: 15689
[09:50:10.443076] Test:  [170/345]  eta: 0:00:33  loss: 0.7543 (0.7541)  time: 0.1913  data: 0.0001  max mem: 15689
[09:50:12.363181] Test:  [180/345]  eta: 0:00:31  loss: 0.7451 (0.7535)  time: 0.1917  data: 0.0001  max mem: 15689
[09:50:14.286787] Test:  [190/345]  eta: 0:00:29  loss: 0.7442 (0.7530)  time: 0.1921  data: 0.0001  max mem: 15689
[09:50:16.213100] Test:  [200/345]  eta: 0:00:27  loss: 0.7417 (0.7530)  time: 0.1924  data: 0.0001  max mem: 15689
[09:50:18.142662] Test:  [210/345]  eta: 0:00:25  loss: 0.7478 (0.7527)  time: 0.1927  data: 0.0001  max mem: 15689
[09:50:20.077188] Test:  [220/345]  eta: 0:00:23  loss: 0.7509 (0.7527)  time: 0.1932  data: 0.0001  max mem: 15689
[09:50:22.015526] Test:  [230/345]  eta: 0:00:21  loss: 0.7534 (0.7524)  time: 0.1936  data: 0.0001  max mem: 15689
[09:50:23.956603] Test:  [240/345]  eta: 0:00:20  loss: 0.7474 (0.7524)  time: 0.1939  data: 0.0001  max mem: 15689
[09:50:25.900587] Test:  [250/345]  eta: 0:00:18  loss: 0.7566 (0.7527)  time: 0.1942  data: 0.0001  max mem: 15689
[09:50:27.848130] Test:  [260/345]  eta: 0:00:16  loss: 0.7544 (0.7525)  time: 0.1945  data: 0.0001  max mem: 15689
[09:50:29.798965] Test:  [270/345]  eta: 0:00:14  loss: 0.7531 (0.7530)  time: 0.1949  data: 0.0001  max mem: 15689
[09:50:31.753809] Test:  [280/345]  eta: 0:00:12  loss: 0.7531 (0.7529)  time: 0.1952  data: 0.0001  max mem: 15689
[09:50:33.713272] Test:  [290/345]  eta: 0:00:10  loss: 0.7522 (0.7527)  time: 0.1957  data: 0.0001  max mem: 15689
[09:50:35.674167] Test:  [300/345]  eta: 0:00:08  loss: 0.7481 (0.7525)  time: 0.1960  data: 0.0001  max mem: 15689
[09:50:37.639168] Test:  [310/345]  eta: 0:00:06  loss: 0.7481 (0.7527)  time: 0.1962  data: 0.0001  max mem: 15689
[09:50:39.607396] Test:  [320/345]  eta: 0:00:04  loss: 0.7473 (0.7526)  time: 0.1966  data: 0.0001  max mem: 15689
[09:50:41.578463] Test:  [330/345]  eta: 0:00:02  loss: 0.7419 (0.7524)  time: 0.1969  data: 0.0001  max mem: 15689
[09:50:43.553917] Test:  [340/345]  eta: 0:00:00  loss: 0.7423 (0.7522)  time: 0.1973  data: 0.0001  max mem: 15689
[09:50:44.345363] Test:  [344/345]  eta: 0:00:00  loss: 0.7423 (0.7522)  time: 0.1974  data: 0.0001  max mem: 15689
[09:50:44.401244] Test: Total time: 0:01:06 (0.1924 s / it)
[09:50:55.001671] Test:  [ 0/57]  eta: 0:00:18  loss: 0.9230 (0.9230)  time: 0.3235  data: 0.1413  max mem: 15689
[09:50:56.839886] Test:  [10/57]  eta: 0:00:09  loss: 0.9230 (0.9134)  time: 0.1964  data: 0.0129  max mem: 15689
[09:50:58.683900] Test:  [20/57]  eta: 0:00:07  loss: 0.9102 (0.8939)  time: 0.1840  data: 0.0001  max mem: 15689
[09:51:00.530762] Test:  [30/57]  eta: 0:00:05  loss: 0.7911 (0.8526)  time: 0.1845  data: 0.0001  max mem: 15689
[09:51:02.384762] Test:  [40/57]  eta: 0:00:03  loss: 0.7578 (0.8303)  time: 0.1850  data: 0.0001  max mem: 15689
[09:51:04.242561] Test:  [50/57]  eta: 0:00:01  loss: 0.7591 (0.8243)  time: 0.1855  data: 0.0001  max mem: 15689
[09:51:05.243767] Test:  [56/57]  eta: 0:00:00  loss: 0.7890 (0.8301)  time: 0.1800  data: 0.0001  max mem: 15689
[09:51:05.301847] Test: Total time: 0:00:10 (0.1864 s / it)
[09:51:07.130893] Dice score of the network on the train images: 0.793195, val images: 0.803144
[09:51:07.131122] saving best_prec_model_0 @ epoch 21
[09:51:08.301052] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:51:09.206507] Epoch: [22]  [  0/345]  eta: 0:05:11  lr: 0.000124  loss: 0.7716 (0.7716)  time: 0.9043  data: 0.1479  max mem: 15689
[09:51:24.319879] Epoch: [22]  [ 20/345]  eta: 0:04:07  lr: 0.000124  loss: 0.7804 (0.7761)  time: 0.7556  data: 0.0001  max mem: 15689
[09:51:39.500371] Epoch: [22]  [ 40/345]  eta: 0:03:52  lr: 0.000123  loss: 0.7720 (0.7732)  time: 0.7590  data: 0.0001  max mem: 15689
[09:51:54.719937] Epoch: [22]  [ 60/345]  eta: 0:03:36  lr: 0.000123  loss: 0.7807 (0.7777)  time: 0.7609  data: 0.0001  max mem: 15689
[09:52:09.976928] Epoch: [22]  [ 80/345]  eta: 0:03:21  lr: 0.000123  loss: 0.8029 (0.7830)  time: 0.7628  data: 0.0001  max mem: 15689
[09:52:25.247550] Epoch: [22]  [100/345]  eta: 0:03:06  lr: 0.000123  loss: 0.7772 (0.7826)  time: 0.7635  data: 0.0001  max mem: 15689
[09:52:40.520471] Epoch: [22]  [120/345]  eta: 0:02:51  lr: 0.000123  loss: 0.7652 (0.7795)  time: 0.7636  data: 0.0001  max mem: 15689
[09:52:55.795745] Epoch: [22]  [140/345]  eta: 0:02:36  lr: 0.000123  loss: 0.7727 (0.7787)  time: 0.7637  data: 0.0001  max mem: 15689
[09:53:11.061539] Epoch: [22]  [160/345]  eta: 0:02:21  lr: 0.000123  loss: 0.7585 (0.7770)  time: 0.7632  data: 0.0001  max mem: 15689
[09:53:26.324424] Epoch: [22]  [180/345]  eta: 0:02:05  lr: 0.000123  loss: 0.7682 (0.7762)  time: 0.7631  data: 0.0001  max mem: 15689
[09:53:41.569330] Epoch: [22]  [200/345]  eta: 0:01:50  lr: 0.000123  loss: 0.7776 (0.7764)  time: 0.7622  data: 0.0001  max mem: 15689
[09:53:56.893704] Epoch: [22]  [220/345]  eta: 0:01:35  lr: 0.000123  loss: 0.7600 (0.7748)  time: 0.7662  data: 0.0001  max mem: 15689
[09:54:12.128965] Epoch: [22]  [240/345]  eta: 0:01:20  lr: 0.000123  loss: 0.7666 (0.7743)  time: 0.7617  data: 0.0001  max mem: 15689
[09:54:27.354101] Epoch: [22]  [260/345]  eta: 0:01:04  lr: 0.000122  loss: 0.7684 (0.7741)  time: 0.7612  data: 0.0001  max mem: 15689
[09:54:42.584527] Epoch: [22]  [280/345]  eta: 0:00:49  lr: 0.000122  loss: 0.7555 (0.7728)  time: 0.7615  data: 0.0001  max mem: 15689
[09:54:57.819930] Epoch: [22]  [300/345]  eta: 0:00:34  lr: 0.000122  loss: 0.7642 (0.7725)  time: 0.7617  data: 0.0001  max mem: 15689
[09:55:13.046762] Epoch: [22]  [320/345]  eta: 0:00:19  lr: 0.000122  loss: 0.7616 (0.7720)  time: 0.7613  data: 0.0001  max mem: 15689
[09:55:28.271702] Epoch: [22]  [340/345]  eta: 0:00:03  lr: 0.000122  loss: 0.7673 (0.7715)  time: 0.7612  data: 0.0001  max mem: 15689
[09:55:31.318029] Epoch: [22]  [344/345]  eta: 0:00:00  lr: 0.000122  loss: 0.7587 (0.7712)  time: 0.7612  data: 0.0001  max mem: 15689
[09:55:31.383918] Epoch: [22] Total time: 0:04:23 (0.7626 s / it)
[09:55:31.384396] Averaged stats: lr: 0.000122  loss: 0.7587 (0.7712)
[09:55:31.733588] Test:  [  0/345]  eta: 0:01:58  loss: 0.7249 (0.7249)  time: 0.3447  data: 0.1608  max mem: 15689
[09:55:33.592321] Test:  [ 10/345]  eta: 0:01:07  loss: 0.7265 (0.7283)  time: 0.2002  data: 0.0147  max mem: 15689
[09:55:35.454681] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7265 (0.7263)  time: 0.1860  data: 0.0001  max mem: 15689
[09:55:37.320240] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7311 (0.7285)  time: 0.1863  data: 0.0001  max mem: 15689
[09:55:39.187961] Test:  [ 40/345]  eta: 0:00:58  loss: 0.7324 (0.7295)  time: 0.1866  data: 0.0001  max mem: 15689
[09:55:41.061000] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7299 (0.7293)  time: 0.1870  data: 0.0001  max mem: 15689
[09:55:42.938288] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7288 (0.7292)  time: 0.1875  data: 0.0001  max mem: 15689
[09:55:44.819580] Test:  [ 70/345]  eta: 0:00:52  loss: 0.7273 (0.7291)  time: 0.1879  data: 0.0001  max mem: 15689
[09:55:46.704506] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7291 (0.7292)  time: 0.1883  data: 0.0001  max mem: 15689
[09:55:48.592654] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7305 (0.7292)  time: 0.1886  data: 0.0001  max mem: 15689
[09:55:50.483319] Test:  [100/345]  eta: 0:00:46  loss: 0.7300 (0.7297)  time: 0.1889  data: 0.0001  max mem: 15689
[09:55:52.377167] Test:  [110/345]  eta: 0:00:44  loss: 0.7300 (0.7298)  time: 0.1892  data: 0.0001  max mem: 15689
[09:55:54.276504] Test:  [120/345]  eta: 0:00:42  loss: 0.7312 (0.7301)  time: 0.1896  data: 0.0001  max mem: 15689
[09:55:56.179497] Test:  [130/345]  eta: 0:00:40  loss: 0.7344 (0.7309)  time: 0.1901  data: 0.0001  max mem: 15689
[09:55:58.084009] Test:  [140/345]  eta: 0:00:38  loss: 0.7369 (0.7313)  time: 0.1903  data: 0.0001  max mem: 15689
[09:55:59.992833] Test:  [150/345]  eta: 0:00:36  loss: 0.7270 (0.7306)  time: 0.1906  data: 0.0001  max mem: 15689
[09:56:01.905196] Test:  [160/345]  eta: 0:00:35  loss: 0.7273 (0.7311)  time: 0.1910  data: 0.0001  max mem: 15689
[09:56:03.820995] Test:  [170/345]  eta: 0:00:33  loss: 0.7289 (0.7311)  time: 0.1914  data: 0.0001  max mem: 15689
[09:56:05.741745] Test:  [180/345]  eta: 0:00:31  loss: 0.7305 (0.7312)  time: 0.1918  data: 0.0001  max mem: 15689
[09:56:07.664325] Test:  [190/345]  eta: 0:00:29  loss: 0.7313 (0.7315)  time: 0.1921  data: 0.0001  max mem: 15689
[09:56:09.590170] Test:  [200/345]  eta: 0:00:27  loss: 0.7321 (0.7313)  time: 0.1924  data: 0.0001  max mem: 15689
[09:56:11.520786] Test:  [210/345]  eta: 0:00:25  loss: 0.7321 (0.7313)  time: 0.1928  data: 0.0001  max mem: 15689
[09:56:13.453591] Test:  [220/345]  eta: 0:00:23  loss: 0.7322 (0.7315)  time: 0.1931  data: 0.0001  max mem: 15689
[09:56:15.392134] Test:  [230/345]  eta: 0:00:21  loss: 0.7302 (0.7313)  time: 0.1935  data: 0.0001  max mem: 15689
[09:56:17.333623] Test:  [240/345]  eta: 0:00:20  loss: 0.7295 (0.7315)  time: 0.1940  data: 0.0001  max mem: 15689
[09:56:19.276198] Test:  [250/345]  eta: 0:00:18  loss: 0.7388 (0.7320)  time: 0.1942  data: 0.0001  max mem: 15689
[09:56:21.222998] Test:  [260/345]  eta: 0:00:16  loss: 0.7351 (0.7320)  time: 0.1944  data: 0.0001  max mem: 15689
[09:56:23.173113] Test:  [270/345]  eta: 0:00:14  loss: 0.7301 (0.7321)  time: 0.1948  data: 0.0001  max mem: 15689
[09:56:25.126870] Test:  [280/345]  eta: 0:00:12  loss: 0.7301 (0.7322)  time: 0.1951  data: 0.0001  max mem: 15689
[09:56:27.086359] Test:  [290/345]  eta: 0:00:10  loss: 0.7345 (0.7323)  time: 0.1956  data: 0.0001  max mem: 15689
[09:56:29.046013] Test:  [300/345]  eta: 0:00:08  loss: 0.7335 (0.7323)  time: 0.1959  data: 0.0001  max mem: 15689
[09:56:31.008907] Test:  [310/345]  eta: 0:00:06  loss: 0.7333 (0.7325)  time: 0.1961  data: 0.0001  max mem: 15689
[09:56:32.977683] Test:  [320/345]  eta: 0:00:04  loss: 0.7327 (0.7326)  time: 0.1965  data: 0.0001  max mem: 15689
[09:56:34.948866] Test:  [330/345]  eta: 0:00:02  loss: 0.7404 (0.7330)  time: 0.1969  data: 0.0001  max mem: 15689
[09:56:36.923757] Test:  [340/345]  eta: 0:00:00  loss: 0.7403 (0.7332)  time: 0.1973  data: 0.0001  max mem: 15689
[09:56:37.715129] Test:  [344/345]  eta: 0:00:00  loss: 0.7402 (0.7332)  time: 0.1974  data: 0.0001  max mem: 15689
[09:56:37.774707] Test: Total time: 0:01:06 (0.1924 s / it)
[09:56:48.351594] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8811 (0.8811)  time: 0.3249  data: 0.1434  max mem: 15689
[09:56:50.188323] Test:  [10/57]  eta: 0:00:09  loss: 0.8947 (0.8873)  time: 0.1964  data: 0.0131  max mem: 15689
[09:56:52.031056] Test:  [20/57]  eta: 0:00:07  loss: 0.8997 (0.8769)  time: 0.1839  data: 0.0001  max mem: 15689
[09:56:53.877966] Test:  [30/57]  eta: 0:00:05  loss: 0.7828 (0.8364)  time: 0.1844  data: 0.0001  max mem: 15689
[09:56:55.727496] Test:  [40/57]  eta: 0:00:03  loss: 0.7425 (0.8152)  time: 0.1848  data: 0.0001  max mem: 15689
[09:56:57.582769] Test:  [50/57]  eta: 0:00:01  loss: 0.7425 (0.8082)  time: 0.1852  data: 0.0001  max mem: 15689
[09:56:58.582924] Test:  [56/57]  eta: 0:00:00  loss: 0.7570 (0.8129)  time: 0.1798  data: 0.0001  max mem: 15689
[09:56:58.642462] Test: Total time: 0:00:10 (0.1863 s / it)
[09:57:00.485330] Dice score of the network on the train images: 0.799690, val images: 0.814637
[09:57:00.490397] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[09:57:01.390407] Epoch: [23]  [  0/345]  eta: 0:05:10  lr: 0.000122  loss: 0.7562 (0.7562)  time: 0.8989  data: 0.1458  max mem: 15689
[09:57:16.489090] Epoch: [23]  [ 20/345]  eta: 0:04:07  lr: 0.000122  loss: 0.7624 (0.7651)  time: 0.7549  data: 0.0001  max mem: 15689
[09:57:31.660173] Epoch: [23]  [ 40/345]  eta: 0:03:51  lr: 0.000122  loss: 0.7777 (0.7732)  time: 0.7585  data: 0.0001  max mem: 15689
[09:57:46.884449] Epoch: [23]  [ 60/345]  eta: 0:03:36  lr: 0.000122  loss: 0.7700 (0.7728)  time: 0.7612  data: 0.0001  max mem: 15689
[09:58:02.207702] Epoch: [23]  [ 80/345]  eta: 0:03:21  lr: 0.000121  loss: 0.7622 (0.7709)  time: 0.7661  data: 0.0001  max mem: 15689
[09:58:17.472560] Epoch: [23]  [100/345]  eta: 0:03:06  lr: 0.000121  loss: 0.7599 (0.7690)  time: 0.7632  data: 0.0001  max mem: 15689
[09:58:32.752506] Epoch: [23]  [120/345]  eta: 0:02:51  lr: 0.000121  loss: 0.7574 (0.7683)  time: 0.7640  data: 0.0001  max mem: 15689
[09:58:48.003307] Epoch: [23]  [140/345]  eta: 0:02:36  lr: 0.000121  loss: 0.7670 (0.7674)  time: 0.7625  data: 0.0001  max mem: 15689
[09:59:03.248440] Epoch: [23]  [160/345]  eta: 0:02:21  lr: 0.000121  loss: 0.7568 (0.7670)  time: 0.7622  data: 0.0001  max mem: 15689
[09:59:18.493285] Epoch: [23]  [180/345]  eta: 0:02:05  lr: 0.000121  loss: 0.7765 (0.7684)  time: 0.7622  data: 0.0001  max mem: 15689
[09:59:33.731829] Epoch: [23]  [200/345]  eta: 0:01:50  lr: 0.000121  loss: 0.7677 (0.7688)  time: 0.7619  data: 0.0001  max mem: 15689
[09:59:48.960131] Epoch: [23]  [220/345]  eta: 0:01:35  lr: 0.000121  loss: 0.7683 (0.7693)  time: 0.7614  data: 0.0001  max mem: 15689
[10:00:04.186583] Epoch: [23]  [240/345]  eta: 0:01:20  lr: 0.000120  loss: 0.7731 (0.7696)  time: 0.7613  data: 0.0001  max mem: 15689
[10:00:19.412874] Epoch: [23]  [260/345]  eta: 0:01:04  lr: 0.000120  loss: 0.7741 (0.7698)  time: 0.7613  data: 0.0001  max mem: 15689
[10:00:34.628903] Epoch: [23]  [280/345]  eta: 0:00:49  lr: 0.000120  loss: 0.7606 (0.7690)  time: 0.7608  data: 0.0001  max mem: 15689
[10:00:49.849734] Epoch: [23]  [300/345]  eta: 0:00:34  lr: 0.000120  loss: 0.7547 (0.7682)  time: 0.7610  data: 0.0001  max mem: 15689
[10:01:05.067479] Epoch: [23]  [320/345]  eta: 0:00:19  lr: 0.000120  loss: 0.7586 (0.7677)  time: 0.7608  data: 0.0001  max mem: 15689
[10:01:20.276958] Epoch: [23]  [340/345]  eta: 0:00:03  lr: 0.000120  loss: 0.7523 (0.7668)  time: 0.7604  data: 0.0001  max mem: 15689
[10:01:23.321299] Epoch: [23]  [344/345]  eta: 0:00:00  lr: 0.000120  loss: 0.7492 (0.7666)  time: 0.7604  data: 0.0001  max mem: 15689
[10:01:23.387442] Epoch: [23] Total time: 0:04:22 (0.7620 s / it)
[10:01:23.387759] Averaged stats: lr: 0.000120  loss: 0.7492 (0.7666)
[10:01:23.730117] Test:  [  0/345]  eta: 0:01:56  loss: 0.7399 (0.7399)  time: 0.3380  data: 0.1543  max mem: 15689
[10:01:25.589137] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7397 (0.7325)  time: 0.1996  data: 0.0141  max mem: 15689
[10:01:27.450564] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7327 (0.7325)  time: 0.1859  data: 0.0001  max mem: 15689
[10:01:29.316413] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7270 (0.7299)  time: 0.1863  data: 0.0001  max mem: 15689
[10:01:31.184160] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7270 (0.7300)  time: 0.1866  data: 0.0001  max mem: 15689
[10:01:33.056822] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7335 (0.7312)  time: 0.1870  data: 0.0001  max mem: 15689
[10:01:34.932409] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7277 (0.7311)  time: 0.1874  data: 0.0001  max mem: 15689
[10:01:36.812722] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7253 (0.7297)  time: 0.1877  data: 0.0001  max mem: 15689
[10:01:38.698081] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7223 (0.7295)  time: 0.1882  data: 0.0001  max mem: 15689
[10:01:40.585164] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7319 (0.7298)  time: 0.1886  data: 0.0001  max mem: 15689
[10:01:42.475478] Test:  [100/345]  eta: 0:00:46  loss: 0.7322 (0.7300)  time: 0.1888  data: 0.0001  max mem: 15689
[10:01:44.371565] Test:  [110/345]  eta: 0:00:44  loss: 0.7322 (0.7304)  time: 0.1893  data: 0.0001  max mem: 15689
[10:01:46.269794] Test:  [120/345]  eta: 0:00:42  loss: 0.7313 (0.7298)  time: 0.1897  data: 0.0001  max mem: 15689
[10:01:48.171208] Test:  [130/345]  eta: 0:00:40  loss: 0.7245 (0.7297)  time: 0.1899  data: 0.0001  max mem: 15689
[10:01:50.075906] Test:  [140/345]  eta: 0:00:38  loss: 0.7230 (0.7292)  time: 0.1903  data: 0.0001  max mem: 15689
[10:01:51.983675] Test:  [150/345]  eta: 0:00:36  loss: 0.7185 (0.7287)  time: 0.1906  data: 0.0001  max mem: 15689
[10:01:53.894758] Test:  [160/345]  eta: 0:00:35  loss: 0.7240 (0.7289)  time: 0.1909  data: 0.0001  max mem: 15689
[10:01:55.808708] Test:  [170/345]  eta: 0:00:33  loss: 0.7319 (0.7291)  time: 0.1912  data: 0.0001  max mem: 15689
[10:01:57.727897] Test:  [180/345]  eta: 0:00:31  loss: 0.7272 (0.7291)  time: 0.1916  data: 0.0001  max mem: 15689
[10:01:59.650515] Test:  [190/345]  eta: 0:00:29  loss: 0.7269 (0.7290)  time: 0.1920  data: 0.0001  max mem: 15689
[10:02:01.574505] Test:  [200/345]  eta: 0:00:27  loss: 0.7303 (0.7292)  time: 0.1923  data: 0.0001  max mem: 15689
[10:02:03.505059] Test:  [210/345]  eta: 0:00:25  loss: 0.7263 (0.7290)  time: 0.1927  data: 0.0001  max mem: 15689
[10:02:05.436789] Test:  [220/345]  eta: 0:00:23  loss: 0.7226 (0.7289)  time: 0.1931  data: 0.0001  max mem: 15689
[10:02:07.372447] Test:  [230/345]  eta: 0:00:21  loss: 0.7251 (0.7289)  time: 0.1933  data: 0.0001  max mem: 15689
[10:02:09.311802] Test:  [240/345]  eta: 0:00:20  loss: 0.7208 (0.7286)  time: 0.1937  data: 0.0001  max mem: 15689
[10:02:11.256472] Test:  [250/345]  eta: 0:00:18  loss: 0.7255 (0.7285)  time: 0.1941  data: 0.0001  max mem: 15689
[10:02:13.204366] Test:  [260/345]  eta: 0:00:16  loss: 0.7283 (0.7287)  time: 0.1946  data: 0.0001  max mem: 15689
[10:02:15.154298] Test:  [270/345]  eta: 0:00:14  loss: 0.7306 (0.7286)  time: 0.1948  data: 0.0001  max mem: 15689
[10:02:17.107681] Test:  [280/345]  eta: 0:00:12  loss: 0.7193 (0.7284)  time: 0.1951  data: 0.0001  max mem: 15689
[10:02:19.066576] Test:  [290/345]  eta: 0:00:10  loss: 0.7193 (0.7282)  time: 0.1956  data: 0.0001  max mem: 15689
[10:02:21.028859] Test:  [300/345]  eta: 0:00:08  loss: 0.7214 (0.7282)  time: 0.1960  data: 0.0001  max mem: 15689
[10:02:22.992094] Test:  [310/345]  eta: 0:00:06  loss: 0.7252 (0.7282)  time: 0.1962  data: 0.0001  max mem: 15689
[10:02:24.959867] Test:  [320/345]  eta: 0:00:04  loss: 0.7252 (0.7280)  time: 0.1965  data: 0.0001  max mem: 15689
[10:02:26.929668] Test:  [330/345]  eta: 0:00:02  loss: 0.7216 (0.7278)  time: 0.1968  data: 0.0001  max mem: 15689
[10:02:28.903798] Test:  [340/345]  eta: 0:00:00  loss: 0.7242 (0.7277)  time: 0.1971  data: 0.0001  max mem: 15689
[10:02:29.694239] Test:  [344/345]  eta: 0:00:00  loss: 0.7258 (0.7277)  time: 0.1973  data: 0.0001  max mem: 15689
[10:02:29.755536] Test: Total time: 0:01:06 (0.1924 s / it)
[10:02:40.357912] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8330 (0.8330)  time: 0.3236  data: 0.1421  max mem: 15689
[10:02:42.194264] Test:  [10/57]  eta: 0:00:09  loss: 0.8815 (0.8765)  time: 0.1963  data: 0.0130  max mem: 15689
[10:02:44.036971] Test:  [20/57]  eta: 0:00:07  loss: 0.8815 (0.8671)  time: 0.1839  data: 0.0001  max mem: 15689
[10:02:45.882974] Test:  [30/57]  eta: 0:00:05  loss: 0.7517 (0.8269)  time: 0.1844  data: 0.0001  max mem: 15689
[10:02:47.733787] Test:  [40/57]  eta: 0:00:03  loss: 0.7478 (0.8060)  time: 0.1848  data: 0.0001  max mem: 15689
[10:02:49.589758] Test:  [50/57]  eta: 0:00:01  loss: 0.7396 (0.7981)  time: 0.1853  data: 0.0001  max mem: 15689
[10:02:50.591614] Test:  [56/57]  eta: 0:00:00  loss: 0.7553 (0.8038)  time: 0.1799  data: 0.0001  max mem: 15689
[10:02:50.650636] Test: Total time: 0:00:10 (0.1863 s / it)
[10:02:52.490779] Dice score of the network on the train images: 0.798863, val images: 0.819967
[10:02:52.491025] saving best_dice_model_0 @ epoch 23
[10:02:53.559288] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:02:54.464454] Epoch: [24]  [  0/345]  eta: 0:05:11  lr: 0.000120  loss: 0.7384 (0.7384)  time: 0.9041  data: 0.1494  max mem: 15689
[10:03:09.580418] Epoch: [24]  [ 20/345]  eta: 0:04:07  lr: 0.000119  loss: 0.7601 (0.7600)  time: 0.7558  data: 0.0001  max mem: 15689

[10:03:24.749645] Epoch: [24]  [ 40/345]  eta: 0:03:52  lr: 0.000119  loss: 0.7560 (0.7604)  time: 0.7584  data: 0.0001  max mem: 15689
[10:03:39.955471] Epoch: [24]  [ 60/345]  eta: 0:03:36  lr: 0.000119  loss: 0.7464 (0.7573)  time: 0.7603  data: 0.0001  max mem: 15689
[10:03:55.185074] Epoch: [24]  [ 80/345]  eta: 0:03:21  lr: 0.000119  loss: 0.7541 (0.7556)  time: 0.7614  data: 0.0001  max mem: 15689
[10:04:10.440130] Epoch: [24]  [100/345]  eta: 0:03:06  lr: 0.000119  loss: 0.7510 (0.7553)  time: 0.7627  data: 0.0001  max mem: 15689
[10:04:25.788920] Epoch: [24]  [120/345]  eta: 0:02:51  lr: 0.000119  loss: 0.7432 (0.7537)  time: 0.7674  data: 0.0001  max mem: 15689
[10:04:41.042441] Epoch: [24]  [140/345]  eta: 0:02:36  lr: 0.000118  loss: 0.7493 (0.7535)  time: 0.7626  data: 0.0001  max mem: 15689
[10:04:56.289026] Epoch: [24]  [160/345]  eta: 0:02:21  lr: 0.000118  loss: 0.7502 (0.7537)  time: 0.7623  data: 0.0001  max mem: 15689
[10:05:11.519533] Epoch: [24]  [180/345]  eta: 0:02:05  lr: 0.000118  loss: 0.7647 (0.7550)  time: 0.7615  data: 0.0001  max mem: 15689
[10:05:26.751149] Epoch: [24]  [200/345]  eta: 0:01:50  lr: 0.000118  loss: 0.7724 (0.7568)  time: 0.7615  data: 0.0001  max mem: 15689
[10:05:41.988513] Epoch: [24]  [220/345]  eta: 0:01:35  lr: 0.000118  loss: 0.7604 (0.7574)  time: 0.7618  data: 0.0001  max mem: 15689
[10:05:57.212227] Epoch: [24]  [240/345]  eta: 0:01:20  lr: 0.000118  loss: 0.7576 (0.7574)  time: 0.7611  data: 0.0001  max mem: 15689
[10:06:12.429773] Epoch: [24]  [260/345]  eta: 0:01:04  lr: 0.000117  loss: 0.7561 (0.7576)  time: 0.7608  data: 0.0001  max mem: 15689
[10:06:27.647077] Epoch: [24]  [280/345]  eta: 0:00:49  lr: 0.000117  loss: 0.7525 (0.7575)  time: 0.7608  data: 0.0001  max mem: 15689
[10:06:42.870514] Epoch: [24]  [300/345]  eta: 0:00:34  lr: 0.000117  loss: 0.7588 (0.7576)  time: 0.7611  data: 0.0001  max mem: 15689
[10:06:58.086398] Epoch: [24]  [320/345]  eta: 0:00:19  lr: 0.000117  loss: 0.7541 (0.7575)  time: 0.7608  data: 0.0001  max mem: 15689
[10:07:13.321630] Epoch: [24]  [340/345]  eta: 0:00:03  lr: 0.000117  loss: 0.7512 (0.7573)  time: 0.7617  data: 0.0001  max mem: 15689
[10:07:16.360102] Epoch: [24]  [344/345]  eta: 0:00:00  lr: 0.000117  loss: 0.7508 (0.7572)  time: 0.7613  data: 0.0001  max mem: 15689
[10:07:16.406540] Epoch: [24] Total time: 0:04:22 (0.7619 s / it)
[10:07:16.407051] Averaged stats: lr: 0.000117  loss: 0.7508 (0.7572)
[10:07:16.749507] Test:  [  0/345]  eta: 0:01:57  loss: 0.7342 (0.7342)  time: 0.3392  data: 0.1557  max mem: 15689
[10:07:18.610105] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7342 (0.7335)  time: 0.1999  data: 0.0142  max mem: 15689
[10:07:20.472741] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7315 (0.7334)  time: 0.1861  data: 0.0001  max mem: 15689
[10:07:22.338762] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7253 (0.7302)  time: 0.1864  data: 0.0001  max mem: 15689
[10:07:24.206525] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7305 (0.7327)  time: 0.1866  data: 0.0001  max mem: 15689
[10:07:26.080952] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7305 (0.7317)  time: 0.1871  data: 0.0001  max mem: 15689
[10:07:27.957618] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7238 (0.7298)  time: 0.1875  data: 0.0001  max mem: 15689
[10:07:29.836899] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7196 (0.7296)  time: 0.1877  data: 0.0001  max mem: 15689
[10:07:31.721767] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7225 (0.7284)  time: 0.1882  data: 0.0001  max mem: 15689
[10:07:33.610441] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7254 (0.7292)  time: 0.1886  data: 0.0001  max mem: 15689
[10:07:35.500150] Test:  [100/345]  eta: 0:00:46  loss: 0.7305 (0.7286)  time: 0.1889  data: 0.0001  max mem: 15689
[10:07:37.395614] Test:  [110/345]  eta: 0:00:44  loss: 0.7284 (0.7290)  time: 0.1892  data: 0.0001  max mem: 15689
[10:07:39.294966] Test:  [120/345]  eta: 0:00:42  loss: 0.7290 (0.7293)  time: 0.1897  data: 0.0001  max mem: 15689
[10:07:41.199237] Test:  [130/345]  eta: 0:00:40  loss: 0.7261 (0.7294)  time: 0.1901  data: 0.0001  max mem: 15689
[10:07:43.103812] Test:  [140/345]  eta: 0:00:38  loss: 0.7219 (0.7289)  time: 0.1904  data: 0.0001  max mem: 15689
[10:07:45.013284] Test:  [150/345]  eta: 0:00:36  loss: 0.7202 (0.7285)  time: 0.1907  data: 0.0001  max mem: 15689
[10:07:46.924741] Test:  [160/345]  eta: 0:00:35  loss: 0.7254 (0.7288)  time: 0.1910  data: 0.0001  max mem: 15689
[10:07:48.840179] Test:  [170/345]  eta: 0:00:33  loss: 0.7241 (0.7286)  time: 0.1913  data: 0.0001  max mem: 15689
[10:07:50.760341] Test:  [180/345]  eta: 0:00:31  loss: 0.7227 (0.7285)  time: 0.1917  data: 0.0001  max mem: 15689
[10:07:52.683198] Test:  [190/345]  eta: 0:00:29  loss: 0.7271 (0.7287)  time: 0.1921  data: 0.0001  max mem: 15689
[10:07:54.610187] Test:  [200/345]  eta: 0:00:27  loss: 0.7272 (0.7285)  time: 0.1924  data: 0.0001  max mem: 15689
[10:07:56.540463] Test:  [210/345]  eta: 0:00:25  loss: 0.7264 (0.7285)  time: 0.1928  data: 0.0001  max mem: 15689
[10:07:58.473579] Test:  [220/345]  eta: 0:00:23  loss: 0.7270 (0.7285)  time: 0.1931  data: 0.0001  max mem: 15689
[10:08:00.409676] Test:  [230/345]  eta: 0:00:21  loss: 0.7226 (0.7280)  time: 0.1934  data: 0.0001  max mem: 15689
[10:08:02.351089] Test:  [240/345]  eta: 0:00:20  loss: 0.7232 (0.7280)  time: 0.1938  data: 0.0001  max mem: 15689
[10:08:04.295263] Test:  [250/345]  eta: 0:00:18  loss: 0.7234 (0.7278)  time: 0.1942  data: 0.0001  max mem: 15689
[10:08:06.243797] Test:  [260/345]  eta: 0:00:16  loss: 0.7185 (0.7275)  time: 0.1946  data: 0.0001  max mem: 15689
[10:08:08.195348] Test:  [270/345]  eta: 0:00:14  loss: 0.7271 (0.7278)  time: 0.1950  data: 0.0001  max mem: 15689
[10:08:10.148516] Test:  [280/345]  eta: 0:00:12  loss: 0.7322 (0.7278)  time: 0.1952  data: 0.0001  max mem: 15689
[10:08:12.105704] Test:  [290/345]  eta: 0:00:10  loss: 0.7311 (0.7278)  time: 0.1955  data: 0.0001  max mem: 15689
[10:08:14.065840] Test:  [300/345]  eta: 0:00:08  loss: 0.7234 (0.7277)  time: 0.1958  data: 0.0001  max mem: 15689
[10:08:16.030322] Test:  [310/345]  eta: 0:00:06  loss: 0.7234 (0.7277)  time: 0.1962  data: 0.0001  max mem: 15689
[10:08:17.999250] Test:  [320/345]  eta: 0:00:04  loss: 0.7181 (0.7275)  time: 0.1966  data: 0.0001  max mem: 15689
[10:08:19.972725] Test:  [330/345]  eta: 0:00:02  loss: 0.7267 (0.7278)  time: 0.1971  data: 0.0001  max mem: 15689
[10:08:21.946952] Test:  [340/345]  eta: 0:00:00  loss: 0.7335 (0.7280)  time: 0.1973  data: 0.0001  max mem: 15689
[10:08:22.737857] Test:  [344/345]  eta: 0:00:00  loss: 0.7268 (0.7280)  time: 0.1974  data: 0.0001  max mem: 15689
[10:08:22.795279] Test: Total time: 0:01:06 (0.1924 s / it)
[10:08:33.516012] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8286 (0.8286)  time: 0.3264  data: 0.1448  max mem: 15689
[10:08:35.354450] Test:  [10/57]  eta: 0:00:09  loss: 0.8629 (0.8702)  time: 0.1967  data: 0.0132  max mem: 15689
[10:08:37.198145] Test:  [20/57]  eta: 0:00:07  loss: 0.8728 (0.8639)  time: 0.1840  data: 0.0001  max mem: 15689
[10:08:39.047159] Test:  [30/57]  eta: 0:00:05  loss: 0.7527 (0.8240)  time: 0.1846  data: 0.0001  max mem: 15689
[10:08:40.901826] Test:  [40/57]  eta: 0:00:03  loss: 0.7400 (0.8037)  time: 0.1851  data: 0.0001  max mem: 15689
[10:08:42.759151] Test:  [50/57]  eta: 0:00:01  loss: 0.7324 (0.7957)  time: 0.1855  data: 0.0001  max mem: 15689
[10:08:43.760742] Test:  [56/57]  eta: 0:00:00  loss: 0.7644 (0.8003)  time: 0.1800  data: 0.0001  max mem: 15689
[10:08:43.820070] Test: Total time: 0:00:10 (0.1865 s / it)
[10:08:45.667142] Dice score of the network on the train images: 0.796761, val images: 0.821409
[10:08:45.667390] saving best_dice_model_0 @ epoch 24
[10:08:46.866054] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:08:47.770515] Epoch: [25]  [  0/345]  eta: 0:05:11  lr: 0.000117  loss: 0.7388 (0.7388)  time: 0.9036  data: 0.1489  max mem: 15689
[10:09:02.881750] Epoch: [25]  [ 20/345]  eta: 0:04:07  lr: 0.000116  loss: 0.7559 (0.7533)  time: 0.7555  data: 0.0001  max mem: 15689
[10:09:18.055755] Epoch: [25]  [ 40/345]  eta: 0:03:52  lr: 0.000116  loss: 0.7422 (0.7504)  time: 0.7587  data: 0.0001  max mem: 15689
[10:09:33.264402] Epoch: [25]  [ 60/345]  eta: 0:03:36  lr: 0.000116  loss: 0.7526 (0.7525)  time: 0.7604  data: 0.0001  max mem: 15689
[10:09:48.498145] Epoch: [25]  [ 80/345]  eta: 0:03:21  lr: 0.000116  loss: 0.7491 (0.7535)  time: 0.7616  data: 0.0001  max mem: 15689
[10:10:03.731036] Epoch: [25]  [100/345]  eta: 0:03:06  lr: 0.000116  loss: 0.7477 (0.7534)  time: 0.7616  data: 0.0001  max mem: 15689
[10:10:18.969232] Epoch: [25]  [120/345]  eta: 0:02:51  lr: 0.000115  loss: 0.7577 (0.7538)  time: 0.7619  data: 0.0001  max mem: 15689
[10:10:34.209025] Epoch: [25]  [140/345]  eta: 0:02:36  lr: 0.000115  loss: 0.7520 (0.7543)  time: 0.7619  data: 0.0001  max mem: 15689
[10:10:49.438247] Epoch: [25]  [160/345]  eta: 0:02:20  lr: 0.000115  loss: 0.7517 (0.7545)  time: 0.7614  data: 0.0001  max mem: 15689
[10:11:04.668181] Epoch: [25]  [180/345]  eta: 0:02:05  lr: 0.000115  loss: 0.7744 (0.7572)  time: 0.7615  data: 0.0001  max mem: 15689
[10:11:19.887286] Epoch: [25]  [200/345]  eta: 0:01:50  lr: 0.000115  loss: 0.7610 (0.7577)  time: 0.7609  data: 0.0001  max mem: 15689
[10:11:35.103583] Epoch: [25]  [220/345]  eta: 0:01:35  lr: 0.000114  loss: 0.7575 (0.7577)  time: 0.7608  data: 0.0001  max mem: 15689
[10:11:50.446708] Epoch: [25]  [240/345]  eta: 0:01:19  lr: 0.000114  loss: 0.7490 (0.7575)  time: 0.7671  data: 0.0001  max mem: 15689
[10:12:05.679478] Epoch: [25]  [260/345]  eta: 0:01:04  lr: 0.000114  loss: 0.7665 (0.7581)  time: 0.7616  data: 0.0001  max mem: 15689
[10:12:20.908526] Epoch: [25]  [280/345]  eta: 0:00:49  lr: 0.000114  loss: 0.7615 (0.7583)  time: 0.7614  data: 0.0001  max mem: 15689
[10:12:36.133308] Epoch: [25]  [300/345]  eta: 0:00:34  lr: 0.000114  loss: 0.7516 (0.7583)  time: 0.7612  data: 0.0001  max mem: 15689
[10:12:51.357698] Epoch: [25]  [320/345]  eta: 0:00:19  lr: 0.000113  loss: 0.7752 (0.7599)  time: 0.7612  data: 0.0001  max mem: 15689
[10:13:06.575597] Epoch: [25]  [340/345]  eta: 0:00:03  lr: 0.000113  loss: 0.7705 (0.7607)  time: 0.7608  data: 0.0000  max mem: 15689
[10:13:09.620714] Epoch: [25]  [344/345]  eta: 0:00:00  lr: 0.000113  loss: 0.7708 (0.7608)  time: 0.7609  data: 0.0001  max mem: 15689
[10:13:09.686563] Epoch: [25] Total time: 0:04:22 (0.7618 s / it)
[10:13:09.687043] Averaged stats: lr: 0.000113  loss: 0.7708 (0.7608)
[10:13:10.033533] Test:  [  0/345]  eta: 0:01:58  loss: 0.6933 (0.6933)  time: 0.3425  data: 0.1593  max mem: 15689
[10:13:11.891184] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7405 (0.7381)  time: 0.1999  data: 0.0145  max mem: 15689
[10:13:13.753271] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7326 (0.7356)  time: 0.1859  data: 0.0001  max mem: 15689
[10:13:15.619342] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7268 (0.7340)  time: 0.1864  data: 0.0001  max mem: 15689
[10:13:17.489523] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7343 (0.7357)  time: 0.1868  data: 0.0001  max mem: 15689
[10:13:19.361321] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7341 (0.7349)  time: 0.1870  data: 0.0001  max mem: 15689
[10:13:21.236655] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7288 (0.7355)  time: 0.1873  data: 0.0001  max mem: 15689
[10:13:23.116174] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7392 (0.7361)  time: 0.1877  data: 0.0001  max mem: 15689
[10:13:25.001826] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7355 (0.7355)  time: 0.1882  data: 0.0001  max mem: 15689
[10:13:26.888915] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7336 (0.7355)  time: 0.1886  data: 0.0001  max mem: 15689
[10:13:28.780027] Test:  [100/345]  eta: 0:00:46  loss: 0.7284 (0.7349)  time: 0.1889  data: 0.0001  max mem: 15689
[10:13:30.672241] Test:  [110/345]  eta: 0:00:44  loss: 0.7341 (0.7359)  time: 0.1891  data: 0.0001  max mem: 15689
[10:13:32.570515] Test:  [120/345]  eta: 0:00:42  loss: 0.7447 (0.7365)  time: 0.1895  data: 0.0001  max mem: 15689
[10:13:34.473735] Test:  [130/345]  eta: 0:00:40  loss: 0.7336 (0.7358)  time: 0.1900  data: 0.0001  max mem: 15689
[10:13:36.379153] Test:  [140/345]  eta: 0:00:38  loss: 0.7343 (0.7363)  time: 0.1904  data: 0.0001  max mem: 15689
[10:13:38.287056] Test:  [150/345]  eta: 0:00:36  loss: 0.7393 (0.7361)  time: 0.1906  data: 0.0001  max mem: 15689
[10:13:40.197451] Test:  [160/345]  eta: 0:00:35  loss: 0.7309 (0.7361)  time: 0.1909  data: 0.0001  max mem: 15689
[10:13:42.112261] Test:  [170/345]  eta: 0:00:33  loss: 0.7301 (0.7361)  time: 0.1912  data: 0.0001  max mem: 15689
[10:13:44.032144] Test:  [180/345]  eta: 0:00:31  loss: 0.7346 (0.7360)  time: 0.1917  data: 0.0001  max mem: 15689
[10:13:45.954085] Test:  [190/345]  eta: 0:00:29  loss: 0.7346 (0.7360)  time: 0.1920  data: 0.0001  max mem: 15689
[10:13:47.879361] Test:  [200/345]  eta: 0:00:27  loss: 0.7383 (0.7361)  time: 0.1923  data: 0.0001  max mem: 15689
[10:13:49.805873] Test:  [210/345]  eta: 0:00:25  loss: 0.7373 (0.7360)  time: 0.1925  data: 0.0001  max mem: 15689
[10:13:51.738432] Test:  [220/345]  eta: 0:00:23  loss: 0.7354 (0.7361)  time: 0.1929  data: 0.0001  max mem: 15689
[10:13:53.674258] Test:  [230/345]  eta: 0:00:21  loss: 0.7373 (0.7365)  time: 0.1934  data: 0.0001  max mem: 15689
[10:13:55.613864] Test:  [240/345]  eta: 0:00:20  loss: 0.7420 (0.7367)  time: 0.1937  data: 0.0001  max mem: 15689
[10:13:57.557601] Test:  [250/345]  eta: 0:00:18  loss: 0.7399 (0.7368)  time: 0.1941  data: 0.0001  max mem: 15689
[10:13:59.504555] Test:  [260/345]  eta: 0:00:16  loss: 0.7311 (0.7366)  time: 0.1945  data: 0.0001  max mem: 15689
[10:14:01.455419] Test:  [270/345]  eta: 0:00:14  loss: 0.7285 (0.7363)  time: 0.1948  data: 0.0001  max mem: 15689
[10:14:03.406496] Test:  [280/345]  eta: 0:00:12  loss: 0.7274 (0.7360)  time: 0.1950  data: 0.0001  max mem: 15689
[10:14:05.362678] Test:  [290/345]  eta: 0:00:10  loss: 0.7287 (0.7361)  time: 0.1953  data: 0.0001  max mem: 15689
[10:14:07.322535] Test:  [300/345]  eta: 0:00:08  loss: 0.7287 (0.7357)  time: 0.1958  data: 0.0001  max mem: 15689
[10:14:09.284463] Test:  [310/345]  eta: 0:00:06  loss: 0.7275 (0.7355)  time: 0.1960  data: 0.0001  max mem: 15689
[10:14:11.251265] Test:  [320/345]  eta: 0:00:04  loss: 0.7276 (0.7355)  time: 0.1964  data: 0.0001  max mem: 15689
[10:14:13.220530] Test:  [330/345]  eta: 0:00:02  loss: 0.7300 (0.7356)  time: 0.1968  data: 0.0001  max mem: 15689
[10:14:15.194783] Test:  [340/345]  eta: 0:00:00  loss: 0.7399 (0.7358)  time: 0.1971  data: 0.0001  max mem: 15689
[10:14:15.985269] Test:  [344/345]  eta: 0:00:00  loss: 0.7399 (0.7358)  time: 0.1973  data: 0.0001  max mem: 15689
[10:14:16.043754] Test: Total time: 0:01:06 (0.1923 s / it)
[10:14:26.734607] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8454 (0.8454)  time: 0.3251  data: 0.1433  max mem: 15689
[10:14:28.572960] Test:  [10/57]  eta: 0:00:09  loss: 0.8741 (0.8731)  time: 0.1966  data: 0.0131  max mem: 15689
[10:14:30.413704] Test:  [20/57]  eta: 0:00:07  loss: 0.8741 (0.8702)  time: 0.1839  data: 0.0001  max mem: 15689
[10:14:32.259106] Test:  [30/57]  eta: 0:00:05  loss: 0.7678 (0.8308)  time: 0.1843  data: 0.0001  max mem: 15689
[10:14:34.109581] Test:  [40/57]  eta: 0:00:03  loss: 0.7453 (0.8107)  time: 0.1847  data: 0.0001  max mem: 15689
[10:14:35.966087] Test:  [50/57]  eta: 0:00:01  loss: 0.7510 (0.8050)  time: 0.1853  data: 0.0001  max mem: 15689
[10:14:36.967481] Test:  [56/57]  eta: 0:00:00  loss: 0.7668 (0.8111)  time: 0.1799  data: 0.0001  max mem: 15689
[10:14:37.026190] Test: Total time: 0:00:10 (0.1863 s / it)
[10:14:38.908777] Dice score of the network on the train images: 0.794643, val images: 0.807685
[10:14:38.913011] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:14:39.816158] Epoch: [26]  [  0/345]  eta: 0:05:11  lr: 0.000113  loss: 0.7392 (0.7392)  time: 0.9024  data: 0.1486  max mem: 15689
[10:14:54.929649] Epoch: [26]  [ 20/345]  eta: 0:04:07  lr: 0.000113  loss: 0.7606 (0.7639)  time: 0.7556  data: 0.0001  max mem: 15689
[10:15:10.100995] Epoch: [26]  [ 40/345]  eta: 0:03:51  lr: 0.000113  loss: 0.7518 (0.7583)  time: 0.7585  data: 0.0001  max mem: 15689
[10:15:25.306505] Epoch: [26]  [ 60/345]  eta: 0:03:36  lr: 0.000112  loss: 0.7527 (0.7571)  time: 0.7602  data: 0.0001  max mem: 15689
[10:15:40.535374] Epoch: [26]  [ 80/345]  eta: 0:03:21  lr: 0.000112  loss: 0.7448 (0.7550)  time: 0.7614  data: 0.0001  max mem: 15689
[10:15:55.763876] Epoch: [26]  [100/345]  eta: 0:03:06  lr: 0.000112  loss: 0.7408 (0.7535)  time: 0.7614  data: 0.0001  max mem: 15689
[10:16:11.003886] Epoch: [26]  [120/345]  eta: 0:02:51  lr: 0.000112  loss: 0.7492 (0.7533)  time: 0.7620  data: 0.0001  max mem: 15689
[10:16:26.222775] Epoch: [26]  [140/345]  eta: 0:02:36  lr: 0.000111  loss: 0.7420 (0.7518)  time: 0.7609  data: 0.0001  max mem: 15689
[10:16:41.448529] Epoch: [26]  [160/345]  eta: 0:02:20  lr: 0.000111  loss: 0.7455 (0.7518)  time: 0.7613  data: 0.0001  max mem: 15689
[10:16:56.670652] Epoch: [26]  [180/345]  eta: 0:02:05  lr: 0.000111  loss: 0.7575 (0.7525)  time: 0.7611  data: 0.0001  max mem: 15689
[10:17:11.885870] Epoch: [26]  [200/345]  eta: 0:01:50  lr: 0.000111  loss: 0.7573 (0.7530)  time: 0.7607  data: 0.0001  max mem: 15689
[10:17:27.090278] Epoch: [26]  [220/345]  eta: 0:01:35  lr: 0.000110  loss: 0.7528 (0.7528)  time: 0.7602  data: 0.0001  max mem: 15689
[10:17:42.295482] Epoch: [26]  [240/345]  eta: 0:01:19  lr: 0.000110  loss: 0.7764 (0.7544)  time: 0.7602  data: 0.0001  max mem: 15689
[10:17:57.490068] Epoch: [26]  [260/345]  eta: 0:01:04  lr: 0.000110  loss: 0.7584 (0.7550)  time: 0.7597  data: 0.0001  max mem: 15689
[10:18:12.684142] Epoch: [26]  [280/345]  eta: 0:00:49  lr: 0.000110  loss: 0.7605 (0.7552)  time: 0.7597  data: 0.0001  max mem: 15689
[10:18:27.879965] Epoch: [26]  [300/345]  eta: 0:00:34  lr: 0.000110  loss: 0.7533 (0.7554)  time: 0.7598  data: 0.0001  max mem: 15689
[10:18:43.076008] Epoch: [26]  [320/345]  eta: 0:00:19  lr: 0.000109  loss: 0.7559 (0.7554)  time: 0.7598  data: 0.0001  max mem: 15689
[10:18:58.267549] Epoch: [26]  [340/345]  eta: 0:00:03  lr: 0.000109  loss: 0.7500 (0.7552)  time: 0.7595  data: 0.0001  max mem: 15689
[10:19:01.306094] Epoch: [26]  [344/345]  eta: 0:00:00  lr: 0.000109  loss: 0.7500 (0.7550)  time: 0.7596  data: 0.0001  max mem: 15689
[10:19:01.371513] Epoch: [26] Total time: 0:04:22 (0.7607 s / it)
[10:19:01.371866] Averaged stats: lr: 0.000109  loss: 0.7500 (0.7550)
[10:19:01.713849] Test:  [  0/345]  eta: 0:01:56  loss: 0.7235 (0.7235)  time: 0.3383  data: 0.1541  max mem: 15689
[10:19:03.572413] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7219 (0.7257)  time: 0.1997  data: 0.0141  max mem: 15689
[10:19:05.433618] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7219 (0.7254)  time: 0.1859  data: 0.0001  max mem: 15689
[10:19:07.298450] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7197 (0.7237)  time: 0.1862  data: 0.0001  max mem: 15689
[10:19:09.166985] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7194 (0.7217)  time: 0.1866  data: 0.0001  max mem: 15689
[10:19:11.036213] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7202 (0.7227)  time: 0.1868  data: 0.0001  max mem: 15689
[10:19:12.911698] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7243 (0.7225)  time: 0.1872  data: 0.0001  max mem: 15689
[10:19:14.792772] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7224 (0.7238)  time: 0.1878  data: 0.0001  max mem: 15689
[10:19:16.676794] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7224 (0.7237)  time: 0.1882  data: 0.0001  max mem: 15689
[10:19:18.563065] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7162 (0.7231)  time: 0.1885  data: 0.0001  max mem: 15689
[10:19:20.450338] Test:  [100/345]  eta: 0:00:46  loss: 0.7245 (0.7240)  time: 0.1886  data: 0.0001  max mem: 15689
[10:19:22.344431] Test:  [110/345]  eta: 0:00:44  loss: 0.7298 (0.7242)  time: 0.1890  data: 0.0001  max mem: 15689
[10:19:24.241039] Test:  [120/345]  eta: 0:00:42  loss: 0.7228 (0.7240)  time: 0.1895  data: 0.0001  max mem: 15689
[10:19:26.143362] Test:  [130/345]  eta: 0:00:40  loss: 0.7247 (0.7243)  time: 0.1899  data: 0.0001  max mem: 15689
[10:19:28.046876] Test:  [140/345]  eta: 0:00:38  loss: 0.7200 (0.7239)  time: 0.1902  data: 0.0001  max mem: 15689
[10:19:29.955086] Test:  [150/345]  eta: 0:00:36  loss: 0.7257 (0.7245)  time: 0.1905  data: 0.0001  max mem: 15689
[10:19:31.866105] Test:  [160/345]  eta: 0:00:35  loss: 0.7171 (0.7237)  time: 0.1909  data: 0.0001  max mem: 15689
[10:19:33.780025] Test:  [170/345]  eta: 0:00:33  loss: 0.7138 (0.7236)  time: 0.1912  data: 0.0001  max mem: 15689
[10:19:35.697330] Test:  [180/345]  eta: 0:00:31  loss: 0.7235 (0.7237)  time: 0.1915  data: 0.0001  max mem: 15689
[10:19:37.619219] Test:  [190/345]  eta: 0:00:29  loss: 0.7181 (0.7236)  time: 0.1919  data: 0.0001  max mem: 15689
[10:19:39.544935] Test:  [200/345]  eta: 0:00:27  loss: 0.7181 (0.7236)  time: 0.1923  data: 0.0001  max mem: 15689
[10:19:41.473984] Test:  [210/345]  eta: 0:00:25  loss: 0.7189 (0.7239)  time: 0.1927  data: 0.0001  max mem: 15689
[10:19:43.405710] Test:  [220/345]  eta: 0:00:23  loss: 0.7248 (0.7239)  time: 0.1930  data: 0.0001  max mem: 15689
[10:19:45.341736] Test:  [230/345]  eta: 0:00:21  loss: 0.7200 (0.7237)  time: 0.1933  data: 0.0001  max mem: 15689
[10:19:47.280673] Test:  [240/345]  eta: 0:00:19  loss: 0.7231 (0.7239)  time: 0.1937  data: 0.0001  max mem: 15689
[10:19:49.224861] Test:  [250/345]  eta: 0:00:18  loss: 0.7236 (0.7237)  time: 0.1941  data: 0.0001  max mem: 15689
[10:19:51.169669] Test:  [260/345]  eta: 0:00:16  loss: 0.7188 (0.7237)  time: 0.1944  data: 0.0001  max mem: 15689
[10:19:53.120582] Test:  [270/345]  eta: 0:00:14  loss: 0.7217 (0.7238)  time: 0.1947  data: 0.0001  max mem: 15689
[10:19:55.072082] Test:  [280/345]  eta: 0:00:12  loss: 0.7209 (0.7238)  time: 0.1951  data: 0.0001  max mem: 15689
[10:19:57.029422] Test:  [290/345]  eta: 0:00:10  loss: 0.7210 (0.7237)  time: 0.1954  data: 0.0001  max mem: 15689
[10:19:58.989148] Test:  [300/345]  eta: 0:00:08  loss: 0.7231 (0.7238)  time: 0.1958  data: 0.0001  max mem: 15689
[10:20:00.953468] Test:  [310/345]  eta: 0:00:06  loss: 0.7252 (0.7240)  time: 0.1962  data: 0.0001  max mem: 15689
[10:20:02.920735] Test:  [320/345]  eta: 0:00:04  loss: 0.7234 (0.7239)  time: 0.1965  data: 0.0001  max mem: 15689
[10:20:04.888747] Test:  [330/345]  eta: 0:00:02  loss: 0.7197 (0.7238)  time: 0.1967  data: 0.0001  max mem: 15689
[10:20:06.863089] Test:  [340/345]  eta: 0:00:00  loss: 0.7171 (0.7236)  time: 0.1971  data: 0.0001  max mem: 15689
[10:20:07.654005] Test:  [344/345]  eta: 0:00:00  loss: 0.7189 (0.7235)  time: 0.1973  data: 0.0001  max mem: 15689
[10:20:07.714773] Test: Total time: 0:01:06 (0.1923 s / it)
[10:20:18.317254] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8596 (0.8596)  time: 0.3255  data: 0.1437  max mem: 15689
[10:20:20.151465] Test:  [10/57]  eta: 0:00:09  loss: 0.8727 (0.8683)  time: 0.1963  data: 0.0131  max mem: 15689
[10:20:21.992613] Test:  [20/57]  eta: 0:00:07  loss: 0.8748 (0.8636)  time: 0.1837  data: 0.0001  max mem: 15689
[10:20:23.837755] Test:  [30/57]  eta: 0:00:05  loss: 0.7672 (0.8264)  time: 0.1843  data: 0.0001  max mem: 15689
[10:20:25.687844] Test:  [40/57]  eta: 0:00:03  loss: 0.7506 (0.8069)  time: 0.1847  data: 0.0001  max mem: 15689
[10:20:27.543614] Test:  [50/57]  eta: 0:00:01  loss: 0.7436 (0.8006)  time: 0.1852  data: 0.0001  max mem: 15689
[10:20:28.543878] Test:  [56/57]  eta: 0:00:00  loss: 0.7626 (0.8052)  time: 0.1798  data: 0.0001  max mem: 15689
[10:20:28.601201] Test: Total time: 0:00:10 (0.1861 s / it)
[10:20:30.503276] Dice score of the network on the train images: 0.801896, val images: 0.815291
[10:20:30.507505] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:20:31.412224] Epoch: [27]  [  0/345]  eta: 0:05:11  lr: 0.000109  loss: 0.7751 (0.7751)  time: 0.9039  data: 0.1487  max mem: 15689
[10:20:46.511801] Epoch: [27]  [ 20/345]  eta: 0:04:07  lr: 0.000109  loss: 0.7509 (0.7567)  time: 0.7549  data: 0.0001  max mem: 15689
[10:21:01.652237] Epoch: [27]  [ 40/345]  eta: 0:03:51  lr: 0.000108  loss: 0.7602 (0.7575)  time: 0.7570  data: 0.0001  max mem: 15689
[10:21:16.842970] Epoch: [27]  [ 60/345]  eta: 0:03:36  lr: 0.000108  loss: 0.7501 (0.7562)  time: 0.7595  data: 0.0001  max mem: 15689

[10:21:32.043126] Epoch: [27]  [ 80/345]  eta: 0:03:21  lr: 0.000108  loss: 0.7531 (0.7570)  time: 0.7600  data: 0.0001  max mem: 15689
[10:21:47.268435] Epoch: [27]  [100/345]  eta: 0:03:06  lr: 0.000108  loss: 0.7487 (0.7562)  time: 0.7612  data: 0.0001  max mem: 15689
[10:22:02.504607] Epoch: [27]  [120/345]  eta: 0:02:51  lr: 0.000107  loss: 0.7478 (0.7557)  time: 0.7618  data: 0.0001  max mem: 15689
[10:22:17.735984] Epoch: [27]  [140/345]  eta: 0:02:35  lr: 0.000107  loss: 0.7499 (0.7548)  time: 0.7615  data: 0.0001  max mem: 15689
[10:22:32.961138] Epoch: [27]  [160/345]  eta: 0:02:20  lr: 0.000107  loss: 0.7475 (0.7546)  time: 0.7612  data: 0.0001  max mem: 15689
[10:22:48.180532] Epoch: [27]  [180/345]  eta: 0:02:05  lr: 0.000107  loss: 0.7546 (0.7546)  time: 0.7609  data: 0.0001  max mem: 15689
[10:23:03.389382] Epoch: [27]  [200/345]  eta: 0:01:50  lr: 0.000106  loss: 0.7442 (0.7541)  time: 0.7604  data: 0.0001  max mem: 15689
[10:23:18.588286] Epoch: [27]  [220/345]  eta: 0:01:35  lr: 0.000106  loss: 0.7427 (0.7533)  time: 0.7599  data: 0.0001  max mem: 15689
[10:23:33.782659] Epoch: [27]  [240/345]  eta: 0:01:19  lr: 0.000106  loss: 0.7496 (0.7529)  time: 0.7597  data: 0.0001  max mem: 15689
[10:23:48.975044] Epoch: [27]  [260/345]  eta: 0:01:04  lr: 0.000106  loss: 0.7475 (0.7531)  time: 0.7596  data: 0.0001  max mem: 15689
[10:24:04.169986] Epoch: [27]  [280/345]  eta: 0:00:49  lr: 0.000105  loss: 0.7647 (0.7538)  time: 0.7597  data: 0.0001  max mem: 15689
[10:24:19.362817] Epoch: [27]  [300/345]  eta: 0:00:34  lr: 0.000105  loss: 0.7512 (0.7539)  time: 0.7596  data: 0.0001  max mem: 15689
[10:24:34.556896] Epoch: [27]  [320/345]  eta: 0:00:19  lr: 0.000105  loss: 0.7390 (0.7533)  time: 0.7597  data: 0.0001  max mem: 15689
[10:24:49.752227] Epoch: [27]  [340/345]  eta: 0:00:03  lr: 0.000104  loss: 0.7500 (0.7533)  time: 0.7597  data: 0.0001  max mem: 15689
[10:24:52.791767] Epoch: [27]  [344/345]  eta: 0:00:00  lr: 0.000104  loss: 0.7506 (0.7534)  time: 0.7597  data: 0.0001  max mem: 15689
[10:24:52.856467] Epoch: [27] Total time: 0:04:22 (0.7604 s / it)
[10:24:52.856824] Averaged stats: lr: 0.000104  loss: 0.7506 (0.7534)
[10:24:53.198098] Test:  [  0/345]  eta: 0:01:56  loss: 0.7196 (0.7196)  time: 0.3379  data: 0.1540  max mem: 15689
[10:24:55.056151] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7262 (0.7251)  time: 0.1996  data: 0.0141  max mem: 15689
[10:24:56.916466] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7222 (0.7199)  time: 0.1859  data: 0.0001  max mem: 15689
[10:24:58.779528] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7205 (0.7202)  time: 0.1861  data: 0.0001  max mem: 15689
[10:25:00.647020] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7205 (0.7199)  time: 0.1865  data: 0.0001  max mem: 15689
[10:25:02.517227] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7122 (0.7198)  time: 0.1868  data: 0.0001  max mem: 15689
[10:25:04.388996] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7123 (0.7206)  time: 0.1870  data: 0.0001  max mem: 15689
[10:25:06.265989] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7210 (0.7208)  time: 0.1874  data: 0.0001  max mem: 15689
[10:25:08.151094] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7217 (0.7211)  time: 0.1881  data: 0.0001  max mem: 15689
[10:25:10.035530] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7264 (0.7219)  time: 0.1884  data: 0.0001  max mem: 15689
[10:25:11.924675] Test:  [100/345]  eta: 0:00:46  loss: 0.7264 (0.7225)  time: 0.1886  data: 0.0001  max mem: 15689
[10:25:13.818174] Test:  [110/345]  eta: 0:00:44  loss: 0.7234 (0.7224)  time: 0.1891  data: 0.0001  max mem: 15689
[10:25:15.715532] Test:  [120/345]  eta: 0:00:42  loss: 0.7248 (0.7229)  time: 0.1895  data: 0.0001  max mem: 15689
[10:25:17.616376] Test:  [130/345]  eta: 0:00:40  loss: 0.7248 (0.7231)  time: 0.1899  data: 0.0001  max mem: 15689
[10:25:19.518685] Test:  [140/345]  eta: 0:00:38  loss: 0.7264 (0.7232)  time: 0.1901  data: 0.0001  max mem: 15689
[10:25:21.425734] Test:  [150/345]  eta: 0:00:36  loss: 0.7200 (0.7228)  time: 0.1904  data: 0.0001  max mem: 15689
[10:25:23.335891] Test:  [160/345]  eta: 0:00:35  loss: 0.7200 (0.7228)  time: 0.1908  data: 0.0001  max mem: 15689
[10:25:25.249366] Test:  [170/345]  eta: 0:00:33  loss: 0.7212 (0.7228)  time: 0.1911  data: 0.0001  max mem: 15689
[10:25:27.167712] Test:  [180/345]  eta: 0:00:31  loss: 0.7156 (0.7223)  time: 0.1915  data: 0.0001  max mem: 15689
[10:25:29.087289] Test:  [190/345]  eta: 0:00:29  loss: 0.7128 (0.7220)  time: 0.1918  data: 0.0001  max mem: 15689
[10:25:31.010675] Test:  [200/345]  eta: 0:00:27  loss: 0.7147 (0.7219)  time: 0.1921  data: 0.0001  max mem: 15689
[10:25:32.937617] Test:  [210/345]  eta: 0:00:25  loss: 0.7220 (0.7220)  time: 0.1925  data: 0.0001  max mem: 15689
[10:25:34.868967] Test:  [220/345]  eta: 0:00:23  loss: 0.7251 (0.7223)  time: 0.1929  data: 0.0001  max mem: 15689
[10:25:36.802014] Test:  [230/345]  eta: 0:00:21  loss: 0.7304 (0.7227)  time: 0.1932  data: 0.0001  max mem: 15689
[10:25:38.740902] Test:  [240/345]  eta: 0:00:19  loss: 0.7304 (0.7227)  time: 0.1935  data: 0.0001  max mem: 15689
[10:25:40.682911] Test:  [250/345]  eta: 0:00:18  loss: 0.7204 (0.7227)  time: 0.1940  data: 0.0001  max mem: 15689
[10:25:42.629156] Test:  [260/345]  eta: 0:00:16  loss: 0.7146 (0.7223)  time: 0.1944  data: 0.0001  max mem: 15689
[10:25:44.577576] Test:  [270/345]  eta: 0:00:14  loss: 0.7146 (0.7222)  time: 0.1947  data: 0.0001  max mem: 15689
[10:25:46.530464] Test:  [280/345]  eta: 0:00:12  loss: 0.7230 (0.7224)  time: 0.1950  data: 0.0001  max mem: 15689
[10:25:48.484308] Test:  [290/345]  eta: 0:00:10  loss: 0.7206 (0.7222)  time: 0.1953  data: 0.0001  max mem: 15689
[10:25:50.441912] Test:  [300/345]  eta: 0:00:08  loss: 0.7179 (0.7223)  time: 0.1955  data: 0.0001  max mem: 15689
[10:25:52.404930] Test:  [310/345]  eta: 0:00:06  loss: 0.7258 (0.7225)  time: 0.1960  data: 0.0001  max mem: 15689
[10:25:54.371667] Test:  [320/345]  eta: 0:00:04  loss: 0.7234 (0.7226)  time: 0.1964  data: 0.0001  max mem: 15689
[10:25:56.342680] Test:  [330/345]  eta: 0:00:02  loss: 0.7209 (0.7225)  time: 0.1968  data: 0.0001  max mem: 15689
[10:25:58.314200] Test:  [340/345]  eta: 0:00:00  loss: 0.7181 (0.7225)  time: 0.1971  data: 0.0001  max mem: 15689
[10:25:59.104521] Test:  [344/345]  eta: 0:00:00  loss: 0.7181 (0.7225)  time: 0.1972  data: 0.0001  max mem: 15689
[10:25:59.166122] Test: Total time: 0:01:06 (0.1922 s / it)
[10:26:09.807251] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8578 (0.8578)  time: 0.3232  data: 0.1416  max mem: 15689
[10:26:11.641128] Test:  [10/57]  eta: 0:00:09  loss: 0.8819 (0.8734)  time: 0.1960  data: 0.0129  max mem: 15689
[10:26:13.482634] Test:  [20/57]  eta: 0:00:07  loss: 0.8831 (0.8634)  time: 0.1837  data: 0.0001  max mem: 15689
[10:26:15.328658] Test:  [30/57]  eta: 0:00:05  loss: 0.7633 (0.8257)  time: 0.1843  data: 0.0001  max mem: 15689
[10:26:17.177620] Test:  [40/57]  eta: 0:00:03  loss: 0.7516 (0.8060)  time: 0.1847  data: 0.0001  max mem: 15689
[10:26:19.032084] Test:  [50/57]  eta: 0:00:01  loss: 0.7347 (0.7979)  time: 0.1851  data: 0.0001  max mem: 15689
[10:26:20.031247] Test:  [56/57]  eta: 0:00:00  loss: 0.7571 (0.8029)  time: 0.1796  data: 0.0001  max mem: 15689
[10:26:20.090823] Test: Total time: 0:00:10 (0.1861 s / it)
[10:26:21.964434] Dice score of the network on the train images: 0.797369, val images: 0.820402
[10:26:21.969495] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:26:22.876787] Epoch: [28]  [  0/345]  eta: 0:05:12  lr: 0.000104  loss: 0.7294 (0.7294)  time: 0.9064  data: 0.1500  max mem: 15689
[10:26:37.970188] Epoch: [28]  [ 20/345]  eta: 0:04:07  lr: 0.000104  loss: 0.7480 (0.7492)  time: 0.7546  data: 0.0001  max mem: 15689
[10:26:53.104189] Epoch: [28]  [ 40/345]  eta: 0:03:51  lr: 0.000104  loss: 0.7454 (0.7477)  time: 0.7567  data: 0.0001  max mem: 15689
[10:27:08.284924] Epoch: [28]  [ 60/345]  eta: 0:03:36  lr: 0.000103  loss: 0.7372 (0.7446)  time: 0.7590  data: 0.0001  max mem: 15689
[10:27:23.574610] Epoch: [28]  [ 80/345]  eta: 0:03:21  lr: 0.000103  loss: 0.7464 (0.7448)  time: 0.7644  data: 0.0001  max mem: 15689
[10:27:38.784943] Epoch: [28]  [100/345]  eta: 0:03:06  lr: 0.000103  loss: 0.7418 (0.7449)  time: 0.7605  data: 0.0001  max mem: 15689
[10:27:54.010650] Epoch: [28]  [120/345]  eta: 0:02:51  lr: 0.000103  loss: 0.7431 (0.7452)  time: 0.7612  data: 0.0001  max mem: 15689
[10:28:09.225417] Epoch: [28]  [140/345]  eta: 0:02:35  lr: 0.000102  loss: 0.7422 (0.7458)  time: 0.7607  data: 0.0001  max mem: 15689
[10:28:24.445149] Epoch: [28]  [160/345]  eta: 0:02:20  lr: 0.000102  loss: 0.7718 (0.7491)  time: 0.7609  data: 0.0001  max mem: 15689
[10:28:39.658603] Epoch: [28]  [180/345]  eta: 0:02:05  lr: 0.000102  loss: 0.7648 (0.7510)  time: 0.7606  data: 0.0001  max mem: 15689
[10:28:54.873842] Epoch: [28]  [200/345]  eta: 0:01:50  lr: 0.000101  loss: 0.7505 (0.7508)  time: 0.7607  data: 0.0001  max mem: 15689
[10:29:10.070606] Epoch: [28]  [220/345]  eta: 0:01:35  lr: 0.000101  loss: 0.7478 (0.7502)  time: 0.7598  data: 0.0001  max mem: 15689
[10:29:25.264595] Epoch: [28]  [240/345]  eta: 0:01:19  lr: 0.000101  loss: 0.7439 (0.7499)  time: 0.7597  data: 0.0001  max mem: 15689
[10:29:40.458975] Epoch: [28]  [260/345]  eta: 0:01:04  lr: 0.000101  loss: 0.7436 (0.7495)  time: 0.7597  data: 0.0001  max mem: 15689
[10:29:55.655132] Epoch: [28]  [280/345]  eta: 0:00:49  lr: 0.000100  loss: 0.7397 (0.7489)  time: 0.7598  data: 0.0001  max mem: 15689
[10:30:10.892020] Epoch: [28]  [300/345]  eta: 0:00:34  lr: 0.000100  loss: 0.7427 (0.7484)  time: 0.7618  data: 0.0001  max mem: 15689
[10:30:26.077932] Epoch: [28]  [320/345]  eta: 0:00:19  lr: 0.000100  loss: 0.7510 (0.7486)  time: 0.7592  data: 0.0001  max mem: 15689
[10:30:41.278094] Epoch: [28]  [340/345]  eta: 0:00:03  lr: 0.000099  loss: 0.7470 (0.7489)  time: 0.7600  data: 0.0001  max mem: 15689
[10:30:44.317442] Epoch: [28]  [344/345]  eta: 0:00:00  lr: 0.000099  loss: 0.7551 (0.7491)  time: 0.7598  data: 0.0001  max mem: 15689
[10:30:44.382788] Epoch: [28] Total time: 0:04:22 (0.7606 s / it)
[10:30:44.383174] Averaged stats: lr: 0.000099  loss: 0.7551 (0.7491)
[10:30:44.727234] Test:  [  0/345]  eta: 0:01:57  loss: 0.7392 (0.7392)  time: 0.3406  data: 0.1573  max mem: 15689
[10:30:46.585409] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7361 (0.7360)  time: 0.1998  data: 0.0144  max mem: 15689
[10:30:48.445274] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7343 (0.7363)  time: 0.1858  data: 0.0001  max mem: 15689
[10:30:50.309386] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7352 (0.7381)  time: 0.1861  data: 0.0001  max mem: 15689
[10:30:52.177148] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7352 (0.7378)  time: 0.1865  data: 0.0001  max mem: 15689
[10:30:54.047907] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7307 (0.7367)  time: 0.1869  data: 0.0001  max mem: 15689
[10:30:55.923406] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7256 (0.7344)  time: 0.1873  data: 0.0001  max mem: 15689
[10:30:57.801441] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7261 (0.7346)  time: 0.1876  data: 0.0001  max mem: 15689
[10:30:59.684564] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7329 (0.7343)  time: 0.1880  data: 0.0001  max mem: 15689
[10:31:01.569844] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7289 (0.7337)  time: 0.1884  data: 0.0001  max mem: 15689
[10:31:03.459139] Test:  [100/345]  eta: 0:00:46  loss: 0.7318 (0.7341)  time: 0.1887  data: 0.0001  max mem: 15689
[10:31:05.352266] Test:  [110/345]  eta: 0:00:44  loss: 0.7405 (0.7343)  time: 0.1891  data: 0.0001  max mem: 15689
[10:31:07.248559] Test:  [120/345]  eta: 0:00:42  loss: 0.7397 (0.7347)  time: 0.1894  data: 0.0001  max mem: 15689
[10:31:09.147983] Test:  [130/345]  eta: 0:00:40  loss: 0.7397 (0.7353)  time: 0.1897  data: 0.0001  max mem: 15689
[10:31:11.052227] Test:  [140/345]  eta: 0:00:38  loss: 0.7374 (0.7350)  time: 0.1901  data: 0.0001  max mem: 15689
[10:31:12.959151] Test:  [150/345]  eta: 0:00:36  loss: 0.7346 (0.7349)  time: 0.1905  data: 0.0001  max mem: 15689
[10:31:14.868973] Test:  [160/345]  eta: 0:00:35  loss: 0.7346 (0.7350)  time: 0.1908  data: 0.0001  max mem: 15689
[10:31:16.782371] Test:  [170/345]  eta: 0:00:33  loss: 0.7383 (0.7353)  time: 0.1911  data: 0.0001  max mem: 15689
[10:31:18.699629] Test:  [180/345]  eta: 0:00:31  loss: 0.7358 (0.7353)  time: 0.1915  data: 0.0001  max mem: 15689
[10:31:20.621799] Test:  [190/345]  eta: 0:00:29  loss: 0.7271 (0.7349)  time: 0.1919  data: 0.0001  max mem: 15689
[10:31:22.546302] Test:  [200/345]  eta: 0:00:27  loss: 0.7211 (0.7346)  time: 0.1923  data: 0.0001  max mem: 15689
[10:31:24.474795] Test:  [210/345]  eta: 0:00:25  loss: 0.7295 (0.7348)  time: 0.1926  data: 0.0001  max mem: 15689
[10:31:26.406529] Test:  [220/345]  eta: 0:00:23  loss: 0.7386 (0.7348)  time: 0.1930  data: 0.0001  max mem: 15689
[10:31:28.340914] Test:  [230/345]  eta: 0:00:21  loss: 0.7344 (0.7349)  time: 0.1933  data: 0.0001  max mem: 15689
[10:31:30.281028] Test:  [240/345]  eta: 0:00:19  loss: 0.7391 (0.7351)  time: 0.1937  data: 0.0001  max mem: 15689
[10:31:32.222920] Test:  [250/345]  eta: 0:00:18  loss: 0.7417 (0.7352)  time: 0.1940  data: 0.0001  max mem: 15689
[10:31:34.168720] Test:  [260/345]  eta: 0:00:16  loss: 0.7275 (0.7350)  time: 0.1943  data: 0.0001  max mem: 15689
[10:31:36.118944] Test:  [270/345]  eta: 0:00:14  loss: 0.7265 (0.7347)  time: 0.1947  data: 0.0001  max mem: 15689
[10:31:38.074390] Test:  [280/345]  eta: 0:00:12  loss: 0.7291 (0.7347)  time: 0.1952  data: 0.0001  max mem: 15689
[10:31:40.030219] Test:  [290/345]  eta: 0:00:10  loss: 0.7362 (0.7350)  time: 0.1955  data: 0.0001  max mem: 15689
[10:31:41.988548] Test:  [300/345]  eta: 0:00:08  loss: 0.7322 (0.7345)  time: 0.1957  data: 0.0001  max mem: 15689
[10:31:43.950275] Test:  [310/345]  eta: 0:00:06  loss: 0.7245 (0.7345)  time: 0.1960  data: 0.0001  max mem: 15689
[10:31:45.915696] Test:  [320/345]  eta: 0:00:04  loss: 0.7238 (0.7343)  time: 0.1963  data: 0.0001  max mem: 15689
[10:31:47.883386] Test:  [330/345]  eta: 0:00:02  loss: 0.7256 (0.7344)  time: 0.1966  data: 0.0001  max mem: 15689
[10:31:49.854213] Test:  [340/345]  eta: 0:00:00  loss: 0.7367 (0.7345)  time: 0.1969  data: 0.0001  max mem: 15689
[10:31:50.644601] Test:  [344/345]  eta: 0:00:00  loss: 0.7344 (0.7344)  time: 0.1971  data: 0.0001  max mem: 15689
[10:31:50.705083] Test: Total time: 0:01:06 (0.1922 s / it)
[10:32:01.174770] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8552 (0.8552)  time: 0.3238  data: 0.1419  max mem: 15689
[10:32:03.012798] Test:  [10/57]  eta: 0:00:09  loss: 0.8885 (0.8713)  time: 0.1965  data: 0.0130  max mem: 15689
[10:32:04.853889] Test:  [20/57]  eta: 0:00:07  loss: 0.8885 (0.8660)  time: 0.1839  data: 0.0001  max mem: 15689
[10:32:06.698874] Test:  [30/57]  eta: 0:00:05  loss: 0.7747 (0.8277)  time: 0.1843  data: 0.0001  max mem: 15689
[10:32:08.549298] Test:  [40/57]  eta: 0:00:03  loss: 0.7438 (0.8055)  time: 0.1847  data: 0.0001  max mem: 15689
[10:32:10.404783] Test:  [50/57]  eta: 0:00:01  loss: 0.7454 (0.7995)  time: 0.1852  data: 0.0001  max mem: 15689
[10:32:11.405095] Test:  [56/57]  eta: 0:00:00  loss: 0.7580 (0.8054)  time: 0.1798  data: 0.0001  max mem: 15689
[10:32:11.465086] Test: Total time: 0:00:10 (0.1862 s / it)
[10:32:13.364345] Dice score of the network on the train images: 0.778688, val images: 0.810491
[10:32:13.369484] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:32:14.267064] Epoch: [29]  [  0/345]  eta: 0:05:09  lr: 0.000099  loss: 0.7424 (0.7424)  time: 0.8965  data: 0.1429  max mem: 15689
[10:32:29.363205] Epoch: [29]  [ 20/345]  eta: 0:04:07  lr: 0.000099  loss: 0.7485 (0.7492)  time: 0.7548  data: 0.0001  max mem: 15689
[10:32:44.495310] Epoch: [29]  [ 40/345]  eta: 0:03:51  lr: 0.000099  loss: 0.7461 (0.7497)  time: 0.7566  data: 0.0001  max mem: 15689
[10:32:59.673585] Epoch: [29]  [ 60/345]  eta: 0:03:36  lr: 0.000098  loss: 0.7465 (0.7502)  time: 0.7589  data: 0.0001  max mem: 15689
[10:33:14.871725] Epoch: [29]  [ 80/345]  eta: 0:03:21  lr: 0.000098  loss: 0.7453 (0.7499)  time: 0.7599  data: 0.0001  max mem: 15689
[10:33:30.084763] Epoch: [29]  [100/345]  eta: 0:03:06  lr: 0.000098  loss: 0.7409 (0.7488)  time: 0.7606  data: 0.0001  max mem: 15689
[10:33:45.300499] Epoch: [29]  [120/345]  eta: 0:02:50  lr: 0.000097  loss: 0.7443 (0.7483)  time: 0.7607  data: 0.0001  max mem: 15689
[10:34:00.514956] Epoch: [29]  [140/345]  eta: 0:02:35  lr: 0.000097  loss: 0.7435 (0.7480)  time: 0.7607  data: 0.0001  max mem: 15689
[10:34:15.731002] Epoch: [29]  [160/345]  eta: 0:02:20  lr: 0.000097  loss: 0.7427 (0.7475)  time: 0.7608  data: 0.0001  max mem: 15689
[10:34:31.029427] Epoch: [29]  [180/345]  eta: 0:02:05  lr: 0.000096  loss: 0.7408 (0.7466)  time: 0.7649  data: 0.0001  max mem: 15689
[10:34:46.241947] Epoch: [29]  [200/345]  eta: 0:01:50  lr: 0.000096  loss: 0.7439 (0.7462)  time: 0.7606  data: 0.0001  max mem: 15689
[10:35:01.438564] Epoch: [29]  [220/345]  eta: 0:01:35  lr: 0.000096  loss: 0.7357 (0.7456)  time: 0.7598  data: 0.0001  max mem: 15689
[10:35:16.631421] Epoch: [29]  [240/345]  eta: 0:01:19  lr: 0.000095  loss: 0.7372 (0.7448)  time: 0.7596  data: 0.0001  max mem: 15689
[10:35:31.825429] Epoch: [29]  [260/345]  eta: 0:01:04  lr: 0.000095  loss: 0.7406 (0.7445)  time: 0.7597  data: 0.0001  max mem: 15689
[10:35:47.020619] Epoch: [29]  [280/345]  eta: 0:00:49  lr: 0.000095  loss: 0.7359 (0.7440)  time: 0.7597  data: 0.0001  max mem: 15689
[10:36:02.214938] Epoch: [29]  [300/345]  eta: 0:00:34  lr: 0.000094  loss: 0.7407 (0.7438)  time: 0.7597  data: 0.0001  max mem: 15689
[10:36:17.409331] Epoch: [29]  [320/345]  eta: 0:00:19  lr: 0.000094  loss: 0.7433 (0.7439)  time: 0.7597  data: 0.0001  max mem: 15689
[10:36:32.603891] Epoch: [29]  [340/345]  eta: 0:00:03  lr: 0.000094  loss: 0.7400 (0.7438)  time: 0.7597  data: 0.0001  max mem: 15689
[10:36:35.643851] Epoch: [29]  [344/345]  eta: 0:00:00  lr: 0.000094  loss: 0.7400 (0.7437)  time: 0.7597  data: 0.0001  max mem: 15689
[10:36:35.710948] Epoch: [29] Total time: 0:04:22 (0.7604 s / it)
[10:36:35.711234] Averaged stats: lr: 0.000094  loss: 0.7400 (0.7437)
[10:36:36.051870] Test:  [  0/345]  eta: 0:01:56  loss: 0.7122 (0.7122)  time: 0.3370  data: 0.1527  max mem: 15689
[10:36:37.909830] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7229 (0.7239)  time: 0.1995  data: 0.0139  max mem: 15689
[10:36:39.769761] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7195 (0.7202)  time: 0.1858  data: 0.0001  max mem: 15689
[10:36:41.632132] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7154 (0.7197)  time: 0.1861  data: 0.0001  max mem: 15689
[10:36:43.499666] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7253 (0.7219)  time: 0.1864  data: 0.0001  max mem: 15689
[10:36:45.369982] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7233 (0.7221)  time: 0.1868  data: 0.0001  max mem: 15689
[10:36:47.245686] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7204 (0.7221)  time: 0.1873  data: 0.0001  max mem: 15689
[10:36:49.124025] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7167 (0.7218)  time: 0.1877  data: 0.0001  max mem: 15689
[10:36:51.007574] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7240 (0.7222)  time: 0.1880  data: 0.0001  max mem: 15689
[10:36:52.894063] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7240 (0.7221)  time: 0.1885  data: 0.0001  max mem: 15689
[10:36:54.783446] Test:  [100/345]  eta: 0:00:46  loss: 0.7205 (0.7223)  time: 0.1887  data: 0.0001  max mem: 15689
[10:36:56.674971] Test:  [110/345]  eta: 0:00:44  loss: 0.7174 (0.7211)  time: 0.1890  data: 0.0001  max mem: 15689
[10:36:58.571956] Test:  [120/345]  eta: 0:00:42  loss: 0.7093 (0.7202)  time: 0.1894  data: 0.0001  max mem: 15689
[10:37:00.472561] Test:  [130/345]  eta: 0:00:40  loss: 0.7110 (0.7202)  time: 0.1898  data: 0.0001  max mem: 15689
[10:37:02.376544] Test:  [140/345]  eta: 0:00:38  loss: 0.7110 (0.7197)  time: 0.1902  data: 0.0001  max mem: 15689
[10:37:04.285499] Test:  [150/345]  eta: 0:00:36  loss: 0.7107 (0.7196)  time: 0.1906  data: 0.0001  max mem: 15689
[10:37:06.196625] Test:  [160/345]  eta: 0:00:35  loss: 0.7140 (0.7196)  time: 0.1910  data: 0.0001  max mem: 15689
[10:37:08.110627] Test:  [170/345]  eta: 0:00:33  loss: 0.7118 (0.7192)  time: 0.1912  data: 0.0001  max mem: 15689
[10:37:10.028115] Test:  [180/345]  eta: 0:00:31  loss: 0.7083 (0.7187)  time: 0.1915  data: 0.0001  max mem: 15689
[10:37:11.949342] Test:  [190/345]  eta: 0:00:29  loss: 0.7149 (0.7191)  time: 0.1919  data: 0.0001  max mem: 15689
[10:37:13.873744] Test:  [200/345]  eta: 0:00:27  loss: 0.7158 (0.7190)  time: 0.1922  data: 0.0001  max mem: 15689
[10:37:15.801707] Test:  [210/345]  eta: 0:00:25  loss: 0.7129 (0.7187)  time: 0.1926  data: 0.0001  max mem: 15689
[10:37:17.733603] Test:  [220/345]  eta: 0:00:23  loss: 0.7155 (0.7188)  time: 0.1929  data: 0.0001  max mem: 15689
[10:37:19.668769] Test:  [230/345]  eta: 0:00:21  loss: 0.7149 (0.7185)  time: 0.1933  data: 0.0001  max mem: 15689
[10:37:21.608326] Test:  [240/345]  eta: 0:00:19  loss: 0.7120 (0.7186)  time: 0.1937  data: 0.0001  max mem: 15689
[10:37:23.551995] Test:  [250/345]  eta: 0:00:18  loss: 0.7184 (0.7187)  time: 0.1941  data: 0.0001  max mem: 15689
[10:37:25.498777] Test:  [260/345]  eta: 0:00:16  loss: 0.7154 (0.7185)  time: 0.1945  data: 0.0001  max mem: 15689
[10:37:27.447036] Test:  [270/345]  eta: 0:00:14  loss: 0.7147 (0.7185)  time: 0.1947  data: 0.0001  max mem: 15689
[10:37:29.400391] Test:  [280/345]  eta: 0:00:12  loss: 0.7127 (0.7184)  time: 0.1950  data: 0.0001  max mem: 15689
[10:37:31.358024] Test:  [290/345]  eta: 0:00:10  loss: 0.7157 (0.7185)  time: 0.1955  data: 0.0001  max mem: 15689
[10:37:33.318396] Test:  [300/345]  eta: 0:00:08  loss: 0.7144 (0.7183)  time: 0.1959  data: 0.0001  max mem: 15689
[10:37:35.281429] Test:  [310/345]  eta: 0:00:06  loss: 0.7138 (0.7183)  time: 0.1961  data: 0.0001  max mem: 15689
[10:37:37.248293] Test:  [320/345]  eta: 0:00:04  loss: 0.7186 (0.7185)  time: 0.1964  data: 0.0001  max mem: 15689
[10:37:39.220466] Test:  [330/345]  eta: 0:00:02  loss: 0.7188 (0.7188)  time: 0.1969  data: 0.0001  max mem: 15689
[10:37:41.194374] Test:  [340/345]  eta: 0:00:00  loss: 0.7166 (0.7187)  time: 0.1973  data: 0.0001  max mem: 15689
[10:37:41.985132] Test:  [344/345]  eta: 0:00:00  loss: 0.7138 (0.7186)  time: 0.1973  data: 0.0001  max mem: 15689
[10:37:42.044358] Test: Total time: 0:01:06 (0.1923 s / it)
[10:37:52.628227] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8398 (0.8398)  time: 0.3257  data: 0.1443  max mem: 15689
[10:37:54.465952] Test:  [10/57]  eta: 0:00:09  loss: 0.8687 (0.8741)  time: 0.1966  data: 0.0132  max mem: 15689
[10:37:56.306787] Test:  [20/57]  eta: 0:00:07  loss: 0.8759 (0.8672)  time: 0.1839  data: 0.0001  max mem: 15689
[10:37:58.151090] Test:  [30/57]  eta: 0:00:05  loss: 0.7666 (0.8291)  time: 0.1842  data: 0.0001  max mem: 15689
[10:37:59.998838] Test:  [40/57]  eta: 0:00:03  loss: 0.7393 (0.8078)  time: 0.1846  data: 0.0001  max mem: 15689
[10:38:01.852889] Test:  [50/57]  eta: 0:00:01  loss: 0.7365 (0.8007)  time: 0.1850  data: 0.0001  max mem: 15689
[10:38:02.853383] Test:  [56/57]  eta: 0:00:00  loss: 0.7558 (0.8061)  time: 0.1797  data: 0.0001  max mem: 15689
[10:38:02.909963] Test: Total time: 0:00:10 (0.1861 s / it)
[10:38:04.750359] Dice score of the network on the train images: 0.799517, val images: 0.819826
[10:38:04.754713] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:38:05.653812] Epoch: [30]  [  0/345]  eta: 0:05:09  lr: 0.000094  loss: 0.7243 (0.7243)  time: 0.8980  data: 0.1443  max mem: 15689
[10:38:20.742428] Epoch: [30]  [ 20/345]  eta: 0:04:07  lr: 0.000093  loss: 0.7386 (0.7403)  time: 0.7544  data: 0.0001  max mem: 15689
[10:38:35.891128] Epoch: [30]  [ 40/345]  eta: 0:03:51  lr: 0.000093  loss: 0.7436 (0.7416)  time: 0.7574  data: 0.0001  max mem: 15689
[10:38:51.073319] Epoch: [30]  [ 60/345]  eta: 0:03:36  lr: 0.000093  loss: 0.7366 (0.7406)  time: 0.7591  data: 0.0001  max mem: 15689
[10:39:06.277326] Epoch: [30]  [ 80/345]  eta: 0:03:21  lr: 0.000092  loss: 0.7401 (0.7403)  time: 0.7602  data: 0.0001  max mem: 15689
[10:39:21.606241] Epoch: [30]  [100/345]  eta: 0:03:06  lr: 0.000092  loss: 0.7389 (0.7402)  time: 0.7664  data: 0.0001  max mem: 15689
[10:39:36.827900] Epoch: [30]  [120/345]  eta: 0:02:51  lr: 0.000092  loss: 0.7466 (0.7408)  time: 0.7610  data: 0.0001  max mem: 15689
[10:39:52.049068] Epoch: [30]  [140/345]  eta: 0:02:35  lr: 0.000091  loss: 0.7486 (0.7417)  time: 0.7610  data: 0.0001  max mem: 15689
[10:40:07.272121] Epoch: [30]  [160/345]  eta: 0:02:20  lr: 0.000091  loss: 0.7400 (0.7416)  time: 0.7611  data: 0.0001  max mem: 15689
[10:40:22.482979] Epoch: [30]  [180/345]  eta: 0:02:05  lr: 0.000091  loss: 0.7458 (0.7420)  time: 0.7605  data: 0.0001  max mem: 15689
[10:40:37.690346] Epoch: [30]  [200/345]  eta: 0:01:50  lr: 0.000090  loss: 0.7408 (0.7420)  time: 0.7603  data: 0.0001  max mem: 15689
[10:40:52.900462] Epoch: [30]  [220/345]  eta: 0:01:35  lr: 0.000090  loss: 0.7317 (0.7413)  time: 0.7605  data: 0.0001  max mem: 15689
[10:41:08.098288] Epoch: [30]  [240/345]  eta: 0:01:19  lr: 0.000090  loss: 0.7386 (0.7411)  time: 0.7598  data: 0.0001  max mem: 15689
[10:41:23.297238] Epoch: [30]  [260/345]  eta: 0:01:04  lr: 0.000089  loss: 0.7325 (0.7407)  time: 0.7599  data: 0.0001  max mem: 15689
[10:41:38.489974] Epoch: [30]  [280/345]  eta: 0:00:49  lr: 0.000089  loss: 0.7342 (0.7404)  time: 0.7596  data: 0.0001  max mem: 15689
[10:41:53.680945] Epoch: [30]  [300/345]  eta: 0:00:34  lr: 0.000089  loss: 0.7337 (0.7400)  time: 0.7595  data: 0.0001  max mem: 15689
[10:42:08.875593] Epoch: [30]  [320/345]  eta: 0:00:19  lr: 0.000088  loss: 0.7294 (0.7395)  time: 0.7597  data: 0.0001  max mem: 15689
[10:42:24.109997] Epoch: [30]  [340/345]  eta: 0:00:03  lr: 0.000088  loss: 0.7341 (0.7392)  time: 0.7617  data: 0.0001  max mem: 15689
[10:42:27.150989] Epoch: [30]  [344/345]  eta: 0:00:00  lr: 0.000088  loss: 0.7341 (0.7391)  time: 0.7618  data: 0.0001  max mem: 15689
[10:42:27.217586] Epoch: [30] Total time: 0:04:22 (0.7608 s / it)
[10:42:27.217982] Averaged stats: lr: 0.000088  loss: 0.7341 (0.7391)
[10:42:27.562788] Test:  [  0/345]  eta: 0:01:57  loss: 0.7347 (0.7347)  time: 0.3415  data: 0.1576  max mem: 15689
[10:42:29.421978] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7202 (0.7185)  time: 0.2000  data: 0.0144  max mem: 15689
[10:42:31.283707] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7108 (0.7136)  time: 0.1860  data: 0.0001  max mem: 15689
[10:42:33.148994] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7070 (0.7108)  time: 0.1863  data: 0.0001  max mem: 15689
[10:42:35.017874] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7023 (0.7090)  time: 0.1867  data: 0.0001  max mem: 15689
[10:42:36.890204] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7039 (0.7087)  time: 0.1870  data: 0.0001  max mem: 15689
[10:42:38.765464] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7052 (0.7099)  time: 0.1873  data: 0.0001  max mem: 15689
[10:42:40.644262] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7022 (0.7091)  time: 0.1876  data: 0.0001  max mem: 15689
[10:42:42.528616] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7014 (0.7081)  time: 0.1881  data: 0.0001  max mem: 15689
[10:42:44.415685] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7058 (0.7086)  time: 0.1885  data: 0.0001  max mem: 15689
[10:42:46.307538] Test:  [100/345]  eta: 0:00:46  loss: 0.7088 (0.7083)  time: 0.1889  data: 0.0001  max mem: 15689
[10:42:48.201389] Test:  [110/345]  eta: 0:00:44  loss: 0.7031 (0.7077)  time: 0.1892  data: 0.0001  max mem: 15689
[10:42:50.098795] Test:  [120/345]  eta: 0:00:42  loss: 0.7010 (0.7072)  time: 0.1895  data: 0.0001  max mem: 15689
[10:42:52.001866] Test:  [130/345]  eta: 0:00:40  loss: 0.7049 (0.7073)  time: 0.1900  data: 0.0001  max mem: 15689
[10:42:53.907855] Test:  [140/345]  eta: 0:00:38  loss: 0.7135 (0.7078)  time: 0.1904  data: 0.0001  max mem: 15689
[10:42:55.816308] Test:  [150/345]  eta: 0:00:36  loss: 0.7093 (0.7079)  time: 0.1907  data: 0.0001  max mem: 15689
[10:42:57.728007] Test:  [160/345]  eta: 0:00:35  loss: 0.7106 (0.7081)  time: 0.1910  data: 0.0001  max mem: 15689
[10:42:59.641682] Test:  [170/345]  eta: 0:00:33  loss: 0.7054 (0.7078)  time: 0.1912  data: 0.0001  max mem: 15689
[10:43:01.560408] Test:  [180/345]  eta: 0:00:31  loss: 0.7012 (0.7075)  time: 0.1916  data: 0.0001  max mem: 15689
[10:43:03.483449] Test:  [190/345]  eta: 0:00:29  loss: 0.7030 (0.7078)  time: 0.1920  data: 0.0001  max mem: 15689
[10:43:05.407456] Test:  [200/345]  eta: 0:00:27  loss: 0.7065 (0.7079)  time: 0.1923  data: 0.0001  max mem: 15689
[10:43:07.336659] Test:  [210/345]  eta: 0:00:25  loss: 0.7061 (0.7080)  time: 0.1926  data: 0.0001  max mem: 15689
[10:43:09.269863] Test:  [220/345]  eta: 0:00:23  loss: 0.7103 (0.7081)  time: 0.1931  data: 0.0001  max mem: 15689
[10:43:11.205616] Test:  [230/345]  eta: 0:00:21  loss: 0.7078 (0.7079)  time: 0.1934  data: 0.0001  max mem: 15689
[10:43:13.146255] Test:  [240/345]  eta: 0:00:20  loss: 0.7005 (0.7077)  time: 0.1938  data: 0.0001  max mem: 15689
[10:43:15.090061] Test:  [250/345]  eta: 0:00:18  loss: 0.7043 (0.7079)  time: 0.1942  data: 0.0001  max mem: 15689
[10:43:17.036267] Test:  [260/345]  eta: 0:00:16  loss: 0.7143 (0.7081)  time: 0.1944  data: 0.0001  max mem: 15689
[10:43:18.986883] Test:  [270/345]  eta: 0:00:14  loss: 0.7156 (0.7083)  time: 0.1948  data: 0.0001  max mem: 15689
[10:43:20.938411] Test:  [280/345]  eta: 0:00:12  loss: 0.7081 (0.7082)  time: 0.1951  data: 0.0001  max mem: 15689
[10:43:22.895193] Test:  [290/345]  eta: 0:00:10  loss: 0.7051 (0.7082)  time: 0.1954  data: 0.0001  max mem: 15689
[10:43:24.855216] Test:  [300/345]  eta: 0:00:08  loss: 0.7112 (0.7083)  time: 0.1958  data: 0.0001  max mem: 15689
[10:43:26.816648] Test:  [310/345]  eta: 0:00:06  loss: 0.7068 (0.7083)  time: 0.1960  data: 0.0001  max mem: 15689
[10:43:28.784168] Test:  [320/345]  eta: 0:00:04  loss: 0.7089 (0.7083)  time: 0.1964  data: 0.0001  max mem: 15689
[10:43:30.755061] Test:  [330/345]  eta: 0:00:02  loss: 0.7109 (0.7084)  time: 0.1969  data: 0.0001  max mem: 15689
[10:43:32.731160] Test:  [340/345]  eta: 0:00:00  loss: 0.7099 (0.7085)  time: 0.1973  data: 0.0001  max mem: 15689
[10:43:33.521648] Test:  [344/345]  eta: 0:00:00  loss: 0.7070 (0.7084)  time: 0.1974  data: 0.0001  max mem: 15689
[10:43:33.582747] Test: Total time: 0:01:06 (0.1924 s / it)
[10:43:44.377423] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8501 (0.8501)  time: 0.3328  data: 0.1510  max mem: 15689
[10:43:46.213041] Test:  [10/57]  eta: 0:00:09  loss: 0.8920 (0.8807)  time: 0.1971  data: 0.0138  max mem: 15689
[10:43:48.055132] Test:  [20/57]  eta: 0:00:07  loss: 0.8907 (0.8702)  time: 0.1838  data: 0.0001  max mem: 15689
[10:43:49.901392] Test:  [30/57]  eta: 0:00:05  loss: 0.7746 (0.8327)  time: 0.1844  data: 0.0001  max mem: 15689
[10:43:51.750551] Test:  [40/57]  eta: 0:00:03  loss: 0.7492 (0.8126)  time: 0.1847  data: 0.0001  max mem: 15689
[10:43:53.606919] Test:  [50/57]  eta: 0:00:01  loss: 0.7448 (0.8044)  time: 0.1852  data: 0.0001  max mem: 15689
[10:43:54.607382] Test:  [56/57]  eta: 0:00:00  loss: 0.7551 (0.8094)  time: 0.1798  data: 0.0001  max mem: 15689
[10:43:54.667075] Test: Total time: 0:00:10 (0.1864 s / it)
[10:43:56.451789] Dice score of the network on the train images: 0.823089, val images: 0.815091
[10:43:56.452008] saving best_prec_model_0 @ epoch 30
[10:43:57.567558] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:43:58.473448] Epoch: [31]  [  0/345]  eta: 0:05:12  lr: 0.000088  loss: 0.7226 (0.7226)  time: 0.9049  data: 0.1496  max mem: 15689
[10:44:13.588129] Epoch: [31]  [ 20/345]  eta: 0:04:07  lr: 0.000088  loss: 0.7284 (0.7353)  time: 0.7557  data: 0.0001  max mem: 15689
[10:44:28.736017] Epoch: [31]  [ 40/345]  eta: 0:03:51  lr: 0.000087  loss: 0.7314 (0.7356)  time: 0.7573  data: 0.0001  max mem: 15689
[10:44:43.934110] Epoch: [31]  [ 60/345]  eta: 0:03:36  lr: 0.000087  loss: 0.7347 (0.7357)  time: 0.7599  data: 0.0001  max mem: 15689
[10:44:59.152001] Epoch: [31]  [ 80/345]  eta: 0:03:21  lr: 0.000087  loss: 0.7345 (0.7353)  time: 0.7608  data: 0.0001  max mem: 15689
[10:45:14.393845] Epoch: [31]  [100/345]  eta: 0:03:06  lr: 0.000086  loss: 0.7289 (0.7351)  time: 0.7620  data: 0.0001  max mem: 15689
[10:45:29.642434] Epoch: [31]  [120/345]  eta: 0:02:51  lr: 0.000086  loss: 0.7267 (0.7343)  time: 0.7624  data: 0.0001  max mem: 15689
[10:45:44.884047] Epoch: [31]  [140/345]  eta: 0:02:36  lr: 0.000085  loss: 0.7263 (0.7338)  time: 0.7620  data: 0.0001  max mem: 15689
[10:46:00.125763] Epoch: [31]  [160/345]  eta: 0:02:20  lr: 0.000085  loss: 0.7361 (0.7340)  time: 0.7620  data: 0.0001  max mem: 15689
[10:46:15.453453] Epoch: [31]  [180/345]  eta: 0:02:05  lr: 0.000085  loss: 0.7424 (0.7348)  time: 0.7663  data: 0.0001  max mem: 15689
[10:46:30.690369] Epoch: [31]  [200/345]  eta: 0:01:50  lr: 0.000084  loss: 0.7420 (0.7357)  time: 0.7618  data: 0.0001  max mem: 15689
[10:46:45.917444] Epoch: [31]  [220/345]  eta: 0:01:35  lr: 0.000084  loss: 0.7323 (0.7356)  time: 0.7613  data: 0.0001  max mem: 15689
[10:47:01.152316] Epoch: [31]  [240/345]  eta: 0:01:19  lr: 0.000084  loss: 0.7276 (0.7350)  time: 0.7617  data: 0.0001  max mem: 15689
[10:47:16.385758] Epoch: [31]  [260/345]  eta: 0:01:04  lr: 0.000083  loss: 0.7321 (0.7352)  time: 0.7616  data: 0.0001  max mem: 15689
[10:47:31.607426] Epoch: [31]  [280/345]  eta: 0:00:49  lr: 0.000083  loss: 0.7260 (0.7351)  time: 0.7610  data: 0.0001  max mem: 15689
[10:47:46.827122] Epoch: [31]  [300/345]  eta: 0:00:34  lr: 0.000083  loss: 0.7359 (0.7354)  time: 0.7609  data: 0.0001  max mem: 15689
[10:48:02.025844] Epoch: [31]  [320/345]  eta: 0:00:19  lr: 0.000082  loss: 0.7353 (0.7355)  time: 0.7599  data: 0.0001  max mem: 15689
[10:48:17.225752] Epoch: [31]  [340/345]  eta: 0:00:03  lr: 0.000082  loss: 0.7279 (0.7351)  time: 0.7599  data: 0.0001  max mem: 15689
[10:48:20.271006] Epoch: [31]  [344/345]  eta: 0:00:00  lr: 0.000082  loss: 0.7256 (0.7351)  time: 0.7602  data: 0.0001  max mem: 15689
[10:48:20.337077] Epoch: [31] Total time: 0:04:22 (0.7617 s / it)
[10:48:20.337513] Averaged stats: lr: 0.000082  loss: 0.7256 (0.7351)
[10:48:20.676108] Test:  [  0/345]  eta: 0:01:55  loss: 0.7131 (0.7131)  time: 0.3342  data: 0.1505  max mem: 15689
[10:48:22.532447] Test:  [ 10/345]  eta: 0:01:06  loss: 0.6977 (0.6983)  time: 0.1991  data: 0.0138  max mem: 15689
[10:48:24.392968] Test:  [ 20/345]  eta: 0:01:02  loss: 0.6981 (0.7003)  time: 0.1858  data: 0.0001  max mem: 15689
[10:48:26.256866] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7035 (0.7022)  time: 0.1862  data: 0.0001  max mem: 15689
[10:48:28.123540] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7031 (0.7042)  time: 0.1865  data: 0.0001  max mem: 15689
[10:48:29.995750] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7034 (0.7046)  time: 0.1869  data: 0.0001  max mem: 15689
[10:48:31.871418] Test:  [ 60/345]  eta: 0:00:53  loss: 0.6998 (0.7035)  time: 0.1873  data: 0.0001  max mem: 15689
[10:48:33.750510] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7004 (0.7042)  time: 0.1877  data: 0.0001  max mem: 15689
[10:48:35.635313] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7055 (0.7044)  time: 0.1881  data: 0.0001  max mem: 15689
[10:48:37.522023] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7055 (0.7045)  time: 0.1885  data: 0.0001  max mem: 15689
[10:48:39.413247] Test:  [100/345]  eta: 0:00:46  loss: 0.7072 (0.7050)  time: 0.1888  data: 0.0001  max mem: 15689
[10:48:41.305655] Test:  [110/345]  eta: 0:00:44  loss: 0.7072 (0.7053)  time: 0.1891  data: 0.0001  max mem: 15689
[10:48:43.202874] Test:  [120/345]  eta: 0:00:42  loss: 0.7034 (0.7047)  time: 0.1894  data: 0.0001  max mem: 15689
[10:48:45.105012] Test:  [130/345]  eta: 0:00:40  loss: 0.7001 (0.7046)  time: 0.1899  data: 0.0001  max mem: 15689
[10:48:47.011153] Test:  [140/345]  eta: 0:00:38  loss: 0.7027 (0.7047)  time: 0.1904  data: 0.0001  max mem: 15689
[10:48:48.919666] Test:  [150/345]  eta: 0:00:36  loss: 0.7038 (0.7048)  time: 0.1907  data: 0.0001  max mem: 15689
[10:48:50.830252] Test:  [160/345]  eta: 0:00:35  loss: 0.7092 (0.7052)  time: 0.1909  data: 0.0001  max mem: 15689
[10:48:52.743889] Test:  [170/345]  eta: 0:00:33  loss: 0.7097 (0.7058)  time: 0.1912  data: 0.0001  max mem: 15689
[10:48:54.663112] Test:  [180/345]  eta: 0:00:31  loss: 0.7131 (0.7063)  time: 0.1916  data: 0.0001  max mem: 15689
[10:48:56.582948] Test:  [190/345]  eta: 0:00:29  loss: 0.7060 (0.7061)  time: 0.1919  data: 0.0001  max mem: 15689
[10:48:58.509679] Test:  [200/345]  eta: 0:00:27  loss: 0.7057 (0.7062)  time: 0.1923  data: 0.0001  max mem: 15689
[10:49:00.436359] Test:  [210/345]  eta: 0:00:25  loss: 0.7056 (0.7062)  time: 0.1926  data: 0.0001  max mem: 15689
[10:49:02.369271] Test:  [220/345]  eta: 0:00:23  loss: 0.7038 (0.7062)  time: 0.1929  data: 0.0001  max mem: 15689
[10:49:04.303122] Test:  [230/345]  eta: 0:00:21  loss: 0.7000 (0.7060)  time: 0.1933  data: 0.0001  max mem: 15689
[10:49:06.242871] Test:  [240/345]  eta: 0:00:19  loss: 0.7000 (0.7059)  time: 0.1936  data: 0.0001  max mem: 15689
[10:49:08.187428] Test:  [250/345]  eta: 0:00:18  loss: 0.7038 (0.7059)  time: 0.1942  data: 0.0001  max mem: 15689
[10:49:10.134315] Test:  [260/345]  eta: 0:00:16  loss: 0.7038 (0.7059)  time: 0.1945  data: 0.0001  max mem: 15689
[10:49:12.084557] Test:  [270/345]  eta: 0:00:14  loss: 0.7009 (0.7061)  time: 0.1948  data: 0.0001  max mem: 15689
[10:49:14.038501] Test:  [280/345]  eta: 0:00:12  loss: 0.7011 (0.7060)  time: 0.1952  data: 0.0001  max mem: 15689
[10:49:15.995637] Test:  [290/345]  eta: 0:00:10  loss: 0.7035 (0.7060)  time: 0.1955  data: 0.0001  max mem: 15689
[10:49:17.958425] Test:  [300/345]  eta: 0:00:08  loss: 0.7056 (0.7060)  time: 0.1959  data: 0.0001  max mem: 15689
[10:49:19.921354] Test:  [310/345]  eta: 0:00:06  loss: 0.7001 (0.7057)  time: 0.1962  data: 0.0001  max mem: 15689
[10:49:21.889703] Test:  [320/345]  eta: 0:00:04  loss: 0.7034 (0.7059)  time: 0.1965  data: 0.0001  max mem: 15689
[10:49:23.862280] Test:  [330/345]  eta: 0:00:02  loss: 0.7061 (0.7059)  time: 0.1970  data: 0.0001  max mem: 15689
[10:49:25.834332] Test:  [340/345]  eta: 0:00:00  loss: 0.7064 (0.7060)  time: 0.1972  data: 0.0001  max mem: 15689
[10:49:26.624866] Test:  [344/345]  eta: 0:00:00  loss: 0.7074 (0.7061)  time: 0.1972  data: 0.0001  max mem: 15689
[10:49:26.685983] Test: Total time: 0:01:06 (0.1923 s / it)
[10:49:37.192280] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8369 (0.8369)  time: 0.3244  data: 0.1427  max mem: 15689
[10:49:39.028644] Test:  [10/57]  eta: 0:00:09  loss: 0.8625 (0.8720)  time: 0.1964  data: 0.0131  max mem: 15689
[10:49:40.871584] Test:  [20/57]  eta: 0:00:07  loss: 0.8778 (0.8621)  time: 0.1839  data: 0.0001  max mem: 15689
[10:49:42.717646] Test:  [30/57]  eta: 0:00:05  loss: 0.7587 (0.8232)  time: 0.1844  data: 0.0001  max mem: 15689
[10:49:44.570085] Test:  [40/57]  eta: 0:00:03  loss: 0.7431 (0.8034)  time: 0.1849  data: 0.0001  max mem: 15689
[10:49:46.425890] Test:  [50/57]  eta: 0:00:01  loss: 0.7375 (0.7951)  time: 0.1854  data: 0.0001  max mem: 15689
[10:49:47.426109] Test:  [56/57]  eta: 0:00:00  loss: 0.7532 (0.8005)  time: 0.1798  data: 0.0001  max mem: 15689
[10:49:47.480483] Test: Total time: 0:00:10 (0.1862 s / it)
[10:49:49.270803] Dice score of the network on the train images: 0.818012, val images: 0.821476
[10:49:49.271029] saving best_dice_model_0 @ epoch 31
[10:49:50.390922] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:49:51.296289] Epoch: [32]  [  0/345]  eta: 0:05:11  lr: 0.000082  loss: 0.7279 (0.7279)  time: 0.9043  data: 0.1473  max mem: 15689
[10:50:06.499560] Epoch: [32]  [ 20/345]  eta: 0:04:09  lr: 0.000081  loss: 0.7220 (0.7270)  time: 0.7601  data: 0.0001  max mem: 15689
[10:50:21.656070] Epoch: [32]  [ 40/345]  eta: 0:03:52  lr: 0.000081  loss: 0.7374 (0.7326)  time: 0.7578  data: 0.0001  max mem: 15689
[10:50:36.856165] Epoch: [32]  [ 60/345]  eta: 0:03:37  lr: 0.000081  loss: 0.7294 (0.7314)  time: 0.7600  data: 0.0001  max mem: 15689
[10:50:52.086285] Epoch: [32]  [ 80/345]  eta: 0:03:21  lr: 0.000080  loss: 0.7247 (0.7308)  time: 0.7615  data: 0.0001  max mem: 15689
[10:51:07.321576] Epoch: [32]  [100/345]  eta: 0:03:06  lr: 0.000080  loss: 0.7303 (0.7307)  time: 0.7617  data: 0.0001  max mem: 15689
[10:51:22.564140] Epoch: [32]  [120/345]  eta: 0:02:51  lr: 0.000080  loss: 0.7312 (0.7306)  time: 0.7621  data: 0.0001  max mem: 15689
[10:51:37.810815] Epoch: [32]  [140/345]  eta: 0:02:36  lr: 0.000079  loss: 0.7218 (0.7298)  time: 0.7623  data: 0.0001  max mem: 15689
[10:51:53.058403] Epoch: [32]  [160/345]  eta: 0:02:20  lr: 0.000079  loss: 0.7345 (0.7308)  time: 0.7623  data: 0.0001  max mem: 15689
[10:52:08.299482] Epoch: [32]  [180/345]  eta: 0:02:05  lr: 0.000079  loss: 0.7291 (0.7308)  time: 0.7620  data: 0.0001  max mem: 15689
[10:52:23.535715] Epoch: [32]  [200/345]  eta: 0:01:50  lr: 0.000078  loss: 0.7308 (0.7312)  time: 0.7618  data: 0.0001  max mem: 15689
[10:52:38.766982] Epoch: [32]  [220/345]  eta: 0:01:35  lr: 0.000078  loss: 0.7273 (0.7307)  time: 0.7615  data: 0.0001  max mem: 15689
[10:52:53.991941] Epoch: [32]  [240/345]  eta: 0:01:19  lr: 0.000077  loss: 0.7327 (0.7312)  time: 0.7612  data: 0.0001  max mem: 15689
[10:53:09.217238] Epoch: [32]  [260/345]  eta: 0:01:04  lr: 0.000077  loss: 0.7369 (0.7316)  time: 0.7612  data: 0.0001  max mem: 15689
[10:53:24.420196] Epoch: [32]  [280/345]  eta: 0:00:49  lr: 0.000077  loss: 0.7340 (0.7321)  time: 0.7601  data: 0.0001  max mem: 15689
[10:53:39.616334] Epoch: [32]  [300/345]  eta: 0:00:34  lr: 0.000076  loss: 0.7255 (0.7318)  time: 0.7598  data: 0.0001  max mem: 15689
[10:53:54.827642] Epoch: [32]  [320/345]  eta: 0:00:19  lr: 0.000076  loss: 0.7271 (0.7317)  time: 0.7605  data: 0.0001  max mem: 15689
[10:54:10.048398] Epoch: [32]  [340/345]  eta: 0:00:03  lr: 0.000076  loss: 0.7303 (0.7316)  time: 0.7610  data: 0.0001  max mem: 15689
[10:54:13.089442] Epoch: [32]  [344/345]  eta: 0:00:00  lr: 0.000076  loss: 0.7303 (0.7315)  time: 0.7607  data: 0.0001  max mem: 15689
[10:54:13.153710] Epoch: [32] Total time: 0:04:22 (0.7616 s / it)
[10:54:13.154127] Averaged stats: lr: 0.000076  loss: 0.7303 (0.7315)
[10:54:13.495025] Test:  [  0/345]  eta: 0:01:56  loss: 0.6982 (0.6982)  time: 0.3365  data: 0.1528  max mem: 15689
[10:54:15.351916] Test:  [ 10/345]  eta: 0:01:06  loss: 0.6974 (0.6979)  time: 0.1993  data: 0.0139  max mem: 15689
[10:54:17.213759] Test:  [ 20/345]  eta: 0:01:02  loss: 0.6991 (0.7040)  time: 0.1859  data: 0.0001  max mem: 15689
[10:54:19.078424] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7042 (0.7036)  time: 0.1863  data: 0.0001  max mem: 15689
[10:54:20.946938] Test:  [ 40/345]  eta: 0:00:57  loss: 0.6969 (0.7028)  time: 0.1866  data: 0.0001  max mem: 15689
[10:54:22.818257] Test:  [ 50/345]  eta: 0:00:55  loss: 0.6997 (0.7030)  time: 0.1869  data: 0.0001  max mem: 15689
[10:54:24.692585] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7024 (0.7029)  time: 0.1872  data: 0.0001  max mem: 15689
[10:54:26.571416] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7043 (0.7039)  time: 0.1876  data: 0.0001  max mem: 15689
[10:54:28.456459] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7072 (0.7046)  time: 0.1881  data: 0.0001  max mem: 15689
[10:54:30.344886] Test:  [ 90/345]  eta: 0:00:48  loss: 0.7069 (0.7051)  time: 0.1886  data: 0.0001  max mem: 15689
[10:54:32.236608] Test:  [100/345]  eta: 0:00:46  loss: 0.7036 (0.7051)  time: 0.1889  data: 0.0001  max mem: 15689
[10:54:34.130573] Test:  [110/345]  eta: 0:00:44  loss: 0.7012 (0.7051)  time: 0.1892  data: 0.0001  max mem: 15689
[10:54:36.028239] Test:  [120/345]  eta: 0:00:42  loss: 0.6990 (0.7046)  time: 0.1895  data: 0.0001  max mem: 15689
[10:54:37.929758] Test:  [130/345]  eta: 0:00:40  loss: 0.6986 (0.7038)  time: 0.1899  data: 0.0001  max mem: 15689
[10:54:39.834418] Test:  [140/345]  eta: 0:00:38  loss: 0.6997 (0.7039)  time: 0.1903  data: 0.0001  max mem: 15689
[10:54:41.743011] Test:  [150/345]  eta: 0:00:36  loss: 0.7028 (0.7038)  time: 0.1906  data: 0.0001  max mem: 15689
[10:54:43.652811] Test:  [160/345]  eta: 0:00:35  loss: 0.7028 (0.7039)  time: 0.1909  data: 0.0001  max mem: 15689
[10:54:45.566921] Test:  [170/345]  eta: 0:00:33  loss: 0.7055 (0.7041)  time: 0.1911  data: 0.0001  max mem: 15689
[10:54:47.485277] Test:  [180/345]  eta: 0:00:31  loss: 0.7022 (0.7038)  time: 0.1916  data: 0.0001  max mem: 15689
[10:54:49.406832] Test:  [190/345]  eta: 0:00:29  loss: 0.7022 (0.7040)  time: 0.1919  data: 0.0001  max mem: 15689
[10:54:51.330006] Test:  [200/345]  eta: 0:00:27  loss: 0.7049 (0.7038)  time: 0.1922  data: 0.0001  max mem: 15689
[10:54:53.259143] Test:  [210/345]  eta: 0:00:25  loss: 0.7049 (0.7039)  time: 0.1926  data: 0.0001  max mem: 15689
[10:54:55.191254] Test:  [220/345]  eta: 0:00:23  loss: 0.7072 (0.7042)  time: 0.1930  data: 0.0001  max mem: 15689
[10:54:57.127533] Test:  [230/345]  eta: 0:00:21  loss: 0.7068 (0.7043)  time: 0.1934  data: 0.0001  max mem: 15689
[10:54:59.068698] Test:  [240/345]  eta: 0:00:19  loss: 0.7054 (0.7042)  time: 0.1938  data: 0.0001  max mem: 15689
[10:55:01.011092] Test:  [250/345]  eta: 0:00:18  loss: 0.7034 (0.7040)  time: 0.1941  data: 0.0001  max mem: 15689
[10:55:02.958560] Test:  [260/345]  eta: 0:00:16  loss: 0.7071 (0.7040)  time: 0.1944  data: 0.0001  max mem: 15689
[10:55:04.908345] Test:  [270/345]  eta: 0:00:14  loss: 0.7052 (0.7040)  time: 0.1948  data: 0.0001  max mem: 15689
[10:55:06.863615] Test:  [280/345]  eta: 0:00:12  loss: 0.7016 (0.7039)  time: 0.1952  data: 0.0001  max mem: 15689
[10:55:08.819670] Test:  [290/345]  eta: 0:00:10  loss: 0.7023 (0.7040)  time: 0.1955  data: 0.0001  max mem: 15689
[10:55:10.780641] Test:  [300/345]  eta: 0:00:08  loss: 0.7072 (0.7041)  time: 0.1958  data: 0.0001  max mem: 15689
[10:55:12.742967] Test:  [310/345]  eta: 0:00:06  loss: 0.7010 (0.7041)  time: 0.1961  data: 0.0001  max mem: 15689
[10:55:14.709089] Test:  [320/345]  eta: 0:00:04  loss: 0.7010 (0.7040)  time: 0.1964  data: 0.0001  max mem: 15689
[10:55:16.680045] Test:  [330/345]  eta: 0:00:02  loss: 0.7050 (0.7043)  time: 0.1968  data: 0.0001  max mem: 15689
[10:55:18.653809] Test:  [340/345]  eta: 0:00:00  loss: 0.7051 (0.7042)  time: 0.1972  data: 0.0001  max mem: 15689
[10:55:19.444422] Test:  [344/345]  eta: 0:00:00  loss: 0.7050 (0.7043)  time: 0.1973  data: 0.0001  max mem: 15689
[10:55:19.504655] Test: Total time: 0:01:06 (0.1923 s / it)
[10:55:30.149708] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8359 (0.8359)  time: 0.3247  data: 0.1431  max mem: 15689
[10:55:31.983798] Test:  [10/57]  eta: 0:00:09  loss: 0.8631 (0.8672)  time: 0.1962  data: 0.0131  max mem: 15689
[10:55:33.825318] Test:  [20/57]  eta: 0:00:07  loss: 0.8634 (0.8577)  time: 0.1837  data: 0.0001  max mem: 15689
[10:55:35.670304] Test:  [30/57]  eta: 0:00:05  loss: 0.7553 (0.8211)  time: 0.1843  data: 0.0001  max mem: 15689
[10:55:37.519326] Test:  [40/57]  eta: 0:00:03  loss: 0.7429 (0.8017)  time: 0.1846  data: 0.0001  max mem: 15689
[10:55:39.373634] Test:  [50/57]  eta: 0:00:01  loss: 0.7364 (0.7941)  time: 0.1851  data: 0.0001  max mem: 15689
[10:55:40.374360] Test:  [56/57]  eta: 0:00:00  loss: 0.7552 (0.7997)  time: 0.1797  data: 0.0001  max mem: 15689
[10:55:40.430066] Test: Total time: 0:00:10 (0.1861 s / it)
[10:55:42.271883] Dice score of the network on the train images: 0.815055, val images: 0.823346
[10:55:42.272116] saving best_dice_model_0 @ epoch 32
[10:55:43.351468] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[10:55:44.263694] Epoch: [33]  [  0/345]  eta: 0:05:14  lr: 0.000075  loss: 0.7223 (0.7223)  time: 0.9113  data: 0.1572  max mem: 15689
[10:55:59.379139] Epoch: [33]  [ 20/345]  eta: 0:04:08  lr: 0.000075  loss: 0.7293 (0.7279)  time: 0.7557  data: 0.0001  max mem: 15689
[10:56:14.524828] Epoch: [33]  [ 40/345]  eta: 0:03:51  lr: 0.000075  loss: 0.7217 (0.7267)  time: 0.7572  data: 0.0001  max mem: 15689
[10:56:29.709003] Epoch: [33]  [ 60/345]  eta: 0:03:36  lr: 0.000074  loss: 0.7207 (0.7259)  time: 0.7592  data: 0.0001  max mem: 15689
[10:56:45.020478] Epoch: [33]  [ 80/345]  eta: 0:03:21  lr: 0.000074  loss: 0.7305 (0.7265)  time: 0.7655  data: 0.0001  max mem: 15689
[10:57:00.258296] Epoch: [33]  [100/345]  eta: 0:03:06  lr: 0.000074  loss: 0.7317 (0.7270)  time: 0.7618  data: 0.0001  max mem: 15689
[10:57:15.504328] Epoch: [33]  [120/345]  eta: 0:02:51  lr: 0.000073  loss: 0.7229 (0.7274)  time: 0.7623  data: 0.0001  max mem: 15689
[10:57:30.738918] Epoch: [33]  [140/345]  eta: 0:02:36  lr: 0.000073  loss: 0.7273 (0.7273)  time: 0.7617  data: 0.0001  max mem: 15689
[10:57:45.979944] Epoch: [33]  [160/345]  eta: 0:02:20  lr: 0.000073  loss: 0.7406 (0.7289)  time: 0.7620  data: 0.0001  max mem: 15689
[10:58:01.227967] Epoch: [33]  [180/345]  eta: 0:02:05  lr: 0.000072  loss: 0.7332 (0.7293)  time: 0.7624  data: 0.0001  max mem: 15689
[10:58:16.468551] Epoch: [33]  [200/345]  eta: 0:01:50  lr: 0.000072  loss: 0.7293 (0.7297)  time: 0.7620  data: 0.0001  max mem: 15689
[10:58:31.721529] Epoch: [33]  [220/345]  eta: 0:01:35  lr: 0.000071  loss: 0.7290 (0.7298)  time: 0.7626  data: 0.0001  max mem: 15689
[10:58:46.949677] Epoch: [33]  [240/345]  eta: 0:01:19  lr: 0.000071  loss: 0.7285 (0.7298)  time: 0.7614  data: 0.0001  max mem: 15689
[10:59:02.174269] Epoch: [33]  [260/345]  eta: 0:01:04  lr: 0.000071  loss: 0.7218 (0.7293)  time: 0.7612  data: 0.0001  max mem: 15689
[10:59:17.455796] Epoch: [33]  [280/345]  eta: 0:00:49  lr: 0.000070  loss: 0.7277 (0.7292)  time: 0.7640  data: 0.0001  max mem: 15689
[10:59:32.678465] Epoch: [33]  [300/345]  eta: 0:00:34  lr: 0.000070  loss: 0.7315 (0.7294)  time: 0.7611  data: 0.0001  max mem: 15689
[10:59:47.887615] Epoch: [33]  [320/345]  eta: 0:00:19  lr: 0.000070  loss: 0.7228 (0.7292)  time: 0.7604  data: 0.0001  max mem: 15689
[11:00:03.111779] Epoch: [33]  [340/345]  eta: 0:00:03  lr: 0.000069  loss: 0.7261 (0.7289)  time: 0.7612  data: 0.0001  max mem: 15689
[11:00:06.154802] Epoch: [33]  [344/345]  eta: 0:00:00  lr: 0.000069  loss: 0.7236 (0.7289)  time: 0.7610  data: 0.0001  max mem: 15689
[11:00:06.217647] Epoch: [33] Total time: 0:04:22 (0.7619 s / it)
[11:00:06.217920] Averaged stats: lr: 0.000069  loss: 0.7236 (0.7289)
[11:00:06.552654] Test:  [  0/345]  eta: 0:01:53  loss: 0.7037 (0.7037)  time: 0.3291  data: 0.1450  max mem: 15689
[11:00:08.411759] Test:  [ 10/345]  eta: 0:01:06  loss: 0.6971 (0.7015)  time: 0.1988  data: 0.0133  max mem: 15689
[11:00:10.272007] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7004 (0.7016)  time: 0.1859  data: 0.0001  max mem: 15689
[11:00:12.135101] Test:  [ 30/345]  eta: 0:01:00  loss: 0.7004 (0.7006)  time: 0.1861  data: 0.0001  max mem: 15689
[11:00:14.002708] Test:  [ 40/345]  eta: 0:00:57  loss: 0.6993 (0.7008)  time: 0.1865  data: 0.0001  max mem: 15689
[11:00:15.874106] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7034 (0.7015)  time: 0.1869  data: 0.0001  max mem: 15689
[11:00:17.748457] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7023 (0.7017)  time: 0.1872  data: 0.0001  max mem: 15689
[11:00:19.626744] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7009 (0.7017)  time: 0.1876  data: 0.0001  max mem: 15689
[11:00:21.511780] Test:  [ 80/345]  eta: 0:00:50  loss: 0.7006 (0.7014)  time: 0.1881  data: 0.0001  max mem: 15689
[11:00:23.398936] Test:  [ 90/345]  eta: 0:00:48  loss: 0.6982 (0.7015)  time: 0.1886  data: 0.0001  max mem: 15689
[11:00:25.288527] Test:  [100/345]  eta: 0:00:46  loss: 0.6985 (0.7017)  time: 0.1888  data: 0.0001  max mem: 15689
[11:00:27.180756] Test:  [110/345]  eta: 0:00:44  loss: 0.7030 (0.7020)  time: 0.1890  data: 0.0001  max mem: 15689
[11:00:29.078116] Test:  [120/345]  eta: 0:00:42  loss: 0.7031 (0.7019)  time: 0.1894  data: 0.0001  max mem: 15689
[11:00:30.980664] Test:  [130/345]  eta: 0:00:40  loss: 0.6968 (0.7016)  time: 0.1899  data: 0.0001  max mem: 15689
[11:00:32.883933] Test:  [140/345]  eta: 0:00:38  loss: 0.6988 (0.7014)  time: 0.1902  data: 0.0001  max mem: 15689
[11:00:34.790117] Test:  [150/345]  eta: 0:00:36  loss: 0.7011 (0.7021)  time: 0.1904  data: 0.0001  max mem: 15689
[11:00:36.701908] Test:  [160/345]  eta: 0:00:35  loss: 0.7059 (0.7021)  time: 0.1908  data: 0.0001  max mem: 15689
[11:00:38.615157] Test:  [170/345]  eta: 0:00:33  loss: 0.7015 (0.7021)  time: 0.1912  data: 0.0001  max mem: 15689
[11:00:40.534497] Test:  [180/345]  eta: 0:00:31  loss: 0.7018 (0.7021)  time: 0.1916  data: 0.0001  max mem: 15689
[11:00:42.455825] Test:  [190/345]  eta: 0:00:29  loss: 0.6981 (0.7021)  time: 0.1920  data: 0.0001  max mem: 15689
[11:00:44.380654] Test:  [200/345]  eta: 0:00:27  loss: 0.6981 (0.7021)  time: 0.1923  data: 0.0001  max mem: 15689
[11:00:46.308476] Test:  [210/345]  eta: 0:00:25  loss: 0.7023 (0.7024)  time: 0.1926  data: 0.0001  max mem: 15689
[11:00:48.240114] Test:  [220/345]  eta: 0:00:23  loss: 0.7009 (0.7024)  time: 0.1929  data: 0.0001  max mem: 15689
[11:00:50.174666] Test:  [230/345]  eta: 0:00:21  loss: 0.6973 (0.7021)  time: 0.1933  data: 0.0001  max mem: 15689
[11:00:52.113122] Test:  [240/345]  eta: 0:00:19  loss: 0.6975 (0.7021)  time: 0.1936  data: 0.0001  max mem: 15689
[11:00:54.055495] Test:  [250/345]  eta: 0:00:18  loss: 0.7041 (0.7024)  time: 0.1940  data: 0.0001  max mem: 15689
[11:00:56.002687] Test:  [260/345]  eta: 0:00:16  loss: 0.7041 (0.7024)  time: 0.1944  data: 0.0001  max mem: 15689
[11:00:57.951920] Test:  [270/345]  eta: 0:00:14  loss: 0.6970 (0.7024)  time: 0.1948  data: 0.0001  max mem: 15689
[11:00:59.904762] Test:  [280/345]  eta: 0:00:12  loss: 0.7064 (0.7026)  time: 0.1950  data: 0.0001  max mem: 15689
[11:01:01.862455] Test:  [290/345]  eta: 0:00:10  loss: 0.7046 (0.7026)  time: 0.1955  data: 0.0001  max mem: 15689
[11:01:03.823486] Test:  [300/345]  eta: 0:00:08  loss: 0.7001 (0.7027)  time: 0.1959  data: 0.0001  max mem: 15689
[11:01:05.787299] Test:  [310/345]  eta: 0:00:06  loss: 0.6992 (0.7027)  time: 0.1962  data: 0.0001  max mem: 15689
[11:01:07.754203] Test:  [320/345]  eta: 0:00:04  loss: 0.6992 (0.7027)  time: 0.1965  data: 0.0001  max mem: 15689
[11:01:09.723144] Test:  [330/345]  eta: 0:00:02  loss: 0.6975 (0.7027)  time: 0.1967  data: 0.0001  max mem: 15689
[11:01:11.695203] Test:  [340/345]  eta: 0:00:00  loss: 0.7040 (0.7028)  time: 0.1970  data: 0.0001  max mem: 15689
[11:01:12.486015] Test:  [344/345]  eta: 0:00:00  loss: 0.7045 (0.7028)  time: 0.1972  data: 0.0001  max mem: 15689
[11:01:12.546223] Test: Total time: 0:01:06 (0.1922 s / it)
[11:01:23.055860] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8400 (0.8400)  time: 0.3285  data: 0.1469  max mem: 15689
[11:01:24.890915] Test:  [10/57]  eta: 0:00:09  loss: 0.8730 (0.8730)  time: 0.1966  data: 0.0134  max mem: 15689
[11:01:26.734717] Test:  [20/57]  eta: 0:00:07  loss: 0.8730 (0.8625)  time: 0.1839  data: 0.0001  max mem: 15689
[11:01:28.581343] Test:  [30/57]  eta: 0:00:05  loss: 0.7577 (0.8238)  time: 0.1845  data: 0.0001  max mem: 15689
[11:01:30.431287] Test:  [40/57]  eta: 0:00:03  loss: 0.7431 (0.8035)  time: 0.1848  data: 0.0001  max mem: 15689
[11:01:32.287198] Test:  [50/57]  eta: 0:00:01  loss: 0.7324 (0.7953)  time: 0.1852  data: 0.0001  max mem: 15689
[11:01:33.288914] Test:  [56/57]  eta: 0:00:00  loss: 0.7505 (0.8008)  time: 0.1799  data: 0.0001  max mem: 15689
[11:01:33.342886] Test: Total time: 0:00:10 (0.1862 s / it)
[11:01:35.152082] Dice score of the network on the train images: 0.821898, val images: 0.820382
[11:01:35.156334] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[11:01:36.058920] Epoch: [34]  [  0/345]  eta: 0:05:11  lr: 0.000069  loss: 0.7135 (0.7135)  time: 0.9018  data: 0.1499  max mem: 15689
[11:01:51.189847] Epoch: [34]  [ 20/345]  eta: 0:04:08  lr: 0.000069  loss: 0.7166 (0.7219)  time: 0.7565  data: 0.0001  max mem: 15689
[11:02:06.352606] Epoch: [34]  [ 40/345]  eta: 0:03:52  lr: 0.000068  loss: 0.7265 (0.7245)  time: 0.7581  data: 0.0001  max mem: 15689
[11:02:21.538891] Epoch: [34]  [ 60/345]  eta: 0:03:36  lr: 0.000068  loss: 0.7194 (0.7235)  time: 0.7593  data: 0.0001  max mem: 15689
[11:02:36.744682] Epoch: [34]  [ 80/345]  eta: 0:03:21  lr: 0.000068  loss: 0.7243 (0.7237)  time: 0.7602  data: 0.0001  max mem: 15689
[11:02:51.973541] Epoch: [34]  [100/345]  eta: 0:03:06  lr: 0.000067  loss: 0.7346 (0.7257)  time: 0.7614  data: 0.0001  max mem: 15689
[11:03:07.203090] Epoch: [34]  [120/345]  eta: 0:02:51  lr: 0.000067  loss: 0.7172 (0.7246)  time: 0.7614  data: 0.0001  max mem: 15689
[11:03:22.433361] Epoch: [34]  [140/345]  eta: 0:02:35  lr: 0.000066  loss: 0.7228 (0.7240)  time: 0.7615  data: 0.0001  max mem: 15689
[11:03:37.678954] Epoch: [34]  [160/345]  eta: 0:02:20  lr: 0.000066  loss: 0.7266 (0.7242)  time: 0.7622  data: 0.0001  max mem: 15689
[11:03:53.015316] Epoch: [34]  [180/345]  eta: 0:02:05  lr: 0.000066  loss: 0.7245 (0.7243)  time: 0.7668  data: 0.0001  max mem: 15689
[11:04:08.250165] Epoch: [34]  [200/345]  eta: 0:01:50  lr: 0.000065  loss: 0.7327 (0.7251)  time: 0.7617  data: 0.0001  max mem: 15689
[11:04:23.485287] Epoch: [34]  [220/345]  eta: 0:01:35  lr: 0.000065  loss: 0.7285 (0.7254)  time: 0.7617  data: 0.0001  max mem: 15689
[11:04:38.705841] Epoch: [34]  [240/345]  eta: 0:01:19  lr: 0.000064  loss: 0.7212 (0.7253)  time: 0.7610  data: 0.0001  max mem: 15689
[11:04:53.936244] Epoch: [34]  [260/345]  eta: 0:01:04  lr: 0.000064  loss: 0.7269 (0.7260)  time: 0.7615  data: 0.0001  max mem: 15689
[11:05:09.154644] Epoch: [34]  [280/345]  eta: 0:00:49  lr: 0.000064  loss: 0.7245 (0.7260)  time: 0.7609  data: 0.0001  max mem: 15689
[11:05:17.712614] NaN detected in inputs
[11:05:17.716919] Loss is nan, stopping training
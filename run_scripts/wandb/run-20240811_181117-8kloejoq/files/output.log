Not using distributed mode
[18:11:19.634770] job dir: /root/seg_framework/MS-Mamba/run_scripts
[18:11:19.634913] Namespace(accum_iter=1,
model='SegFormer3D',
in_channels=1,
lr=None,
blr=0.001,
min_lr=0,
dist_on_itp=False,
warmup_epochs=20,
device='cuda:0',
seed=42,
layer_decay=0.75,
clip_grad=None,
num_workers=8,
pin_mem=True,
resume='',
world_size=1,
embed_dim=1,
local_rank=-1,
dist_url='env://',
nb_classes=2,
data_dir='/root/MSLesSeg24/data',
datalist=None,
modalities='T1,T2,FLAIR,T1ce',
preprocess=False,
distributed=False)
[18:11:19.635025] device  cuda:0
[18:11:19.636701] Starting for fold 0
[18:11:20.447147] base lr: 1.00e-03
[18:11:20.447404] actual lr: 1.25e-04
[18:11:20.447501] accumulate grad iterations: 1
[18:11:20.447592] effective batch size: 32
[18:11:20.448497] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.000125
    maximize: False
    weight_decay: 0.01
)
[18:11:20.450283] Start training for 50 epochs
[18:11:20.451390] log_dir: /root/seg_framework/MS-Mamba/output_dir/segformer/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/segformer/fold_0/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/segformer/fold_0/val_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/segformer/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/segformer/val_ft
Traceback (most recent call last):
  File "/root/anaconda3/envs/vivim/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/anaconda3/envs/vivim/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/seg_framework/MS-Mamba/run_scripts/K_fold_mslesseg.py", line 458, in <module>
    main(args)
  File "/root/seg_framework/MS-Mamba/run_scripts/K_fold_mslesseg.py", line 358, in main
    train_stats = train_one_epoch(
  File "/root/seg_framework/MS-Mamba/run_scripts/K_fold_mslesseg.py", line 120, in train_one_epoch
    outputs = model(samples)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/seg_framework/MS-Mamba/model/Vivim.py", line 301, in forward
    outs = self.encoder(x_in)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/seg_framework/MS-Mamba/model/Vivim.py", line 246, in forward
    x = self.forward_features(x)
  File "/root/seg_framework/MS-Mamba/model/Vivim.py", line 226, in forward_features
    hs = blk(hs, height, width, depth)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/seg_framework/MS-Mamba/model/SegFormer3D.py", line 280, in forward
    x = x + self.attention(self.norm1(x), h, w, d)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/seg_framework/MS-Mamba/model/SegFormer3D.py", line 237, in forward
    attention_score = (q @ k.transpose(-2, -1)) / math.sqrt(self.num_heads)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 21.89 GiB. GPU 0 has a total capacty of 47.44 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 38.95 GiB memory in use. Of the allocated memory 37.93 GiB is allocated by PyTorch, and 713.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[18:11:56.311592] [18:11:56.311869] [18:11:56.311939] [18:11:56.312018] [18:11:56.312078] [18:11:56.312136] [18:11:56.312192] [18:11:56.312250] [18:11:56.312307] [18:11:56.312373] [18:11:56.312429] [18:11:56.312486] [18:11:56.312543] [18:11:56.312599] [18:11:56.312654] [18:11:56.312708] [18:11:56.312764] [18:11:56.312820] [18:11:56.312876] [18:11:56.312941]
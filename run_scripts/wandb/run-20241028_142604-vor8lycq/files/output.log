Not using distributed mode
[14:26:06.146318] job dir: /root/seg_framework/MS-Mamba/run_scripts
[14:26:06.146415] Namespace(accum_iter=1,
model='SegFormer3D',
in_channels=1,
lr=None,
blr=0.001,
min_lr=0,
dist_on_itp=False,
warmup_epochs=20,
device='cuda:0',
seed=42,
layer_decay=0.75,
clip_grad=None,
num_workers=1,
pin_mem=True,
resume='',
mask_mode='concatenate to image',
world_size=1,
embed_dim=1,
local_rank=-1,
dist_url='env://',
nb_classes=2,
data_dir='/root/MSLesSeg24/data',
datalist=None,
preprocess=False,
dim=2,
loss='mask tp1 tp2',
distributed=False)
[14:26:06.146501] device  cuda:0
[14:26:06.147183] Random seed set as 42
[14:26:06.147478] Starting for fold 0
[14:26:06.336669] Elements in data_dir_paths: 11052
[14:26:06.371007] Elements in data_dir_paths: 1803
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[14:26:08.131560] number of params: 59617303
[14:26:08.131804] model: Vivim2D(
  (encoder): mamba_block(
    (downsample_layers): SegformerEncoder(
      (patch_embeddings): ModuleList(
        (0): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(2, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (3): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (block): ModuleList(
        (0): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): Identity()
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.003703703870996833)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.007407407741993666)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.011111111380159855)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.014814815483987331)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.018518518656492233)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.02222222276031971)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.025925926864147186)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.029629630967974663)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03333333507180214)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03703703731298447)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04074074327945709)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04444444552063942)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.048148151487112045)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.051851850003004074)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0555555559694767)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.05925925821065903)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06296296417713165)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06666667014360428)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07037036865949631)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07407407462596893)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07777778059244156)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08148147910833359)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08518518507480621)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08888889104127884)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.09259259700775146)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0962962955236435)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.10000000149011612)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (layer_norm): ModuleList(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (2): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (3): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (decoder): SegformerDecodeHead(
    (linear_c): ModuleList(
      (0): SegformerMLP(
        (proj): Linear(in_features=64, out_features=768, bias=True)
      )
      (1): SegformerMLP(
        (proj): Linear(in_features=128, out_features=768, bias=True)
      )
      (2): SegformerMLP(
        (proj): Linear(in_features=320, out_features=768, bias=True)
      )
      (3): SegformerMLP(
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
    )
    (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Conv2d(768, 150, kernel_size=(1, 1), stride=(1, 1))
  )
  (out): Conv2d(768, 1, kernel_size=(1, 1), stride=(1, 1))
)
[14:26:08.134992] base lr: 1.00e-03
[14:26:08.135055] actual lr: 1.25e-04
[14:26:08.135111] accumulate grad iterations: 1
[14:26:08.135158] effective batch size: 32
[14:26:08.138310] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.000125
    maximize: False
    weight_decay: 0.01
)
[14:26:08.140327] Start training for 50 epochs
[14:26:08.142181] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[14:26:09.714514] Epoch: [0]  [  0/345]  eta: 0:09:02  lr: 0.000000  loss: 1.6965 (1.6965)  time: 1.5713  data: 0.1912  max mem: 14473
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
[14:26:24.106757] Epoch: [0]  [ 20/345]  eta: 0:04:07  lr: 0.000000  loss: 1.6955 (1.6956)  time: 0.7196  data: 0.0001  max mem: 14938
[14:26:38.577691] Epoch: [0]  [ 40/345]  eta: 0:03:46  lr: 0.000001  loss: 1.6917 (1.6936)  time: 0.7235  data: 0.0001  max mem: 14938
[14:26:53.250516] Epoch: [0]  [ 60/345]  eta: 0:03:30  lr: 0.000001  loss: 1.6874 (1.6916)  time: 0.7336  data: 0.0001  max mem: 14938
[14:27:07.920627] Epoch: [0]  [ 80/345]  eta: 0:03:15  lr: 0.000001  loss: 1.6850 (1.6901)  time: 0.7335  data: 0.0001  max mem: 14938
[14:27:22.684505] Epoch: [0]  [100/345]  eta: 0:03:00  lr: 0.000002  loss: 1.6822 (1.6885)  time: 0.7381  data: 0.0001  max mem: 14938
[14:27:37.483096] Epoch: [0]  [120/345]  eta: 0:02:46  lr: 0.000002  loss: 1.6764 (1.6865)  time: 0.7399  data: 0.0001  max mem: 14938
[14:27:52.331602] Epoch: [0]  [140/345]  eta: 0:02:31  lr: 0.000003  loss: 1.6697 (1.6843)  time: 0.7424  data: 0.0001  max mem: 14938
[14:28:07.233502] Epoch: [0]  [160/345]  eta: 0:02:16  lr: 0.000003  loss: 1.6670 (1.6822)  time: 0.7450  data: 0.0001  max mem: 14938
[14:28:22.146938] Epoch: [0]  [180/345]  eta: 0:02:02  lr: 0.000003  loss: 1.6606 (1.6798)  time: 0.7456  data: 0.0001  max mem: 14938
[14:28:37.082980] Epoch: [0]  [200/345]  eta: 0:01:47  lr: 0.000004  loss: 1.6525 (1.6772)  time: 0.7468  data: 0.0001  max mem: 14938
[14:28:52.033765] Epoch: [0]  [220/345]  eta: 0:01:32  lr: 0.000004  loss: 1.6446 (1.6743)  time: 0.7475  data: 0.0001  max mem: 14938
[14:29:06.991805] Epoch: [0]  [240/345]  eta: 0:01:17  lr: 0.000004  loss: 1.6343 (1.6711)  time: 0.7479  data: 0.0001  max mem: 14938
[14:29:21.951859] Epoch: [0]  [260/345]  eta: 0:01:03  lr: 0.000005  loss: 1.6247 (1.6675)  time: 0.7480  data: 0.0001  max mem: 14938
[14:29:36.922368] Epoch: [0]  [280/345]  eta: 0:00:48  lr: 0.000005  loss: 1.6127 (1.6636)  time: 0.7485  data: 0.0001  max mem: 14938
[14:29:51.903175] Epoch: [0]  [300/345]  eta: 0:00:33  lr: 0.000005  loss: 1.5994 (1.6594)  time: 0.7490  data: 0.0001  max mem: 14938
[14:30:06.896602] Epoch: [0]  [320/345]  eta: 0:00:18  lr: 0.000006  loss: 1.5854 (1.6548)  time: 0.7496  data: 0.0001  max mem: 14938
[14:30:21.890980] Epoch: [0]  [340/345]  eta: 0:00:03  lr: 0.000006  loss: 1.5717 (1.6499)  time: 0.7497  data: 0.0001  max mem: 14938
[14:30:24.890185] Epoch: [0]  [344/345]  eta: 0:00:00  lr: 0.000006  loss: 1.5658 (1.6489)  time: 0.7497  data: 0.0001  max mem: 14938
[14:30:24.949357] Epoch: [0] Total time: 0:04:16 (0.7444 s / it)
[14:30:24.949617] Averaged stats: lr: 0.000006  loss: 1.5658 (1.6489)
[14:30:25.286914] Test:  [  0/345]  eta: 0:01:55  loss: 1.5937 (1.5937)  time: 0.3335  data: 0.1517  max mem: 14938
[14:30:27.124330] Test:  [ 10/345]  eta: 0:01:06  loss: 1.5937 (1.5936)  time: 0.1973  data: 0.0139  max mem: 14938
[14:30:28.964819] Test:  [ 20/345]  eta: 0:01:02  loss: 1.5941 (1.5937)  time: 0.1838  data: 0.0001  max mem: 14938
[14:30:30.809365] Test:  [ 30/345]  eta: 0:00:59  loss: 1.5925 (1.5932)  time: 0.1842  data: 0.0001  max mem: 14938
[14:30:32.657728] Test:  [ 40/345]  eta: 0:00:57  loss: 1.5925 (1.5930)  time: 0.1846  data: 0.0001  max mem: 14938
[14:30:34.508978] Test:  [ 50/345]  eta: 0:00:55  loss: 1.5935 (1.5932)  time: 0.1849  data: 0.0001  max mem: 14938
[14:30:36.365819] Test:  [ 60/345]  eta: 0:00:53  loss: 1.5935 (1.5931)  time: 0.1854  data: 0.0001  max mem: 14938
[14:30:38.225007] Test:  [ 70/345]  eta: 0:00:51  loss: 1.5913 (1.5928)  time: 0.1857  data: 0.0001  max mem: 14938
[14:30:40.088025] Test:  [ 80/345]  eta: 0:00:49  loss: 1.5913 (1.5927)  time: 0.1861  data: 0.0001  max mem: 14938
[14:30:41.954821] Test:  [ 90/345]  eta: 0:00:47  loss: 1.5924 (1.5926)  time: 0.1864  data: 0.0001  max mem: 14938
[14:30:43.824091] Test:  [100/345]  eta: 0:00:45  loss: 1.5927 (1.5925)  time: 0.1868  data: 0.0001  max mem: 14938
[14:30:45.698234] Test:  [110/345]  eta: 0:00:43  loss: 1.5931 (1.5926)  time: 0.1871  data: 0.0001  max mem: 14938
[14:30:47.573797] Test:  [120/345]  eta: 0:00:42  loss: 1.5936 (1.5926)  time: 0.1874  data: 0.0001  max mem: 14938
[14:30:49.453852] Test:  [130/345]  eta: 0:00:40  loss: 1.5920 (1.5925)  time: 0.1877  data: 0.0001  max mem: 14938
[14:30:51.336708] Test:  [140/345]  eta: 0:00:38  loss: 1.5925 (1.5925)  time: 0.1881  data: 0.0001  max mem: 14938
[14:30:53.221760] Test:  [150/345]  eta: 0:00:36  loss: 1.5931 (1.5925)  time: 0.1883  data: 0.0001  max mem: 14938
[14:30:55.111969] Test:  [160/345]  eta: 0:00:34  loss: 1.5937 (1.5926)  time: 0.1887  data: 0.0001  max mem: 14938
[14:30:57.006204] Test:  [170/345]  eta: 0:00:32  loss: 1.5926 (1.5926)  time: 0.1892  data: 0.0001  max mem: 14938
[14:30:58.902576] Test:  [180/345]  eta: 0:00:30  loss: 1.5930 (1.5925)  time: 0.1895  data: 0.0001  max mem: 14938
[14:31:00.801966] Test:  [190/345]  eta: 0:00:29  loss: 1.5934 (1.5925)  time: 0.1897  data: 0.0001  max mem: 14938
[14:31:02.705176] Test:  [200/345]  eta: 0:00:27  loss: 1.5923 (1.5925)  time: 0.1901  data: 0.0001  max mem: 14938
[14:31:05.022948] Test:  [210/345]  eta: 0:00:25  loss: 1.5923 (1.5925)  time: 0.2110  data: 0.0001  max mem: 14938
[14:31:06.944901] Test:  [220/345]  eta: 0:00:23  loss: 1.5940 (1.5926)  time: 0.2119  data: 0.0001  max mem: 14938
[14:31:08.994330] Test:  [230/345]  eta: 0:00:21  loss: 1.5944 (1.5926)  time: 0.1985  data: 0.0001  max mem: 14938
[14:31:10.914707] Test:  [240/345]  eta: 0:00:20  loss: 1.5926 (1.5926)  time: 0.1984  data: 0.0001  max mem: 14938
[14:31:13.010229] Test:  [250/345]  eta: 0:00:18  loss: 1.5926 (1.5926)  time: 0.2007  data: 0.0001  max mem: 14938
[14:31:15.109120] Test:  [260/345]  eta: 0:00:16  loss: 1.5920 (1.5926)  time: 0.2097  data: 0.0001  max mem: 14938
[14:31:17.202441] Test:  [270/345]  eta: 0:00:14  loss: 1.5934 (1.5927)  time: 0.2096  data: 0.0001  max mem: 14938
[14:31:19.163481] Test:  [280/345]  eta: 0:00:12  loss: 1.5934 (1.5926)  time: 0.2027  data: 0.0001  max mem: 14938
[14:31:21.286718] Test:  [290/345]  eta: 0:00:10  loss: 1.5909 (1.5926)  time: 0.2042  data: 0.0001  max mem: 14938
[14:31:23.480187] Test:  [300/345]  eta: 0:00:08  loss: 1.5934 (1.5926)  time: 0.2158  data: 0.0001  max mem: 14938
[14:31:25.603056] Test:  [310/345]  eta: 0:00:06  loss: 1.5937 (1.5927)  time: 0.2158  data: 0.0001  max mem: 14938
[14:31:27.705242] Test:  [320/345]  eta: 0:00:04  loss: 1.5932 (1.5927)  time: 0.2112  data: 0.0001  max mem: 14938
[14:31:29.847786] Test:  [330/345]  eta: 0:00:02  loss: 1.5927 (1.5926)  time: 0.2122  data: 0.0001  max mem: 14938
[14:31:31.879785] Test:  [340/345]  eta: 0:00:00  loss: 1.5917 (1.5926)  time: 0.2087  data: 0.0001  max mem: 14938
[14:31:32.905704] Test:  [344/345]  eta: 0:00:00  loss: 1.5918 (1.5926)  time: 0.2199  data: 0.0001  max mem: 14938
[14:31:32.963503] Test: Total time: 0:01:08 (0.1971 s / it)
[14:31:43.376194] Test:  [ 0/57]  eta: 0:00:18  loss: 1.6031 (1.6031)  time: 0.3191  data: 0.1394  max mem: 14938
[14:31:45.191932] Test:  [10/57]  eta: 0:00:09  loss: 1.5973 (1.5979)  time: 0.1940  data: 0.0127  max mem: 14938
[14:31:47.011578] Test:  [20/57]  eta: 0:00:06  loss: 1.5978 (1.5966)  time: 0.1817  data: 0.0001  max mem: 14938
[14:31:48.834446] Test:  [30/57]  eta: 0:00:05  loss: 1.5949 (1.5923)  time: 0.1821  data: 0.0001  max mem: 14938
[14:31:50.662826] Test:  [40/57]  eta: 0:00:03  loss: 1.5839 (1.5894)  time: 0.1825  data: 0.0001  max mem: 14938
[14:31:52.496846] Test:  [50/57]  eta: 0:00:01  loss: 1.5839 (1.5884)  time: 0.1831  data: 0.0001  max mem: 14938
[14:31:53.512943] Test:  [56/57]  eta: 0:00:00  loss: 1.5872 (1.5884)  time: 0.1790  data: 0.0001  max mem: 14938
[14:31:53.571622] Test: Total time: 0:00:10 (0.1845 s / it)
[14:31:55.307057] Dice score of the network on the train images: 0.000000, val images: 0.000000
[14:31:55.307282] saving best_dice_model_0 @ epoch 0
[14:31:56.515804] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[14:31:57.403000] Epoch: [1]  [  0/345]  eta: 0:05:05  lr: 0.000006  loss: 1.5589 (1.5589)  time: 0.8860  data: 0.1415  max mem: 14938
[14:32:12.292357] Epoch: [1]  [ 20/345]  eta: 0:04:04  lr: 0.000007  loss: 1.5513 (1.5526)  time: 0.7444  data: 0.0001  max mem: 14938
[14:32:27.237910] Epoch: [1]  [ 40/345]  eta: 0:03:48  lr: 0.000007  loss: 1.5421 (1.5472)  time: 0.7472  data: 0.0001  max mem: 14938
[14:32:42.217714] Epoch: [1]  [ 60/345]  eta: 0:03:33  lr: 0.000007  loss: 1.5235 (1.5400)  time: 0.7489  data: 0.0001  max mem: 14938
[14:32:57.218195] Epoch: [1]  [ 80/345]  eta: 0:03:18  lr: 0.000008  loss: 1.5102 (1.5334)  time: 0.7500  data: 0.0001  max mem: 14938
[14:33:12.239106] Epoch: [1]  [100/345]  eta: 0:03:03  lr: 0.000008  loss: 1.4967 (1.5266)  time: 0.7510  data: 0.0001  max mem: 14938
[14:33:27.281791] Epoch: [1]  [120/345]  eta: 0:02:48  lr: 0.000008  loss: 1.4868 (1.5198)  time: 0.7521  data: 0.0001  max mem: 14938
[14:33:42.325759] Epoch: [1]  [140/345]  eta: 0:02:33  lr: 0.000009  loss: 1.4720 (1.5131)  time: 0.7522  data: 0.0001  max mem: 14938
[14:33:57.362282] Epoch: [1]  [160/345]  eta: 0:02:18  lr: 0.000009  loss: 1.4666 (1.5074)  time: 0.7518  data: 0.0001  max mem: 14938
[14:34:12.384710] Epoch: [1]  [180/345]  eta: 0:02:03  lr: 0.000010  loss: 1.4543 (1.5018)  time: 0.7511  data: 0.0001  max mem: 14938
[14:34:27.402491] Epoch: [1]  [200/345]  eta: 0:01:48  lr: 0.000010  loss: 1.4464 (1.4965)  time: 0.7508  data: 0.0001  max mem: 14938
[14:34:42.415982] Epoch: [1]  [220/345]  eta: 0:01:33  lr: 0.000010  loss: 1.4327 (1.4908)  time: 0.7506  data: 0.0001  max mem: 14938
[14:34:57.416767] Epoch: [1]  [240/345]  eta: 0:01:18  lr: 0.000011  loss: 1.4221 (1.4854)  time: 0.7500  data: 0.0001  max mem: 14938
[14:35:12.411564] Epoch: [1]  [260/345]  eta: 0:01:03  lr: 0.000011  loss: 1.4219 (1.4805)  time: 0.7497  data: 0.0001  max mem: 14938
[14:35:27.399121] Epoch: [1]  [280/345]  eta: 0:00:48  lr: 0.000011  loss: 1.4123 (1.4760)  time: 0.7493  data: 0.0001  max mem: 14938
[14:35:42.379934] Epoch: [1]  [300/345]  eta: 0:00:33  lr: 0.000012  loss: 1.4043 (1.4715)  time: 0.7490  data: 0.0001  max mem: 14938
[14:35:57.364345] Epoch: [1]  [320/345]  eta: 0:00:18  lr: 0.000012  loss: 1.4006 (1.4672)  time: 0.7492  data: 0.0001  max mem: 14938
[14:36:12.346982] Epoch: [1]  [340/345]  eta: 0:00:03  lr: 0.000012  loss: 1.3953 (1.4632)  time: 0.7491  data: 0.0001  max mem: 14938
[14:36:15.344323] Epoch: [1]  [344/345]  eta: 0:00:00  lr: 0.000012  loss: 1.3953 (1.4624)  time: 0.7491  data: 0.0001  max mem: 14938
[14:36:15.409417] Epoch: [1] Total time: 0:04:18 (0.7504 s / it)
[14:36:15.409936] Averaged stats: lr: 0.000012  loss: 1.3953 (1.4624)
[14:36:15.749968] Test:  [  0/345]  eta: 0:01:55  loss: 1.3930 (1.3930)  time: 0.3355  data: 0.1538  max mem: 14938
[14:36:17.585849] Test:  [ 10/345]  eta: 0:01:06  loss: 1.3928 (1.3932)  time: 0.1973  data: 0.0140  max mem: 14938
[14:36:19.424370] Test:  [ 20/345]  eta: 0:01:02  loss: 1.3929 (1.3935)  time: 0.1836  data: 0.0001  max mem: 14938
[14:36:21.266055] Test:  [ 30/345]  eta: 0:00:59  loss: 1.3929 (1.3933)  time: 0.1840  data: 0.0001  max mem: 14938
[14:36:23.111142] Test:  [ 40/345]  eta: 0:00:57  loss: 1.3927 (1.3930)  time: 0.1843  data: 0.0001  max mem: 14938
[14:36:24.961003] Test:  [ 50/345]  eta: 0:00:55  loss: 1.3925 (1.3929)  time: 0.1847  data: 0.0001  max mem: 14938
[14:36:26.811941] Test:  [ 60/345]  eta: 0:00:53  loss: 1.3926 (1.3929)  time: 0.1850  data: 0.0001  max mem: 14938
[14:36:28.667737] Test:  [ 70/345]  eta: 0:00:51  loss: 1.3935 (1.3931)  time: 0.1853  data: 0.0001  max mem: 14938
[14:36:30.526716] Test:  [ 80/345]  eta: 0:00:49  loss: 1.3934 (1.3931)  time: 0.1857  data: 0.0001  max mem: 14938
[14:36:32.387769] Test:  [ 90/345]  eta: 0:00:47  loss: 1.3921 (1.3930)  time: 0.1860  data: 0.0001  max mem: 14938
[14:36:34.253817] Test:  [100/345]  eta: 0:00:45  loss: 1.3919 (1.3929)  time: 0.1863  data: 0.0001  max mem: 14938
[14:36:36.122480] Test:  [110/345]  eta: 0:00:43  loss: 1.3930 (1.3929)  time: 0.1867  data: 0.0001  max mem: 14938
[14:36:37.995946] Test:  [120/345]  eta: 0:00:41  loss: 1.3930 (1.3929)  time: 0.1870  data: 0.0001  max mem: 14938
[14:36:39.872341] Test:  [130/345]  eta: 0:00:40  loss: 1.3929 (1.3929)  time: 0.1874  data: 0.0001  max mem: 14938
[14:36:41.752857] Test:  [140/345]  eta: 0:00:38  loss: 1.3933 (1.3930)  time: 0.1878  data: 0.0001  max mem: 14938
[14:36:43.636192] Test:  [150/345]  eta: 0:00:36  loss: 1.3927 (1.3929)  time: 0.1881  data: 0.0001  max mem: 14938
[14:36:45.522075] Test:  [160/345]  eta: 0:00:34  loss: 1.3926 (1.3929)  time: 0.1884  data: 0.0001  max mem: 14938
[14:36:47.410476] Test:  [170/345]  eta: 0:00:32  loss: 1.3929 (1.3929)  time: 0.1887  data: 0.0001  max mem: 14938
[14:36:49.305308] Test:  [180/345]  eta: 0:00:30  loss: 1.3934 (1.3930)  time: 0.1891  data: 0.0001  max mem: 14938
[14:36:51.202725] Test:  [190/345]  eta: 0:00:29  loss: 1.3930 (1.3930)  time: 0.1896  data: 0.0001  max mem: 14938
[14:36:53.104609] Test:  [200/345]  eta: 0:00:27  loss: 1.3933 (1.3930)  time: 0.1899  data: 0.0001  max mem: 14938
[14:36:55.009022] Test:  [210/345]  eta: 0:00:25  loss: 1.3935 (1.3930)  time: 0.1903  data: 0.0001  max mem: 14938
[14:36:56.917119] Test:  [220/345]  eta: 0:00:23  loss: 1.3933 (1.3930)  time: 0.1906  data: 0.0001  max mem: 14938
[14:36:58.828256] Test:  [230/345]  eta: 0:00:21  loss: 1.3929 (1.3930)  time: 0.1909  data: 0.0001  max mem: 14938
[14:37:00.743456] Test:  [240/345]  eta: 0:00:19  loss: 1.3929 (1.3930)  time: 0.1913  data: 0.0001  max mem: 14938
[14:37:02.660061] Test:  [250/345]  eta: 0:00:17  loss: 1.3929 (1.3930)  time: 0.1915  data: 0.0001  max mem: 14938
[14:37:04.581368] Test:  [260/345]  eta: 0:00:16  loss: 1.3925 (1.3929)  time: 0.1918  data: 0.0001  max mem: 14938
[14:37:06.505627] Test:  [270/345]  eta: 0:00:14  loss: 1.3925 (1.3929)  time: 0.1922  data: 0.0001  max mem: 14938
[14:37:08.434740] Test:  [280/345]  eta: 0:00:12  loss: 1.3926 (1.3929)  time: 0.1926  data: 0.0001  max mem: 14938
[14:37:10.366356] Test:  [290/345]  eta: 0:00:10  loss: 1.3927 (1.3929)  time: 0.1930  data: 0.0001  max mem: 14938
[14:37:12.301760] Test:  [300/345]  eta: 0:00:08  loss: 1.3926 (1.3929)  time: 0.1933  data: 0.0001  max mem: 14938
[14:37:14.241323] Test:  [310/345]  eta: 0:00:06  loss: 1.3926 (1.3929)  time: 0.1937  data: 0.0001  max mem: 14938
[14:37:16.185337] Test:  [320/345]  eta: 0:00:04  loss: 1.3925 (1.3929)  time: 0.1941  data: 0.0001  max mem: 14938
[14:37:18.131341] Test:  [330/345]  eta: 0:00:02  loss: 1.3925 (1.3929)  time: 0.1945  data: 0.0001  max mem: 14938
[14:37:20.080279] Test:  [340/345]  eta: 0:00:00  loss: 1.3931 (1.3929)  time: 0.1947  data: 0.0001  max mem: 14938
[14:37:20.861131] Test:  [344/345]  eta: 0:00:00  loss: 1.3934 (1.3929)  time: 0.1948  data: 0.0001  max mem: 14938
[14:37:20.919074] Test: Total time: 0:01:05 (0.1899 s / it)
[14:37:31.349780] Test:  [ 0/57]  eta: 0:00:18  loss: 1.4002 (1.4002)  time: 0.3207  data: 0.1411  max mem: 14938
[14:37:33.166348] Test:  [10/57]  eta: 0:00:09  loss: 1.3979 (1.3969)  time: 0.1942  data: 0.0129  max mem: 14938
[14:37:34.986744] Test:  [20/57]  eta: 0:00:06  loss: 1.3979 (1.3960)  time: 0.1818  data: 0.0001  max mem: 14938
[14:37:36.810100] Test:  [30/57]  eta: 0:00:05  loss: 1.3926 (1.3927)  time: 0.1821  data: 0.0001  max mem: 14938
[14:37:38.636275] Test:  [40/57]  eta: 0:00:03  loss: 1.3871 (1.3906)  time: 0.1824  data: 0.0001  max mem: 14938
[14:37:40.467532] Test:  [50/57]  eta: 0:00:01  loss: 1.3871 (1.3899)  time: 0.1828  data: 0.0001  max mem: 14938
[14:37:41.456672] Test:  [56/57]  eta: 0:00:00  loss: 1.3899 (1.3898)  time: 0.1775  data: 0.0001  max mem: 14938
[14:37:41.514655] Test: Total time: 0:00:10 (0.1840 s / it)
[14:37:43.298508] Dice score of the network on the train images: 0.000000, val images: 0.000000
[14:37:43.302561] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[14:37:44.195730] Epoch: [2]  [  0/345]  eta: 0:05:07  lr: 0.000013  loss: 1.3903 (1.3903)  time: 0.8921  data: 0.1468  max mem: 14938
[14:37:59.081026] Epoch: [2]  [ 20/345]  eta: 0:04:04  lr: 0.000013  loss: 1.3872 (1.3882)  time: 0.7442  data: 0.0001  max mem: 14938
[14:38:14.023684] Epoch: [2]  [ 40/345]  eta: 0:03:48  lr: 0.000013  loss: 1.3834 (1.3868)  time: 0.7471  data: 0.0001  max mem: 14938
[14:38:28.988877] Epoch: [2]  [ 60/345]  eta: 0:03:33  lr: 0.000014  loss: 1.3799 (1.3847)  time: 0.7482  data: 0.0001  max mem: 14938
[14:38:43.980438] Epoch: [2]  [ 80/345]  eta: 0:03:18  lr: 0.000014  loss: 1.3745 (1.3833)  time: 0.7495  data: 0.0001  max mem: 14938
[14:38:58.991508] Epoch: [2]  [100/345]  eta: 0:03:03  lr: 0.000014  loss: 1.3708 (1.3813)  time: 0.7505  data: 0.0001  max mem: 14938
[14:39:14.033004] Epoch: [2]  [120/345]  eta: 0:02:48  lr: 0.000015  loss: 1.3643 (1.3789)  time: 0.7520  data: 0.0001  max mem: 14938
[14:39:29.074145] Epoch: [2]  [140/345]  eta: 0:02:33  lr: 0.000015  loss: 1.3576 (1.3760)  time: 0.7520  data: 0.0001  max mem: 14938
[14:39:44.102167] Epoch: [2]  [160/345]  eta: 0:02:18  lr: 0.000015  loss: 1.3560 (1.3741)  time: 0.7514  data: 0.0001  max mem: 14938
[14:39:59.247352] Epoch: [2]  [180/345]  eta: 0:02:03  lr: 0.000016  loss: 1.3545 (1.3721)  time: 0.7572  data: 0.0001  max mem: 14938
[14:40:14.262906] Epoch: [2]  [200/345]  eta: 0:01:48  lr: 0.000016  loss: 1.3490 (1.3700)  time: 0.7507  data: 0.0001  max mem: 14938
[14:40:29.276197] Epoch: [2]  [220/345]  eta: 0:01:33  lr: 0.000016  loss: 1.3484 (1.3680)  time: 0.7506  data: 0.0001  max mem: 14938
[14:40:44.286855] Epoch: [2]  [240/345]  eta: 0:01:18  lr: 0.000017  loss: 1.3415 (1.3660)  time: 0.7505  data: 0.0001  max mem: 14938
[14:40:59.287229] Epoch: [2]  [260/345]  eta: 0:01:03  lr: 0.000017  loss: 1.3398 (1.3645)  time: 0.7500  data: 0.0001  max mem: 14938
[14:41:14.271290] Epoch: [2]  [280/345]  eta: 0:00:48  lr: 0.000018  loss: 1.3390 (1.3629)  time: 0.7492  data: 0.0001  max mem: 14938
[14:41:29.238112] Epoch: [2]  [300/345]  eta: 0:00:33  lr: 0.000018  loss: 1.3290 (1.3610)  time: 0.7483  data: 0.0001  max mem: 14938
[14:41:44.221604] Epoch: [2]  [320/345]  eta: 0:00:18  lr: 0.000018  loss: 1.3324 (1.3595)  time: 0.7491  data: 0.0001  max mem: 14938
[14:41:59.207020] Epoch: [2]  [340/345]  eta: 0:00:03  lr: 0.000019  loss: 1.3339 (1.3582)  time: 0.7492  data: 0.0001  max mem: 14938
[14:42:02.202264] Epoch: [2]  [344/345]  eta: 0:00:00  lr: 0.000019  loss: 1.3339 (1.3580)  time: 0.7491  data: 0.0001  max mem: 14938
[14:42:02.262565] Epoch: [2] Total time: 0:04:18 (0.7506 s / it)
[14:42:02.263029] Averaged stats: lr: 0.000019  loss: 1.3339 (1.3580)
[14:42:02.601074] Test:  [  0/345]  eta: 0:01:55  loss: 1.3401 (1.3401)  time: 0.3336  data: 0.1522  max mem: 14938
[14:42:04.437476] Test:  [ 10/345]  eta: 0:01:06  loss: 1.3420 (1.3417)  time: 0.1972  data: 0.0139  max mem: 14938
[14:42:06.277485] Test:  [ 20/345]  eta: 0:01:02  loss: 1.3420 (1.3415)  time: 0.1838  data: 0.0001  max mem: 14938
[14:42:08.119949] Test:  [ 30/345]  eta: 0:00:59  loss: 1.3429 (1.3418)  time: 0.1841  data: 0.0001  max mem: 14938
[14:42:09.965451] Test:  [ 40/345]  eta: 0:00:57  loss: 1.3433 (1.3423)  time: 0.1843  data: 0.0001  max mem: 14938
[14:42:11.816263] Test:  [ 50/345]  eta: 0:00:55  loss: 1.3433 (1.3425)  time: 0.1848  data: 0.0001  max mem: 14938
[14:42:13.669068] Test:  [ 60/345]  eta: 0:00:53  loss: 1.3426 (1.3427)  time: 0.1851  data: 0.0001  max mem: 14938
[14:42:15.524582] Test:  [ 70/345]  eta: 0:00:51  loss: 1.3423 (1.3426)  time: 0.1854  data: 0.0001  max mem: 14938
[14:42:17.383246] Test:  [ 80/345]  eta: 0:00:49  loss: 1.3432 (1.3427)  time: 0.1857  data: 0.0001  max mem: 14938
[14:42:19.246295] Test:  [ 90/345]  eta: 0:00:47  loss: 1.3439 (1.3428)  time: 0.1860  data: 0.0001  max mem: 14938
[14:42:21.112260] Test:  [100/345]  eta: 0:00:45  loss: 1.3436 (1.3428)  time: 0.1864  data: 0.0001  max mem: 14938
[14:42:22.981822] Test:  [110/345]  eta: 0:00:43  loss: 1.3429 (1.3428)  time: 0.1867  data: 0.0001  max mem: 14938
[14:42:24.854880] Test:  [120/345]  eta: 0:00:41  loss: 1.3423 (1.3427)  time: 0.1871  data: 0.0001  max mem: 14938
[14:42:26.730593] Test:  [130/345]  eta: 0:00:40  loss: 1.3427 (1.3429)  time: 0.1874  data: 0.0001  max mem: 14938
[14:42:28.611393] Test:  [140/345]  eta: 0:00:38  loss: 1.3431 (1.3428)  time: 0.1878  data: 0.0001  max mem: 14938
[14:42:30.494645] Test:  [150/345]  eta: 0:00:36  loss: 1.3428 (1.3428)  time: 0.1881  data: 0.0001  max mem: 14938
[14:42:32.381439] Test:  [160/345]  eta: 0:00:34  loss: 1.3429 (1.3429)  time: 0.1884  data: 0.0001  max mem: 14938
[14:42:34.271603] Test:  [170/345]  eta: 0:00:32  loss: 1.3426 (1.3428)  time: 0.1888  data: 0.0001  max mem: 14938
[14:42:36.167372] Test:  [180/345]  eta: 0:00:30  loss: 1.3412 (1.3428)  time: 0.1892  data: 0.0001  max mem: 14938
[14:42:38.065178] Test:  [190/345]  eta: 0:00:29  loss: 1.3423 (1.3427)  time: 0.1896  data: 0.0001  max mem: 14938
[14:42:39.965538] Test:  [200/345]  eta: 0:00:27  loss: 1.3428 (1.3428)  time: 0.1899  data: 0.0001  max mem: 14938
[14:42:41.869711] Test:  [210/345]  eta: 0:00:25  loss: 1.3427 (1.3427)  time: 0.1902  data: 0.0001  max mem: 14938
[14:42:43.777766] Test:  [220/345]  eta: 0:00:23  loss: 1.3419 (1.3427)  time: 0.1906  data: 0.0001  max mem: 14938
[14:42:45.690698] Test:  [230/345]  eta: 0:00:21  loss: 1.3440 (1.3428)  time: 0.1910  data: 0.0001  max mem: 14938
[14:42:47.604740] Test:  [240/345]  eta: 0:00:19  loss: 1.3441 (1.3428)  time: 0.1913  data: 0.0001  max mem: 14938
[14:42:49.524748] Test:  [250/345]  eta: 0:00:17  loss: 1.3432 (1.3428)  time: 0.1916  data: 0.0001  max mem: 14938
[14:42:51.446128] Test:  [260/345]  eta: 0:00:16  loss: 1.3430 (1.3428)  time: 0.1920  data: 0.0001  max mem: 14938
[14:42:53.373764] Test:  [270/345]  eta: 0:00:14  loss: 1.3437 (1.3429)  time: 0.1924  data: 0.0001  max mem: 14938
[14:42:55.302863] Test:  [280/345]  eta: 0:00:12  loss: 1.3448 (1.3430)  time: 0.1928  data: 0.0001  max mem: 14938
[14:42:57.234578] Test:  [290/345]  eta: 0:00:10  loss: 1.3436 (1.3429)  time: 0.1930  data: 0.0001  max mem: 14938
[14:42:59.171199] Test:  [300/345]  eta: 0:00:08  loss: 1.3427 (1.3429)  time: 0.1934  data: 0.0001  max mem: 14938
[14:43:01.111109] Test:  [310/345]  eta: 0:00:06  loss: 1.3421 (1.3429)  time: 0.1938  data: 0.0001  max mem: 14938
[14:43:03.053651] Test:  [320/345]  eta: 0:00:04  loss: 1.3428 (1.3429)  time: 0.1941  data: 0.0001  max mem: 14938
[14:43:04.998800] Test:  [330/345]  eta: 0:00:02  loss: 1.3440 (1.3429)  time: 0.1943  data: 0.0001  max mem: 14938
[14:43:06.947073] Test:  [340/345]  eta: 0:00:00  loss: 1.3432 (1.3429)  time: 0.1946  data: 0.0001  max mem: 14938
[14:43:07.728747] Test:  [344/345]  eta: 0:00:00  loss: 1.3435 (1.3429)  time: 0.1948  data: 0.0001  max mem: 14938
[14:43:07.788569] Test: Total time: 0:01:05 (0.1899 s / it)
[14:43:18.134719] Test:  [ 0/57]  eta: 0:00:18  loss: 1.3514 (1.3514)  time: 0.3227  data: 0.1430  max mem: 14938
[14:43:19.950941] Test:  [10/57]  eta: 0:00:09  loss: 1.3484 (1.3475)  time: 0.1944  data: 0.0131  max mem: 14938
[14:43:21.771230] Test:  [20/57]  eta: 0:00:06  loss: 1.3484 (1.3467)  time: 0.1818  data: 0.0001  max mem: 14938
[14:43:23.595181] Test:  [30/57]  eta: 0:00:05  loss: 1.3411 (1.3425)  time: 0.1822  data: 0.0001  max mem: 14938
[14:43:25.424791] Test:  [40/57]  eta: 0:00:03  loss: 1.3346 (1.3398)  time: 0.1826  data: 0.0001  max mem: 14938
[14:43:27.256824] Test:  [50/57]  eta: 0:00:01  loss: 1.3346 (1.3389)  time: 0.1830  data: 0.0001  max mem: 14938
[14:43:28.244524] Test:  [56/57]  eta: 0:00:00  loss: 1.3385 (1.3387)  time: 0.1776  data: 0.0001  max mem: 14938
[14:43:28.302635] Test: Total time: 0:00:10 (0.1841 s / it)
[14:43:30.089651] Dice score of the network on the train images: 0.000000, val images: 0.000000
[14:43:30.093829] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[14:43:30.981184] Epoch: [3]  [  0/345]  eta: 0:05:05  lr: 0.000019  loss: 1.3233 (1.3233)  time: 0.8866  data: 0.1439  max mem: 14938
[14:43:45.855945] Epoch: [3]  [ 20/345]  eta: 0:04:03  lr: 0.000019  loss: 1.3269 (1.3270)  time: 0.7437  data: 0.0001  max mem: 14938
[14:44:00.793534] Epoch: [3]  [ 40/345]  eta: 0:03:48  lr: 0.000019  loss: 1.3227 (1.3284)  time: 0.7468  data: 0.0001  max mem: 14938
[14:44:15.754054] Epoch: [3]  [ 60/345]  eta: 0:03:33  lr: 0.000020  loss: 1.3264 (1.3277)  time: 0.7480  data: 0.0001  max mem: 14938
[14:44:30.737591] Epoch: [3]  [ 80/345]  eta: 0:03:18  lr: 0.000020  loss: 1.3173 (1.3259)  time: 0.7491  data: 0.0001  max mem: 14938
[14:44:45.749327] Epoch: [3]  [100/345]  eta: 0:03:03  lr: 0.000021  loss: 1.3134 (1.3249)  time: 0.7505  data: 0.0001  max mem: 14938
[14:45:00.780970] Epoch: [3]  [120/345]  eta: 0:02:48  lr: 0.000021  loss: 1.3151 (1.3246)  time: 0.7515  data: 0.0001  max mem: 14938
[14:45:15.934101] Epoch: [3]  [140/345]  eta: 0:02:33  lr: 0.000021  loss: 1.3108 (1.3230)  time: 0.7576  data: 0.0001  max mem: 14938
[14:45:30.959654] Epoch: [3]  [160/345]  eta: 0:02:18  lr: 0.000022  loss: 1.3078 (1.3214)  time: 0.7512  data: 0.0001  max mem: 14938
[14:45:45.970190] Epoch: [3]  [180/345]  eta: 0:02:03  lr: 0.000022  loss: 1.3089 (1.3203)  time: 0.7505  data: 0.0001  max mem: 14938
[14:46:00.987774] Epoch: [3]  [200/345]  eta: 0:01:48  lr: 0.000022  loss: 1.3048 (1.3189)  time: 0.7508  data: 0.0001  max mem: 14938
[14:46:15.995444] Epoch: [3]  [220/345]  eta: 0:01:33  lr: 0.000023  loss: 1.3027 (1.3180)  time: 0.7503  data: 0.0001  max mem: 14938
[14:46:30.979001] Epoch: [3]  [240/345]  eta: 0:01:18  lr: 0.000023  loss: 1.2995 (1.3170)  time: 0.7491  data: 0.0001  max mem: 14938
[14:46:45.958092] Epoch: [3]  [260/345]  eta: 0:01:03  lr: 0.000023  loss: 1.2995 (1.3157)  time: 0.7489  data: 0.0001  max mem: 14938
[14:47:00.928425] Epoch: [3]  [280/345]  eta: 0:00:48  lr: 0.000024  loss: 1.2979 (1.3145)  time: 0.7485  data: 0.0001  max mem: 14938
[14:47:15.898058] Epoch: [3]  [300/345]  eta: 0:00:33  lr: 0.000024  loss: 1.2990 (1.3137)  time: 0.7484  data: 0.0001  max mem: 14938
[14:47:30.873830] Epoch: [3]  [320/345]  eta: 0:00:18  lr: 0.000025  loss: 1.2966 (1.3129)  time: 0.7487  data: 0.0001  max mem: 14938
[14:47:45.868520] Epoch: [3]  [340/345]  eta: 0:00:03  lr: 0.000025  loss: 1.2925 (1.3118)  time: 0.7497  data: 0.0001  max mem: 14938
[14:47:48.866533] Epoch: [3]  [344/345]  eta: 0:00:00  lr: 0.000025  loss: 1.2922 (1.3117)  time: 0.7495  data: 0.0001  max mem: 14938
[14:47:48.930447] Epoch: [3] Total time: 0:04:18 (0.7503 s / it)
[14:47:48.930722] Averaged stats: lr: 0.000025  loss: 1.2922 (1.3117)
[14:47:49.264726] Test:  [  0/345]  eta: 0:01:53  loss: 1.3001 (1.3001)  time: 0.3283  data: 0.1468  max mem: 14938
[14:47:51.101368] Test:  [ 10/345]  eta: 0:01:05  loss: 1.2927 (1.2933)  time: 0.1967  data: 0.0134  max mem: 14938
[14:47:52.940610] Test:  [ 20/345]  eta: 0:01:01  loss: 1.2916 (1.2923)  time: 0.1837  data: 0.0001  max mem: 14938
[14:47:54.783091] Test:  [ 30/345]  eta: 0:00:59  loss: 1.2909 (1.2924)  time: 0.1840  data: 0.0001  max mem: 14938
[14:47:56.629403] Test:  [ 40/345]  eta: 0:00:57  loss: 1.2922 (1.2922)  time: 0.1844  data: 0.0001  max mem: 14938
[14:47:58.480568] Test:  [ 50/345]  eta: 0:00:55  loss: 1.2924 (1.2922)  time: 0.1848  data: 0.0001  max mem: 14938
[14:48:00.333558] Test:  [ 60/345]  eta: 0:00:53  loss: 1.2911 (1.2920)  time: 0.1852  data: 0.0001  max mem: 14938
[14:48:02.188226] Test:  [ 70/345]  eta: 0:00:51  loss: 1.2923 (1.2921)  time: 0.1853  data: 0.0001  max mem: 14938
[14:48:04.047016] Test:  [ 80/345]  eta: 0:00:49  loss: 1.2923 (1.2920)  time: 0.1856  data: 0.0001  max mem: 14938
[14:48:05.909373] Test:  [ 90/345]  eta: 0:00:47  loss: 1.2897 (1.2917)  time: 0.1860  data: 0.0001  max mem: 14938
[14:48:07.776741] Test:  [100/345]  eta: 0:00:45  loss: 1.2895 (1.2916)  time: 0.1864  data: 0.0001  max mem: 14938
[14:48:09.645467] Test:  [110/345]  eta: 0:00:43  loss: 1.2896 (1.2916)  time: 0.1868  data: 0.0001  max mem: 14938
[14:48:11.519752] Test:  [120/345]  eta: 0:00:41  loss: 1.2919 (1.2916)  time: 0.1871  data: 0.0001  max mem: 14938
[14:48:13.396941] Test:  [130/345]  eta: 0:00:40  loss: 1.2904 (1.2914)  time: 0.1875  data: 0.0001  max mem: 14938
[14:48:15.278180] Test:  [140/345]  eta: 0:00:38  loss: 1.2904 (1.2914)  time: 0.1879  data: 0.0001  max mem: 14938
[14:48:17.162793] Test:  [150/345]  eta: 0:00:36  loss: 1.2922 (1.2914)  time: 0.1882  data: 0.0001  max mem: 14938
[14:48:19.050147] Test:  [160/345]  eta: 0:00:34  loss: 1.2918 (1.2915)  time: 0.1885  data: 0.0001  max mem: 14938
[14:48:20.941269] Test:  [170/345]  eta: 0:00:32  loss: 1.2909 (1.2914)  time: 0.1889  data: 0.0001  max mem: 14938
[14:48:22.839098] Test:  [180/345]  eta: 0:00:30  loss: 1.2902 (1.2913)  time: 0.1894  data: 0.0001  max mem: 14938
[14:48:24.739001] Test:  [190/345]  eta: 0:00:29  loss: 1.2911 (1.2913)  time: 0.1898  data: 0.0001  max mem: 14938
[14:48:26.640719] Test:  [200/345]  eta: 0:00:27  loss: 1.2925 (1.2915)  time: 0.1900  data: 0.0001  max mem: 14938
[14:48:28.545653] Test:  [210/345]  eta: 0:00:25  loss: 1.2925 (1.2914)  time: 0.1903  data: 0.0001  max mem: 14938
[14:48:30.454704] Test:  [220/345]  eta: 0:00:23  loss: 1.2900 (1.2914)  time: 0.1906  data: 0.0001  max mem: 14938
[14:48:32.366535] Test:  [230/345]  eta: 0:00:21  loss: 1.2900 (1.2913)  time: 0.1910  data: 0.0001  max mem: 14938
[14:48:34.282505] Test:  [240/345]  eta: 0:00:19  loss: 1.2873 (1.2912)  time: 0.1913  data: 0.0001  max mem: 14938
[14:48:36.202984] Test:  [250/345]  eta: 0:00:17  loss: 1.2919 (1.2913)  time: 0.1918  data: 0.0001  max mem: 14938
[14:48:38.125197] Test:  [260/345]  eta: 0:00:16  loss: 1.2922 (1.2913)  time: 0.1921  data: 0.0001  max mem: 14938
[14:48:40.053317] Test:  [270/345]  eta: 0:00:14  loss: 1.2915 (1.2913)  time: 0.1925  data: 0.0001  max mem: 14938
[14:48:41.983789] Test:  [280/345]  eta: 0:00:12  loss: 1.2904 (1.2913)  time: 0.1929  data: 0.0001  max mem: 14938
[14:48:43.917120] Test:  [290/345]  eta: 0:00:10  loss: 1.2920 (1.2913)  time: 0.1931  data: 0.0001  max mem: 14938
[14:48:45.854262] Test:  [300/345]  eta: 0:00:08  loss: 1.2933 (1.2913)  time: 0.1935  data: 0.0001  max mem: 14938
[14:48:47.794267] Test:  [310/345]  eta: 0:00:06  loss: 1.2933 (1.2913)  time: 0.1938  data: 0.0001  max mem: 14938
[14:48:49.737832] Test:  [320/345]  eta: 0:00:04  loss: 1.2901 (1.2913)  time: 0.1941  data: 0.0001  max mem: 14938
[14:48:51.687101] Test:  [330/345]  eta: 0:00:02  loss: 1.2898 (1.2913)  time: 0.1946  data: 0.0001  max mem: 14938
[14:48:53.636961] Test:  [340/345]  eta: 0:00:00  loss: 1.2910 (1.2913)  time: 0.1949  data: 0.0001  max mem: 14938
[14:48:54.418547] Test:  [344/345]  eta: 0:00:00  loss: 1.2898 (1.2913)  time: 0.1950  data: 0.0001  max mem: 14938
[14:48:54.474834] Test: Total time: 0:01:05 (0.1900 s / it)
[14:49:04.798095] Test:  [ 0/57]  eta: 0:00:18  loss: 1.3050 (1.3050)  time: 0.3195  data: 0.1402  max mem: 14938
[14:49:06.614556] Test:  [10/57]  eta: 0:00:09  loss: 1.2995 (1.2985)  time: 0.1941  data: 0.0128  max mem: 14938
[14:49:08.435497] Test:  [20/57]  eta: 0:00:06  loss: 1.2995 (1.2979)  time: 0.1818  data: 0.0001  max mem: 14938
[14:49:10.260544] Test:  [30/57]  eta: 0:00:05  loss: 1.2839 (1.2901)  time: 0.1822  data: 0.0001  max mem: 14938
[14:49:12.089955] Test:  [40/57]  eta: 0:00:03  loss: 1.2726 (1.2851)  time: 0.1827  data: 0.0001  max mem: 14938
[14:49:13.922436] Test:  [50/57]  eta: 0:00:01  loss: 1.2726 (1.2832)  time: 0.1830  data: 0.0001  max mem: 14938
[14:49:14.911461] Test:  [56/57]  eta: 0:00:00  loss: 1.2801 (1.2830)  time: 0.1777  data: 0.0001  max mem: 14938
[14:49:14.964857] Test: Total time: 0:00:10 (0.1840 s / it)
[14:49:16.784970] Dice score of the network on the train images: 0.000000, val images: 0.000000
[14:49:16.789757] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[14:49:17.677072] Epoch: [4]  [  0/345]  eta: 0:05:05  lr: 0.000025  loss: 1.2984 (1.2984)  time: 0.8863  data: 0.1435  max mem: 14938
[14:49:32.559271] Epoch: [4]  [ 20/345]  eta: 0:04:04  lr: 0.000025  loss: 1.2863 (1.2910)  time: 0.7441  data: 0.0001  max mem: 14938
[14:49:47.493076] Epoch: [4]  [ 40/345]  eta: 0:03:48  lr: 0.000026  loss: 1.2881 (1.2895)  time: 0.7467  data: 0.0001  max mem: 14938
[14:50:02.462590] Epoch: [4]  [ 60/345]  eta: 0:03:33  lr: 0.000026  loss: 1.2772 (1.2875)  time: 0.7484  data: 0.0001  max mem: 14938
[14:50:17.447798] Epoch: [4]  [ 80/345]  eta: 0:03:18  lr: 0.000026  loss: 1.2740 (1.2849)  time: 0.7492  data: 0.0001  max mem: 14938
[14:50:32.464860] Epoch: [4]  [100/345]  eta: 0:03:03  lr: 0.000027  loss: 1.2696 (1.2826)  time: 0.7508  data: 0.0001  max mem: 14938
[14:50:47.503452] Epoch: [4]  [120/345]  eta: 0:02:48  lr: 0.000027  loss: 1.2637 (1.2796)  time: 0.7519  data: 0.0000  max mem: 14938
[14:51:02.660591] Epoch: [4]  [140/345]  eta: 0:02:33  lr: 0.000028  loss: 1.2583 (1.2768)  time: 0.7578  data: 0.0001  max mem: 14938
[14:51:17.692046] Epoch: [4]  [160/345]  eta: 0:02:18  lr: 0.000028  loss: 1.2646 (1.2751)  time: 0.7515  data: 0.0001  max mem: 14938
[14:51:32.718534] Epoch: [4]  [180/345]  eta: 0:02:03  lr: 0.000028  loss: 1.2414 (1.2719)  time: 0.7513  data: 0.0001  max mem: 14938
[14:51:47.738455] Epoch: [4]  [200/345]  eta: 0:01:48  lr: 0.000029  loss: 1.2342 (1.2682)  time: 0.7510  data: 0.0001  max mem: 14938
[14:52:02.761709] Epoch: [4]  [220/345]  eta: 0:01:33  lr: 0.000029  loss: 1.2287 (1.2650)  time: 0.7511  data: 0.0001  max mem: 14938
[14:52:17.775026] Epoch: [4]  [240/345]  eta: 0:01:18  lr: 0.000029  loss: 1.2205 (1.2614)  time: 0.7506  data: 0.0001  max mem: 14938
[14:52:32.775791] Epoch: [4]  [260/345]  eta: 0:01:03  lr: 0.000030  loss: 1.2115 (1.2577)  time: 0.7500  data: 0.0001  max mem: 14938
[14:52:47.768254] Epoch: [4]  [280/345]  eta: 0:00:48  lr: 0.000030  loss: 1.2046 (1.2539)  time: 0.7496  data: 0.0001  max mem: 14938
[14:53:02.762487] Epoch: [4]  [300/345]  eta: 0:00:33  lr: 0.000030  loss: 1.1858 (1.2498)  time: 0.7497  data: 0.0001  max mem: 14938
[14:53:17.749599] Epoch: [4]  [320/345]  eta: 0:00:18  lr: 0.000031  loss: 1.1772 (1.2456)  time: 0.7493  data: 0.0001  max mem: 14938
[14:53:32.736927] Epoch: [4]  [340/345]  eta: 0:00:03  lr: 0.000031  loss: 1.1820 (1.2419)  time: 0.7493  data: 0.0001  max mem: 14938
[14:53:35.732154] Epoch: [4]  [344/345]  eta: 0:00:00  lr: 0.000031  loss: 1.1820 (1.2414)  time: 0.7493  data: 0.0001  max mem: 14938
[14:53:35.775639] Epoch: [4] Total time: 0:04:18 (0.7507 s / it)
[14:53:35.776098] Averaged stats: lr: 0.000031  loss: 1.1820 (1.2414)
[14:53:36.106704] Test:  [  0/345]  eta: 0:01:52  loss: 1.1804 (1.1804)  time: 0.3264  data: 0.1447  max mem: 14938
[14:53:37.942474] Test:  [ 10/345]  eta: 0:01:05  loss: 1.1649 (1.1604)  time: 0.1965  data: 0.0132  max mem: 14938
[14:53:39.781397] Test:  [ 20/345]  eta: 0:01:01  loss: 1.1546 (1.1590)  time: 0.1837  data: 0.0001  max mem: 14938
[14:53:41.623674] Test:  [ 30/345]  eta: 0:00:59  loss: 1.1520 (1.1530)  time: 0.1840  data: 0.0001  max mem: 14938
[14:53:43.466976] Test:  [ 40/345]  eta: 0:00:57  loss: 1.1520 (1.1552)  time: 0.1842  data: 0.0001  max mem: 14938
[14:53:45.317576] Test:  [ 50/345]  eta: 0:00:55  loss: 1.1535 (1.1532)  time: 0.1846  data: 0.0001  max mem: 14938
[14:53:47.169728] Test:  [ 60/345]  eta: 0:00:53  loss: 1.1498 (1.1535)  time: 0.1851  data: 0.0001  max mem: 14938
[14:53:49.025203] Test:  [ 70/345]  eta: 0:00:51  loss: 1.1495 (1.1528)  time: 0.1853  data: 0.0001  max mem: 14938
[14:53:50.884318] Test:  [ 80/345]  eta: 0:00:49  loss: 1.1449 (1.1520)  time: 0.1857  data: 0.0001  max mem: 14938
[14:53:52.746806] Test:  [ 90/345]  eta: 0:00:47  loss: 1.1493 (1.1522)  time: 0.1860  data: 0.0001  max mem: 14938
[14:53:54.612287] Test:  [100/345]  eta: 0:00:45  loss: 1.1513 (1.1516)  time: 0.1863  data: 0.0001  max mem: 14938
[14:53:56.481756] Test:  [110/345]  eta: 0:00:43  loss: 1.1513 (1.1522)  time: 0.1867  data: 0.0001  max mem: 14938
[14:53:58.353107] Test:  [120/345]  eta: 0:00:41  loss: 1.1474 (1.1516)  time: 0.1870  data: 0.0001  max mem: 14938
[14:54:00.228101] Test:  [130/345]  eta: 0:00:40  loss: 1.1429 (1.1503)  time: 0.1873  data: 0.0001  max mem: 14938
[14:54:02.108538] Test:  [140/345]  eta: 0:00:38  loss: 1.1487 (1.1507)  time: 0.1877  data: 0.0001  max mem: 14938
[14:54:03.991764] Test:  [150/345]  eta: 0:00:36  loss: 1.1546 (1.1511)  time: 0.1881  data: 0.0001  max mem: 14938
[14:54:05.877499] Test:  [160/345]  eta: 0:00:34  loss: 1.1488 (1.1498)  time: 0.1884  data: 0.0001  max mem: 14938
[14:54:07.767856] Test:  [170/345]  eta: 0:00:32  loss: 1.1450 (1.1503)  time: 0.1888  data: 0.0001  max mem: 14938
[14:54:09.662436] Test:  [180/345]  eta: 0:00:30  loss: 1.1455 (1.1499)  time: 0.1892  data: 0.0001  max mem: 14938
[14:54:11.561240] Test:  [190/345]  eta: 0:00:29  loss: 1.1441 (1.1496)  time: 0.1896  data: 0.0001  max mem: 14938
[14:54:13.461253] Test:  [200/345]  eta: 0:00:27  loss: 1.1532 (1.1500)  time: 0.1899  data: 0.0001  max mem: 14938
[14:54:15.365016] Test:  [210/345]  eta: 0:00:25  loss: 1.1616 (1.1507)  time: 0.1901  data: 0.0001  max mem: 14938
[14:54:17.272742] Test:  [220/345]  eta: 0:00:23  loss: 1.1639 (1.1512)  time: 0.1905  data: 0.0001  max mem: 14938
[14:54:19.184686] Test:  [230/345]  eta: 0:00:21  loss: 1.1521 (1.1512)  time: 0.1909  data: 0.0001  max mem: 14938
[14:54:21.099061] Test:  [240/345]  eta: 0:00:19  loss: 1.1590 (1.1517)  time: 0.1913  data: 0.0001  max mem: 14938
[14:54:23.017839] Test:  [250/345]  eta: 0:00:17  loss: 1.1616 (1.1518)  time: 0.1916  data: 0.0001  max mem: 14938
[14:54:24.938687] Test:  [260/345]  eta: 0:00:16  loss: 1.1578 (1.1519)  time: 0.1919  data: 0.0001  max mem: 14938
[14:54:26.865201] Test:  [270/345]  eta: 0:00:14  loss: 1.1542 (1.1520)  time: 0.1923  data: 0.0001  max mem: 14938
[14:54:28.794662] Test:  [280/345]  eta: 0:00:12  loss: 1.1560 (1.1526)  time: 0.1927  data: 0.0001  max mem: 14938
[14:54:30.725802] Test:  [290/345]  eta: 0:00:10  loss: 1.1688 (1.1531)  time: 0.1930  data: 0.0001  max mem: 14938
[14:54:32.664550] Test:  [300/345]  eta: 0:00:08  loss: 1.1533 (1.1531)  time: 0.1934  data: 0.0001  max mem: 14938
[14:54:34.604897] Test:  [310/345]  eta: 0:00:06  loss: 1.1509 (1.1533)  time: 0.1939  data: 0.0001  max mem: 14938
[14:54:36.547423] Test:  [320/345]  eta: 0:00:04  loss: 1.1509 (1.1531)  time: 0.1941  data: 0.0001  max mem: 14938
[14:54:38.492904] Test:  [330/345]  eta: 0:00:02  loss: 1.1545 (1.1532)  time: 0.1943  data: 0.0001  max mem: 14938
[14:54:40.444845] Test:  [340/345]  eta: 0:00:00  loss: 1.1588 (1.1532)  time: 0.1948  data: 0.0001  max mem: 14938
[14:54:41.225503] Test:  [344/345]  eta: 0:00:00  loss: 1.1545 (1.1530)  time: 0.1950  data: 0.0001  max mem: 14938
[14:54:41.284657] Test: Total time: 0:01:05 (0.1899 s / it)
[14:54:51.708284] Test:  [ 0/57]  eta: 0:00:18  loss: 1.2294 (1.2294)  time: 0.3245  data: 0.1454  max mem: 14938
[14:54:53.523059] Test:  [10/57]  eta: 0:00:09  loss: 1.2042 (1.1933)  time: 0.1944  data: 0.0133  max mem: 14938
[14:54:55.342303] Test:  [20/57]  eta: 0:00:06  loss: 1.2146 (1.1948)  time: 0.1816  data: 0.0001  max mem: 14938
[14:54:57.165514] Test:  [30/57]  eta: 0:00:05  loss: 1.1269 (1.1477)  time: 0.1821  data: 0.0001  max mem: 14938
[14:54:58.992176] Test:  [40/57]  eta: 0:00:03  loss: 1.0372 (1.1180)  time: 0.1824  data: 0.0001  max mem: 14938
[14:55:00.824300] Test:  [50/57]  eta: 0:00:01  loss: 1.0397 (1.1096)  time: 0.1829  data: 0.0001  max mem: 14938
[14:55:01.812381] Test:  [56/57]  eta: 0:00:00  loss: 1.0968 (1.1135)  time: 0.1775  data: 0.0001  max mem: 14938
[14:55:01.868591] Test: Total time: 0:00:10 (0.1840 s / it)
[14:55:03.610339] Dice score of the network on the train images: 0.523091, val images: 0.622443
[14:55:03.610570] saving best_prec_model_0 @ epoch 4
[14:55:04.688011] saving best_rec_model_0 @ epoch 4
[14:55:05.742513] saving best_dice_model_0 @ epoch 4
[14:55:06.874426] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[14:55:07.756814] Epoch: [5]  [  0/345]  eta: 0:05:04  lr: 0.000031  loss: 1.2067 (1.2067)  time: 0.8812  data: 0.1398  max mem: 14938

[14:55:22.597105] Epoch: [5]  [ 20/345]  eta: 0:04:03  lr: 0.000032  loss: 1.1558 (1.1633)  time: 0.7420  data: 0.0001  max mem: 14938
[14:55:37.495487] Epoch: [5]  [ 40/345]  eta: 0:03:47  lr: 0.000032  loss: 1.1517 (1.1596)  time: 0.7449  data: 0.0001  max mem: 14938
[14:55:52.451849] Epoch: [5]  [ 60/345]  eta: 0:03:32  lr: 0.000032  loss: 1.1607 (1.1604)  time: 0.7478  data: 0.0001  max mem: 14938
[14:56:07.452381] Epoch: [5]  [ 80/345]  eta: 0:03:18  lr: 0.000033  loss: 1.1415 (1.1572)  time: 0.7500  data: 0.0001  max mem: 14938
[14:56:22.493439] Epoch: [5]  [100/345]  eta: 0:03:03  lr: 0.000033  loss: 1.1393 (1.1538)  time: 0.7520  data: 0.0001  max mem: 14938
[14:56:37.545974] Epoch: [5]  [120/345]  eta: 0:02:48  lr: 0.000033  loss: 1.1144 (1.1487)  time: 0.7526  data: 0.0001  max mem: 14938
[14:56:52.584980] Epoch: [5]  [140/345]  eta: 0:02:33  lr: 0.000034  loss: 1.1219 (1.1450)  time: 0.7519  data: 0.0001  max mem: 14938
[14:57:07.613643] Epoch: [5]  [160/345]  eta: 0:02:18  lr: 0.000034  loss: 1.1243 (1.1421)  time: 0.7514  data: 0.0001  max mem: 14938
[14:57:22.625331] Epoch: [5]  [180/345]  eta: 0:02:03  lr: 0.000035  loss: 1.1246 (1.1398)  time: 0.7505  data: 0.0001  max mem: 14938
[14:57:37.646261] Epoch: [5]  [200/345]  eta: 0:01:48  lr: 0.000035  loss: 1.0984 (1.1364)  time: 0.7510  data: 0.0001  max mem: 14938
[14:57:52.654477] Epoch: [5]  [220/345]  eta: 0:01:33  lr: 0.000035  loss: 1.0900 (1.1322)  time: 0.7504  data: 0.0001  max mem: 14938
[14:58:07.660219] Epoch: [5]  [240/345]  eta: 0:01:18  lr: 0.000036  loss: 1.0832 (1.1285)  time: 0.7502  data: 0.0001  max mem: 14938
[14:58:22.629613] Epoch: [5]  [260/345]  eta: 0:01:03  lr: 0.000036  loss: 1.0778 (1.1251)  time: 0.7484  data: 0.0001  max mem: 14938
[14:58:37.599147] Epoch: [5]  [280/345]  eta: 0:00:48  lr: 0.000036  loss: 1.0883 (1.1230)  time: 0.7484  data: 0.0001  max mem: 14938
[14:58:52.576613] Epoch: [5]  [300/345]  eta: 0:00:33  lr: 0.000037  loss: 1.0670 (1.1191)  time: 0.7488  data: 0.0001  max mem: 14938
[14:59:07.580791] Epoch: [5]  [320/345]  eta: 0:00:18  lr: 0.000037  loss: 1.0664 (1.1165)  time: 0.7502  data: 0.0001  max mem: 14938
[14:59:22.578672] Epoch: [5]  [340/345]  eta: 0:00:03  lr: 0.000037  loss: 1.0663 (1.1136)  time: 0.7498  data: 0.0001  max mem: 14938
[14:59:25.577258] Epoch: [5]  [344/345]  eta: 0:00:00  lr: 0.000037  loss: 1.0669 (1.1129)  time: 0.7498  data: 0.0001  max mem: 14938
[14:59:25.640197] Epoch: [5] Total time: 0:04:18 (0.7500 s / it)
[14:59:25.640681] Averaged stats: lr: 0.000037  loss: 1.0669 (1.1129)
[14:59:25.978665] Test:  [  0/345]  eta: 0:01:54  loss: 0.9774 (0.9774)  time: 0.3331  data: 0.1519  max mem: 14938
[14:59:27.813201] Test:  [ 10/345]  eta: 0:01:05  loss: 1.0184 (1.0140)  time: 0.1970  data: 0.0139  max mem: 14938
[14:59:29.650931] Test:  [ 20/345]  eta: 0:01:01  loss: 1.0189 (1.0180)  time: 0.1835  data: 0.0001  max mem: 14938
[14:59:31.492458] Test:  [ 30/345]  eta: 0:00:59  loss: 1.0231 (1.0225)  time: 0.1839  data: 0.0001  max mem: 14938
[14:59:33.337033] Test:  [ 40/345]  eta: 0:00:57  loss: 1.0194 (1.0212)  time: 0.1843  data: 0.0001  max mem: 14938
[14:59:35.185527] Test:  [ 50/345]  eta: 0:00:55  loss: 1.0165 (1.0218)  time: 0.1846  data: 0.0001  max mem: 14938
[14:59:37.037219] Test:  [ 60/345]  eta: 0:00:53  loss: 1.0106 (1.0202)  time: 0.1850  data: 0.0001  max mem: 14938
[14:59:38.891792] Test:  [ 70/345]  eta: 0:00:51  loss: 1.0157 (1.0215)  time: 0.1853  data: 0.0001  max mem: 14938
[14:59:40.751626] Test:  [ 80/345]  eta: 0:00:49  loss: 1.0215 (1.0216)  time: 0.1857  data: 0.0001  max mem: 14938
[14:59:42.615107] Test:  [ 90/345]  eta: 0:00:47  loss: 1.0128 (1.0207)  time: 0.1861  data: 0.0001  max mem: 14938
[14:59:44.479936] Test:  [100/345]  eta: 0:00:45  loss: 1.0128 (1.0200)  time: 0.1864  data: 0.0001  max mem: 14938
[14:59:46.348246] Test:  [110/345]  eta: 0:00:43  loss: 1.0108 (1.0196)  time: 0.1866  data: 0.0001  max mem: 14938
[14:59:48.220616] Test:  [120/345]  eta: 0:00:41  loss: 1.0173 (1.0194)  time: 0.1870  data: 0.0001  max mem: 14938
[14:59:50.096626] Test:  [130/345]  eta: 0:00:40  loss: 1.0236 (1.0203)  time: 0.1874  data: 0.0001  max mem: 14938
[14:59:51.976041] Test:  [140/345]  eta: 0:00:38  loss: 1.0301 (1.0210)  time: 0.1877  data: 0.0001  max mem: 14938
[14:59:53.859511] Test:  [150/345]  eta: 0:00:36  loss: 1.0301 (1.0213)  time: 0.1881  data: 0.0001  max mem: 14938
[14:59:55.745684] Test:  [160/345]  eta: 0:00:34  loss: 1.0276 (1.0209)  time: 0.1884  data: 0.0001  max mem: 14938
[14:59:57.635613] Test:  [170/345]  eta: 0:00:32  loss: 1.0161 (1.0208)  time: 0.1888  data: 0.0001  max mem: 14938
[14:59:59.529944] Test:  [180/345]  eta: 0:00:30  loss: 1.0188 (1.0209)  time: 0.1892  data: 0.0001  max mem: 14938
[15:00:01.429506] Test:  [190/345]  eta: 0:00:29  loss: 1.0179 (1.0205)  time: 0.1896  data: 0.0001  max mem: 14938
[15:00:03.330305] Test:  [200/345]  eta: 0:00:27  loss: 1.0273 (1.0212)  time: 0.1900  data: 0.0001  max mem: 14938
[15:00:05.233048] Test:  [210/345]  eta: 0:00:25  loss: 1.0298 (1.0213)  time: 0.1901  data: 0.0001  max mem: 14938
[15:00:07.141533] Test:  [220/345]  eta: 0:00:23  loss: 1.0057 (1.0203)  time: 0.1905  data: 0.0001  max mem: 14938
[15:00:09.053190] Test:  [230/345]  eta: 0:00:21  loss: 1.0060 (1.0199)  time: 0.1910  data: 0.0001  max mem: 14938
[15:00:10.967887] Test:  [240/345]  eta: 0:00:19  loss: 1.0194 (1.0205)  time: 0.1913  data: 0.0001  max mem: 14938
[15:00:12.886917] Test:  [250/345]  eta: 0:00:17  loss: 1.0292 (1.0205)  time: 0.1916  data: 0.0001  max mem: 14938
[15:00:14.808312] Test:  [260/345]  eta: 0:00:16  loss: 1.0285 (1.0210)  time: 0.1920  data: 0.0001  max mem: 14938
[15:00:16.735679] Test:  [270/345]  eta: 0:00:14  loss: 1.0300 (1.0211)  time: 0.1924  data: 0.0001  max mem: 14938
[15:00:18.663598] Test:  [280/345]  eta: 0:00:12  loss: 1.0300 (1.0215)  time: 0.1927  data: 0.0001  max mem: 14938
[15:00:20.593931] Test:  [290/345]  eta: 0:00:10  loss: 1.0141 (1.0217)  time: 0.1929  data: 0.0001  max mem: 14938
[15:00:22.528204] Test:  [300/345]  eta: 0:00:08  loss: 1.0270 (1.0223)  time: 0.1932  data: 0.0001  max mem: 14938
[15:00:24.467207] Test:  [310/345]  eta: 0:00:06  loss: 1.0234 (1.0221)  time: 0.1936  data: 0.0001  max mem: 14938
[15:00:26.408540] Test:  [320/345]  eta: 0:00:04  loss: 1.0144 (1.0221)  time: 0.1940  data: 0.0001  max mem: 14938
[15:00:28.352770] Test:  [330/345]  eta: 0:00:02  loss: 1.0156 (1.0220)  time: 0.1942  data: 0.0001  max mem: 14938
[15:00:30.301429] Test:  [340/345]  eta: 0:00:00  loss: 1.0161 (1.0216)  time: 0.1946  data: 0.0001  max mem: 14938
[15:00:31.082007] Test:  [344/345]  eta: 0:00:00  loss: 1.0161 (1.0218)  time: 0.1948  data: 0.0001  max mem: 14938
[15:00:31.138020] Test: Total time: 0:01:05 (0.1898 s / it)
[15:00:41.492170] Test:  [ 0/57]  eta: 0:00:18  loss: 1.1080 (1.1080)  time: 0.3229  data: 0.1436  max mem: 14938
[15:00:43.305989] Test:  [10/57]  eta: 0:00:09  loss: 1.0815 (1.0740)  time: 0.1942  data: 0.0131  max mem: 14938
[15:00:45.125796] Test:  [20/57]  eta: 0:00:06  loss: 1.1069 (1.0806)  time: 0.1816  data: 0.0001  max mem: 14938
[15:00:46.949175] Test:  [30/57]  eta: 0:00:05  loss: 0.9634 (1.0241)  time: 0.1821  data: 0.0001  max mem: 14938
[15:00:48.778641] Test:  [40/57]  eta: 0:00:03  loss: 0.8905 (0.9900)  time: 0.1826  data: 0.0001  max mem: 14938
[15:00:50.611185] Test:  [50/57]  eta: 0:00:01  loss: 0.8905 (0.9803)  time: 0.1831  data: 0.0001  max mem: 14938
[15:00:51.599364] Test:  [56/57]  eta: 0:00:00  loss: 0.9683 (0.9854)  time: 0.1776  data: 0.0001  max mem: 14938
[15:00:51.656763] Test: Total time: 0:00:10 (0.1840 s / it)
[15:00:53.408295] Dice score of the network on the train images: 0.670774, val images: 0.729874
[15:00:53.408528] saving best_prec_model_0 @ epoch 5
[15:00:54.501690] saving best_dice_model_0 @ epoch 5
[15:00:55.512681] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[15:00:56.400937] Epoch: [6]  [  0/345]  eta: 0:05:06  lr: 0.000038  loss: 1.0378 (1.0378)  time: 0.8873  data: 0.1439  max mem: 14938
[15:01:11.276982] Epoch: [6]  [ 20/345]  eta: 0:04:03  lr: 0.000038  loss: 1.0474 (1.0480)  time: 0.7438  data: 0.0001  max mem: 14938
[15:01:26.193295] Epoch: [6]  [ 40/345]  eta: 0:03:48  lr: 0.000038  loss: 1.0442 (1.0500)  time: 0.7458  data: 0.0001  max mem: 14938
[15:01:41.157920] Epoch: [6]  [ 60/345]  eta: 0:03:33  lr: 0.000039  loss: 1.0403 (1.0452)  time: 0.7482  data: 0.0001  max mem: 14938
[15:01:56.152867] Epoch: [6]  [ 80/345]  eta: 0:03:18  lr: 0.000039  loss: 1.0377 (1.0423)  time: 0.7497  data: 0.0001  max mem: 14938
[15:02:11.185223] Epoch: [6]  [100/345]  eta: 0:03:03  lr: 0.000039  loss: 1.0324 (1.0420)  time: 0.7516  data: 0.0001  max mem: 14938
[15:02:26.229434] Epoch: [6]  [120/345]  eta: 0:02:48  lr: 0.000040  loss: 1.0203 (1.0391)  time: 0.7522  data: 0.0001  max mem: 14938
[15:02:41.291506] Epoch: [6]  [140/345]  eta: 0:02:33  lr: 0.000040  loss: 1.0413 (1.0393)  time: 0.7531  data: 0.0001  max mem: 14938
[15:02:56.329842] Epoch: [6]  [160/345]  eta: 0:02:18  lr: 0.000040  loss: 1.0087 (1.0354)  time: 0.7519  data: 0.0001  max mem: 14938
[15:03:11.379839] Epoch: [6]  [180/345]  eta: 0:02:03  lr: 0.000041  loss: 1.0077 (1.0327)  time: 0.7525  data: 0.0001  max mem: 14938
[15:03:26.410386] Epoch: [6]  [200/345]  eta: 0:01:48  lr: 0.000041  loss: 1.0064 (1.0305)  time: 0.7515  data: 0.0001  max mem: 14938
[15:03:41.431241] Epoch: [6]  [220/345]  eta: 0:01:33  lr: 0.000041  loss: 1.0108 (1.0290)  time: 0.7510  data: 0.0001  max mem: 14938
[15:03:56.443535] Epoch: [6]  [240/345]  eta: 0:01:18  lr: 0.000042  loss: 0.9967 (1.0262)  time: 0.7506  data: 0.0001  max mem: 14938
[15:04:11.448375] Epoch: [6]  [260/345]  eta: 0:01:03  lr: 0.000042  loss: 0.9856 (1.0244)  time: 0.7502  data: 0.0001  max mem: 14938
[15:04:26.445465] Epoch: [6]  [280/345]  eta: 0:00:48  lr: 0.000043  loss: 0.9750 (1.0210)  time: 0.7498  data: 0.0001  max mem: 14938
[15:04:41.445442] Epoch: [6]  [300/345]  eta: 0:00:33  lr: 0.000043  loss: 0.9785 (1.0188)  time: 0.7500  data: 0.0001  max mem: 14938
[15:04:56.438038] Epoch: [6]  [320/345]  eta: 0:00:18  lr: 0.000043  loss: 0.9745 (1.0167)  time: 0.7496  data: 0.0001  max mem: 14938
[15:05:11.407322] Epoch: [6]  [340/345]  eta: 0:00:03  lr: 0.000044  loss: 0.9717 (1.0146)  time: 0.7484  data: 0.0001  max mem: 14938
[15:05:14.400840] Epoch: [6]  [344/345]  eta: 0:00:00  lr: 0.000044  loss: 0.9851 (1.0146)  time: 0.7484  data: 0.0001  max mem: 14938
[15:05:14.467474] Epoch: [6] Total time: 0:04:18 (0.7506 s / it)
[15:05:14.467833] Averaged stats: lr: 0.000044  loss: 0.9851 (1.0146)
[15:05:14.799266] Test:  [  0/345]  eta: 0:01:53  loss: 0.9549 (0.9549)  time: 0.3281  data: 0.1472  max mem: 14938
[15:05:16.634497] Test:  [ 10/345]  eta: 0:01:05  loss: 0.9461 (0.9424)  time: 0.1966  data: 0.0134  max mem: 14938
[15:05:18.473677] Test:  [ 20/345]  eta: 0:01:01  loss: 0.9478 (0.9526)  time: 0.1837  data: 0.0001  max mem: 14938
[15:05:20.315838] Test:  [ 30/345]  eta: 0:00:59  loss: 0.9537 (0.9512)  time: 0.1840  data: 0.0001  max mem: 14938
[15:05:22.162259] Test:  [ 40/345]  eta: 0:00:57  loss: 0.9585 (0.9545)  time: 0.1844  data: 0.0001  max mem: 14938
[15:05:24.010547] Test:  [ 50/345]  eta: 0:00:55  loss: 0.9587 (0.9523)  time: 0.1847  data: 0.0001  max mem: 14938
[15:05:25.863933] Test:  [ 60/345]  eta: 0:00:53  loss: 0.9441 (0.9510)  time: 0.1850  data: 0.0001  max mem: 14938
[15:05:27.719287] Test:  [ 70/345]  eta: 0:00:51  loss: 0.9627 (0.9544)  time: 0.1854  data: 0.0001  max mem: 14938
[15:05:29.578074] Test:  [ 80/345]  eta: 0:00:49  loss: 0.9685 (0.9564)  time: 0.1857  data: 0.0001  max mem: 14938
[15:05:31.441363] Test:  [ 90/345]  eta: 0:00:47  loss: 0.9611 (0.9562)  time: 0.1861  data: 0.0001  max mem: 14938
[15:05:33.307273] Test:  [100/345]  eta: 0:00:45  loss: 0.9457 (0.9550)  time: 0.1864  data: 0.0001  max mem: 14938
[15:05:35.175313] Test:  [110/345]  eta: 0:00:43  loss: 0.9445 (0.9541)  time: 0.1866  data: 0.0001  max mem: 14938
[15:05:37.048013] Test:  [120/345]  eta: 0:00:41  loss: 0.9445 (0.9539)  time: 0.1870  data: 0.0001  max mem: 14938
[15:05:38.924604] Test:  [130/345]  eta: 0:00:40  loss: 0.9397 (0.9533)  time: 0.1874  data: 0.0001  max mem: 14938
[15:05:40.804072] Test:  [140/345]  eta: 0:00:38  loss: 0.9397 (0.9522)  time: 0.1878  data: 0.0001  max mem: 14938
[15:05:42.686264] Test:  [150/345]  eta: 0:00:36  loss: 0.9443 (0.9526)  time: 0.1880  data: 0.0001  max mem: 14938
[15:05:44.572335] Test:  [160/345]  eta: 0:00:34  loss: 0.9387 (0.9523)  time: 0.1884  data: 0.0001  max mem: 14938
[15:05:46.462179] Test:  [170/345]  eta: 0:00:32  loss: 0.9382 (0.9529)  time: 0.1887  data: 0.0001  max mem: 14938
[15:05:48.356550] Test:  [180/345]  eta: 0:00:30  loss: 0.9622 (0.9524)  time: 0.1892  data: 0.0001  max mem: 14938
[15:05:50.253859] Test:  [190/345]  eta: 0:00:29  loss: 0.9397 (0.9514)  time: 0.1895  data: 0.0001  max mem: 14938
[15:05:52.154503] Test:  [200/345]  eta: 0:00:27  loss: 0.9347 (0.9511)  time: 0.1898  data: 0.0001  max mem: 14938
[15:05:54.058778] Test:  [210/345]  eta: 0:00:25  loss: 0.9501 (0.9518)  time: 0.1902  data: 0.0001  max mem: 14938
[15:05:55.966235] Test:  [220/345]  eta: 0:00:23  loss: 0.9526 (0.9513)  time: 0.1905  data: 0.0001  max mem: 14938
[15:05:57.877633] Test:  [230/345]  eta: 0:00:21  loss: 0.9394 (0.9509)  time: 0.1909  data: 0.0001  max mem: 14938
[15:05:59.792264] Test:  [240/345]  eta: 0:00:19  loss: 0.9517 (0.9510)  time: 0.1912  data: 0.0001  max mem: 14938
[15:06:01.711662] Test:  [250/345]  eta: 0:00:17  loss: 0.9509 (0.9507)  time: 0.1916  data: 0.0001  max mem: 14938
[15:06:03.633604] Test:  [260/345]  eta: 0:00:16  loss: 0.9298 (0.9501)  time: 0.1920  data: 0.0001  max mem: 14938
[15:06:05.560858] Test:  [270/345]  eta: 0:00:14  loss: 0.9424 (0.9503)  time: 0.1924  data: 0.0001  max mem: 14938
[15:06:07.490045] Test:  [280/345]  eta: 0:00:12  loss: 0.9578 (0.9505)  time: 0.1928  data: 0.0001  max mem: 14938
[15:06:09.423623] Test:  [290/345]  eta: 0:00:10  loss: 0.9467 (0.9499)  time: 0.1931  data: 0.0001  max mem: 14938
[15:06:11.358828] Test:  [300/345]  eta: 0:00:08  loss: 0.9361 (0.9499)  time: 0.1934  data: 0.0001  max mem: 14938
[15:06:13.297461] Test:  [310/345]  eta: 0:00:06  loss: 0.9528 (0.9497)  time: 0.1936  data: 0.0001  max mem: 14938
[15:06:15.239052] Test:  [320/345]  eta: 0:00:04  loss: 0.9528 (0.9499)  time: 0.1940  data: 0.0001  max mem: 14938
[15:06:17.185036] Test:  [330/345]  eta: 0:00:02  loss: 0.9450 (0.9496)  time: 0.1943  data: 0.0001  max mem: 14938
[15:06:19.134041] Test:  [340/345]  eta: 0:00:00  loss: 0.9416 (0.9494)  time: 0.1947  data: 0.0001  max mem: 14938
[15:06:19.916109] Test:  [344/345]  eta: 0:00:00  loss: 0.9416 (0.9492)  time: 0.1949  data: 0.0001  max mem: 14938
[15:06:19.976521] Test: Total time: 0:01:05 (0.1899 s / it)
[15:06:30.339462] Test:  [ 0/57]  eta: 0:00:17  loss: 1.0306 (1.0306)  time: 0.3140  data: 0.1347  max mem: 14938
[15:06:32.152634] Test:  [10/57]  eta: 0:00:09  loss: 0.9927 (0.9977)  time: 0.1933  data: 0.0123  max mem: 14938
[15:06:33.971538] Test:  [20/57]  eta: 0:00:06  loss: 0.9888 (0.9894)  time: 0.1815  data: 0.0001  max mem: 14938
[15:06:35.794533] Test:  [30/57]  eta: 0:00:05  loss: 0.9045 (0.9444)  time: 0.1820  data: 0.0001  max mem: 14938
[15:06:37.623368] Test:  [40/57]  eta: 0:00:03  loss: 0.8328 (0.9164)  time: 0.1825  data: 0.0001  max mem: 14938
[15:06:39.456638] Test:  [50/57]  eta: 0:00:01  loss: 0.8328 (0.9081)  time: 0.1830  data: 0.0001  max mem: 14938
[15:06:40.444464] Test:  [56/57]  eta: 0:00:00  loss: 0.8926 (0.9130)  time: 0.1776  data: 0.0001  max mem: 14938
[15:06:40.497971] Test: Total time: 0:00:10 (0.1837 s / it)
[15:06:42.216569] Dice score of the network on the train images: 0.666803, val images: 0.750757
[15:06:42.216780] saving best_rec_model_0 @ epoch 6
[15:06:43.308119] saving best_dice_model_0 @ epoch 6
[15:06:44.443974] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[15:06:45.322996] Epoch: [7]  [  0/345]  eta: 0:05:02  lr: 0.000044  loss: 0.9401 (0.9401)  time: 0.8780  data: 0.1361  max mem: 14938
[15:07:00.187704] Epoch: [7]  [ 20/345]  eta: 0:04:03  lr: 0.000044  loss: 0.9627 (0.9692)  time: 0.7432  data: 0.0001  max mem: 14938
[15:07:15.106951] Epoch: [7]  [ 40/345]  eta: 0:03:48  lr: 0.000044  loss: 0.9674 (0.9709)  time: 0.7459  data: 0.0001  max mem: 14938
[15:07:30.073493] Epoch: [7]  [ 60/345]  eta: 0:03:33  lr: 0.000045  loss: 0.9578 (0.9665)  time: 0.7483  data: 0.0001  max mem: 14938
[15:07:45.063898] Epoch: [7]  [ 80/345]  eta: 0:03:18  lr: 0.000045  loss: 0.9637 (0.9643)  time: 0.7495  data: 0.0001  max mem: 14938
[15:08:00.089481] Epoch: [7]  [100/345]  eta: 0:03:03  lr: 0.000046  loss: 0.9717 (0.9681)  time: 0.7512  data: 0.0001  max mem: 14938
[15:08:15.138935] Epoch: [7]  [120/345]  eta: 0:02:48  lr: 0.000046  loss: 0.9673 (0.9679)  time: 0.7524  data: 0.0001  max mem: 14938
[15:08:30.186750] Epoch: [7]  [140/345]  eta: 0:02:33  lr: 0.000046  loss: 0.9587 (0.9662)  time: 0.7524  data: 0.0001  max mem: 14938
[15:08:45.223877] Epoch: [7]  [160/345]  eta: 0:02:18  lr: 0.000047  loss: 0.9516 (0.9646)  time: 0.7518  data: 0.0001  max mem: 14938
[15:09:00.247294] Epoch: [7]  [180/345]  eta: 0:02:03  lr: 0.000047  loss: 0.9657 (0.9665)  time: 0.7511  data: 0.0001  max mem: 14938
[15:09:15.269294] Epoch: [7]  [200/345]  eta: 0:01:48  lr: 0.000047  loss: 0.9552 (0.9652)  time: 0.7511  data: 0.0001  max mem: 14938
[15:09:30.282636] Epoch: [7]  [220/345]  eta: 0:01:33  lr: 0.000048  loss: 0.9402 (0.9636)  time: 0.7506  data: 0.0001  max mem: 14938
[15:09:45.273223] Epoch: [7]  [240/345]  eta: 0:01:18  lr: 0.000048  loss: 0.9412 (0.9612)  time: 0.7495  data: 0.0001  max mem: 14938
[15:10:00.271743] Epoch: [7]  [260/345]  eta: 0:01:03  lr: 0.000048  loss: 0.9220 (0.9586)  time: 0.7499  data: 0.0001  max mem: 14938
[15:10:15.255378] Epoch: [7]  [280/345]  eta: 0:00:48  lr: 0.000049  loss: 0.9302 (0.9571)  time: 0.7491  data: 0.0001  max mem: 14938
[15:10:30.232813] Epoch: [7]  [300/345]  eta: 0:00:33  lr: 0.000049  loss: 0.9271 (0.9553)  time: 0.7488  data: 0.0001  max mem: 14938
[15:10:45.207094] Epoch: [7]  [320/345]  eta: 0:00:18  lr: 0.000050  loss: 0.9310 (0.9542)  time: 0.7487  data: 0.0001  max mem: 14938
[15:11:00.169330] Epoch: [7]  [340/345]  eta: 0:00:03  lr: 0.000050  loss: 0.9223 (0.9528)  time: 0.7481  data: 0.0001  max mem: 14938
[15:11:03.173825] Epoch: [7]  [344/345]  eta: 0:00:00  lr: 0.000050  loss: 0.9220 (0.9523)  time: 0.7488  data: 0.0001  max mem: 14938
[15:11:03.234485] Epoch: [7] Total time: 0:04:18 (0.7501 s / it)
[15:11:03.234659] Averaged stats: lr: 0.000050  loss: 0.9220 (0.9523)
[15:11:03.565447] Test:  [  0/345]  eta: 0:01:52  loss: 0.8675 (0.8675)  time: 0.3264  data: 0.1451  max mem: 14938
[15:11:05.400988] Test:  [ 10/345]  eta: 0:01:05  loss: 0.8675 (0.8667)  time: 0.1965  data: 0.0133  max mem: 14938
[15:11:07.238725] Test:  [ 20/345]  eta: 0:01:01  loss: 0.8742 (0.8760)  time: 0.1836  data: 0.0001  max mem: 14938
[15:11:09.080336] Test:  [ 30/345]  eta: 0:00:59  loss: 0.8745 (0.8740)  time: 0.1839  data: 0.0001  max mem: 14938
[15:11:10.924677] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8543 (0.8671)  time: 0.1842  data: 0.0001  max mem: 14938
[15:11:12.773665] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8656 (0.8689)  time: 0.1846  data: 0.0001  max mem: 14938
[15:11:14.624990] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8656 (0.8705)  time: 0.1850  data: 0.0001  max mem: 14938
[15:11:16.480507] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8623 (0.8694)  time: 0.1853  data: 0.0001  max mem: 14938
[15:11:18.340552] Test:  [ 80/345]  eta: 0:00:49  loss: 0.8657 (0.8697)  time: 0.1856  data: 0.0001  max mem: 14938
[15:11:20.203261] Test:  [ 90/345]  eta: 0:00:47  loss: 0.8657 (0.8703)  time: 0.1860  data: 0.0001  max mem: 14938
[15:11:22.070357] Test:  [100/345]  eta: 0:00:45  loss: 0.8696 (0.8709)  time: 0.1864  data: 0.0001  max mem: 14938
[15:11:23.939786] Test:  [110/345]  eta: 0:00:43  loss: 0.8775 (0.8718)  time: 0.1868  data: 0.0001  max mem: 14938
[15:11:25.812695] Test:  [120/345]  eta: 0:00:41  loss: 0.8790 (0.8721)  time: 0.1871  data: 0.0001  max mem: 14938
[15:11:27.688250] Test:  [130/345]  eta: 0:00:40  loss: 0.8774 (0.8724)  time: 0.1874  data: 0.0001  max mem: 14938
[15:11:29.568733] Test:  [140/345]  eta: 0:00:38  loss: 0.8629 (0.8708)  time: 0.1878  data: 0.0001  max mem: 14938
[15:11:31.454031] Test:  [150/345]  eta: 0:00:36  loss: 0.8724 (0.8723)  time: 0.1882  data: 0.0001  max mem: 14938
[15:11:33.339738] Test:  [160/345]  eta: 0:00:34  loss: 0.8899 (0.8725)  time: 0.1885  data: 0.0001  max mem: 14938
[15:11:35.231149] Test:  [170/345]  eta: 0:00:32  loss: 0.8757 (0.8726)  time: 0.1888  data: 0.0001  max mem: 14938
[15:11:37.126645] Test:  [180/345]  eta: 0:00:30  loss: 0.8755 (0.8730)  time: 0.1893  data: 0.0001  max mem: 14938
[15:11:39.024475] Test:  [190/345]  eta: 0:00:29  loss: 0.8728 (0.8724)  time: 0.1896  data: 0.0001  max mem: 14938
[15:11:40.925690] Test:  [200/345]  eta: 0:00:27  loss: 0.8601 (0.8722)  time: 0.1899  data: 0.0001  max mem: 14938
[15:11:42.830628] Test:  [210/345]  eta: 0:00:25  loss: 0.8664 (0.8721)  time: 0.1903  data: 0.0001  max mem: 14938
[15:11:44.738120] Test:  [220/345]  eta: 0:00:23  loss: 0.8664 (0.8722)  time: 0.1906  data: 0.0001  max mem: 14938
[15:11:46.649530] Test:  [230/345]  eta: 0:00:21  loss: 0.8735 (0.8725)  time: 0.1909  data: 0.0001  max mem: 14938
[15:11:48.564570] Test:  [240/345]  eta: 0:00:19  loss: 0.8736 (0.8729)  time: 0.1913  data: 0.0001  max mem: 14938
[15:11:50.483351] Test:  [250/345]  eta: 0:00:17  loss: 0.8716 (0.8722)  time: 0.1916  data: 0.0001  max mem: 14938
[15:11:52.406850] Test:  [260/345]  eta: 0:00:16  loss: 0.8578 (0.8721)  time: 0.1921  data: 0.0001  max mem: 14938
[15:11:54.332636] Test:  [270/345]  eta: 0:00:14  loss: 0.8637 (0.8724)  time: 0.1924  data: 0.0001  max mem: 14938
[15:11:56.261435] Test:  [280/345]  eta: 0:00:12  loss: 0.8691 (0.8725)  time: 0.1927  data: 0.0001  max mem: 14938
[15:11:58.194328] Test:  [290/345]  eta: 0:00:10  loss: 0.8736 (0.8727)  time: 0.1930  data: 0.0001  max mem: 14938
[15:12:00.130824] Test:  [300/345]  eta: 0:00:08  loss: 0.8757 (0.8727)  time: 0.1934  data: 0.0001  max mem: 14938
[15:12:02.070712] Test:  [310/345]  eta: 0:00:06  loss: 0.8757 (0.8732)  time: 0.1938  data: 0.0001  max mem: 14938
[15:12:04.013150] Test:  [320/345]  eta: 0:00:04  loss: 0.8771 (0.8735)  time: 0.1941  data: 0.0001  max mem: 14938
[15:12:05.960264] Test:  [330/345]  eta: 0:00:02  loss: 0.8676 (0.8733)  time: 0.1944  data: 0.0001  max mem: 14938
[15:12:07.911955] Test:  [340/345]  eta: 0:00:00  loss: 0.8702 (0.8733)  time: 0.1949  data: 0.0001  max mem: 14938
[15:12:08.692687] Test:  [344/345]  eta: 0:00:00  loss: 0.8770 (0.8735)  time: 0.1950  data: 0.0001  max mem: 14938
[15:12:08.750123] Test: Total time: 0:01:05 (0.1899 s / it)
[15:12:19.184907] Test:  [ 0/57]  eta: 0:00:17  loss: 0.9710 (0.9710)  time: 0.3137  data: 0.1343  max mem: 14938
[15:12:21.001072] Test:  [10/57]  eta: 0:00:09  loss: 0.9301 (0.9369)  time: 0.1935  data: 0.0123  max mem: 14938
[15:12:22.820259] Test:  [20/57]  eta: 0:00:06  loss: 0.9301 (0.9309)  time: 0.1817  data: 0.0001  max mem: 14938
[15:12:24.645076] Test:  [30/57]  eta: 0:00:05  loss: 0.8282 (0.8870)  time: 0.1821  data: 0.0001  max mem: 14938
[15:12:26.473471] Test:  [40/57]  eta: 0:00:03  loss: 0.7920 (0.8605)  time: 0.1826  data: 0.0001  max mem: 14938
[15:12:28.308228] Test:  [50/57]  eta: 0:00:01  loss: 0.7920 (0.8524)  time: 0.1831  data: 0.0001  max mem: 14938
[15:12:29.297049] Test:  [56/57]  eta: 0:00:00  loss: 0.8224 (0.8572)  time: 0.1777  data: 0.0001  max mem: 14938
[15:12:29.355100] Test: Total time: 0:00:10 (0.1839 s / it)
[15:12:31.109041] Dice score of the network on the train images: 0.723070, val images: 0.786550
[15:12:31.109280] saving best_prec_model_0 @ epoch 7
[15:12:32.222159] saving best_dice_model_0 @ epoch 7
[15:12:33.336809] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[15:12:34.224011] Epoch: [8]  [  0/345]  eta: 0:05:05  lr: 0.000050  loss: 0.9438 (0.9438)  time: 0.8860  data: 0.1451  max mem: 14938
[15:12:49.101792] Epoch: [8]  [ 20/345]  eta: 0:04:03  lr: 0.000050  loss: 0.9150 (0.9239)  time: 0.7438  data: 0.0001  max mem: 14938
[15:13:04.035048] Epoch: [8]  [ 40/345]  eta: 0:03:48  lr: 0.000051  loss: 0.9108 (0.9221)  time: 0.7466  data: 0.0001  max mem: 14938
[15:13:19.003960] Epoch: [8]  [ 60/345]  eta: 0:03:33  lr: 0.000051  loss: 0.9236 (0.9245)  time: 0.7484  data: 0.0001  max mem: 14938
[15:13:33.995660] Epoch: [8]  [ 80/345]  eta: 0:03:18  lr: 0.000051  loss: 0.9230 (0.9221)  time: 0.7495  data: 0.0001  max mem: 14938
[15:13:49.017980] Epoch: [8]  [100/345]  eta: 0:03:03  lr: 0.000052  loss: 0.9117 (0.9217)  time: 0.7511  data: 0.0001  max mem: 14938
[15:14:04.082232] Epoch: [8]  [120/345]  eta: 0:02:48  lr: 0.000052  loss: 0.9192 (0.9210)  time: 0.7532  data: 0.0001  max mem: 14938
[15:14:19.116204] Epoch: [8]  [140/345]  eta: 0:02:33  lr: 0.000053  loss: 0.9320 (0.9222)  time: 0.7517  data: 0.0001  max mem: 14938
[15:14:34.155837] Epoch: [8]  [160/345]  eta: 0:02:18  lr: 0.000053  loss: 0.9116 (0.9216)  time: 0.7519  data: 0.0001  max mem: 14938
[15:14:49.189322] Epoch: [8]  [180/345]  eta: 0:02:03  lr: 0.000053  loss: 0.9001 (0.9206)  time: 0.7516  data: 0.0001  max mem: 14938
[15:15:04.208943] Epoch: [8]  [200/345]  eta: 0:01:48  lr: 0.000054  loss: 0.8878 (0.9178)  time: 0.7509  data: 0.0001  max mem: 14938
[15:15:19.223796] Epoch: [8]  [220/345]  eta: 0:01:33  lr: 0.000054  loss: 0.8885 (0.9156)  time: 0.7507  data: 0.0001  max mem: 14938
[15:15:34.226085] Epoch: [8]  [240/345]  eta: 0:01:18  lr: 0.000054  loss: 0.8983 (0.9143)  time: 0.7501  data: 0.0001  max mem: 14938
[15:15:49.221901] Epoch: [8]  [260/345]  eta: 0:01:03  lr: 0.000055  loss: 0.9052 (0.9138)  time: 0.7497  data: 0.0001  max mem: 14938
[15:16:04.218399] Epoch: [8]  [280/345]  eta: 0:00:48  lr: 0.000055  loss: 0.8972 (0.9126)  time: 0.7498  data: 0.0001  max mem: 14938
[15:16:19.216626] Epoch: [8]  [300/345]  eta: 0:00:33  lr: 0.000055  loss: 0.8873 (0.9111)  time: 0.7499  data: 0.0001  max mem: 14938
[15:16:34.211365] Epoch: [8]  [320/345]  eta: 0:00:18  lr: 0.000056  loss: 0.8817 (0.9100)  time: 0.7497  data: 0.0001  max mem: 14938
[15:16:49.207808] Epoch: [8]  [340/345]  eta: 0:00:03  lr: 0.000056  loss: 0.8810 (0.9083)  time: 0.7498  data: 0.0001  max mem: 14938
[15:16:52.207990] Epoch: [8]  [344/345]  eta: 0:00:00  lr: 0.000056  loss: 0.8813 (0.9081)  time: 0.7498  data: 0.0001  max mem: 14938
[15:16:52.273500] Epoch: [8] Total time: 0:04:18 (0.7505 s / it)
[15:16:52.273754] Averaged stats: lr: 0.000056  loss: 0.8813 (0.9081)
[15:16:52.615687] Test:  [  0/345]  eta: 0:01:56  loss: 0.8729 (0.8729)  time: 0.3377  data: 0.1563  max mem: 14938
[15:16:54.451244] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8469 (0.8480)  time: 0.1975  data: 0.0143  max mem: 14938
[15:16:56.289689] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8361 (0.8387)  time: 0.1836  data: 0.0001  max mem: 14938
[15:16:58.131372] Test:  [ 30/345]  eta: 0:00:59  loss: 0.8434 (0.8426)  time: 0.1840  data: 0.0001  max mem: 14938
[15:16:59.976684] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8515 (0.8454)  time: 0.1843  data: 0.0001  max mem: 14938
[15:17:01.824944] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8482 (0.8448)  time: 0.1846  data: 0.0001  max mem: 14938
[15:17:03.678401] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8445 (0.8464)  time: 0.1850  data: 0.0001  max mem: 14938
[15:17:05.533180] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8470 (0.8476)  time: 0.1854  data: 0.0001  max mem: 14938
[15:17:07.391464] Test:  [ 80/345]  eta: 0:00:49  loss: 0.8564 (0.8479)  time: 0.1856  data: 0.0001  max mem: 14938
[15:17:09.254577] Test:  [ 90/345]  eta: 0:00:47  loss: 0.8556 (0.8494)  time: 0.1860  data: 0.0001  max mem: 14938
[15:17:11.120207] Test:  [100/345]  eta: 0:00:45  loss: 0.8495 (0.8492)  time: 0.1864  data: 0.0001  max mem: 14938
[15:17:12.988440] Test:  [110/345]  eta: 0:00:43  loss: 0.8486 (0.8498)  time: 0.1866  data: 0.0001  max mem: 14938
[15:17:14.862031] Test:  [120/345]  eta: 0:00:41  loss: 0.8608 (0.8498)  time: 0.1870  data: 0.0001  max mem: 14938
[15:17:16.739118] Test:  [130/345]  eta: 0:00:40  loss: 0.8341 (0.8491)  time: 0.1875  data: 0.0001  max mem: 14938
[15:17:18.618953] Test:  [140/345]  eta: 0:00:38  loss: 0.8453 (0.8496)  time: 0.1878  data: 0.0001  max mem: 14938
[15:17:20.502333] Test:  [150/345]  eta: 0:00:36  loss: 0.8453 (0.8494)  time: 0.1881  data: 0.0001  max mem: 14938
[15:17:22.388158] Test:  [160/345]  eta: 0:00:34  loss: 0.8413 (0.8497)  time: 0.1884  data: 0.0001  max mem: 14938
[15:17:24.278319] Test:  [170/345]  eta: 0:00:32  loss: 0.8655 (0.8506)  time: 0.1887  data: 0.0001  max mem: 14938
[15:17:26.173383] Test:  [180/345]  eta: 0:00:30  loss: 0.8580 (0.8501)  time: 0.1892  data: 0.0001  max mem: 14938
[15:17:28.071875] Test:  [190/345]  eta: 0:00:29  loss: 0.8495 (0.8505)  time: 0.1896  data: 0.0001  max mem: 14938
[15:17:29.972956] Test:  [200/345]  eta: 0:00:27  loss: 0.8492 (0.8500)  time: 0.1899  data: 0.0001  max mem: 14938
[15:17:31.877634] Test:  [210/345]  eta: 0:00:25  loss: 0.8495 (0.8502)  time: 0.1902  data: 0.0001  max mem: 14938
[15:17:33.787477] Test:  [220/345]  eta: 0:00:23  loss: 0.8495 (0.8501)  time: 0.1907  data: 0.0001  max mem: 14938
[15:17:35.699421] Test:  [230/345]  eta: 0:00:21  loss: 0.8469 (0.8508)  time: 0.1910  data: 0.0001  max mem: 14938
[15:17:37.616264] Test:  [240/345]  eta: 0:00:19  loss: 0.8563 (0.8507)  time: 0.1914  data: 0.0001  max mem: 14938
[15:17:39.535902] Test:  [250/345]  eta: 0:00:17  loss: 0.8458 (0.8503)  time: 0.1918  data: 0.0001  max mem: 14938
[15:17:41.457024] Test:  [260/345]  eta: 0:00:16  loss: 0.8542 (0.8509)  time: 0.1920  data: 0.0001  max mem: 14938
[15:17:43.381997] Test:  [270/345]  eta: 0:00:14  loss: 0.8573 (0.8508)  time: 0.1923  data: 0.0001  max mem: 14938
[15:17:45.313921] Test:  [280/345]  eta: 0:00:12  loss: 0.8583 (0.8516)  time: 0.1928  data: 0.0001  max mem: 14938
[15:17:47.245915] Test:  [290/345]  eta: 0:00:10  loss: 0.8582 (0.8519)  time: 0.1931  data: 0.0001  max mem: 14938
[15:17:49.182444] Test:  [300/345]  eta: 0:00:08  loss: 0.8560 (0.8521)  time: 0.1934  data: 0.0001  max mem: 14938
[15:17:51.122157] Test:  [310/345]  eta: 0:00:06  loss: 0.8581 (0.8523)  time: 0.1938  data: 0.0001  max mem: 14938
[15:17:53.065645] Test:  [320/345]  eta: 0:00:04  loss: 0.8366 (0.8517)  time: 0.1941  data: 0.0001  max mem: 14938
[15:17:55.012573] Test:  [330/345]  eta: 0:00:02  loss: 0.8412 (0.8516)  time: 0.1945  data: 0.0001  max mem: 14938
[15:17:56.962652] Test:  [340/345]  eta: 0:00:00  loss: 0.8551 (0.8522)  time: 0.1948  data: 0.0001  max mem: 14938
[15:17:57.743780] Test:  [344/345]  eta: 0:00:00  loss: 0.8669 (0.8524)  time: 0.1950  data: 0.0001  max mem: 14938
[15:17:57.802355] Test: Total time: 0:01:05 (0.1899 s / it)
[15:18:08.620058] Test:  [ 0/57]  eta: 0:00:18  loss: 0.9296 (0.9296)  time: 0.3233  data: 0.1436  max mem: 14938
[15:18:10.435739] Test:  [10/57]  eta: 0:00:09  loss: 0.8907 (0.9124)  time: 0.1944  data: 0.0131  max mem: 14938
[15:18:12.255480] Test:  [20/57]  eta: 0:00:06  loss: 0.8907 (0.9060)  time: 0.1817  data: 0.0001  max mem: 14938
[15:18:14.079032] Test:  [30/57]  eta: 0:00:05  loss: 0.8269 (0.8675)  time: 0.1821  data: 0.0001  max mem: 14938
[15:18:15.906353] Test:  [40/57]  eta: 0:00:03  loss: 0.7735 (0.8442)  time: 0.1825  data: 0.0001  max mem: 14938
[15:18:17.740624] Test:  [50/57]  eta: 0:00:01  loss: 0.7735 (0.8371)  time: 0.1830  data: 0.0001  max mem: 14938
[15:18:18.729104] Test:  [56/57]  eta: 0:00:00  loss: 0.8205 (0.8428)  time: 0.1776  data: 0.0001  max mem: 14938
[15:18:18.761515] Test: Total time: 0:00:10 (0.1836 s / it)
[15:18:20.737524] Dice score of the network on the train images: 0.718722, val images: 0.791598
[15:18:20.737765] saving best_rec_model_0 @ epoch 8
[15:18:21.844456] saving best_dice_model_0 @ epoch 8
[15:18:22.889259] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[15:18:23.777370] Epoch: [9]  [  0/345]  eta: 0:05:06  lr: 0.000056  loss: 0.8728 (0.8728)  time: 0.8873  data: 0.1483  max mem: 14938
[15:18:38.655524] Epoch: [9]  [ 20/345]  eta: 0:04:03  lr: 0.000057  loss: 0.8671 (0.8766)  time: 0.7439  data: 0.0001  max mem: 14938
[15:18:53.587008] Epoch: [9]  [ 40/345]  eta: 0:03:48  lr: 0.000057  loss: 0.9099 (0.8887)  time: 0.7465  data: 0.0001  max mem: 14938
[15:19:08.565994] Epoch: [9]  [ 60/345]  eta: 0:03:33  lr: 0.000057  loss: 0.8884 (0.8913)  time: 0.7489  data: 0.0001  max mem: 14938
[15:19:23.570793] Epoch: [9]  [ 80/345]  eta: 0:03:18  lr: 0.000058  loss: 0.8774 (0.8885)  time: 0.7502  data: 0.0001  max mem: 14938
[15:19:38.613577] Epoch: [9]  [100/345]  eta: 0:03:03  lr: 0.000058  loss: 0.8806 (0.8879)  time: 0.7521  data: 0.0001  max mem: 14938
[15:19:53.678143] Epoch: [9]  [120/345]  eta: 0:02:48  lr: 0.000058  loss: 0.8768 (0.8873)  time: 0.7532  data: 0.0001  max mem: 14938
[15:20:08.729910] Epoch: [9]  [140/345]  eta: 0:02:33  lr: 0.000059  loss: 0.8780 (0.8861)  time: 0.7526  data: 0.0001  max mem: 14938
[15:20:23.775711] Epoch: [9]  [160/345]  eta: 0:02:18  lr: 0.000059  loss: 0.8886 (0.8851)  time: 0.7523  data: 0.0001  max mem: 14938
[15:20:38.800040] Epoch: [9]  [180/345]  eta: 0:02:03  lr: 0.000060  loss: 0.8903 (0.8861)  time: 0.7512  data: 0.0001  max mem: 14938
[15:20:53.821103] Epoch: [9]  [200/345]  eta: 0:01:48  lr: 0.000060  loss: 0.8805 (0.8849)  time: 0.7510  data: 0.0001  max mem: 14938
[15:21:08.828962] Epoch: [9]  [220/345]  eta: 0:01:33  lr: 0.000060  loss: 0.8838 (0.8848)  time: 0.7504  data: 0.0001  max mem: 14938
[15:21:23.836897] Epoch: [9]  [240/345]  eta: 0:01:18  lr: 0.000061  loss: 0.8575 (0.8830)  time: 0.7504  data: 0.0001  max mem: 14938
[15:21:38.842916] Epoch: [9]  [260/345]  eta: 0:01:03  lr: 0.000061  loss: 0.8899 (0.8836)  time: 0.7503  data: 0.0001  max mem: 14938
[15:21:53.844760] Epoch: [9]  [280/345]  eta: 0:00:48  lr: 0.000061  loss: 0.8785 (0.8835)  time: 0.7500  data: 0.0001  max mem: 14938
[15:22:08.822085] Epoch: [9]  [300/345]  eta: 0:00:33  lr: 0.000062  loss: 0.8805 (0.8830)  time: 0.7488  data: 0.0001  max mem: 14938
[15:22:23.796182] Epoch: [9]  [320/345]  eta: 0:00:18  lr: 0.000062  loss: 0.8731 (0.8826)  time: 0.7487  data: 0.0001  max mem: 14938
[15:22:38.769294] Epoch: [9]  [340/345]  eta: 0:00:03  lr: 0.000062  loss: 0.8740 (0.8820)  time: 0.7486  data: 0.0001  max mem: 14938
[15:22:41.761916] Epoch: [9]  [344/345]  eta: 0:00:00  lr: 0.000062  loss: 0.8551 (0.8818)  time: 0.7486  data: 0.0001  max mem: 14938
[15:22:41.827544] Epoch: [9] Total time: 0:04:18 (0.7505 s / it)
[15:22:41.827875] Averaged stats: lr: 0.000062  loss: 0.8551 (0.8818)
[15:22:42.163170] Test:  [  0/345]  eta: 0:01:54  loss: 0.8452 (0.8452)  time: 0.3320  data: 0.1505  max mem: 14938
[15:22:43.998647] Test:  [ 10/345]  eta: 0:01:05  loss: 0.8451 (0.8431)  time: 0.1970  data: 0.0137  max mem: 14938
[15:22:45.837714] Test:  [ 20/345]  eta: 0:01:01  loss: 0.8406 (0.8398)  time: 0.1837  data: 0.0001  max mem: 14938
[15:22:47.681277] Test:  [ 30/345]  eta: 0:00:59  loss: 0.8242 (0.8359)  time: 0.1841  data: 0.0001  max mem: 14938
[15:22:49.528415] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8307 (0.8383)  time: 0.1845  data: 0.0001  max mem: 14938
[15:22:51.379364] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8485 (0.8411)  time: 0.1849  data: 0.0001  max mem: 14938
[15:22:53.233933] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8359 (0.8387)  time: 0.1852  data: 0.0001  max mem: 14938
[15:22:55.089595] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8190 (0.8386)  time: 0.1855  data: 0.0001  max mem: 14938
[15:22:56.949558] Test:  [ 80/345]  eta: 0:00:49  loss: 0.8207 (0.8368)  time: 0.1857  data: 0.0001  max mem: 14938
[15:22:58.812967] Test:  [ 90/345]  eta: 0:00:47  loss: 0.8269 (0.8372)  time: 0.1861  data: 0.0001  max mem: 14938
[15:23:00.679078] Test:  [100/345]  eta: 0:00:45  loss: 0.8302 (0.8371)  time: 0.1864  data: 0.0001  max mem: 14938
[15:23:02.547588] Test:  [110/345]  eta: 0:00:43  loss: 0.8240 (0.8367)  time: 0.1867  data: 0.0001  max mem: 14938
[15:23:04.421444] Test:  [120/345]  eta: 0:00:41  loss: 0.8303 (0.8372)  time: 0.1871  data: 0.0001  max mem: 14938
[15:23:06.297819] Test:  [130/345]  eta: 0:00:40  loss: 0.8355 (0.8372)  time: 0.1875  data: 0.0001  max mem: 14938
[15:23:08.179010] Test:  [140/345]  eta: 0:00:38  loss: 0.8267 (0.8362)  time: 0.1878  data: 0.0001  max mem: 14938
[15:23:10.063315] Test:  [150/345]  eta: 0:00:36  loss: 0.8236 (0.8361)  time: 0.1882  data: 0.0001  max mem: 14938
[15:23:11.949966] Test:  [160/345]  eta: 0:00:34  loss: 0.8236 (0.8358)  time: 0.1885  data: 0.0001  max mem: 14938
[15:23:13.841822] Test:  [170/345]  eta: 0:00:32  loss: 0.8414 (0.8366)  time: 0.1889  data: 0.0001  max mem: 14938
[15:23:15.736071] Test:  [180/345]  eta: 0:00:30  loss: 0.8476 (0.8374)  time: 0.1892  data: 0.0001  max mem: 14938
[15:23:17.635918] Test:  [190/345]  eta: 0:00:29  loss: 0.8391 (0.8372)  time: 0.1897  data: 0.0001  max mem: 14938
[15:23:19.539188] Test:  [200/345]  eta: 0:00:27  loss: 0.8349 (0.8370)  time: 0.1901  data: 0.0001  max mem: 14938
[15:23:21.443916] Test:  [210/345]  eta: 0:00:25  loss: 0.8392 (0.8370)  time: 0.1903  data: 0.0001  max mem: 14938
[15:23:23.351843] Test:  [220/345]  eta: 0:00:23  loss: 0.8394 (0.8373)  time: 0.1906  data: 0.0001  max mem: 14938
[15:23:25.264889] Test:  [230/345]  eta: 0:00:21  loss: 0.8506 (0.8380)  time: 0.1910  data: 0.0001  max mem: 14938
[15:23:27.182541] Test:  [240/345]  eta: 0:00:19  loss: 0.8506 (0.8382)  time: 0.1915  data: 0.0001  max mem: 14938
[15:23:29.102048] Test:  [250/345]  eta: 0:00:17  loss: 0.8365 (0.8382)  time: 0.1918  data: 0.0001  max mem: 14938
[15:23:31.023993] Test:  [260/345]  eta: 0:00:16  loss: 0.8340 (0.8381)  time: 0.1920  data: 0.0001  max mem: 14938
[15:23:32.950155] Test:  [270/345]  eta: 0:00:14  loss: 0.8285 (0.8377)  time: 0.1924  data: 0.0001  max mem: 14938
[15:23:34.879057] Test:  [280/345]  eta: 0:00:12  loss: 0.8306 (0.8378)  time: 0.1927  data: 0.0001  max mem: 14938
[15:23:36.811091] Test:  [290/345]  eta: 0:00:10  loss: 0.8315 (0.8375)  time: 0.1930  data: 0.0001  max mem: 14938
[15:23:38.747790] Test:  [300/345]  eta: 0:00:08  loss: 0.8257 (0.8374)  time: 0.1934  data: 0.0001  max mem: 14938
[15:23:40.687723] Test:  [310/345]  eta: 0:00:06  loss: 0.8317 (0.8376)  time: 0.1938  data: 0.0001  max mem: 14938
[15:23:42.630431] Test:  [320/345]  eta: 0:00:04  loss: 0.8429 (0.8379)  time: 0.1941  data: 0.0001  max mem: 14938
[15:23:44.576720] Test:  [330/345]  eta: 0:00:02  loss: 0.8310 (0.8375)  time: 0.1944  data: 0.0001  max mem: 14938
[15:23:46.525957] Test:  [340/345]  eta: 0:00:00  loss: 0.8307 (0.8377)  time: 0.1947  data: 0.0001  max mem: 14938
[15:23:47.307165] Test:  [344/345]  eta: 0:00:00  loss: 0.8307 (0.8376)  time: 0.1949  data: 0.0001  max mem: 14938
[15:23:47.364633] Test: Total time: 0:01:05 (0.1900 s / it)
[15:23:58.051823] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8858 (0.8858)  time: 0.3228  data: 0.1433  max mem: 14938
[15:23:59.867672] Test:  [10/57]  eta: 0:00:09  loss: 0.8858 (0.9005)  time: 0.1943  data: 0.0131  max mem: 14938
[15:24:01.687721] Test:  [20/57]  eta: 0:00:06  loss: 0.8937 (0.8966)  time: 0.1817  data: 0.0001  max mem: 14938
[15:24:03.511506] Test:  [30/57]  eta: 0:00:05  loss: 0.8118 (0.8600)  time: 0.1821  data: 0.0001  max mem: 14938
[15:24:05.339923] Test:  [40/57]  eta: 0:00:03  loss: 0.7750 (0.8383)  time: 0.1826  data: 0.0001  max mem: 14938
[15:24:07.173199] Test:  [50/57]  eta: 0:00:01  loss: 0.7750 (0.8318)  time: 0.1830  data: 0.0001  max mem: 14938
[15:24:08.161893] Test:  [56/57]  eta: 0:00:00  loss: 0.8119 (0.8377)  time: 0.1777  data: 0.0001  max mem: 14938
[15:24:08.221051] Test: Total time: 0:00:10 (0.1841 s / it)
[15:24:10.172494] Dice score of the network on the train images: 0.734315, val images: 0.797215
[15:24:10.172729] saving best_dice_model_0 @ epoch 9
[15:24:11.233086] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[15:24:12.110847] Epoch: [10]  [  0/345]  eta: 0:05:02  lr: 0.000063  loss: 0.8916 (0.8916)  time: 0.8768  data: 0.1351  max mem: 14938
[15:24:27.122369] Epoch: [10]  [ 20/345]  eta: 0:04:05  lr: 0.000063  loss: 0.8731 (0.8722)  time: 0.7505  data: 0.0001  max mem: 14938
[15:24:42.055301] Epoch: [10]  [ 40/345]  eta: 0:03:49  lr: 0.000063  loss: 0.8628 (0.8671)  time: 0.7466  data: 0.0001  max mem: 14938
[15:24:57.032698] Epoch: [10]  [ 60/345]  eta: 0:03:33  lr: 0.000064  loss: 0.8685 (0.8666)  time: 0.7488  data: 0.0001  max mem: 14938
[15:25:12.041355] Epoch: [10]  [ 80/345]  eta: 0:03:18  lr: 0.000064  loss: 0.8599 (0.8667)  time: 0.7504  data: 0.0001  max mem: 14938
[15:25:27.080576] Epoch: [10]  [100/345]  eta: 0:03:03  lr: 0.000064  loss: 0.8575 (0.8665)  time: 0.7519  data: 0.0001  max mem: 14938
[15:25:42.129530] Epoch: [10]  [120/345]  eta: 0:02:49  lr: 0.000065  loss: 0.8605 (0.8648)  time: 0.7524  data: 0.0001  max mem: 14938
[15:25:57.179093] Epoch: [10]  [140/345]  eta: 0:02:34  lr: 0.000065  loss: 0.8444 (0.8623)  time: 0.7524  data: 0.0001  max mem: 14938
[15:26:12.218871] Epoch: [10]  [160/345]  eta: 0:02:19  lr: 0.000065  loss: 0.8597 (0.8621)  time: 0.7519  data: 0.0001  max mem: 14938
[15:26:27.249270] Epoch: [10]  [180/345]  eta: 0:02:03  lr: 0.000066  loss: 0.8462 (0.8617)  time: 0.7515  data: 0.0001  max mem: 14938
[15:26:42.283437] Epoch: [10]  [200/345]  eta: 0:01:48  lr: 0.000066  loss: 0.8403 (0.8607)  time: 0.7517  data: 0.0001  max mem: 14938
[15:26:57.313294] Epoch: [10]  [220/345]  eta: 0:01:33  lr: 0.000066  loss: 0.8435 (0.8593)  time: 0.7514  data: 0.0001  max mem: 14938
[15:27:12.307335] Epoch: [10]  [240/345]  eta: 0:01:18  lr: 0.000067  loss: 0.8499 (0.8589)  time: 0.7497  data: 0.0001  max mem: 14938
[15:27:27.385739] Epoch: [10]  [260/345]  eta: 0:01:03  lr: 0.000067  loss: 0.8565 (0.8588)  time: 0.7539  data: 0.0001  max mem: 14938
[15:27:42.396560] Epoch: [10]  [280/345]  eta: 0:00:48  lr: 0.000068  loss: 0.8360 (0.8578)  time: 0.7505  data: 0.0001  max mem: 14938
[15:27:57.398128] Epoch: [10]  [300/345]  eta: 0:00:33  lr: 0.000068  loss: 0.8295 (0.8567)  time: 0.7500  data: 0.0001  max mem: 14938
[15:28:12.394664] Epoch: [10]  [320/345]  eta: 0:00:18  lr: 0.000068  loss: 0.8338 (0.8561)  time: 0.7498  data: 0.0001  max mem: 14938
[15:28:27.393963] Epoch: [10]  [340/345]  eta: 0:00:03  lr: 0.000069  loss: 0.8347 (0.8551)  time: 0.7499  data: 0.0001  max mem: 14938
[15:28:30.393437] Epoch: [10]  [344/345]  eta: 0:00:00  lr: 0.000069  loss: 0.8335 (0.8549)  time: 0.7498  data: 0.0001  max mem: 14938
[15:28:30.455987] Epoch: [10] Total time: 0:04:19 (0.7514 s / it)
[15:28:30.456337] Averaged stats: lr: 0.000069  loss: 0.8335 (0.8549)
[15:28:30.794641] Test:  [  0/345]  eta: 0:01:55  loss: 0.7748 (0.7748)  time: 0.3341  data: 0.1524  max mem: 14938
[15:28:32.631346] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8165 (0.8110)  time: 0.1973  data: 0.0139  max mem: 14938
[15:28:34.470491] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8165 (0.8111)  time: 0.1837  data: 0.0001  max mem: 14938
[15:28:36.313986] Test:  [ 30/345]  eta: 0:00:59  loss: 0.8146 (0.8125)  time: 0.1841  data: 0.0001  max mem: 14938
[15:28:38.160028] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8220 (0.8153)  time: 0.1844  data: 0.0001  max mem: 14938
[15:28:40.009658] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8127 (0.8133)  time: 0.1847  data: 0.0001  max mem: 14938
[15:28:41.863637] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8047 (0.8146)  time: 0.1851  data: 0.0001  max mem: 14938
[15:28:43.718898] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8124 (0.8148)  time: 0.1854  data: 0.0001  max mem: 14938
[15:28:45.578547] Test:  [ 80/345]  eta: 0:00:49  loss: 0.8038 (0.8135)  time: 0.1857  data: 0.0001  max mem: 14938
[15:28:47.442272] Test:  [ 90/345]  eta: 0:00:47  loss: 0.8033 (0.8135)  time: 0.1861  data: 0.0001  max mem: 14938
[15:28:49.309831] Test:  [100/345]  eta: 0:00:45  loss: 0.8252 (0.8146)  time: 0.1865  data: 0.0001  max mem: 14938
[15:28:51.179227] Test:  [110/345]  eta: 0:00:43  loss: 0.8232 (0.8150)  time: 0.1868  data: 0.0001  max mem: 14938
[15:28:53.052191] Test:  [120/345]  eta: 0:00:41  loss: 0.8147 (0.8145)  time: 0.1871  data: 0.0001  max mem: 14938
[15:28:54.927731] Test:  [130/345]  eta: 0:00:40  loss: 0.8046 (0.8140)  time: 0.1874  data: 0.0001  max mem: 14938
[15:28:56.809124] Test:  [140/345]  eta: 0:00:38  loss: 0.8042 (0.8133)  time: 0.1878  data: 0.0001  max mem: 14938
[15:28:58.693312] Test:  [150/345]  eta: 0:00:36  loss: 0.8054 (0.8130)  time: 0.1882  data: 0.0001  max mem: 14938
[15:29:00.580343] Test:  [160/345]  eta: 0:00:34  loss: 0.8155 (0.8137)  time: 0.1885  data: 0.0001  max mem: 14938
[15:29:02.471041] Test:  [170/345]  eta: 0:00:32  loss: 0.8182 (0.8136)  time: 0.1888  data: 0.0001  max mem: 14938
[15:29:04.368065] Test:  [180/345]  eta: 0:00:30  loss: 0.8157 (0.8136)  time: 0.1893  data: 0.0001  max mem: 14938
[15:29:06.266114] Test:  [190/345]  eta: 0:00:29  loss: 0.8235 (0.8146)  time: 0.1897  data: 0.0001  max mem: 14938
[15:29:08.166266] Test:  [200/345]  eta: 0:00:27  loss: 0.8281 (0.8154)  time: 0.1899  data: 0.0001  max mem: 14938
[15:29:10.072278] Test:  [210/345]  eta: 0:00:25  loss: 0.8226 (0.8156)  time: 0.1903  data: 0.0001  max mem: 14938
[15:29:11.980330] Test:  [220/345]  eta: 0:00:23  loss: 0.8152 (0.8160)  time: 0.1907  data: 0.0001  max mem: 14938
[15:29:13.893351] Test:  [230/345]  eta: 0:00:21  loss: 0.8133 (0.8169)  time: 0.1910  data: 0.0001  max mem: 14938
[15:29:15.809499] Test:  [240/345]  eta: 0:00:19  loss: 0.8132 (0.8164)  time: 0.1914  data: 0.0001  max mem: 14938
[15:29:17.730682] Test:  [250/345]  eta: 0:00:17  loss: 0.8128 (0.8164)  time: 0.1918  data: 0.0001  max mem: 14938
[15:29:19.654225] Test:  [260/345]  eta: 0:00:16  loss: 0.8154 (0.8164)  time: 0.1922  data: 0.0001  max mem: 14938
[15:29:21.580901] Test:  [270/345]  eta: 0:00:14  loss: 0.8117 (0.8162)  time: 0.1925  data: 0.0001  max mem: 14938
[15:29:23.511455] Test:  [280/345]  eta: 0:00:12  loss: 0.8117 (0.8162)  time: 0.1928  data: 0.0001  max mem: 14938
[15:29:25.444845] Test:  [290/345]  eta: 0:00:10  loss: 0.8144 (0.8163)  time: 0.1931  data: 0.0001  max mem: 14938
[15:29:27.382282] Test:  [300/345]  eta: 0:00:08  loss: 0.7977 (0.8162)  time: 0.1935  data: 0.0001  max mem: 14938
[15:29:29.322769] Test:  [310/345]  eta: 0:00:06  loss: 0.7964 (0.8154)  time: 0.1938  data: 0.0001  max mem: 14938
[15:29:31.267979] Test:  [320/345]  eta: 0:00:04  loss: 0.8071 (0.8156)  time: 0.1942  data: 0.0001  max mem: 14938
[15:29:33.215554] Test:  [330/345]  eta: 0:00:02  loss: 0.8207 (0.8155)  time: 0.1946  data: 0.0001  max mem: 14938
[15:29:35.166647] Test:  [340/345]  eta: 0:00:00  loss: 0.8169 (0.8157)  time: 0.1949  data: 0.0001  max mem: 14938
[15:29:35.948022] Test:  [344/345]  eta: 0:00:00  loss: 0.8225 (0.8157)  time: 0.1950  data: 0.0001  max mem: 14938
[15:29:36.007427] Test: Total time: 0:01:05 (0.1900 s / it)
[15:29:46.633090] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8951 (0.8951)  time: 0.3182  data: 0.1387  max mem: 14938
[15:29:48.449731] Test:  [10/57]  eta: 0:00:09  loss: 0.8862 (0.9002)  time: 0.1940  data: 0.0127  max mem: 14938
[15:29:50.270701] Test:  [20/57]  eta: 0:00:06  loss: 0.8862 (0.8962)  time: 0.1818  data: 0.0001  max mem: 14938
[15:29:52.096743] Test:  [30/57]  eta: 0:00:05  loss: 0.8026 (0.8585)  time: 0.1823  data: 0.0001  max mem: 14938
[15:29:53.928894] Test:  [40/57]  eta: 0:00:03  loss: 0.7656 (0.8359)  time: 0.1829  data: 0.0001  max mem: 14938
[15:29:55.763642] Test:  [50/57]  eta: 0:00:01  loss: 0.7633 (0.8287)  time: 0.1833  data: 0.0001  max mem: 14938
[15:29:56.753041] Test:  [56/57]  eta: 0:00:00  loss: 0.8038 (0.8334)  time: 0.1778  data: 0.0001  max mem: 14938
[15:29:56.810346] Test: Total time: 0:00:10 (0.1841 s / it)
[15:29:58.755163] Dice score of the network on the train images: 0.770876, val images: 0.798675
[15:29:58.755404] saving best_prec_model_0 @ epoch 10
[15:29:59.872078] saving best_dice_model_0 @ epoch 10
[15:30:00.919760] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[15:30:01.805767] Epoch: [11]  [  0/345]  eta: 0:05:05  lr: 0.000069  loss: 0.8824 (0.8824)  time: 0.8850  data: 0.1441  max mem: 14938
[15:30:16.671510] Epoch: [11]  [ 20/345]  eta: 0:04:03  lr: 0.000069  loss: 0.8503 (0.8491)  time: 0.7432  data: 0.0001  max mem: 14938
[15:30:31.612509] Epoch: [11]  [ 40/345]  eta: 0:03:48  lr: 0.000069  loss: 0.8257 (0.8425)  time: 0.7470  data: 0.0001  max mem: 14938
[15:30:46.581317] Epoch: [11]  [ 60/345]  eta: 0:03:33  lr: 0.000070  loss: 0.8273 (0.8392)  time: 0.7484  data: 0.0001  max mem: 14938
[15:31:01.578404] Epoch: [11]  [ 80/345]  eta: 0:03:18  lr: 0.000070  loss: 0.8424 (0.8403)  time: 0.7498  data: 0.0001  max mem: 14938
[15:31:16.626602] Epoch: [11]  [100/345]  eta: 0:03:03  lr: 0.000071  loss: 0.8310 (0.8403)  time: 0.7524  data: 0.0001  max mem: 14938
[15:31:31.690004] Epoch: [11]  [120/345]  eta: 0:02:48  lr: 0.000071  loss: 0.8369 (0.8406)  time: 0.7531  data: 0.0001  max mem: 14938
[15:31:46.744545] Epoch: [11]  [140/345]  eta: 0:02:33  lr: 0.000071  loss: 0.8468 (0.8417)  time: 0.7527  data: 0.0001  max mem: 14938
[15:32:01.772233] Epoch: [11]  [160/345]  eta: 0:02:18  lr: 0.000072  loss: 0.8387 (0.8410)  time: 0.7513  data: 0.0001  max mem: 14938
[15:32:16.805462] Epoch: [11]  [180/345]  eta: 0:02:03  lr: 0.000072  loss: 0.8282 (0.8404)  time: 0.7516  data: 0.0001  max mem: 14938
[15:32:31.827847] Epoch: [11]  [200/345]  eta: 0:01:48  lr: 0.000072  loss: 0.8277 (0.8400)  time: 0.7511  data: 0.0001  max mem: 14938
[15:32:46.837423] Epoch: [11]  [220/345]  eta: 0:01:33  lr: 0.000073  loss: 0.8350 (0.8402)  time: 0.7504  data: 0.0001  max mem: 14938
[15:33:01.821526] Epoch: [11]  [240/345]  eta: 0:01:18  lr: 0.000073  loss: 0.8308 (0.8396)  time: 0.7492  data: 0.0001  max mem: 14938
[15:33:16.806583] Epoch: [11]  [260/345]  eta: 0:01:03  lr: 0.000073  loss: 0.8262 (0.8388)  time: 0.7492  data: 0.0001  max mem: 14938
[15:33:31.787376] Epoch: [11]  [280/345]  eta: 0:00:48  lr: 0.000074  loss: 0.8362 (0.8388)  time: 0.7490  data: 0.0001  max mem: 14938
[15:33:46.754264] Epoch: [11]  [300/345]  eta: 0:00:33  lr: 0.000074  loss: 0.8218 (0.8379)  time: 0.7483  data: 0.0001  max mem: 14938
[15:34:01.729870] Epoch: [11]  [320/345]  eta: 0:00:18  lr: 0.000075  loss: 0.8365 (0.8380)  time: 0.7487  data: 0.0001  max mem: 14938
[15:34:16.695147] Epoch: [11]  [340/345]  eta: 0:00:03  lr: 0.000075  loss: 0.8262 (0.8372)  time: 0.7482  data: 0.0001  max mem: 14938
[15:34:19.692798] Epoch: [11]  [344/345]  eta: 0:00:00  lr: 0.000075  loss: 0.8262 (0.8370)  time: 0.7483  data: 0.0001  max mem: 14938
[15:34:19.757946] Epoch: [11] Total time: 0:04:18 (0.7503 s / it)
[15:34:19.758304] Averaged stats: lr: 0.000075  loss: 0.8262 (0.8370)
[15:34:20.094645] Test:  [  0/345]  eta: 0:01:54  loss: 0.8137 (0.8137)  time: 0.3325  data: 0.1511  max mem: 14938
[15:34:21.930680] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7977 (0.7993)  time: 0.1971  data: 0.0138  max mem: 14938
[15:34:23.771539] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7922 (0.7898)  time: 0.1838  data: 0.0001  max mem: 14938
[15:34:25.614763] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7704 (0.7872)  time: 0.1842  data: 0.0001  max mem: 14938
[15:34:27.460357] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8004 (0.7937)  time: 0.1844  data: 0.0001  max mem: 14938
[15:34:29.309983] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8024 (0.7944)  time: 0.1847  data: 0.0001  max mem: 14938
[15:34:31.162441] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7975 (0.7939)  time: 0.1851  data: 0.0001  max mem: 14938
[15:34:33.019697] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7975 (0.7939)  time: 0.1854  data: 0.0001  max mem: 14938
[15:34:34.879173] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7955 (0.7938)  time: 0.1858  data: 0.0001  max mem: 14938
[15:34:36.742819] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7966 (0.7947)  time: 0.1861  data: 0.0001  max mem: 14938
[15:34:38.609386] Test:  [100/345]  eta: 0:00:45  loss: 0.7966 (0.7947)  time: 0.1865  data: 0.0001  max mem: 14938
[15:34:40.478214] Test:  [110/345]  eta: 0:00:43  loss: 0.7987 (0.7951)  time: 0.1867  data: 0.0001  max mem: 14938
[15:34:42.352468] Test:  [120/345]  eta: 0:00:41  loss: 0.7983 (0.7946)  time: 0.1871  data: 0.0001  max mem: 14938
[15:34:44.229966] Test:  [130/345]  eta: 0:00:40  loss: 0.7953 (0.7947)  time: 0.1875  data: 0.0001  max mem: 14938
[15:34:46.110814] Test:  [140/345]  eta: 0:00:38  loss: 0.7953 (0.7947)  time: 0.1879  data: 0.0001  max mem: 14938
[15:34:47.995254] Test:  [150/345]  eta: 0:00:36  loss: 0.7924 (0.7949)  time: 0.1882  data: 0.0001  max mem: 14938
[15:34:49.881748] Test:  [160/345]  eta: 0:00:34  loss: 0.7923 (0.7944)  time: 0.1885  data: 0.0001  max mem: 14938
[15:34:51.774369] Test:  [170/345]  eta: 0:00:32  loss: 0.7895 (0.7939)  time: 0.1889  data: 0.0001  max mem: 14938
[15:34:53.670044] Test:  [180/345]  eta: 0:00:30  loss: 0.7934 (0.7944)  time: 0.1894  data: 0.0001  max mem: 14938
[15:34:55.569671] Test:  [190/345]  eta: 0:00:29  loss: 0.8017 (0.7947)  time: 0.1897  data: 0.0001  max mem: 14938
[15:34:57.472202] Test:  [200/345]  eta: 0:00:27  loss: 0.7922 (0.7944)  time: 0.1901  data: 0.0001  max mem: 14938
[15:34:59.377540] Test:  [210/345]  eta: 0:00:25  loss: 0.7926 (0.7945)  time: 0.1903  data: 0.0001  max mem: 14938
[15:35:01.287439] Test:  [220/345]  eta: 0:00:23  loss: 0.7989 (0.7942)  time: 0.1907  data: 0.0001  max mem: 14938
[15:35:03.199612] Test:  [230/345]  eta: 0:00:21  loss: 0.7903 (0.7938)  time: 0.1911  data: 0.0001  max mem: 14938
[15:35:05.117172] Test:  [240/345]  eta: 0:00:19  loss: 0.7884 (0.7937)  time: 0.1914  data: 0.0001  max mem: 14938
[15:35:07.036471] Test:  [250/345]  eta: 0:00:17  loss: 0.7780 (0.7931)  time: 0.1918  data: 0.0001  max mem: 14938
[15:35:08.958714] Test:  [260/345]  eta: 0:00:16  loss: 0.7803 (0.7930)  time: 0.1920  data: 0.0001  max mem: 14938
[15:35:10.885501] Test:  [270/345]  eta: 0:00:14  loss: 0.7878 (0.7929)  time: 0.1924  data: 0.0001  max mem: 14938
[15:35:12.815450] Test:  [280/345]  eta: 0:00:12  loss: 0.7904 (0.7932)  time: 0.1928  data: 0.0001  max mem: 14938
[15:35:14.750362] Test:  [290/345]  eta: 0:00:10  loss: 0.8037 (0.7935)  time: 0.1932  data: 0.0001  max mem: 14938
[15:35:16.688425] Test:  [300/345]  eta: 0:00:08  loss: 0.7925 (0.7934)  time: 0.1936  data: 0.0001  max mem: 14938
[15:35:18.627657] Test:  [310/345]  eta: 0:00:06  loss: 0.7914 (0.7933)  time: 0.1938  data: 0.0001  max mem: 14938
[15:35:20.570617] Test:  [320/345]  eta: 0:00:04  loss: 0.7936 (0.7936)  time: 0.1941  data: 0.0001  max mem: 14938
[15:35:22.517438] Test:  [330/345]  eta: 0:00:02  loss: 0.7936 (0.7935)  time: 0.1944  data: 0.0001  max mem: 14938
[15:35:24.467324] Test:  [340/345]  eta: 0:00:00  loss: 0.7936 (0.7935)  time: 0.1948  data: 0.0001  max mem: 14938
[15:35:25.248200] Test:  [344/345]  eta: 0:00:00  loss: 0.7891 (0.7934)  time: 0.1949  data: 0.0001  max mem: 14938
[15:35:25.306290] Test: Total time: 0:01:05 (0.1900 s / it)
[15:35:35.940600] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8555 (0.8555)  time: 0.3212  data: 0.1422  max mem: 14938
[15:35:37.756678] Test:  [10/57]  eta: 0:00:09  loss: 0.8863 (0.8844)  time: 0.1942  data: 0.0130  max mem: 14938
[15:35:39.576892] Test:  [20/57]  eta: 0:00:06  loss: 0.8803 (0.8800)  time: 0.1817  data: 0.0001  max mem: 14938
[15:35:41.402283] Test:  [30/57]  eta: 0:00:05  loss: 0.7806 (0.8433)  time: 0.1822  data: 0.0001  max mem: 14938
[15:35:43.232051] Test:  [40/57]  eta: 0:00:03  loss: 0.7618 (0.8234)  time: 0.1827  data: 0.0001  max mem: 14938
[15:35:45.066087] Test:  [50/57]  eta: 0:00:01  loss: 0.7614 (0.8169)  time: 0.1831  data: 0.0001  max mem: 14938
[15:35:46.054457] Test:  [56/57]  eta: 0:00:00  loss: 0.7913 (0.8220)  time: 0.1777  data: 0.0001  max mem: 14938
[15:35:46.110828] Test: Total time: 0:00:10 (0.1841 s / it)
[15:35:48.008611] Dice score of the network on the train images: 0.762678, val images: 0.805103
[15:35:48.008851] saving best_dice_model_0 @ epoch 11
[15:35:49.033955] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[15:35:49.922938] Epoch: [12]  [  0/345]  eta: 0:05:06  lr: 0.000075  loss: 0.8741 (0.8741)  time: 0.8878  data: 0.1469  max mem: 14938
[15:36:04.784035] Epoch: [12]  [ 20/345]  eta: 0:04:03  lr: 0.000075  loss: 0.8143 (0.8299)  time: 0.7430  data: 0.0001  max mem: 14938
[15:36:19.695944] Epoch: [12]  [ 40/345]  eta: 0:03:48  lr: 0.000076  loss: 0.8206 (0.8275)  time: 0.7455  data: 0.0001  max mem: 14938
[15:36:34.646160] Epoch: [12]  [ 60/345]  eta: 0:03:33  lr: 0.000076  loss: 0.8258 (0.8274)  time: 0.7475  data: 0.0001  max mem: 14938
[15:36:49.626985] Epoch: [12]  [ 80/345]  eta: 0:03:18  lr: 0.000076  loss: 0.8250 (0.8269)  time: 0.7490  data: 0.0001  max mem: 14938
[15:37:04.631941] Epoch: [12]  [100/345]  eta: 0:03:03  lr: 0.000077  loss: 0.8057 (0.8233)  time: 0.7502  data: 0.0001  max mem: 14938
[15:37:19.659647] Epoch: [12]  [120/345]  eta: 0:02:48  lr: 0.000077  loss: 0.8266 (0.8240)  time: 0.7513  data: 0.0001  max mem: 14938
[15:37:34.683016] Epoch: [12]  [140/345]  eta: 0:02:33  lr: 0.000078  loss: 0.8368 (0.8245)  time: 0.7511  data: 0.0001  max mem: 14938
[15:37:49.706196] Epoch: [12]  [160/345]  eta: 0:02:18  lr: 0.000078  loss: 0.8269 (0.8246)  time: 0.7511  data: 0.0001  max mem: 14938
[15:38:04.715951] Epoch: [12]  [180/345]  eta: 0:02:03  lr: 0.000078  loss: 0.8050 (0.8231)  time: 0.7504  data: 0.0001  max mem: 14938
[15:38:19.725402] Epoch: [12]  [200/345]  eta: 0:01:48  lr: 0.000079  loss: 0.8102 (0.8219)  time: 0.7504  data: 0.0001  max mem: 14938
[15:38:34.726646] Epoch: [12]  [220/345]  eta: 0:01:33  lr: 0.000079  loss: 0.8212 (0.8217)  time: 0.7500  data: 0.0001  max mem: 14938

[15:38:49.712585] Epoch: [12]  [240/345]  eta: 0:01:18  lr: 0.000079  loss: 0.8045 (0.8210)  time: 0.7493  data: 0.0001  max mem: 14938
[15:39:04.712408] Epoch: [12]  [260/345]  eta: 0:01:03  lr: 0.000080  loss: 0.8272 (0.8217)  time: 0.7499  data: 0.0001  max mem: 14938
[15:39:19.696265] Epoch: [12]  [280/345]  eta: 0:00:48  lr: 0.000080  loss: 0.8158 (0.8214)  time: 0.7491  data: 0.0001  max mem: 14938
[15:39:34.682840] Epoch: [12]  [300/345]  eta: 0:00:33  lr: 0.000080  loss: 0.8174 (0.8212)  time: 0.7493  data: 0.0001  max mem: 14938
[15:39:49.653511] Epoch: [12]  [320/345]  eta: 0:00:18  lr: 0.000081  loss: 0.8150 (0.8209)  time: 0.7485  data: 0.0001  max mem: 14938
[15:40:04.646216] Epoch: [12]  [340/345]  eta: 0:00:03  lr: 0.000081  loss: 0.8242 (0.8212)  time: 0.7496  data: 0.0001  max mem: 14938
[15:40:07.644586] Epoch: [12]  [344/345]  eta: 0:00:00  lr: 0.000081  loss: 0.8290 (0.8214)  time: 0.7498  data: 0.0001  max mem: 14938
[15:40:07.704517] Epoch: [12] Total time: 0:04:18 (0.7498 s / it)
[15:40:07.704713] Averaged stats: lr: 0.000081  loss: 0.8290 (0.8214)
[15:40:08.032742] Test:  [  0/345]  eta: 0:01:51  loss: 0.8502 (0.8502)  time: 0.3245  data: 0.1430  max mem: 14938
[15:40:09.870025] Test:  [ 10/345]  eta: 0:01:05  loss: 0.8072 (0.8023)  time: 0.1964  data: 0.0131  max mem: 14938
[15:40:11.709797] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7920 (0.7968)  time: 0.1838  data: 0.0001  max mem: 14938
[15:40:13.553926] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7872 (0.7967)  time: 0.1841  data: 0.0001  max mem: 14938
[15:40:15.402359] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7918 (0.7964)  time: 0.1845  data: 0.0001  max mem: 14938
[15:40:17.252400] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7865 (0.7959)  time: 0.1848  data: 0.0001  max mem: 14938
[15:40:19.106099] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8074 (0.7987)  time: 0.1851  data: 0.0001  max mem: 14938
[15:40:20.961417] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8061 (0.7978)  time: 0.1854  data: 0.0001  max mem: 14938
[15:40:22.820528] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7904 (0.7979)  time: 0.1857  data: 0.0001  max mem: 14938
[15:40:24.685367] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7961 (0.7974)  time: 0.1861  data: 0.0001  max mem: 14938
[15:40:26.554164] Test:  [100/345]  eta: 0:00:45  loss: 0.7871 (0.7963)  time: 0.1866  data: 0.0001  max mem: 14938
[15:40:28.425092] Test:  [110/345]  eta: 0:00:43  loss: 0.7829 (0.7959)  time: 0.1869  data: 0.0001  max mem: 14938
[15:40:30.298956] Test:  [120/345]  eta: 0:00:41  loss: 0.7981 (0.7963)  time: 0.1872  data: 0.0001  max mem: 14938
[15:40:32.175682] Test:  [130/345]  eta: 0:00:40  loss: 0.7981 (0.7961)  time: 0.1875  data: 0.0001  max mem: 14938
[15:40:34.056719] Test:  [140/345]  eta: 0:00:38  loss: 0.7913 (0.7960)  time: 0.1878  data: 0.0001  max mem: 14938
[15:40:35.941423] Test:  [150/345]  eta: 0:00:36  loss: 0.7969 (0.7957)  time: 0.1882  data: 0.0001  max mem: 14938
[15:40:37.829692] Test:  [160/345]  eta: 0:00:34  loss: 0.7921 (0.7956)  time: 0.1886  data: 0.0001  max mem: 14938
[15:40:39.721676] Test:  [170/345]  eta: 0:00:32  loss: 0.7921 (0.7953)  time: 0.1890  data: 0.0001  max mem: 14938
[15:40:41.617725] Test:  [180/345]  eta: 0:00:30  loss: 0.7910 (0.7952)  time: 0.1893  data: 0.0001  max mem: 14938
[15:40:43.517367] Test:  [190/345]  eta: 0:00:29  loss: 0.7845 (0.7948)  time: 0.1897  data: 0.0001  max mem: 14938
[15:40:45.419428] Test:  [200/345]  eta: 0:00:27  loss: 0.7926 (0.7951)  time: 0.1900  data: 0.0001  max mem: 14938
[15:40:47.325344] Test:  [210/345]  eta: 0:00:25  loss: 0.7930 (0.7949)  time: 0.1903  data: 0.0001  max mem: 14938
[15:40:49.233534] Test:  [220/345]  eta: 0:00:23  loss: 0.7943 (0.7952)  time: 0.1907  data: 0.0001  max mem: 14938
[15:40:51.146479] Test:  [230/345]  eta: 0:00:21  loss: 0.7907 (0.7949)  time: 0.1910  data: 0.0001  max mem: 14938
[15:40:53.063997] Test:  [240/345]  eta: 0:00:19  loss: 0.7906 (0.7949)  time: 0.1915  data: 0.0001  max mem: 14938
[15:40:54.985026] Test:  [250/345]  eta: 0:00:17  loss: 0.8008 (0.7953)  time: 0.1919  data: 0.0001  max mem: 14938
[15:40:56.910622] Test:  [260/345]  eta: 0:00:16  loss: 0.8003 (0.7953)  time: 0.1923  data: 0.0001  max mem: 14938
[15:40:58.838613] Test:  [270/345]  eta: 0:00:14  loss: 0.7940 (0.7952)  time: 0.1926  data: 0.0001  max mem: 14938
[15:41:00.771004] Test:  [280/345]  eta: 0:00:12  loss: 0.8040 (0.7959)  time: 0.1929  data: 0.0001  max mem: 14938
[15:41:02.705534] Test:  [290/345]  eta: 0:00:10  loss: 0.8091 (0.7959)  time: 0.1933  data: 0.0001  max mem: 14938
[15:41:04.643520] Test:  [300/345]  eta: 0:00:08  loss: 0.7943 (0.7959)  time: 0.1936  data: 0.0001  max mem: 14938
[15:41:06.583104] Test:  [310/345]  eta: 0:00:06  loss: 0.7888 (0.7956)  time: 0.1938  data: 0.0001  max mem: 14938
[15:41:08.525940] Test:  [320/345]  eta: 0:00:04  loss: 0.7896 (0.7956)  time: 0.1941  data: 0.0001  max mem: 14938
[15:41:10.472752] Test:  [330/345]  eta: 0:00:02  loss: 0.7919 (0.7954)  time: 0.1944  data: 0.0001  max mem: 14938
[15:41:12.422421] Test:  [340/345]  eta: 0:00:00  loss: 0.7919 (0.7954)  time: 0.1948  data: 0.0001  max mem: 14938
[15:41:13.204485] Test:  [344/345]  eta: 0:00:00  loss: 0.7930 (0.7955)  time: 0.1949  data: 0.0001  max mem: 14938
[15:41:13.261726] Test: Total time: 0:01:05 (0.1900 s / it)
[15:41:23.779715] Test:  [ 0/57]  eta: 0:00:17  loss: 0.8380 (0.8380)  time: 0.3148  data: 0.1349  max mem: 14938
[15:41:25.596379] Test:  [10/57]  eta: 0:00:09  loss: 0.8544 (0.8761)  time: 0.1937  data: 0.0123  max mem: 14938
[15:41:27.417399] Test:  [20/57]  eta: 0:00:06  loss: 0.8655 (0.8706)  time: 0.1818  data: 0.0001  max mem: 14938
[15:41:29.241197] Test:  [30/57]  eta: 0:00:05  loss: 0.7743 (0.8346)  time: 0.1822  data: 0.0001  max mem: 14938
[15:41:31.069633] Test:  [40/57]  eta: 0:00:03  loss: 0.7557 (0.8140)  time: 0.1826  data: 0.0001  max mem: 14938
[15:41:32.903318] Test:  [50/57]  eta: 0:00:01  loss: 0.7438 (0.8066)  time: 0.1831  data: 0.0001  max mem: 14938
[15:41:33.892327] Test:  [56/57]  eta: 0:00:00  loss: 0.7973 (0.8112)  time: 0.1777  data: 0.0001  max mem: 14938
[15:41:33.945771] Test: Total time: 0:00:10 (0.1839 s / it)
[15:41:35.751279] Dice score of the network on the train images: 0.730497, val images: 0.796921
[15:41:35.751522] saving best_rec_model_0 @ epoch 12
[15:41:36.842552] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[15:41:37.716854] Epoch: [13]  [  0/345]  eta: 0:05:01  lr: 0.000081  loss: 0.8171 (0.8171)  time: 0.8731  data: 0.1339  max mem: 14938
[15:41:52.571956] Epoch: [13]  [ 20/345]  eta: 0:04:03  lr: 0.000082  loss: 0.8106 (0.8071)  time: 0.7427  data: 0.0001  max mem: 14938
[15:42:07.493043] Epoch: [13]  [ 40/345]  eta: 0:03:47  lr: 0.000082  loss: 0.8121 (0.8119)  time: 0.7460  data: 0.0001  max mem: 14938
[15:42:22.447101] Epoch: [13]  [ 60/345]  eta: 0:03:33  lr: 0.000082  loss: 0.8425 (0.8215)  time: 0.7477  data: 0.0001  max mem: 14938
[15:42:37.421697] Epoch: [13]  [ 80/345]  eta: 0:03:18  lr: 0.000083  loss: 0.8152 (0.8214)  time: 0.7487  data: 0.0001  max mem: 14938
[15:42:52.440767] Epoch: [13]  [100/345]  eta: 0:03:03  lr: 0.000083  loss: 0.8163 (0.8205)  time: 0.7509  data: 0.0001  max mem: 14938
[15:43:07.472255] Epoch: [13]  [120/345]  eta: 0:02:48  lr: 0.000083  loss: 0.8021 (0.8188)  time: 0.7515  data: 0.0001  max mem: 14938
[15:43:22.492603] Epoch: [13]  [140/345]  eta: 0:02:33  lr: 0.000084  loss: 0.8113 (0.8184)  time: 0.7509  data: 0.0001  max mem: 14938
[15:43:37.504267] Epoch: [13]  [160/345]  eta: 0:02:18  lr: 0.000084  loss: 0.8431 (0.8223)  time: 0.7505  data: 0.0001  max mem: 14938
[15:43:52.513687] Epoch: [13]  [180/345]  eta: 0:02:03  lr: 0.000085  loss: 0.8142 (0.8217)  time: 0.7504  data: 0.0001  max mem: 14938
[15:44:07.519528] Epoch: [13]  [200/345]  eta: 0:01:48  lr: 0.000085  loss: 0.8038 (0.8202)  time: 0.7502  data: 0.0001  max mem: 14938
[15:44:22.513473] Epoch: [13]  [220/345]  eta: 0:01:33  lr: 0.000085  loss: 0.8013 (0.8192)  time: 0.7497  data: 0.0001  max mem: 14938
[15:44:37.526169] Epoch: [13]  [240/345]  eta: 0:01:18  lr: 0.000086  loss: 0.8188 (0.8195)  time: 0.7506  data: 0.0001  max mem: 14938
[15:44:52.540495] Epoch: [13]  [260/345]  eta: 0:01:03  lr: 0.000086  loss: 0.8029 (0.8186)  time: 0.7507  data: 0.0001  max mem: 14938
[15:45:07.535363] Epoch: [13]  [280/345]  eta: 0:00:48  lr: 0.000086  loss: 0.8025 (0.8180)  time: 0.7497  data: 0.0001  max mem: 14938
[15:45:22.533577] Epoch: [13]  [300/345]  eta: 0:00:33  lr: 0.000087  loss: 0.8205 (0.8178)  time: 0.7499  data: 0.0001  max mem: 14938
[15:45:37.535311] Epoch: [13]  [320/345]  eta: 0:00:18  lr: 0.000087  loss: 0.8109 (0.8175)  time: 0.7500  data: 0.0001  max mem: 14938
[15:45:52.535353] Epoch: [13]  [340/345]  eta: 0:00:03  lr: 0.000087  loss: 0.8158 (0.8171)  time: 0.7500  data: 0.0001  max mem: 14938
[15:45:55.535482] Epoch: [13]  [344/345]  eta: 0:00:00  lr: 0.000087  loss: 0.8164 (0.8171)  time: 0.7500  data: 0.0001  max mem: 14938
[15:45:55.596550] Epoch: [13] Total time: 0:04:18 (0.7500 s / it)
[15:45:55.596953] Averaged stats: lr: 0.000087  loss: 0.8164 (0.8171)
[15:45:55.932164] Test:  [  0/345]  eta: 0:01:54  loss: 0.8091 (0.8091)  time: 0.3307  data: 0.1491  max mem: 14938
[15:45:57.769845] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8066 (0.8051)  time: 0.1970  data: 0.0136  max mem: 14938
[15:45:59.609376] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8046 (0.8060)  time: 0.1838  data: 0.0001  max mem: 14938
[15:46:01.452205] Test:  [ 30/345]  eta: 0:00:59  loss: 0.8046 (0.8084)  time: 0.1841  data: 0.0001  max mem: 14938
[15:46:03.297746] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8001 (0.8079)  time: 0.1844  data: 0.0001  max mem: 14938
[15:46:05.149301] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8060 (0.8101)  time: 0.1848  data: 0.0001  max mem: 14938
[15:46:07.003041] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8060 (0.8087)  time: 0.1852  data: 0.0001  max mem: 14938
[15:46:08.862369] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8015 (0.8090)  time: 0.1856  data: 0.0001  max mem: 14938
[15:46:10.723028] Test:  [ 80/345]  eta: 0:00:49  loss: 0.8105 (0.8110)  time: 0.1859  data: 0.0001  max mem: 14938
[15:46:12.589070] Test:  [ 90/345]  eta: 0:00:47  loss: 0.8101 (0.8090)  time: 0.1863  data: 0.0001  max mem: 14938
[15:46:14.454147] Test:  [100/345]  eta: 0:00:45  loss: 0.8028 (0.8086)  time: 0.1865  data: 0.0001  max mem: 14938
[15:46:16.324750] Test:  [110/345]  eta: 0:00:43  loss: 0.8079 (0.8085)  time: 0.1867  data: 0.0001  max mem: 14938
[15:46:18.199649] Test:  [120/345]  eta: 0:00:42  loss: 0.8087 (0.8087)  time: 0.1872  data: 0.0001  max mem: 14938
[15:46:20.077412] Test:  [130/345]  eta: 0:00:40  loss: 0.8120 (0.8090)  time: 0.1876  data: 0.0001  max mem: 14938
[15:46:21.958058] Test:  [140/345]  eta: 0:00:38  loss: 0.8102 (0.8090)  time: 0.1879  data: 0.0001  max mem: 14938
[15:46:23.842098] Test:  [150/345]  eta: 0:00:36  loss: 0.8139 (0.8094)  time: 0.1882  data: 0.0001  max mem: 14938
[15:46:25.730614] Test:  [160/345]  eta: 0:00:34  loss: 0.8090 (0.8090)  time: 0.1886  data: 0.0001  max mem: 14938
[15:46:27.622444] Test:  [170/345]  eta: 0:00:32  loss: 0.8058 (0.8087)  time: 0.1890  data: 0.0001  max mem: 14938
[15:46:29.520768] Test:  [180/345]  eta: 0:00:30  loss: 0.8071 (0.8084)  time: 0.1895  data: 0.0001  max mem: 14938
[15:46:31.421204] Test:  [190/345]  eta: 0:00:29  loss: 0.8094 (0.8086)  time: 0.1899  data: 0.0001  max mem: 14938
[15:46:33.323439] Test:  [200/345]  eta: 0:00:27  loss: 0.8102 (0.8089)  time: 0.1901  data: 0.0001  max mem: 14938
[15:46:35.226685] Test:  [210/345]  eta: 0:00:25  loss: 0.8064 (0.8090)  time: 0.1902  data: 0.0001  max mem: 14938
[15:46:37.136561] Test:  [220/345]  eta: 0:00:23  loss: 0.8040 (0.8088)  time: 0.1906  data: 0.0001  max mem: 14938
[15:46:39.049884] Test:  [230/345]  eta: 0:00:21  loss: 0.8023 (0.8088)  time: 0.1911  data: 0.0001  max mem: 14938
[15:46:40.966839] Test:  [240/345]  eta: 0:00:19  loss: 0.8003 (0.8086)  time: 0.1915  data: 0.0001  max mem: 14938
[15:46:42.886524] Test:  [250/345]  eta: 0:00:17  loss: 0.7989 (0.8082)  time: 0.1918  data: 0.0001  max mem: 14938
[15:46:44.811264] Test:  [260/345]  eta: 0:00:16  loss: 0.7999 (0.8082)  time: 0.1922  data: 0.0001  max mem: 14938
[15:46:46.738247] Test:  [270/345]  eta: 0:00:14  loss: 0.8084 (0.8083)  time: 0.1925  data: 0.0001  max mem: 14938
[15:46:48.669506] Test:  [280/345]  eta: 0:00:12  loss: 0.8037 (0.8082)  time: 0.1929  data: 0.0001  max mem: 14938
[15:46:50.603470] Test:  [290/345]  eta: 0:00:10  loss: 0.8021 (0.8080)  time: 0.1932  data: 0.0001  max mem: 14938
[15:46:52.543596] Test:  [300/345]  eta: 0:00:08  loss: 0.8025 (0.8080)  time: 0.1936  data: 0.0001  max mem: 14938
[15:46:54.483256] Test:  [310/345]  eta: 0:00:06  loss: 0.8025 (0.8079)  time: 0.1939  data: 0.0001  max mem: 14938
[15:46:56.427083] Test:  [320/345]  eta: 0:00:04  loss: 0.8091 (0.8081)  time: 0.1941  data: 0.0001  max mem: 14938
[15:46:58.374172] Test:  [330/345]  eta: 0:00:02  loss: 0.8055 (0.8079)  time: 0.1945  data: 0.0001  max mem: 14938
[15:47:00.324178] Test:  [340/345]  eta: 0:00:00  loss: 0.7981 (0.8077)  time: 0.1948  data: 0.0001  max mem: 14938
[15:47:01.105288] Test:  [344/345]  eta: 0:00:00  loss: 0.7981 (0.8076)  time: 0.1949  data: 0.0001  max mem: 14938
[15:47:01.163741] Test: Total time: 0:01:05 (0.1900 s / it)
[15:47:11.691648] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8574 (0.8574)  time: 0.3191  data: 0.1392  max mem: 14938
[15:47:13.507780] Test:  [10/57]  eta: 0:00:09  loss: 0.8714 (0.8884)  time: 0.1940  data: 0.0127  max mem: 14938
[15:47:15.330816] Test:  [20/57]  eta: 0:00:06  loss: 0.8714 (0.8750)  time: 0.1819  data: 0.0001  max mem: 14938
[15:47:17.157967] Test:  [30/57]  eta: 0:00:05  loss: 0.7732 (0.8375)  time: 0.1825  data: 0.0001  max mem: 14938
[15:47:18.988557] Test:  [40/57]  eta: 0:00:03  loss: 0.7566 (0.8169)  time: 0.1828  data: 0.0001  max mem: 14938
[15:47:20.824396] Test:  [50/57]  eta: 0:00:01  loss: 0.7566 (0.8115)  time: 0.1833  data: 0.0001  max mem: 14938
[15:47:21.813768] Test:  [56/57]  eta: 0:00:00  loss: 0.8067 (0.8183)  time: 0.1778  data: 0.0001  max mem: 14938
[15:47:21.872118] Test: Total time: 0:00:10 (0.1842 s / it)
[15:47:23.606657] Dice score of the network on the train images: 0.707727, val images: 0.788572
[15:47:23.610780] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[15:47:24.501601] Epoch: [14]  [  0/345]  eta: 0:05:07  lr: 0.000087  loss: 0.8392 (0.8392)  time: 0.8899  data: 0.1473  max mem: 14938
[15:47:39.403847] Epoch: [14]  [ 20/345]  eta: 0:04:04  lr: 0.000088  loss: 0.8203 (0.8194)  time: 0.7451  data: 0.0001  max mem: 14938
[15:47:54.350956] Epoch: [14]  [ 40/345]  eta: 0:03:48  lr: 0.000088  loss: 0.8108 (0.8159)  time: 0.7473  data: 0.0001  max mem: 14938
[15:48:09.329846] Epoch: [14]  [ 60/345]  eta: 0:03:33  lr: 0.000089  loss: 0.8185 (0.8178)  time: 0.7489  data: 0.0001  max mem: 14938
[15:48:24.323775] Epoch: [14]  [ 80/345]  eta: 0:03:18  lr: 0.000089  loss: 0.8057 (0.8151)  time: 0.7496  data: 0.0001  max mem: 14938
[15:48:39.340692] Epoch: [14]  [100/345]  eta: 0:03:03  lr: 0.000089  loss: 0.7981 (0.8131)  time: 0.7508  data: 0.0001  max mem: 14938
[15:48:54.383499] Epoch: [14]  [120/345]  eta: 0:02:48  lr: 0.000090  loss: 0.8224 (0.8151)  time: 0.7521  data: 0.0001  max mem: 14938
[15:49:09.426898] Epoch: [14]  [140/345]  eta: 0:02:33  lr: 0.000090  loss: 0.8427 (0.8179)  time: 0.7521  data: 0.0001  max mem: 14938

[15:49:24.463199] Epoch: [14]  [160/345]  eta: 0:02:18  lr: 0.000090  loss: 0.8176 (0.8185)  time: 0.7518  data: 0.0001  max mem: 14938
[15:49:39.507638] Epoch: [14]  [180/345]  eta: 0:02:03  lr: 0.000091  loss: 0.8251 (0.8188)  time: 0.7522  data: 0.0001  max mem: 14938
[15:49:54.540475] Epoch: [14]  [200/345]  eta: 0:01:48  lr: 0.000091  loss: 0.8023 (0.8175)  time: 0.7516  data: 0.0001  max mem: 14938
[15:50:09.536054] Epoch: [14]  [220/345]  eta: 0:01:33  lr: 0.000091  loss: 0.8098 (0.8167)  time: 0.7497  data: 0.0001  max mem: 14938

[15:50:24.526510] Epoch: [14]  [240/345]  eta: 0:01:18  lr: 0.000092  loss: 0.8085 (0.8157)  time: 0.7495  data: 0.0001  max mem: 14938
[15:50:39.603060] Epoch: [14]  [260/345]  eta: 0:01:03  lr: 0.000092  loss: 0.7937 (0.8142)  time: 0.7538  data: 0.0001  max mem: 14938

[15:50:54.589185] Epoch: [14]  [280/345]  eta: 0:00:48  lr: 0.000093  loss: 0.8073 (0.8137)  time: 0.7493  data: 0.0001  max mem: 14938
[15:51:09.589411] Epoch: [14]  [300/345]  eta: 0:00:33  lr: 0.000093  loss: 0.8225 (0.8142)  time: 0.7500  data: 0.0001  max mem: 14938
[15:51:24.581376] Epoch: [14]  [320/345]  eta: 0:00:18  lr: 0.000093  loss: 0.8285 (0.8148)  time: 0.7496  data: 0.0001  max mem: 14938
[15:51:39.569356] Epoch: [14]  [340/345]  eta: 0:00:03  lr: 0.000094  loss: 0.7952 (0.8137)  time: 0.7494  data: 0.0001  max mem: 14938
[15:51:42.567240] Epoch: [14]  [344/345]  eta: 0:00:00  lr: 0.000094  loss: 0.7937 (0.8135)  time: 0.7493  data: 0.0001  max mem: 14938
[15:51:42.632366] Epoch: [14] Total time: 0:04:19 (0.7508 s / it)
[15:51:42.632704] Averaged stats: lr: 0.000094  loss: 0.7937 (0.8135)
[15:51:42.965602] Test:  [  0/345]  eta: 0:01:53  loss: 0.7571 (0.7571)  time: 0.3279  data: 0.1457  max mem: 14938
[15:51:44.802714] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7604 (0.7639)  time: 0.1967  data: 0.0133  max mem: 14938
[15:51:46.643023] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7612 (0.7664)  time: 0.1838  data: 0.0001  max mem: 14938
[15:51:48.485216] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7607 (0.7625)  time: 0.1841  data: 0.0001  max mem: 14938
[15:51:50.332409] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7574 (0.7625)  time: 0.1844  data: 0.0001  max mem: 14938
[15:51:52.183721] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7559 (0.7625)  time: 0.1849  data: 0.0001  max mem: 14938
[15:51:54.037544] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7562 (0.7636)  time: 0.1852  data: 0.0001  max mem: 14938
[15:51:55.893843] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7584 (0.7631)  time: 0.1855  data: 0.0001  max mem: 14938
[15:51:57.753615] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7635 (0.7646)  time: 0.1858  data: 0.0001  max mem: 14938
[15:51:59.618520] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7660 (0.7644)  time: 0.1862  data: 0.0001  max mem: 14938
[15:52:01.486845] Test:  [100/345]  eta: 0:00:45  loss: 0.7616 (0.7655)  time: 0.1866  data: 0.0001  max mem: 14938
[15:52:03.356831] Test:  [110/345]  eta: 0:00:43  loss: 0.7643 (0.7655)  time: 0.1869  data: 0.0001  max mem: 14938
[15:52:05.232365] Test:  [120/345]  eta: 0:00:42  loss: 0.7591 (0.7655)  time: 0.1872  data: 0.0001  max mem: 14938
[15:52:07.110999] Test:  [130/345]  eta: 0:00:40  loss: 0.7591 (0.7659)  time: 0.1877  data: 0.0001  max mem: 14938
[15:52:08.992832] Test:  [140/345]  eta: 0:00:38  loss: 0.7622 (0.7656)  time: 0.1880  data: 0.0001  max mem: 14938
[15:52:10.878460] Test:  [150/345]  eta: 0:00:36  loss: 0.7579 (0.7650)  time: 0.1883  data: 0.0001  max mem: 14938
[15:52:12.767560] Test:  [160/345]  eta: 0:00:34  loss: 0.7595 (0.7652)  time: 0.1887  data: 0.0001  max mem: 14938
[15:52:14.659357] Test:  [170/345]  eta: 0:00:32  loss: 0.7609 (0.7649)  time: 0.1890  data: 0.0001  max mem: 14938
[15:52:16.556008] Test:  [180/345]  eta: 0:00:30  loss: 0.7607 (0.7647)  time: 0.1894  data: 0.0001  max mem: 14938
[15:52:18.457435] Test:  [190/345]  eta: 0:00:29  loss: 0.7634 (0.7650)  time: 0.1899  data: 0.0001  max mem: 14938
[15:52:20.359827] Test:  [200/345]  eta: 0:00:27  loss: 0.7716 (0.7650)  time: 0.1901  data: 0.0001  max mem: 14938
[15:52:22.264667] Test:  [210/345]  eta: 0:00:25  loss: 0.7600 (0.7649)  time: 0.1903  data: 0.0001  max mem: 14938
[15:52:24.174827] Test:  [220/345]  eta: 0:00:23  loss: 0.7609 (0.7649)  time: 0.1907  data: 0.0001  max mem: 14938
[15:52:26.088500] Test:  [230/345]  eta: 0:00:21  loss: 0.7619 (0.7645)  time: 0.1911  data: 0.0001  max mem: 14938
[15:52:28.005436] Test:  [240/345]  eta: 0:00:19  loss: 0.7619 (0.7649)  time: 0.1915  data: 0.0001  max mem: 14938
[15:52:29.928789] Test:  [250/345]  eta: 0:00:17  loss: 0.7703 (0.7649)  time: 0.1920  data: 0.0001  max mem: 14938
[15:52:31.852963] Test:  [260/345]  eta: 0:00:16  loss: 0.7722 (0.7651)  time: 0.1923  data: 0.0001  max mem: 14938
[15:52:33.778038] Test:  [270/345]  eta: 0:00:14  loss: 0.7722 (0.7654)  time: 0.1924  data: 0.0001  max mem: 14938
[15:52:35.710833] Test:  [280/345]  eta: 0:00:12  loss: 0.7603 (0.7656)  time: 0.1928  data: 0.0001  max mem: 14938
[15:52:37.645265] Test:  [290/345]  eta: 0:00:10  loss: 0.7588 (0.7654)  time: 0.1933  data: 0.0001  max mem: 14938
[15:52:39.582304] Test:  [300/345]  eta: 0:00:08  loss: 0.7600 (0.7654)  time: 0.1935  data: 0.0001  max mem: 14938
[15:52:41.524236] Test:  [310/345]  eta: 0:00:06  loss: 0.7615 (0.7652)  time: 0.1939  data: 0.0001  max mem: 14938
[15:52:43.470261] Test:  [320/345]  eta: 0:00:04  loss: 0.7620 (0.7654)  time: 0.1943  data: 0.0001  max mem: 14938
[15:52:45.417189] Test:  [330/345]  eta: 0:00:02  loss: 0.7656 (0.7655)  time: 0.1946  data: 0.0001  max mem: 14938
[15:52:47.367372] Test:  [340/345]  eta: 0:00:00  loss: 0.7522 (0.7652)  time: 0.1948  data: 0.0001  max mem: 14938
[15:52:48.149294] Test:  [344/345]  eta: 0:00:00  loss: 0.7558 (0.7653)  time: 0.1950  data: 0.0001  max mem: 14938
[15:52:48.203023] Test: Total time: 0:01:05 (0.1900 s / it)
[15:52:58.686354] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8682 (0.8682)  time: 0.3217  data: 0.1421  max mem: 14938
[15:53:00.502580] Test:  [10/57]  eta: 0:00:09  loss: 0.8930 (0.8896)  time: 0.1943  data: 0.0130  max mem: 14938
[15:53:02.325981] Test:  [20/57]  eta: 0:00:06  loss: 0.8885 (0.8762)  time: 0.1819  data: 0.0001  max mem: 14938
[15:53:04.152647] Test:  [30/57]  eta: 0:00:05  loss: 0.7690 (0.8372)  time: 0.1825  data: 0.0001  max mem: 14938
[15:53:05.983202] Test:  [40/57]  eta: 0:00:03  loss: 0.7565 (0.8167)  time: 0.1828  data: 0.0001  max mem: 14938
[15:53:07.819549] Test:  [50/57]  eta: 0:00:01  loss: 0.7549 (0.8096)  time: 0.1833  data: 0.0001  max mem: 14938
[15:53:08.809989] Test:  [56/57]  eta: 0:00:00  loss: 0.7699 (0.8148)  time: 0.1779  data: 0.0001  max mem: 14938
[15:53:08.867843] Test: Total time: 0:00:10 (0.1843 s / it)
[15:53:10.685823] Dice score of the network on the train images: 0.780461, val images: 0.813192
[15:53:10.686057] saving best_prec_model_0 @ epoch 14
[15:53:11.806038] saving best_dice_model_0 @ epoch 14
[15:53:12.848935] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[15:53:13.734101] Epoch: [15]  [  0/345]  eta: 0:05:04  lr: 0.000094  loss: 0.7918 (0.7918)  time: 0.8840  data: 0.1429  max mem: 14938
[15:53:28.711984] Epoch: [15]  [ 20/345]  eta: 0:04:05  lr: 0.000094  loss: 0.7986 (0.7963)  time: 0.7488  data: 0.0001  max mem: 14938
[15:53:43.642347] Epoch: [15]  [ 40/345]  eta: 0:03:49  lr: 0.000094  loss: 0.8048 (0.8022)  time: 0.7465  data: 0.0001  max mem: 14938
[15:53:58.610755] Epoch: [15]  [ 60/345]  eta: 0:03:33  lr: 0.000095  loss: 0.8011 (0.8029)  time: 0.7484  data: 0.0001  max mem: 14938
[15:54:13.617134] Epoch: [15]  [ 80/345]  eta: 0:03:18  lr: 0.000095  loss: 0.8037 (0.8049)  time: 0.7503  data: 0.0001  max mem: 14938
[15:54:28.652144] Epoch: [15]  [100/345]  eta: 0:03:03  lr: 0.000096  loss: 0.8026 (0.8034)  time: 0.7517  data: 0.0001  max mem: 14938
[15:54:43.705115] Epoch: [15]  [120/345]  eta: 0:02:48  lr: 0.000096  loss: 0.8064 (0.8046)  time: 0.7526  data: 0.0001  max mem: 14938
[15:54:58.740153] Epoch: [15]  [140/345]  eta: 0:02:33  lr: 0.000096  loss: 0.7926 (0.8034)  time: 0.7517  data: 0.0001  max mem: 14938
[15:55:13.779933] Epoch: [15]  [160/345]  eta: 0:02:18  lr: 0.000097  loss: 0.7984 (0.8029)  time: 0.7519  data: 0.0001  max mem: 14938
[15:55:28.806232] Epoch: [15]  [180/345]  eta: 0:02:03  lr: 0.000097  loss: 0.7980 (0.8035)  time: 0.7513  data: 0.0001  max mem: 14938
[15:55:43.827797] Epoch: [15]  [200/345]  eta: 0:01:48  lr: 0.000097  loss: 0.8000 (0.8025)  time: 0.7510  data: 0.0001  max mem: 14938
[15:55:58.831121] Epoch: [15]  [220/345]  eta: 0:01:33  lr: 0.000098  loss: 0.8022 (0.8028)  time: 0.7501  data: 0.0001  max mem: 14938
[15:56:13.825629] Epoch: [15]  [240/345]  eta: 0:01:18  lr: 0.000098  loss: 0.8029 (0.8031)  time: 0.7497  data: 0.0001  max mem: 14938
[15:56:28.809427] Epoch: [15]  [260/345]  eta: 0:01:03  lr: 0.000098  loss: 0.7934 (0.8030)  time: 0.7492  data: 0.0001  max mem: 14938
[15:56:43.787430] Epoch: [15]  [280/345]  eta: 0:00:48  lr: 0.000099  loss: 0.8002 (0.8033)  time: 0.7489  data: 0.0001  max mem: 14938
[15:56:58.764898] Epoch: [15]  [300/345]  eta: 0:00:33  lr: 0.000099  loss: 0.7944 (0.8030)  time: 0.7488  data: 0.0001  max mem: 14938
[15:57:13.733597] Epoch: [15]  [320/345]  eta: 0:00:18  lr: 0.000100  loss: 0.7978 (0.8029)  time: 0.7484  data: 0.0001  max mem: 14938
[15:57:28.711977] Epoch: [15]  [340/345]  eta: 0:00:03  lr: 0.000100  loss: 0.8052 (0.8028)  time: 0.7489  data: 0.0001  max mem: 14938
[15:57:31.713004] Epoch: [15]  [344/345]  eta: 0:00:00  lr: 0.000100  loss: 0.8053 (0.8029)  time: 0.7492  data: 0.0001  max mem: 14938
[15:57:31.774330] Epoch: [15] Total time: 0:04:18 (0.7505 s / it)
[15:57:31.774548] Averaged stats: lr: 0.000100  loss: 0.8053 (0.8029)
[15:57:32.104666] Test:  [  0/345]  eta: 0:01:52  loss: 0.7672 (0.7672)  time: 0.3260  data: 0.1445  max mem: 14938
[15:57:33.943304] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7682 (0.7706)  time: 0.1967  data: 0.0132  max mem: 14938
[15:57:35.784464] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7676 (0.7647)  time: 0.1839  data: 0.0001  max mem: 14938
[15:57:37.628676] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7599 (0.7656)  time: 0.1842  data: 0.0001  max mem: 14938
[15:57:39.476149] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7678 (0.7653)  time: 0.1845  data: 0.0001  max mem: 14938
[15:57:41.328294] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7679 (0.7673)  time: 0.1849  data: 0.0001  max mem: 14938
[15:57:43.182865] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7691 (0.7681)  time: 0.1853  data: 0.0001  max mem: 14938
[15:57:45.041359] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7668 (0.7674)  time: 0.1856  data: 0.0001  max mem: 14938
[15:57:46.901702] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7574 (0.7663)  time: 0.1859  data: 0.0001  max mem: 14938
[15:57:48.764999] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7546 (0.7653)  time: 0.1861  data: 0.0001  max mem: 14938
[15:57:50.632027] Test:  [100/345]  eta: 0:00:45  loss: 0.7628 (0.7659)  time: 0.1865  data: 0.0001  max mem: 14938
[15:57:52.502964] Test:  [110/345]  eta: 0:00:43  loss: 0.7693 (0.7663)  time: 0.1868  data: 0.0001  max mem: 14938
[15:57:54.378215] Test:  [120/345]  eta: 0:00:42  loss: 0.7717 (0.7663)  time: 0.1872  data: 0.0001  max mem: 14938
[15:57:56.257149] Test:  [130/345]  eta: 0:00:40  loss: 0.7575 (0.7659)  time: 0.1876  data: 0.0001  max mem: 14938
[15:57:58.139305] Test:  [140/345]  eta: 0:00:38  loss: 0.7613 (0.7662)  time: 0.1880  data: 0.0001  max mem: 14938
[15:58:00.023593] Test:  [150/345]  eta: 0:00:36  loss: 0.7652 (0.7660)  time: 0.1883  data: 0.0001  max mem: 14938
[15:58:01.912570] Test:  [160/345]  eta: 0:00:34  loss: 0.7685 (0.7670)  time: 0.1886  data: 0.0001  max mem: 14938
[15:58:03.803359] Test:  [170/345]  eta: 0:00:32  loss: 0.7729 (0.7672)  time: 0.1889  data: 0.0001  max mem: 14938
[15:58:05.703089] Test:  [180/345]  eta: 0:00:30  loss: 0.7724 (0.7675)  time: 0.1895  data: 0.0001  max mem: 14938
[15:58:07.602744] Test:  [190/345]  eta: 0:00:29  loss: 0.7687 (0.7676)  time: 0.1899  data: 0.0001  max mem: 14938
[15:58:09.506578] Test:  [200/345]  eta: 0:00:27  loss: 0.7638 (0.7674)  time: 0.1901  data: 0.0001  max mem: 14938
[15:58:11.413379] Test:  [210/345]  eta: 0:00:25  loss: 0.7598 (0.7671)  time: 0.1905  data: 0.0001  max mem: 14938
[15:58:13.323108] Test:  [220/345]  eta: 0:00:23  loss: 0.7598 (0.7671)  time: 0.1908  data: 0.0001  max mem: 14938
[15:58:15.237815] Test:  [230/345]  eta: 0:00:21  loss: 0.7684 (0.7674)  time: 0.1912  data: 0.0001  max mem: 14938
[15:58:17.155134] Test:  [240/345]  eta: 0:00:19  loss: 0.7742 (0.7676)  time: 0.1915  data: 0.0001  max mem: 14938
[15:58:19.075049] Test:  [250/345]  eta: 0:00:17  loss: 0.7711 (0.7678)  time: 0.1918  data: 0.0001  max mem: 14938
[15:58:20.998166] Test:  [260/345]  eta: 0:00:16  loss: 0.7622 (0.7674)  time: 0.1921  data: 0.0001  max mem: 14938
[15:58:22.926641] Test:  [270/345]  eta: 0:00:14  loss: 0.7548 (0.7675)  time: 0.1925  data: 0.0001  max mem: 14938
[15:58:24.859204] Test:  [280/345]  eta: 0:00:12  loss: 0.7795 (0.7679)  time: 0.1930  data: 0.0001  max mem: 14938
[15:58:26.793554] Test:  [290/345]  eta: 0:00:10  loss: 0.7810 (0.7683)  time: 0.1933  data: 0.0001  max mem: 14938
[15:58:28.734003] Test:  [300/345]  eta: 0:00:08  loss: 0.7750 (0.7683)  time: 0.1937  data: 0.0001  max mem: 14938
[15:58:30.676291] Test:  [310/345]  eta: 0:00:06  loss: 0.7648 (0.7681)  time: 0.1941  data: 0.0001  max mem: 14938
[15:58:32.620577] Test:  [320/345]  eta: 0:00:04  loss: 0.7636 (0.7680)  time: 0.1943  data: 0.0001  max mem: 14938
[15:58:34.566126] Test:  [330/345]  eta: 0:00:02  loss: 0.7635 (0.7680)  time: 0.1944  data: 0.0001  max mem: 14938
[15:58:36.517069] Test:  [340/345]  eta: 0:00:00  loss: 0.7622 (0.7679)  time: 0.1948  data: 0.0001  max mem: 14938
[15:58:37.298542] Test:  [344/345]  eta: 0:00:00  loss: 0.7622 (0.7679)  time: 0.1949  data: 0.0001  max mem: 14938
[15:58:37.353072] Test: Total time: 0:01:05 (0.1901 s / it)
[15:58:47.905625] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8647 (0.8647)  time: 0.3160  data: 0.1365  max mem: 14938
[15:58:49.722533] Test:  [10/57]  eta: 0:00:09  loss: 0.8733 (0.8771)  time: 0.1938  data: 0.0125  max mem: 14938
[15:58:51.543249] Test:  [20/57]  eta: 0:00:06  loss: 0.8733 (0.8706)  time: 0.1818  data: 0.0001  max mem: 14938
[15:58:53.369288] Test:  [30/57]  eta: 0:00:05  loss: 0.7656 (0.8326)  time: 0.1823  data: 0.0001  max mem: 14938
[15:58:55.199297] Test:  [40/57]  eta: 0:00:03  loss: 0.7553 (0.8132)  time: 0.1827  data: 0.0001  max mem: 14938
[15:58:57.036307] Test:  [50/57]  eta: 0:00:01  loss: 0.7542 (0.8085)  time: 0.1833  data: 0.0001  max mem: 14938
[15:58:58.025825] Test:  [56/57]  eta: 0:00:00  loss: 0.7825 (0.8156)  time: 0.1779  data: 0.0001  max mem: 14938
[15:58:58.082245] Test: Total time: 0:00:10 (0.1841 s / it)
[15:58:59.856261] Dice score of the network on the train images: 0.777426, val images: 0.803989
[15:58:59.860584] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[15:59:00.746320] Epoch: [16]  [  0/345]  eta: 0:05:05  lr: 0.000100  loss: 0.7736 (0.7736)  time: 0.8846  data: 0.1417  max mem: 14938
[15:59:15.642276] Epoch: [16]  [ 20/345]  eta: 0:04:04  lr: 0.000100  loss: 0.7877 (0.7892)  time: 0.7447  data: 0.0001  max mem: 14938
[15:59:30.600521] Epoch: [16]  [ 40/345]  eta: 0:03:48  lr: 0.000101  loss: 0.8014 (0.7956)  time: 0.7479  data: 0.0001  max mem: 14938
[15:59:45.571267] Epoch: [16]  [ 60/345]  eta: 0:03:33  lr: 0.000101  loss: 0.7860 (0.7940)  time: 0.7485  data: 0.0001  max mem: 14938
[16:00:00.568960] Epoch: [16]  [ 80/345]  eta: 0:03:18  lr: 0.000101  loss: 0.7846 (0.7918)  time: 0.7498  data: 0.0001  max mem: 14938
[16:00:15.594024] Epoch: [16]  [100/345]  eta: 0:03:03  lr: 0.000102  loss: 0.7848 (0.7903)  time: 0.7512  data: 0.0001  max mem: 14938
[16:00:30.635982] Epoch: [16]  [120/345]  eta: 0:02:48  lr: 0.000102  loss: 0.7874 (0.7899)  time: 0.7521  data: 0.0001  max mem: 14938
[16:00:45.667279] Epoch: [16]  [140/345]  eta: 0:02:33  lr: 0.000103  loss: 0.7871 (0.7899)  time: 0.7515  data: 0.0001  max mem: 14938
[16:01:00.693617] Epoch: [16]  [160/345]  eta: 0:02:18  lr: 0.000103  loss: 0.7865 (0.7899)  time: 0.7513  data: 0.0001  max mem: 14938
[16:01:15.717885] Epoch: [16]  [180/345]  eta: 0:02:03  lr: 0.000103  loss: 0.7796 (0.7890)  time: 0.7512  data: 0.0001  max mem: 14938
[16:01:30.735287] Epoch: [16]  [200/345]  eta: 0:01:48  lr: 0.000104  loss: 0.7849 (0.7887)  time: 0.7508  data: 0.0001  max mem: 14938
[16:01:45.743905] Epoch: [16]  [220/345]  eta: 0:01:33  lr: 0.000104  loss: 0.7858 (0.7884)  time: 0.7504  data: 0.0001  max mem: 14938
[16:02:00.740139] Epoch: [16]  [240/345]  eta: 0:01:18  lr: 0.000104  loss: 0.8000 (0.7893)  time: 0.7498  data: 0.0001  max mem: 14938
[16:02:15.740421] Epoch: [16]  [260/345]  eta: 0:01:03  lr: 0.000105  loss: 0.8121 (0.7909)  time: 0.7500  data: 0.0001  max mem: 14938
[16:02:30.739017] Epoch: [16]  [280/345]  eta: 0:00:48  lr: 0.000105  loss: 0.8042 (0.7920)  time: 0.7499  data: 0.0001  max mem: 14938
[16:02:45.733334] Epoch: [16]  [300/345]  eta: 0:00:33  lr: 0.000105  loss: 0.8142 (0.7931)  time: 0.7497  data: 0.0001  max mem: 14938
[16:03:00.717718] Epoch: [16]  [320/345]  eta: 0:00:18  lr: 0.000106  loss: 0.7999 (0.7939)  time: 0.7492  data: 0.0001  max mem: 14938
[16:03:15.706863] Epoch: [16]  [340/345]  eta: 0:00:03  lr: 0.000106  loss: 0.8001 (0.7945)  time: 0.7494  data: 0.0001  max mem: 14938
[16:03:18.708475] Epoch: [16]  [344/345]  eta: 0:00:00  lr: 0.000106  loss: 0.7918 (0.7945)  time: 0.7496  data: 0.0001  max mem: 14938
[16:03:18.772847] Epoch: [16] Total time: 0:04:18 (0.7505 s / it)
[16:03:18.773343] Averaged stats: lr: 0.000106  loss: 0.7918 (0.7945)
[16:03:19.108778] Test:  [  0/345]  eta: 0:01:53  loss: 0.7568 (0.7568)  time: 0.3304  data: 0.1489  max mem: 14938
[16:03:20.945310] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7570 (0.7624)  time: 0.1969  data: 0.0136  max mem: 14938
[16:03:22.786360] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7610 (0.7640)  time: 0.1838  data: 0.0001  max mem: 14938
[16:03:24.628831] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7610 (0.7640)  time: 0.1841  data: 0.0001  max mem: 14938
[16:03:26.475145] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7604 (0.7634)  time: 0.1844  data: 0.0001  max mem: 14938
[16:03:28.328344] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7604 (0.7635)  time: 0.1849  data: 0.0001  max mem: 14938
[16:03:30.182925] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7607 (0.7634)  time: 0.1853  data: 0.0001  max mem: 14938
[16:03:32.040130] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7573 (0.7622)  time: 0.1855  data: 0.0001  max mem: 14938
[16:03:33.899730] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7601 (0.7622)  time: 0.1858  data: 0.0001  max mem: 14938
[16:03:35.765605] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7586 (0.7615)  time: 0.1862  data: 0.0001  max mem: 14938
[16:03:37.632845] Test:  [100/345]  eta: 0:00:45  loss: 0.7541 (0.7611)  time: 0.1866  data: 0.0001  max mem: 14938
[16:03:39.502813] Test:  [110/345]  eta: 0:00:43  loss: 0.7537 (0.7611)  time: 0.1868  data: 0.0001  max mem: 14938
[16:03:41.377009] Test:  [120/345]  eta: 0:00:42  loss: 0.7544 (0.7608)  time: 0.1871  data: 0.0001  max mem: 14938
[16:03:43.252730] Test:  [130/345]  eta: 0:00:40  loss: 0.7544 (0.7605)  time: 0.1874  data: 0.0001  max mem: 14938
[16:03:45.133850] Test:  [140/345]  eta: 0:00:38  loss: 0.7635 (0.7609)  time: 0.1878  data: 0.0001  max mem: 14938
[16:03:47.017400] Test:  [150/345]  eta: 0:00:36  loss: 0.7711 (0.7619)  time: 0.1882  data: 0.0001  max mem: 14938
[16:03:48.904984] Test:  [160/345]  eta: 0:00:34  loss: 0.7727 (0.7624)  time: 0.1885  data: 0.0001  max mem: 14938
[16:03:50.795444] Test:  [170/345]  eta: 0:00:32  loss: 0.7589 (0.7622)  time: 0.1889  data: 0.0001  max mem: 14938
[16:03:52.691874] Test:  [180/345]  eta: 0:00:30  loss: 0.7589 (0.7620)  time: 0.1893  data: 0.0001  max mem: 14938
[16:03:54.592166] Test:  [190/345]  eta: 0:00:29  loss: 0.7540 (0.7613)  time: 0.1898  data: 0.0001  max mem: 14938
[16:03:56.494225] Test:  [200/345]  eta: 0:00:27  loss: 0.7501 (0.7611)  time: 0.1901  data: 0.0001  max mem: 14938
[16:03:58.402330] Test:  [210/345]  eta: 0:00:25  loss: 0.7561 (0.7611)  time: 0.1905  data: 0.0001  max mem: 14938
[16:04:00.313025] Test:  [220/345]  eta: 0:00:23  loss: 0.7598 (0.7612)  time: 0.1909  data: 0.0001  max mem: 14938
[16:04:02.227719] Test:  [230/345]  eta: 0:00:21  loss: 0.7598 (0.7615)  time: 0.1912  data: 0.0001  max mem: 14938
[16:04:04.145114] Test:  [240/345]  eta: 0:00:19  loss: 0.7573 (0.7613)  time: 0.1916  data: 0.0001  max mem: 14938
[16:04:06.064261] Test:  [250/345]  eta: 0:00:17  loss: 0.7600 (0.7614)  time: 0.1918  data: 0.0001  max mem: 14938
[16:04:07.988100] Test:  [260/345]  eta: 0:00:16  loss: 0.7610 (0.7612)  time: 0.1921  data: 0.0001  max mem: 14938
[16:04:09.915606] Test:  [270/345]  eta: 0:00:14  loss: 0.7520 (0.7609)  time: 0.1925  data: 0.0001  max mem: 14938
[16:04:11.848598] Test:  [280/345]  eta: 0:00:12  loss: 0.7464 (0.7606)  time: 0.1930  data: 0.0001  max mem: 14938
[16:04:13.783187] Test:  [290/345]  eta: 0:00:10  loss: 0.7564 (0.7609)  time: 0.1933  data: 0.0001  max mem: 14938
[16:04:15.721518] Test:  [300/345]  eta: 0:00:08  loss: 0.7647 (0.7610)  time: 0.1936  data: 0.0001  max mem: 14938
[16:04:17.660264] Test:  [310/345]  eta: 0:00:06  loss: 0.7594 (0.7610)  time: 0.1938  data: 0.0001  max mem: 14938
[16:04:19.605386] Test:  [320/345]  eta: 0:00:04  loss: 0.7571 (0.7610)  time: 0.1941  data: 0.0001  max mem: 14938
[16:04:21.553319] Test:  [330/345]  eta: 0:00:02  loss: 0.7589 (0.7612)  time: 0.1946  data: 0.0001  max mem: 14938
[16:04:23.503989] Test:  [340/345]  eta: 0:00:00  loss: 0.7589 (0.7611)  time: 0.1949  data: 0.0001  max mem: 14938
[16:04:24.285761] Test:  [344/345]  eta: 0:00:00  loss: 0.7592 (0.7612)  time: 0.1950  data: 0.0001  max mem: 14938
[16:04:24.344541] Test: Total time: 0:01:05 (0.1900 s / it)
[16:04:34.832135] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8811 (0.8811)  time: 0.3191  data: 0.1394  max mem: 14938
[16:04:36.648362] Test:  [10/57]  eta: 0:00:09  loss: 0.8841 (0.8881)  time: 0.1940  data: 0.0127  max mem: 14938
[16:04:38.469566] Test:  [20/57]  eta: 0:00:06  loss: 0.8718 (0.8689)  time: 0.1818  data: 0.0001  max mem: 14938
[16:04:40.293154] Test:  [30/57]  eta: 0:00:05  loss: 0.7733 (0.8313)  time: 0.1822  data: 0.0001  max mem: 14938
[16:04:42.122013] Test:  [40/57]  eta: 0:00:03  loss: 0.7477 (0.8119)  time: 0.1826  data: 0.0001  max mem: 14938
[16:04:43.956219] Test:  [50/57]  eta: 0:00:01  loss: 0.7484 (0.8057)  time: 0.1831  data: 0.0001  max mem: 14938
[16:04:44.945906] Test:  [56/57]  eta: 0:00:00  loss: 0.7765 (0.8119)  time: 0.1778  data: 0.0001  max mem: 14938
[16:04:45.013169] Test: Total time: 0:00:10 (0.1842 s / it)
[16:04:46.802370] Dice score of the network on the train images: 0.780122, val images: 0.810923
[16:04:46.802604] saving best_prec_model_0 @ epoch 16
[16:04:47.913678] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[16:04:48.799493] Epoch: [17]  [  0/345]  eta: 0:05:05  lr: 0.000106  loss: 0.7698 (0.7698)  time: 0.8846  data: 0.1441  max mem: 14938
[16:05:03.664478] Epoch: [17]  [ 20/345]  eta: 0:04:03  lr: 0.000107  loss: 0.8069 (0.8108)  time: 0.7432  data: 0.0001  max mem: 14938
[16:05:18.579302] Epoch: [17]  [ 40/345]  eta: 0:03:48  lr: 0.000107  loss: 0.8053 (0.8116)  time: 0.7457  data: 0.0001  max mem: 14938

[16:05:33.647198] Epoch: [17]  [ 60/345]  eta: 0:03:33  lr: 0.000107  loss: 0.7894 (0.8066)  time: 0.7533  data: 0.0001  max mem: 14938
[16:05:48.622149] Epoch: [17]  [ 80/345]  eta: 0:03:18  lr: 0.000108  loss: 0.7935 (0.8032)  time: 0.7487  data: 0.0001  max mem: 14938
[16:06:03.632743] Epoch: [17]  [100/345]  eta: 0:03:03  lr: 0.000108  loss: 0.7910 (0.8009)  time: 0.7505  data: 0.0001  max mem: 14938
[16:06:18.656861] Epoch: [17]  [120/345]  eta: 0:02:48  lr: 0.000108  loss: 0.7918 (0.8004)  time: 0.7512  data: 0.0001  max mem: 14938
[16:06:33.680145] Epoch: [17]  [140/345]  eta: 0:02:33  lr: 0.000109  loss: 0.8161 (0.8024)  time: 0.7511  data: 0.0001  max mem: 14938
[16:06:48.696180] Epoch: [17]  [160/345]  eta: 0:02:18  lr: 0.000109  loss: 0.7948 (0.8016)  time: 0.7508  data: 0.0001  max mem: 14938
[16:07:03.702842] Epoch: [17]  [180/345]  eta: 0:02:03  lr: 0.000110  loss: 0.8052 (0.8017)  time: 0.7503  data: 0.0001  max mem: 14938
[16:07:18.705263] Epoch: [17]  [200/345]  eta: 0:01:48  lr: 0.000110  loss: 0.8183 (0.8033)  time: 0.7501  data: 0.0001  max mem: 14938
[16:07:33.696349] Epoch: [17]  [220/345]  eta: 0:01:33  lr: 0.000110  loss: 0.8013 (0.8030)  time: 0.7495  data: 0.0001  max mem: 14938
[16:07:48.685042] Epoch: [17]  [240/345]  eta: 0:01:18  lr: 0.000111  loss: 0.7956 (0.8026)  time: 0.7494  data: 0.0001  max mem: 14938
[16:08:03.664635] Epoch: [17]  [260/345]  eta: 0:01:03  lr: 0.000111  loss: 0.7949 (0.8024)  time: 0.7489  data: 0.0001  max mem: 14938
[16:08:18.633491] Epoch: [17]  [280/345]  eta: 0:00:48  lr: 0.000111  loss: 0.7867 (0.8015)  time: 0.7484  data: 0.0001  max mem: 14938
[16:08:33.603931] Epoch: [17]  [300/345]  eta: 0:00:33  lr: 0.000112  loss: 0.7853 (0.8008)  time: 0.7485  data: 0.0001  max mem: 14938
[16:08:48.573198] Epoch: [17]  [320/345]  eta: 0:00:18  lr: 0.000112  loss: 0.7774 (0.7997)  time: 0.7484  data: 0.0001  max mem: 14938
[16:09:03.541553] Epoch: [17]  [340/345]  eta: 0:00:03  lr: 0.000112  loss: 0.7792 (0.7990)  time: 0.7484  data: 0.0001  max mem: 14938
[16:09:06.535202] Epoch: [17]  [344/345]  eta: 0:00:00  lr: 0.000112  loss: 0.7926 (0.7996)  time: 0.7482  data: 0.0001  max mem: 14938
[16:09:06.600596] Epoch: [17] Total time: 0:04:18 (0.7498 s / it)
[16:09:06.600890] Averaged stats: lr: 0.000112  loss: 0.7926 (0.7996)
[16:09:06.932967] Test:  [  0/345]  eta: 0:01:53  loss: 0.8271 (0.8271)  time: 0.3287  data: 0.1476  max mem: 14938
[16:09:08.769943] Test:  [ 10/345]  eta: 0:01:05  loss: 0.8043 (0.8104)  time: 0.1968  data: 0.0135  max mem: 14938
[16:09:10.611171] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7958 (0.7980)  time: 0.1838  data: 0.0001  max mem: 14938
[16:09:12.455116] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7958 (0.8056)  time: 0.1842  data: 0.0001  max mem: 14938
[16:09:14.301819] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8145 (0.8064)  time: 0.1845  data: 0.0001  max mem: 14938
[16:09:16.152486] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8131 (0.8094)  time: 0.1848  data: 0.0001  max mem: 14938
[16:09:18.009304] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8092 (0.8080)  time: 0.1853  data: 0.0001  max mem: 14938
[16:09:19.866724] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7962 (0.8068)  time: 0.1857  data: 0.0001  max mem: 14938
[16:09:21.728669] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7991 (0.8074)  time: 0.1859  data: 0.0001  max mem: 14938
[16:09:23.592454] Test:  [ 90/345]  eta: 0:00:47  loss: 0.8150 (0.8084)  time: 0.1862  data: 0.0001  max mem: 14938
[16:09:25.460638] Test:  [100/345]  eta: 0:00:45  loss: 0.8118 (0.8086)  time: 0.1865  data: 0.0001  max mem: 14938
[16:09:27.333705] Test:  [110/345]  eta: 0:00:43  loss: 0.8074 (0.8087)  time: 0.1870  data: 0.0001  max mem: 14938
[16:09:29.208313] Test:  [120/345]  eta: 0:00:42  loss: 0.8085 (0.8082)  time: 0.1873  data: 0.0001  max mem: 14938
[16:09:31.086050] Test:  [130/345]  eta: 0:00:40  loss: 0.8106 (0.8087)  time: 0.1876  data: 0.0001  max mem: 14938
[16:09:32.968155] Test:  [140/345]  eta: 0:00:38  loss: 0.8087 (0.8086)  time: 0.1879  data: 0.0001  max mem: 14938
[16:09:34.853271] Test:  [150/345]  eta: 0:00:36  loss: 0.8017 (0.8084)  time: 0.1883  data: 0.0001  max mem: 14938
[16:09:36.740417] Test:  [160/345]  eta: 0:00:34  loss: 0.8030 (0.8083)  time: 0.1886  data: 0.0001  max mem: 14938
[16:09:38.632441] Test:  [170/345]  eta: 0:00:32  loss: 0.8088 (0.8084)  time: 0.1889  data: 0.0001  max mem: 14938
[16:09:40.530179] Test:  [180/345]  eta: 0:00:30  loss: 0.8012 (0.8084)  time: 0.1894  data: 0.0001  max mem: 14938
[16:09:42.429857] Test:  [190/345]  eta: 0:00:29  loss: 0.8133 (0.8089)  time: 0.1898  data: 0.0001  max mem: 14938
[16:09:44.331215] Test:  [200/345]  eta: 0:00:27  loss: 0.8141 (0.8090)  time: 0.1900  data: 0.0001  max mem: 14938
[16:09:46.237214] Test:  [210/345]  eta: 0:00:25  loss: 0.8107 (0.8096)  time: 0.1903  data: 0.0001  max mem: 14938
[16:09:48.145462] Test:  [220/345]  eta: 0:00:23  loss: 0.8107 (0.8091)  time: 0.1907  data: 0.0001  max mem: 14938
[16:09:50.057351] Test:  [230/345]  eta: 0:00:21  loss: 0.8032 (0.8090)  time: 0.1910  data: 0.0001  max mem: 14938
[16:09:51.974108] Test:  [240/345]  eta: 0:00:19  loss: 0.8050 (0.8088)  time: 0.1914  data: 0.0001  max mem: 14938
[16:09:53.895381] Test:  [250/345]  eta: 0:00:17  loss: 0.8050 (0.8091)  time: 0.1918  data: 0.0001  max mem: 14938
[16:09:55.819139] Test:  [260/345]  eta: 0:00:16  loss: 0.8059 (0.8092)  time: 0.1922  data: 0.0001  max mem: 14938
[16:09:57.746045] Test:  [270/345]  eta: 0:00:14  loss: 0.8126 (0.8096)  time: 0.1925  data: 0.0001  max mem: 14938
[16:09:59.677115] Test:  [280/345]  eta: 0:00:12  loss: 0.8126 (0.8094)  time: 0.1928  data: 0.0001  max mem: 14938
[16:10:01.611617] Test:  [290/345]  eta: 0:00:10  loss: 0.8000 (0.8090)  time: 0.1932  data: 0.0001  max mem: 14938
[16:10:03.549103] Test:  [300/345]  eta: 0:00:08  loss: 0.8065 (0.8092)  time: 0.1935  data: 0.0001  max mem: 14938
[16:10:05.490330] Test:  [310/345]  eta: 0:00:06  loss: 0.8115 (0.8093)  time: 0.1939  data: 0.0001  max mem: 14938
[16:10:07.433847] Test:  [320/345]  eta: 0:00:04  loss: 0.8102 (0.8094)  time: 0.1942  data: 0.0001  max mem: 14938
[16:10:09.382461] Test:  [330/345]  eta: 0:00:02  loss: 0.8069 (0.8091)  time: 0.1946  data: 0.0001  max mem: 14938
[16:10:11.334437] Test:  [340/345]  eta: 0:00:00  loss: 0.8013 (0.8088)  time: 0.1950  data: 0.0001  max mem: 14938
[16:10:12.116712] Test:  [344/345]  eta: 0:00:00  loss: 0.8015 (0.8088)  time: 0.1951  data: 0.0001  max mem: 14938
[16:10:12.175069] Test: Total time: 0:01:05 (0.1901 s / it)
[16:10:22.548777] Test:  [ 0/57]  eta: 0:00:18  loss: 0.9423 (0.9423)  time: 0.3200  data: 0.1407  max mem: 14938
[16:10:24.366572] Test:  [10/57]  eta: 0:00:09  loss: 0.9168 (0.9112)  time: 0.1943  data: 0.0129  max mem: 14938
[16:10:26.187963] Test:  [20/57]  eta: 0:00:06  loss: 0.9127 (0.8942)  time: 0.1819  data: 0.0001  max mem: 14938
[16:10:28.012491] Test:  [30/57]  eta: 0:00:05  loss: 0.8220 (0.8602)  time: 0.1822  data: 0.0001  max mem: 14938
[16:10:29.841964] Test:  [40/57]  eta: 0:00:03  loss: 0.7707 (0.8390)  time: 0.1826  data: 0.0001  max mem: 14938
[16:10:31.678224] Test:  [50/57]  eta: 0:00:01  loss: 0.7707 (0.8305)  time: 0.1832  data: 0.0001  max mem: 14938
[16:10:32.668687] Test:  [56/57]  eta: 0:00:00  loss: 0.7886 (0.8350)  time: 0.1778  data: 0.0001  max mem: 14938
[16:10:32.728191] Test: Total time: 0:00:10 (0.1842 s / it)
[16:10:34.469185] Dice score of the network on the train images: 0.721124, val images: 0.790935
[16:10:34.473564] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[16:10:35.361545] Epoch: [18]  [  0/345]  eta: 0:05:05  lr: 0.000113  loss: 0.8219 (0.8219)  time: 0.8868  data: 0.1419  max mem: 14938
[16:10:50.262050] Epoch: [18]  [ 20/345]  eta: 0:04:04  lr: 0.000113  loss: 0.8134 (0.8155)  time: 0.7450  data: 0.0001  max mem: 14938
[16:11:05.191561] Epoch: [18]  [ 40/345]  eta: 0:03:48  lr: 0.000113  loss: 0.8022 (0.8108)  time: 0.7464  data: 0.0001  max mem: 14938
[16:11:20.138476] Epoch: [18]  [ 60/345]  eta: 0:03:33  lr: 0.000114  loss: 0.7843 (0.8029)  time: 0.7473  data: 0.0001  max mem: 14938
[16:11:35.103017] Epoch: [18]  [ 80/345]  eta: 0:03:18  lr: 0.000114  loss: 0.7890 (0.8002)  time: 0.7482  data: 0.0001  max mem: 14938
[16:11:50.227967] Epoch: [18]  [100/345]  eta: 0:03:03  lr: 0.000114  loss: 0.8042 (0.8005)  time: 0.7562  data: 0.0001  max mem: 14938
[16:12:05.250140] Epoch: [18]  [120/345]  eta: 0:02:48  lr: 0.000115  loss: 0.7949 (0.8002)  time: 0.7511  data: 0.0001  max mem: 14938
[16:12:20.267080] Epoch: [18]  [140/345]  eta: 0:02:33  lr: 0.000115  loss: 0.7937 (0.7985)  time: 0.7508  data: 0.0001  max mem: 14938
[16:12:35.285406] Epoch: [18]  [160/345]  eta: 0:02:18  lr: 0.000115  loss: 0.7915 (0.7979)  time: 0.7509  data: 0.0001  max mem: 14938
[16:12:50.302372] Epoch: [18]  [180/345]  eta: 0:02:03  lr: 0.000116  loss: 0.7892 (0.7971)  time: 0.7508  data: 0.0001  max mem: 14938
[16:13:05.315636] Epoch: [18]  [200/345]  eta: 0:01:48  lr: 0.000116  loss: 0.7802 (0.7957)  time: 0.7506  data: 0.0001  max mem: 14938
[16:13:20.310759] Epoch: [18]  [220/345]  eta: 0:01:33  lr: 0.000116  loss: 0.7849 (0.7949)  time: 0.7497  data: 0.0001  max mem: 14938
[16:13:35.294339] Epoch: [18]  [240/345]  eta: 0:01:18  lr: 0.000117  loss: 0.7860 (0.7940)  time: 0.7491  data: 0.0001  max mem: 14938
[16:13:50.276616] Epoch: [18]  [260/345]  eta: 0:01:03  lr: 0.000117  loss: 0.7794 (0.7928)  time: 0.7491  data: 0.0001  max mem: 14938
[16:14:05.253632] Epoch: [18]  [280/345]  eta: 0:00:48  lr: 0.000118  loss: 0.7721 (0.7917)  time: 0.7488  data: 0.0001  max mem: 14938
[16:14:20.231186] Epoch: [18]  [300/345]  eta: 0:00:33  lr: 0.000118  loss: 0.7857 (0.7914)  time: 0.7488  data: 0.0001  max mem: 14938
[16:14:35.206421] Epoch: [18]  [320/345]  eta: 0:00:18  lr: 0.000118  loss: 0.7962 (0.7916)  time: 0.7487  data: 0.0001  max mem: 14938
[16:14:50.175321] Epoch: [18]  [340/345]  eta: 0:00:03  lr: 0.000119  loss: 0.7954 (0.7913)  time: 0.7484  data: 0.0001  max mem: 14938
[16:14:53.167554] Epoch: [18]  [344/345]  eta: 0:00:00  lr: 0.000119  loss: 0.7937 (0.7911)  time: 0.7484  data: 0.0001  max mem: 14938
[16:14:53.231406] Epoch: [18] Total time: 0:04:18 (0.7500 s / it)
[16:14:53.231856] Averaged stats: lr: 0.000119  loss: 0.7937 (0.7911)
[16:14:53.559670] Test:  [  0/345]  eta: 0:01:51  loss: 0.7436 (0.7436)  time: 0.3243  data: 0.1424  max mem: 14938
[16:14:55.399074] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7406 (0.7394)  time: 0.1966  data: 0.0130  max mem: 14938
[16:14:57.240194] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7421 (0.7521)  time: 0.1839  data: 0.0001  max mem: 14938
[16:14:59.084811] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7562 (0.7526)  time: 0.1842  data: 0.0001  max mem: 14938
[16:15:00.932372] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7519 (0.7519)  time: 0.1846  data: 0.0001  max mem: 14938
[16:15:02.784450] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7544 (0.7529)  time: 0.1849  data: 0.0001  max mem: 14938
[16:15:04.640717] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7471 (0.7525)  time: 0.1854  data: 0.0001  max mem: 14938
[16:15:06.499341] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7455 (0.7513)  time: 0.1857  data: 0.0001  max mem: 14938
[16:15:08.361793] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7495 (0.7517)  time: 0.1860  data: 0.0001  max mem: 14938
[16:15:10.228397] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7504 (0.7513)  time: 0.1864  data: 0.0001  max mem: 14938
[16:15:12.096408] Test:  [100/345]  eta: 0:00:45  loss: 0.7398 (0.7505)  time: 0.1867  data: 0.0001  max mem: 14938
[16:15:13.968560] Test:  [110/345]  eta: 0:00:43  loss: 0.7498 (0.7512)  time: 0.1870  data: 0.0001  max mem: 14938
[16:15:15.842373] Test:  [120/345]  eta: 0:00:42  loss: 0.7477 (0.7511)  time: 0.1872  data: 0.0001  max mem: 14938
[16:15:17.721666] Test:  [130/345]  eta: 0:00:40  loss: 0.7476 (0.7509)  time: 0.1876  data: 0.0001  max mem: 14938
[16:15:19.603582] Test:  [140/345]  eta: 0:00:38  loss: 0.7472 (0.7506)  time: 0.1880  data: 0.0001  max mem: 14938
[16:15:21.489117] Test:  [150/345]  eta: 0:00:36  loss: 0.7413 (0.7504)  time: 0.1883  data: 0.0001  max mem: 14938
[16:15:23.377502] Test:  [160/345]  eta: 0:00:34  loss: 0.7489 (0.7509)  time: 0.1886  data: 0.0001  max mem: 14938
[16:15:25.270487] Test:  [170/345]  eta: 0:00:32  loss: 0.7578 (0.7513)  time: 0.1890  data: 0.0001  max mem: 14938
[16:15:27.167124] Test:  [180/345]  eta: 0:00:30  loss: 0.7586 (0.7518)  time: 0.1894  data: 0.0001  max mem: 14938
[16:15:29.065950] Test:  [190/345]  eta: 0:00:29  loss: 0.7582 (0.7520)  time: 0.1897  data: 0.0001  max mem: 14938
[16:15:30.966962] Test:  [200/345]  eta: 0:00:27  loss: 0.7563 (0.7522)  time: 0.1899  data: 0.0001  max mem: 14938
[16:15:32.875327] Test:  [210/345]  eta: 0:00:25  loss: 0.7470 (0.7521)  time: 0.1904  data: 0.0001  max mem: 14938
[16:15:34.785425] Test:  [220/345]  eta: 0:00:23  loss: 0.7466 (0.7519)  time: 0.1909  data: 0.0001  max mem: 14938
[16:15:36.699513] Test:  [230/345]  eta: 0:00:21  loss: 0.7514 (0.7518)  time: 0.1912  data: 0.0001  max mem: 14938
[16:15:38.615675] Test:  [240/345]  eta: 0:00:19  loss: 0.7522 (0.7519)  time: 0.1915  data: 0.0001  max mem: 14938
[16:15:40.538160] Test:  [250/345]  eta: 0:00:17  loss: 0.7510 (0.7516)  time: 0.1919  data: 0.0001  max mem: 14938
[16:15:42.462986] Test:  [260/345]  eta: 0:00:16  loss: 0.7552 (0.7521)  time: 0.1923  data: 0.0001  max mem: 14938
[16:15:44.390372] Test:  [270/345]  eta: 0:00:14  loss: 0.7559 (0.7521)  time: 0.1926  data: 0.0001  max mem: 14938
[16:15:46.321904] Test:  [280/345]  eta: 0:00:12  loss: 0.7479 (0.7522)  time: 0.1929  data: 0.0001  max mem: 14938
[16:15:48.254639] Test:  [290/345]  eta: 0:00:10  loss: 0.7527 (0.7526)  time: 0.1932  data: 0.0001  max mem: 14938
[16:15:50.193247] Test:  [300/345]  eta: 0:00:08  loss: 0.7511 (0.7525)  time: 0.1935  data: 0.0001  max mem: 14938
[16:15:52.133555] Test:  [310/345]  eta: 0:00:06  loss: 0.7475 (0.7523)  time: 0.1939  data: 0.0001  max mem: 14938
[16:15:54.080916] Test:  [320/345]  eta: 0:00:04  loss: 0.7458 (0.7521)  time: 0.1943  data: 0.0001  max mem: 14938
[16:15:56.029327] Test:  [330/345]  eta: 0:00:02  loss: 0.7506 (0.7521)  time: 0.1947  data: 0.0001  max mem: 14938
[16:15:57.981499] Test:  [340/345]  eta: 0:00:00  loss: 0.7432 (0.7518)  time: 0.1950  data: 0.0001  max mem: 14938
[16:15:58.764480] Test:  [344/345]  eta: 0:00:00  loss: 0.7432 (0.7518)  time: 0.1952  data: 0.0001  max mem: 14938
[16:15:58.821943] Test: Total time: 0:01:05 (0.1901 s / it)
[16:16:09.222486] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8396 (0.8396)  time: 0.3164  data: 0.1361  max mem: 14938
[16:16:11.040864] Test:  [10/57]  eta: 0:00:09  loss: 0.8756 (0.8719)  time: 0.1940  data: 0.0124  max mem: 14938
[16:16:12.865816] Test:  [20/57]  eta: 0:00:06  loss: 0.8756 (0.8613)  time: 0.1821  data: 0.0001  max mem: 14938
[16:16:14.692112] Test:  [30/57]  eta: 0:00:05  loss: 0.7566 (0.8239)  time: 0.1825  data: 0.0001  max mem: 14938
[16:16:16.521574] Test:  [40/57]  eta: 0:00:03  loss: 0.7473 (0.8039)  time: 0.1827  data: 0.0001  max mem: 14938
[16:16:18.359088] Test:  [50/57]  eta: 0:00:01  loss: 0.7350 (0.7969)  time: 0.1833  data: 0.0001  max mem: 14938
[16:16:19.350084] Test:  [56/57]  eta: 0:00:00  loss: 0.7753 (0.8031)  time: 0.1780  data: 0.0001  max mem: 14938
[16:16:19.406371] Test: Total time: 0:00:10 (0.1842 s / it)
[16:16:21.206501] Dice score of the network on the train images: 0.767115, val images: 0.819206
[16:16:21.206740] saving best_dice_model_0 @ epoch 18
[16:16:22.323841] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[16:16:23.208064] Epoch: [19]  [  0/345]  eta: 0:05:04  lr: 0.000119  loss: 0.7867 (0.7867)  time: 0.8831  data: 0.1425  max mem: 14938
[16:16:38.066909] Epoch: [19]  [ 20/345]  eta: 0:04:03  lr: 0.000119  loss: 0.7712 (0.7733)  time: 0.7429  data: 0.0001  max mem: 14938

[16:16:52.983739] Epoch: [19]  [ 40/345]  eta: 0:03:48  lr: 0.000119  loss: 0.7792 (0.7756)  time: 0.7458  data: 0.0001  max mem: 14938
[16:17:08.068805] Epoch: [19]  [ 60/345]  eta: 0:03:33  lr: 0.000120  loss: 0.7688 (0.7753)  time: 0.7542  data: 0.0001  max mem: 14938
[16:17:23.071769] Epoch: [19]  [ 80/345]  eta: 0:03:18  lr: 0.000120  loss: 0.7873 (0.7802)  time: 0.7501  data: 0.0001  max mem: 14938
[16:17:38.115457] Epoch: [19]  [100/345]  eta: 0:03:03  lr: 0.000121  loss: 0.7834 (0.7816)  time: 0.7521  data: 0.0001  max mem: 14938
[16:17:53.169502] Epoch: [19]  [120/345]  eta: 0:02:48  lr: 0.000121  loss: 0.7958 (0.7837)  time: 0.7527  data: 0.0001  max mem: 14938
[16:18:08.231954] Epoch: [19]  [140/345]  eta: 0:02:33  lr: 0.000121  loss: 0.8006 (0.7857)  time: 0.7531  data: 0.0001  max mem: 14938
[16:18:23.271556] Epoch: [19]  [160/345]  eta: 0:02:18  lr: 0.000122  loss: 0.7769 (0.7859)  time: 0.7519  data: 0.0001  max mem: 14938
[16:18:38.314407] Epoch: [19]  [180/345]  eta: 0:02:03  lr: 0.000122  loss: 0.8035 (0.7877)  time: 0.7521  data: 0.0001  max mem: 14938
[16:18:53.346228] Epoch: [19]  [200/345]  eta: 0:01:48  lr: 0.000122  loss: 0.7881 (0.7880)  time: 0.7515  data: 0.0001  max mem: 14938
[16:19:08.359920] Epoch: [19]  [220/345]  eta: 0:01:33  lr: 0.000123  loss: 0.7768 (0.7875)  time: 0.7506  data: 0.0001  max mem: 14938
[16:19:23.366457] Epoch: [19]  [240/345]  eta: 0:01:18  lr: 0.000123  loss: 0.7867 (0.7872)  time: 0.7503  data: 0.0001  max mem: 14938
[16:19:38.358526] Epoch: [19]  [260/345]  eta: 0:01:03  lr: 0.000123  loss: 0.7772 (0.7869)  time: 0.7496  data: 0.0001  max mem: 14938
[16:19:53.361576] Epoch: [19]  [280/345]  eta: 0:00:48  lr: 0.000124  loss: 0.7841 (0.7866)  time: 0.7501  data: 0.0001  max mem: 14938
[16:20:08.361593] Epoch: [19]  [300/345]  eta: 0:00:33  lr: 0.000124  loss: 0.7854 (0.7866)  time: 0.7500  data: 0.0001  max mem: 14938
[16:20:23.358329] Epoch: [19]  [320/345]  eta: 0:00:18  lr: 0.000125  loss: 0.7713 (0.7858)  time: 0.7498  data: 0.0001  max mem: 14938
[16:20:38.352763] Epoch: [19]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.7714 (0.7851)  time: 0.7497  data: 0.0001  max mem: 14938
[16:20:41.354403] Epoch: [19]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.7691 (0.7851)  time: 0.7498  data: 0.0001  max mem: 14938
[16:20:41.418520] Epoch: [19] Total time: 0:04:19 (0.7510 s / it)
[16:20:41.418924] Averaged stats: lr: 0.000125  loss: 0.7691 (0.7851)
[16:20:41.754261] Test:  [  0/345]  eta: 0:01:54  loss: 0.7432 (0.7432)  time: 0.3307  data: 0.1492  max mem: 14938
[16:20:43.593941] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7447 (0.7482)  time: 0.1972  data: 0.0136  max mem: 14938
[16:20:45.436250] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7462 (0.7508)  time: 0.1840  data: 0.0001  max mem: 14938
[16:20:47.281052] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7463 (0.7473)  time: 0.1843  data: 0.0001  max mem: 14938
[16:20:49.128784] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7502 (0.7505)  time: 0.1846  data: 0.0001  max mem: 14938
[16:20:50.980871] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7497 (0.7498)  time: 0.1849  data: 0.0001  max mem: 14938
[16:20:52.836611] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7497 (0.7500)  time: 0.1853  data: 0.0001  max mem: 14938
[16:20:54.696634] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7483 (0.7494)  time: 0.1857  data: 0.0001  max mem: 14938
[16:20:56.558022] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7454 (0.7494)  time: 0.1860  data: 0.0001  max mem: 14938
[16:20:58.423215] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7494 (0.7500)  time: 0.1863  data: 0.0001  max mem: 14938
[16:21:00.292309] Test:  [100/345]  eta: 0:00:45  loss: 0.7541 (0.7501)  time: 0.1867  data: 0.0001  max mem: 14938
[16:21:02.163504] Test:  [110/345]  eta: 0:00:43  loss: 0.7519 (0.7499)  time: 0.1870  data: 0.0001  max mem: 14938
[16:21:04.038327] Test:  [120/345]  eta: 0:00:42  loss: 0.7452 (0.7500)  time: 0.1873  data: 0.0001  max mem: 14938
[16:21:05.918098] Test:  [130/345]  eta: 0:00:40  loss: 0.7499 (0.7501)  time: 0.1877  data: 0.0001  max mem: 14938
[16:21:07.800873] Test:  [140/345]  eta: 0:00:38  loss: 0.7549 (0.7509)  time: 0.1881  data: 0.0001  max mem: 14938
[16:21:09.687177] Test:  [150/345]  eta: 0:00:36  loss: 0.7606 (0.7512)  time: 0.1884  data: 0.0001  max mem: 14938
[16:21:11.575390] Test:  [160/345]  eta: 0:00:34  loss: 0.7555 (0.7515)  time: 0.1887  data: 0.0001  max mem: 14938
[16:21:13.469699] Test:  [170/345]  eta: 0:00:32  loss: 0.7502 (0.7516)  time: 0.1891  data: 0.0001  max mem: 14938
[16:21:15.367447] Test:  [180/345]  eta: 0:00:30  loss: 0.7434 (0.7513)  time: 0.1895  data: 0.0001  max mem: 14938
[16:21:17.269147] Test:  [190/345]  eta: 0:00:29  loss: 0.7443 (0.7513)  time: 0.1899  data: 0.0001  max mem: 14938
[16:21:19.173357] Test:  [200/345]  eta: 0:00:27  loss: 0.7562 (0.7517)  time: 0.1902  data: 0.0001  max mem: 14938
[16:21:21.080307] Test:  [210/345]  eta: 0:00:25  loss: 0.7526 (0.7513)  time: 0.1905  data: 0.0001  max mem: 14938
[16:21:22.990498] Test:  [220/345]  eta: 0:00:23  loss: 0.7526 (0.7516)  time: 0.1908  data: 0.0001  max mem: 14938
[16:21:24.904734] Test:  [230/345]  eta: 0:00:21  loss: 0.7557 (0.7515)  time: 0.1912  data: 0.0001  max mem: 14938
[16:21:26.822688] Test:  [240/345]  eta: 0:00:19  loss: 0.7420 (0.7515)  time: 0.1916  data: 0.0001  max mem: 14938
[16:21:28.744386] Test:  [250/345]  eta: 0:00:17  loss: 0.7456 (0.7516)  time: 0.1919  data: 0.0001  max mem: 14938
[16:21:30.668414] Test:  [260/345]  eta: 0:00:16  loss: 0.7513 (0.7512)  time: 0.1922  data: 0.0001  max mem: 14938
[16:21:32.596869] Test:  [270/345]  eta: 0:00:14  loss: 0.7553 (0.7514)  time: 0.1926  data: 0.0001  max mem: 14938
[16:21:34.528559] Test:  [280/345]  eta: 0:00:12  loss: 0.7595 (0.7519)  time: 0.1930  data: 0.0001  max mem: 14938
[16:21:36.462964] Test:  [290/345]  eta: 0:00:10  loss: 0.7578 (0.7519)  time: 0.1933  data: 0.0001  max mem: 14938
[16:21:38.402437] Test:  [300/345]  eta: 0:00:08  loss: 0.7578 (0.7521)  time: 0.1936  data: 0.0001  max mem: 14938
[16:21:40.344743] Test:  [310/345]  eta: 0:00:06  loss: 0.7598 (0.7522)  time: 0.1940  data: 0.0001  max mem: 14938
[16:21:42.292172] Test:  [320/345]  eta: 0:00:04  loss: 0.7469 (0.7522)  time: 0.1944  data: 0.0001  max mem: 14938
[16:21:44.239525] Test:  [330/345]  eta: 0:00:02  loss: 0.7419 (0.7519)  time: 0.1947  data: 0.0001  max mem: 14938
[16:21:46.192347] Test:  [340/345]  eta: 0:00:00  loss: 0.7477 (0.7522)  time: 0.1950  data: 0.0001  max mem: 14938
[16:21:46.974747] Test:  [344/345]  eta: 0:00:00  loss: 0.7477 (0.7522)  time: 0.1951  data: 0.0001  max mem: 14938
[16:21:47.032610] Test: Total time: 0:01:05 (0.1902 s / it)
[16:21:57.443726] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8489 (0.8489)  time: 0.3212  data: 0.1419  max mem: 14938
[16:21:59.263278] Test:  [10/57]  eta: 0:00:09  loss: 0.8702 (0.8661)  time: 0.1945  data: 0.0130  max mem: 14938
[16:22:01.084975] Test:  [20/57]  eta: 0:00:06  loss: 0.8638 (0.8553)  time: 0.1820  data: 0.0001  max mem: 14938
[16:22:02.912844] Test:  [30/57]  eta: 0:00:05  loss: 0.7536 (0.8192)  time: 0.1824  data: 0.0001  max mem: 14938
[16:22:04.743140] Test:  [40/57]  eta: 0:00:03  loss: 0.7410 (0.7992)  time: 0.1829  data: 0.0001  max mem: 14938
[16:22:06.579106] Test:  [50/57]  eta: 0:00:01  loss: 0.7439 (0.7943)  time: 0.1833  data: 0.0001  max mem: 14938
[16:22:07.568093] Test:  [56/57]  eta: 0:00:00  loss: 0.7751 (0.8004)  time: 0.1778  data: 0.0001  max mem: 14938
[16:22:07.603855] Test: Total time: 0:00:10 (0.1839 s / it)
[16:22:09.386052] Dice score of the network on the train images: 0.755461, val images: 0.813516
[16:22:09.390279] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[16:22:10.277009] Epoch: [20]  [  0/345]  eta: 0:05:05  lr: 0.000125  loss: 0.7700 (0.7700)  time: 0.8858  data: 0.1433  max mem: 14938
[16:22:25.164298] Epoch: [20]  [ 20/345]  eta: 0:04:04  lr: 0.000125  loss: 0.7643 (0.7677)  time: 0.7443  data: 0.0001  max mem: 14938
[16:22:40.127335] Epoch: [20]  [ 40/345]  eta: 0:03:48  lr: 0.000125  loss: 0.7709 (0.7702)  time: 0.7481  data: 0.0001  max mem: 14938
[16:22:55.104888] Epoch: [20]  [ 60/345]  eta: 0:03:33  lr: 0.000125  loss: 0.7656 (0.7706)  time: 0.7488  data: 0.0001  max mem: 14938
[16:23:10.112866] Epoch: [20]  [ 80/345]  eta: 0:03:18  lr: 0.000125  loss: 0.7752 (0.7741)  time: 0.7504  data: 0.0001  max mem: 14938
[16:23:25.121239] Epoch: [20]  [100/345]  eta: 0:03:03  lr: 0.000125  loss: 0.8217 (0.7842)  time: 0.7504  data: 0.0001  max mem: 14938
[16:23:40.160278] Epoch: [20]  [120/345]  eta: 0:02:48  lr: 0.000125  loss: 0.8034 (0.7875)  time: 0.7519  data: 0.0001  max mem: 14938
[16:23:55.191973] Epoch: [20]  [140/345]  eta: 0:02:33  lr: 0.000125  loss: 0.7877 (0.7882)  time: 0.7515  data: 0.0001  max mem: 14938
[16:24:10.233154] Epoch: [20]  [160/345]  eta: 0:02:18  lr: 0.000125  loss: 0.7882 (0.7883)  time: 0.7520  data: 0.0001  max mem: 14938
[16:24:25.272544] Epoch: [20]  [180/345]  eta: 0:02:03  lr: 0.000125  loss: 0.7830 (0.7877)  time: 0.7519  data: 0.0001  max mem: 14938
[16:24:40.300546] Epoch: [20]  [200/345]  eta: 0:01:48  lr: 0.000125  loss: 0.7991 (0.7888)  time: 0.7514  data: 0.0001  max mem: 14938
[16:24:55.323469] Epoch: [20]  [220/345]  eta: 0:01:33  lr: 0.000125  loss: 0.7868 (0.7891)  time: 0.7511  data: 0.0001  max mem: 14938
[16:25:10.347201] Epoch: [20]  [240/345]  eta: 0:01:18  lr: 0.000125  loss: 0.7854 (0.7885)  time: 0.7511  data: 0.0001  max mem: 14938
[16:25:25.348275] Epoch: [20]  [260/345]  eta: 0:01:03  lr: 0.000125  loss: 0.7813 (0.7877)  time: 0.7500  data: 0.0001  max mem: 14938
[16:25:40.360923] Epoch: [20]  [280/345]  eta: 0:00:48  lr: 0.000125  loss: 0.7820 (0.7872)  time: 0.7506  data: 0.0001  max mem: 14938
[16:25:55.367121] Epoch: [20]  [300/345]  eta: 0:00:33  lr: 0.000125  loss: 0.7731 (0.7867)  time: 0.7503  data: 0.0001  max mem: 14938
[16:26:10.359010] Epoch: [20]  [320/345]  eta: 0:00:18  lr: 0.000125  loss: 0.7814 (0.7863)  time: 0.7495  data: 0.0001  max mem: 14938
[16:26:25.354604] Epoch: [20]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.7758 (0.7859)  time: 0.7497  data: 0.0001  max mem: 14938
[16:26:28.354732] Epoch: [20]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.7743 (0.7858)  time: 0.7496  data: 0.0001  max mem: 14938
[16:26:28.418050] Epoch: [20] Total time: 0:04:19 (0.7508 s / it)
[16:26:28.418510] Averaged stats: lr: 0.000125  loss: 0.7743 (0.7858)
[16:26:28.751612] Test:  [  0/345]  eta: 0:01:52  loss: 0.7554 (0.7554)  time: 0.3269  data: 0.1448  max mem: 14938
[16:26:30.591905] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7439 (0.7423)  time: 0.1969  data: 0.0133  max mem: 14938
[16:26:32.433961] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7406 (0.7392)  time: 0.1840  data: 0.0001  max mem: 14938
[16:26:34.279868] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7442 (0.7409)  time: 0.1843  data: 0.0001  max mem: 14938
[16:26:36.128316] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7449 (0.7423)  time: 0.1847  data: 0.0001  max mem: 14938
[16:26:37.979584] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7455 (0.7433)  time: 0.1849  data: 0.0001  max mem: 14938
[16:26:39.835706] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7463 (0.7451)  time: 0.1853  data: 0.0001  max mem: 14938
[16:26:41.693146] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7447 (0.7455)  time: 0.1856  data: 0.0001  max mem: 14938
[16:26:43.554490] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7440 (0.7448)  time: 0.1859  data: 0.0001  max mem: 14938
[16:26:45.419559] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7412 (0.7457)  time: 0.1863  data: 0.0001  max mem: 14938
[16:26:47.288965] Test:  [100/345]  eta: 0:00:45  loss: 0.7428 (0.7456)  time: 0.1867  data: 0.0001  max mem: 14938
[16:26:49.160882] Test:  [110/345]  eta: 0:00:43  loss: 0.7445 (0.7458)  time: 0.1870  data: 0.0001  max mem: 14938
[16:26:51.036724] Test:  [120/345]  eta: 0:00:42  loss: 0.7468 (0.7459)  time: 0.1873  data: 0.0001  max mem: 14938
[16:26:52.917114] Test:  [130/345]  eta: 0:00:40  loss: 0.7449 (0.7460)  time: 0.1877  data: 0.0001  max mem: 14938
[16:26:54.800033] Test:  [140/345]  eta: 0:00:38  loss: 0.7480 (0.7463)  time: 0.1881  data: 0.0001  max mem: 14938
[16:26:56.685344] Test:  [150/345]  eta: 0:00:36  loss: 0.7550 (0.7468)  time: 0.1884  data: 0.0001  max mem: 14938
[16:26:58.573477] Test:  [160/345]  eta: 0:00:34  loss: 0.7550 (0.7470)  time: 0.1886  data: 0.0001  max mem: 14938
[16:27:00.468118] Test:  [170/345]  eta: 0:00:32  loss: 0.7400 (0.7468)  time: 0.1891  data: 0.0001  max mem: 14938
[16:27:02.367793] Test:  [180/345]  eta: 0:00:30  loss: 0.7476 (0.7467)  time: 0.1897  data: 0.0001  max mem: 14938
[16:27:04.270116] Test:  [190/345]  eta: 0:00:29  loss: 0.7431 (0.7463)  time: 0.1900  data: 0.0001  max mem: 14938
[16:27:06.174564] Test:  [200/345]  eta: 0:00:27  loss: 0.7426 (0.7461)  time: 0.1903  data: 0.0001  max mem: 14938
[16:27:08.083142] Test:  [210/345]  eta: 0:00:25  loss: 0.7480 (0.7467)  time: 0.1906  data: 0.0001  max mem: 14938
[16:27:09.992527] Test:  [220/345]  eta: 0:00:23  loss: 0.7453 (0.7465)  time: 0.1908  data: 0.0001  max mem: 14938
[16:27:11.907431] Test:  [230/345]  eta: 0:00:21  loss: 0.7501 (0.7469)  time: 0.1912  data: 0.0001  max mem: 14938
[16:27:13.826033] Test:  [240/345]  eta: 0:00:19  loss: 0.7501 (0.7471)  time: 0.1916  data: 0.0001  max mem: 14938
[16:27:15.747316] Test:  [250/345]  eta: 0:00:17  loss: 0.7468 (0.7472)  time: 0.1919  data: 0.0001  max mem: 14938
[16:27:17.670295] Test:  [260/345]  eta: 0:00:16  loss: 0.7397 (0.7469)  time: 0.1922  data: 0.0001  max mem: 14938
[16:27:19.598126] Test:  [270/345]  eta: 0:00:14  loss: 0.7421 (0.7472)  time: 0.1925  data: 0.0001  max mem: 14938
[16:27:21.529972] Test:  [280/345]  eta: 0:00:12  loss: 0.7561 (0.7477)  time: 0.1929  data: 0.0001  max mem: 14938
[16:27:23.463696] Test:  [290/345]  eta: 0:00:10  loss: 0.7561 (0.7478)  time: 0.1932  data: 0.0001  max mem: 14938
[16:27:25.402491] Test:  [300/345]  eta: 0:00:08  loss: 0.7479 (0.7477)  time: 0.1936  data: 0.0001  max mem: 14938
[16:27:27.343704] Test:  [310/345]  eta: 0:00:06  loss: 0.7450 (0.7476)  time: 0.1939  data: 0.0001  max mem: 14938
[16:27:29.289713] Test:  [320/345]  eta: 0:00:04  loss: 0.7528 (0.7480)  time: 0.1943  data: 0.0001  max mem: 14938
[16:27:31.235991] Test:  [330/345]  eta: 0:00:02  loss: 0.7578 (0.7481)  time: 0.1946  data: 0.0001  max mem: 14938
[16:27:33.187485] Test:  [340/345]  eta: 0:00:00  loss: 0.7497 (0.7483)  time: 0.1948  data: 0.0001  max mem: 14938
[16:27:33.968911] Test:  [344/345]  eta: 0:00:00  loss: 0.7501 (0.7484)  time: 0.1950  data: 0.0001  max mem: 14938
[16:27:34.027581] Test: Total time: 0:01:05 (0.1902 s / it)
[16:27:44.483515] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8306 (0.8306)  time: 0.3242  data: 0.1442  max mem: 14938
[16:27:46.300548] Test:  [10/57]  eta: 0:00:09  loss: 0.8339 (0.8598)  time: 0.1946  data: 0.0132  max mem: 14938
[16:27:48.122527] Test:  [20/57]  eta: 0:00:06  loss: 0.8633 (0.8500)  time: 0.1819  data: 0.0001  max mem: 14938
[16:27:49.947982] Test:  [30/57]  eta: 0:00:05  loss: 0.7479 (0.8160)  time: 0.1823  data: 0.0001  max mem: 14938
[16:27:51.778423] Test:  [40/57]  eta: 0:00:03  loss: 0.7407 (0.7974)  time: 0.1827  data: 0.0001  max mem: 14938
[16:27:53.614689] Test:  [50/57]  eta: 0:00:01  loss: 0.7299 (0.7916)  time: 0.1833  data: 0.0001  max mem: 14938
[16:27:54.604536] Test:  [56/57]  eta: 0:00:00  loss: 0.7634 (0.7979)  time: 0.1779  data: 0.0001  max mem: 14938
[16:27:54.645690] Test: Total time: 0:00:10 (0.1840 s / it)
[16:27:56.362532] Dice score of the network on the train images: 0.770192, val images: 0.814762
[16:27:56.366585] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[16:27:57.254693] Epoch: [21]  [  0/345]  eta: 0:05:06  lr: 0.000125  loss: 0.7911 (0.7911)  time: 0.8870  data: 0.1427  max mem: 14938
[16:28:12.161538] Epoch: [21]  [ 20/345]  eta: 0:04:04  lr: 0.000125  loss: 0.7669 (0.7733)  time: 0.7453  data: 0.0001  max mem: 14938
[16:28:27.118710] Epoch: [21]  [ 40/345]  eta: 0:03:48  lr: 0.000125  loss: 0.7951 (0.7839)  time: 0.7478  data: 0.0001  max mem: 14938
[16:28:42.101506] Epoch: [21]  [ 60/345]  eta: 0:03:33  lr: 0.000125  loss: 0.7734 (0.7830)  time: 0.7491  data: 0.0001  max mem: 14938
[16:28:57.118905] Epoch: [21]  [ 80/345]  eta: 0:03:18  lr: 0.000124  loss: 0.7846 (0.7824)  time: 0.7508  data: 0.0001  max mem: 14938
[16:29:12.158588] Epoch: [21]  [100/345]  eta: 0:03:03  lr: 0.000124  loss: 0.7718 (0.7808)  time: 0.7519  data: 0.0001  max mem: 14938
[16:29:27.218791] Epoch: [21]  [120/345]  eta: 0:02:48  lr: 0.000124  loss: 0.7715 (0.7789)  time: 0.7530  data: 0.0001  max mem: 14938
[16:29:42.275805] Epoch: [21]  [140/345]  eta: 0:02:33  lr: 0.000124  loss: 0.7686 (0.7774)  time: 0.7528  data: 0.0001  max mem: 14938
[16:29:57.323258] Epoch: [21]  [160/345]  eta: 0:02:18  lr: 0.000124  loss: 0.7707 (0.7766)  time: 0.7523  data: 0.0001  max mem: 14938
[16:30:12.358930] Epoch: [21]  [180/345]  eta: 0:02:03  lr: 0.000124  loss: 0.7712 (0.7763)  time: 0.7517  data: 0.0001  max mem: 14938
[16:30:27.391995] Epoch: [21]  [200/345]  eta: 0:01:48  lr: 0.000124  loss: 0.7662 (0.7758)  time: 0.7516  data: 0.0001  max mem: 14938
[16:30:42.420062] Epoch: [21]  [220/345]  eta: 0:01:33  lr: 0.000124  loss: 0.7666 (0.7749)  time: 0.7514  data: 0.0001  max mem: 14938
[16:30:57.440022] Epoch: [21]  [240/345]  eta: 0:01:18  lr: 0.000124  loss: 0.7708 (0.7748)  time: 0.7510  data: 0.0001  max mem: 14938
[16:31:12.452969] Epoch: [21]  [260/345]  eta: 0:01:03  lr: 0.000124  loss: 0.7796 (0.7755)  time: 0.7506  data: 0.0001  max mem: 14938
[16:31:27.459294] Epoch: [21]  [280/345]  eta: 0:00:48  lr: 0.000124  loss: 0.7751 (0.7757)  time: 0.7503  data: 0.0001  max mem: 14938
[16:31:42.466216] Epoch: [21]  [300/345]  eta: 0:00:33  lr: 0.000124  loss: 0.7619 (0.7754)  time: 0.7503  data: 0.0001  max mem: 14938
[16:31:57.467758] Epoch: [21]  [320/345]  eta: 0:00:18  lr: 0.000124  loss: 0.7582 (0.7749)  time: 0.7500  data: 0.0001  max mem: 14938
[16:32:12.464543] Epoch: [21]  [340/345]  eta: 0:00:03  lr: 0.000124  loss: 0.7686 (0.7746)  time: 0.7498  data: 0.0001  max mem: 14938
[16:32:15.465402] Epoch: [21]  [344/345]  eta: 0:00:00  lr: 0.000124  loss: 0.7706 (0.7747)  time: 0.7498  data: 0.0001  max mem: 14938
[16:32:15.530255] Epoch: [21] Total time: 0:04:19 (0.7512 s / it)
[16:32:15.530754] Averaged stats: lr: 0.000124  loss: 0.7706 (0.7747)
[16:32:15.868032] Test:  [  0/345]  eta: 0:01:54  loss: 0.7291 (0.7291)  time: 0.3326  data: 0.1513  max mem: 14938
[16:32:17.707419] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7423 (0.7377)  time: 0.1974  data: 0.0138  max mem: 14938
[16:32:19.548380] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7369 (0.7376)  time: 0.1839  data: 0.0001  max mem: 14938
[16:32:21.392402] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7397 (0.7409)  time: 0.1842  data: 0.0001  max mem: 14938
[16:32:23.239504] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7450 (0.7410)  time: 0.1845  data: 0.0001  max mem: 14938
[16:32:25.091635] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7403 (0.7416)  time: 0.1849  data: 0.0001  max mem: 14938
[16:32:26.947441] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7396 (0.7414)  time: 0.1853  data: 0.0001  max mem: 14938
[16:32:28.806548] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7379 (0.7413)  time: 0.1857  data: 0.0001  max mem: 14938
[16:32:30.667264] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7405 (0.7411)  time: 0.1859  data: 0.0001  max mem: 14938
[16:32:32.533162] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7382 (0.7412)  time: 0.1863  data: 0.0001  max mem: 14938
[16:32:34.401548] Test:  [100/345]  eta: 0:00:45  loss: 0.7391 (0.7411)  time: 0.1867  data: 0.0001  max mem: 14938
[16:32:36.271798] Test:  [110/345]  eta: 0:00:43  loss: 0.7432 (0.7418)  time: 0.1869  data: 0.0001  max mem: 14938
[16:32:38.147608] Test:  [120/345]  eta: 0:00:42  loss: 0.7448 (0.7417)  time: 0.1873  data: 0.0001  max mem: 14938
[16:32:40.025822] Test:  [130/345]  eta: 0:00:40  loss: 0.7411 (0.7418)  time: 0.1876  data: 0.0001  max mem: 14938
[16:32:41.908413] Test:  [140/345]  eta: 0:00:38  loss: 0.7405 (0.7416)  time: 0.1880  data: 0.0001  max mem: 14938
[16:32:43.794751] Test:  [150/345]  eta: 0:00:36  loss: 0.7405 (0.7417)  time: 0.1884  data: 0.0001  max mem: 14938
[16:32:45.684862] Test:  [160/345]  eta: 0:00:34  loss: 0.7407 (0.7417)  time: 0.1888  data: 0.0001  max mem: 14938
[16:32:47.577746] Test:  [170/345]  eta: 0:00:32  loss: 0.7380 (0.7415)  time: 0.1891  data: 0.0001  max mem: 14938
[16:32:49.477405] Test:  [180/345]  eta: 0:00:30  loss: 0.7308 (0.7410)  time: 0.1896  data: 0.0001  max mem: 14938
[16:32:51.378482] Test:  [190/345]  eta: 0:00:29  loss: 0.7308 (0.7406)  time: 0.1900  data: 0.0001  max mem: 14938
[16:32:53.281175] Test:  [200/345]  eta: 0:00:27  loss: 0.7331 (0.7405)  time: 0.1901  data: 0.0001  max mem: 14938
[16:32:55.189927] Test:  [210/345]  eta: 0:00:25  loss: 0.7329 (0.7402)  time: 0.1905  data: 0.0001  max mem: 14938
[16:32:57.102646] Test:  [220/345]  eta: 0:00:23  loss: 0.7337 (0.7401)  time: 0.1910  data: 0.0001  max mem: 14938
[16:32:59.018840] Test:  [230/345]  eta: 0:00:21  loss: 0.7337 (0.7399)  time: 0.1914  data: 0.0001  max mem: 14938
[16:33:00.939245] Test:  [240/345]  eta: 0:00:19  loss: 0.7378 (0.7399)  time: 0.1918  data: 0.0001  max mem: 14938
[16:33:02.862061] Test:  [250/345]  eta: 0:00:17  loss: 0.7411 (0.7399)  time: 0.1921  data: 0.0001  max mem: 14938
[16:33:04.787974] Test:  [260/345]  eta: 0:00:16  loss: 0.7334 (0.7396)  time: 0.1924  data: 0.0001  max mem: 14938
[16:33:06.716142] Test:  [270/345]  eta: 0:00:14  loss: 0.7330 (0.7400)  time: 0.1926  data: 0.0001  max mem: 14938
[16:33:08.648245] Test:  [280/345]  eta: 0:00:12  loss: 0.7449 (0.7402)  time: 0.1930  data: 0.0001  max mem: 14938
[16:33:10.584127] Test:  [290/345]  eta: 0:00:10  loss: 0.7393 (0.7399)  time: 0.1933  data: 0.0001  max mem: 14938
[16:33:12.522191] Test:  [300/345]  eta: 0:00:08  loss: 0.7336 (0.7399)  time: 0.1936  data: 0.0001  max mem: 14938
[16:33:14.464766] Test:  [310/345]  eta: 0:00:06  loss: 0.7453 (0.7403)  time: 0.1940  data: 0.0001  max mem: 14938
[16:33:16.408217] Test:  [320/345]  eta: 0:00:04  loss: 0.7378 (0.7402)  time: 0.1943  data: 0.0001  max mem: 14938
[16:33:18.356014] Test:  [330/345]  eta: 0:00:02  loss: 0.7307 (0.7399)  time: 0.1945  data: 0.0001  max mem: 14938
[16:33:20.307592] Test:  [340/345]  eta: 0:00:00  loss: 0.7333 (0.7398)  time: 0.1949  data: 0.0001  max mem: 14938
[16:33:21.090048] Test:  [344/345]  eta: 0:00:00  loss: 0.7361 (0.7398)  time: 0.1950  data: 0.0001  max mem: 14938
[16:33:21.149487] Test: Total time: 0:01:05 (0.1902 s / it)
[16:33:31.488137] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8862 (0.8862)  time: 0.3158  data: 0.1359  max mem: 14938
[16:33:33.303923] Test:  [10/57]  eta: 0:00:09  loss: 0.8765 (0.8806)  time: 0.1937  data: 0.0124  max mem: 14938
[16:33:35.126705] Test:  [20/57]  eta: 0:00:06  loss: 0.8765 (0.8740)  time: 0.1819  data: 0.0001  max mem: 14938
[16:33:36.951346] Test:  [30/57]  eta: 0:00:05  loss: 0.7639 (0.8351)  time: 0.1823  data: 0.0001  max mem: 14938
[16:33:38.782176] Test:  [40/57]  eta: 0:00:03  loss: 0.7511 (0.8145)  time: 0.1827  data: 0.0001  max mem: 14938
[16:33:40.617155] Test:  [50/57]  eta: 0:00:01  loss: 0.7408 (0.8070)  time: 0.1832  data: 0.0001  max mem: 14938
[16:33:41.608504] Test:  [56/57]  eta: 0:00:00  loss: 0.7640 (0.8124)  time: 0.1779  data: 0.0001  max mem: 14938
[16:33:41.666818] Test: Total time: 0:00:10 (0.1841 s / it)
[16:33:43.401076] Dice score of the network on the train images: 0.796958, val images: 0.816561
[16:33:43.401324] saving best_prec_model_0 @ epoch 21
[16:33:44.514757] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[16:33:45.397460] Epoch: [22]  [  0/345]  eta: 0:05:04  lr: 0.000124  loss: 0.7526 (0.7526)  time: 0.8816  data: 0.1409  max mem: 14938
[16:34:00.417722] Epoch: [22]  [ 20/345]  eta: 0:04:06  lr: 0.000124  loss: 0.7625 (0.7672)  time: 0.7510  data: 0.0001  max mem: 14938
[16:34:15.369019] Epoch: [22]  [ 40/345]  eta: 0:03:49  lr: 0.000123  loss: 0.7574 (0.7658)  time: 0.7475  data: 0.0001  max mem: 14938
[16:34:30.328081] Epoch: [22]  [ 60/345]  eta: 0:03:34  lr: 0.000123  loss: 0.7727 (0.7677)  time: 0.7479  data: 0.0001  max mem: 14938
[16:34:45.302927] Epoch: [22]  [ 80/345]  eta: 0:03:18  lr: 0.000123  loss: 0.7816 (0.7706)  time: 0.7487  data: 0.0001  max mem: 14938

[16:35:00.307388] Epoch: [22]  [100/345]  eta: 0:03:03  lr: 0.000123  loss: 0.7653 (0.7717)  time: 0.7502  data: 0.0001  max mem: 14938
[16:35:15.338807] Epoch: [22]  [120/345]  eta: 0:02:48  lr: 0.000123  loss: 0.7584 (0.7703)  time: 0.7515  data: 0.0001  max mem: 14938
[16:35:30.376827] Epoch: [22]  [140/345]  eta: 0:02:33  lr: 0.000123  loss: 0.7709 (0.7704)  time: 0.7519  data: 0.0001  max mem: 14938
[16:35:45.406460] Epoch: [22]  [160/345]  eta: 0:02:18  lr: 0.000123  loss: 0.7570 (0.7690)  time: 0.7514  data: 0.0001  max mem: 14938
[16:36:00.420142] Epoch: [22]  [180/345]  eta: 0:02:03  lr: 0.000123  loss: 0.7575 (0.7681)  time: 0.7506  data: 0.0001  max mem: 14938
[16:36:15.430979] Epoch: [22]  [200/345]  eta: 0:01:48  lr: 0.000123  loss: 0.7885 (0.7713)  time: 0.7505  data: 0.0001  max mem: 14938
[16:36:30.432353] Epoch: [22]  [220/345]  eta: 0:01:33  lr: 0.000123  loss: 0.7716 (0.7717)  time: 0.7500  data: 0.0001  max mem: 14938
[16:36:45.430409] Epoch: [22]  [240/345]  eta: 0:01:18  lr: 0.000123  loss: 0.7701 (0.7715)  time: 0.7499  data: 0.0001  max mem: 14938

[16:37:00.417837] Epoch: [22]  [260/345]  eta: 0:01:03  lr: 0.000122  loss: 0.7699 (0.7713)  time: 0.7493  data: 0.0001  max mem: 14938
[16:37:15.405679] Epoch: [22]  [280/345]  eta: 0:00:48  lr: 0.000122  loss: 0.7557 (0.7703)  time: 0.7493  data: 0.0001  max mem: 14938
[16:37:30.387375] Epoch: [22]  [300/345]  eta: 0:00:33  lr: 0.000122  loss: 0.7684 (0.7702)  time: 0.7490  data: 0.0001  max mem: 14938
[16:37:45.360960] Epoch: [22]  [320/345]  eta: 0:00:18  lr: 0.000122  loss: 0.7644 (0.7700)  time: 0.7486  data: 0.0001  max mem: 14938
[16:38:00.338185] Epoch: [22]  [340/345]  eta: 0:00:03  lr: 0.000122  loss: 0.7642 (0.7696)  time: 0.7488  data: 0.0001  max mem: 14938
[16:38:03.333285] Epoch: [22]  [344/345]  eta: 0:00:00  lr: 0.000122  loss: 0.7592 (0.7693)  time: 0.7488  data: 0.0001  max mem: 14938
[16:38:03.398417] Epoch: [22] Total time: 0:04:18 (0.7504 s / it)
[16:38:03.398776] Averaged stats: lr: 0.000122  loss: 0.7592 (0.7693)
[16:38:03.737381] Test:  [  0/345]  eta: 0:01:55  loss: 0.7336 (0.7336)  time: 0.3353  data: 0.1537  max mem: 14938
[16:38:05.576189] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7289 (0.7313)  time: 0.1976  data: 0.0140  max mem: 14938
[16:38:07.416881] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7307 (0.7323)  time: 0.1839  data: 0.0001  max mem: 14938
[16:38:09.260714] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7367 (0.7337)  time: 0.1842  data: 0.0001  max mem: 14938
[16:38:11.108894] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7402 (0.7352)  time: 0.1845  data: 0.0001  max mem: 14938
[16:38:12.960409] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7337 (0.7338)  time: 0.1849  data: 0.0001  max mem: 14938
[16:38:14.815774] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7299 (0.7339)  time: 0.1853  data: 0.0001  max mem: 14938
[16:38:16.674896] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7310 (0.7338)  time: 0.1857  data: 0.0001  max mem: 14938
[16:38:18.535031] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7351 (0.7338)  time: 0.1859  data: 0.0001  max mem: 14938
[16:38:20.399596] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7293 (0.7338)  time: 0.1862  data: 0.0001  max mem: 14938
[16:38:22.268426] Test:  [100/345]  eta: 0:00:45  loss: 0.7343 (0.7346)  time: 0.1866  data: 0.0001  max mem: 14938
[16:38:24.140168] Test:  [110/345]  eta: 0:00:43  loss: 0.7364 (0.7346)  time: 0.1869  data: 0.0001  max mem: 14938
[16:38:26.015037] Test:  [120/345]  eta: 0:00:42  loss: 0.7364 (0.7353)  time: 0.1873  data: 0.0001  max mem: 14938
[16:38:27.893150] Test:  [130/345]  eta: 0:00:40  loss: 0.7418 (0.7359)  time: 0.1876  data: 0.0001  max mem: 14938
[16:38:29.774866] Test:  [140/345]  eta: 0:00:38  loss: 0.7432 (0.7365)  time: 0.1879  data: 0.0001  max mem: 14938
[16:38:31.660948] Test:  [150/345]  eta: 0:00:36  loss: 0.7320 (0.7357)  time: 0.1883  data: 0.0001  max mem: 14938
[16:38:33.549258] Test:  [160/345]  eta: 0:00:34  loss: 0.7307 (0.7359)  time: 0.1887  data: 0.0001  max mem: 14938
[16:38:35.442685] Test:  [170/345]  eta: 0:00:32  loss: 0.7348 (0.7359)  time: 0.1890  data: 0.0001  max mem: 14938
[16:38:37.340381] Test:  [180/345]  eta: 0:00:30  loss: 0.7349 (0.7359)  time: 0.1895  data: 0.0001  max mem: 14938
[16:38:39.242169] Test:  [190/345]  eta: 0:00:29  loss: 0.7349 (0.7362)  time: 0.1899  data: 0.0001  max mem: 14938
[16:38:41.146420] Test:  [200/345]  eta: 0:00:27  loss: 0.7324 (0.7360)  time: 0.1903  data: 0.0001  max mem: 14938
[16:38:43.052987] Test:  [210/345]  eta: 0:00:25  loss: 0.7343 (0.7361)  time: 0.1905  data: 0.0001  max mem: 14938
[16:38:44.960459] Test:  [220/345]  eta: 0:00:23  loss: 0.7372 (0.7364)  time: 0.1907  data: 0.0001  max mem: 14938
[16:38:46.873278] Test:  [230/345]  eta: 0:00:21  loss: 0.7348 (0.7361)  time: 0.1910  data: 0.0001  max mem: 14938
[16:38:48.791054] Test:  [240/345]  eta: 0:00:19  loss: 0.7283 (0.7362)  time: 0.1915  data: 0.0001  max mem: 14938
[16:38:50.713255] Test:  [250/345]  eta: 0:00:17  loss: 0.7425 (0.7365)  time: 0.1919  data: 0.0001  max mem: 14938
[16:38:52.637836] Test:  [260/345]  eta: 0:00:16  loss: 0.7402 (0.7364)  time: 0.1923  data: 0.0001  max mem: 14938
[16:38:54.564958] Test:  [270/345]  eta: 0:00:14  loss: 0.7355 (0.7365)  time: 0.1925  data: 0.0001  max mem: 14938
[16:38:56.497691] Test:  [280/345]  eta: 0:00:12  loss: 0.7360 (0.7367)  time: 0.1929  data: 0.0001  max mem: 14938
[16:38:58.432352] Test:  [290/345]  eta: 0:00:10  loss: 0.7406 (0.7368)  time: 0.1933  data: 0.0001  max mem: 14938
[16:39:00.370104] Test:  [300/345]  eta: 0:00:08  loss: 0.7343 (0.7368)  time: 0.1936  data: 0.0001  max mem: 14938
[16:39:02.313809] Test:  [310/345]  eta: 0:00:06  loss: 0.7343 (0.7370)  time: 0.1940  data: 0.0001  max mem: 14938
[16:39:04.259368] Test:  [320/345]  eta: 0:00:04  loss: 0.7353 (0.7372)  time: 0.1944  data: 0.0001  max mem: 14938
[16:39:06.206920] Test:  [330/345]  eta: 0:00:02  loss: 0.7422 (0.7376)  time: 0.1946  data: 0.0001  max mem: 14938
[16:39:08.158568] Test:  [340/345]  eta: 0:00:00  loss: 0.7422 (0.7378)  time: 0.1949  data: 0.0001  max mem: 14938
[16:39:08.939908] Test:  [344/345]  eta: 0:00:00  loss: 0.7430 (0.7379)  time: 0.1950  data: 0.0001  max mem: 14938
[16:39:08.997704] Test: Total time: 0:01:05 (0.1901 s / it)
[16:39:19.348098] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8152 (0.8152)  time: 0.3196  data: 0.1401  max mem: 14938
[16:39:21.164937] Test:  [10/57]  eta: 0:00:09  loss: 0.8578 (0.8697)  time: 0.1941  data: 0.0128  max mem: 14938
[16:39:22.987793] Test:  [20/57]  eta: 0:00:06  loss: 0.8700 (0.8616)  time: 0.1819  data: 0.0001  max mem: 14938
[16:39:24.814770] Test:  [30/57]  eta: 0:00:05  loss: 0.7572 (0.8224)  time: 0.1824  data: 0.0001  max mem: 14938
[16:39:26.644602] Test:  [40/57]  eta: 0:00:03  loss: 0.7419 (0.8033)  time: 0.1828  data: 0.0001  max mem: 14938
[16:39:28.481344] Test:  [50/57]  eta: 0:00:01  loss: 0.7352 (0.7966)  time: 0.1833  data: 0.0001  max mem: 14938
[16:39:29.470984] Test:  [56/57]  eta: 0:00:00  loss: 0.7644 (0.8022)  time: 0.1779  data: 0.0001  max mem: 14938
[16:39:29.529553] Test: Total time: 0:00:10 (0.1842 s / it)
[16:39:31.246909] Dice score of the network on the train images: 0.796304, val images: 0.816255
[16:39:31.250956] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[16:39:32.138570] Epoch: [23]  [  0/345]  eta: 0:05:06  lr: 0.000122  loss: 0.7457 (0.7457)  time: 0.8871  data: 0.1474  max mem: 14938
[16:39:47.038079] Epoch: [23]  [ 20/345]  eta: 0:04:04  lr: 0.000122  loss: 0.7552 (0.7571)  time: 0.7449  data: 0.0001  max mem: 14938
[16:40:01.995265] Epoch: [23]  [ 40/345]  eta: 0:03:48  lr: 0.000122  loss: 0.7784 (0.7667)  time: 0.7478  data: 0.0001  max mem: 14938
[16:40:16.942906] Epoch: [23]  [ 60/345]  eta: 0:03:33  lr: 0.000122  loss: 0.7557 (0.7654)  time: 0.7473  data: 0.0001  max mem: 14938
[16:40:32.040725] Epoch: [23]  [ 80/345]  eta: 0:03:18  lr: 0.000121  loss: 0.7633 (0.7656)  time: 0.7548  data: 0.0001  max mem: 14938
[16:40:47.041057] Epoch: [23]  [100/345]  eta: 0:03:03  lr: 0.000121  loss: 0.7589 (0.7648)  time: 0.7500  data: 0.0001  max mem: 14938
[16:41:02.056322] Epoch: [23]  [120/345]  eta: 0:02:48  lr: 0.000121  loss: 0.7565 (0.7650)  time: 0.7507  data: 0.0001  max mem: 14938
[16:41:17.074901] Epoch: [23]  [140/345]  eta: 0:02:33  lr: 0.000121  loss: 0.7525 (0.7631)  time: 0.7509  data: 0.0001  max mem: 14938
[16:41:32.100178] Epoch: [23]  [160/345]  eta: 0:02:18  lr: 0.000121  loss: 0.7551 (0.7626)  time: 0.7512  data: 0.0001  max mem: 14938
[16:41:47.118664] Epoch: [23]  [180/345]  eta: 0:02:03  lr: 0.000121  loss: 0.7496 (0.7618)  time: 0.7509  data: 0.0001  max mem: 14938
[16:42:02.123921] Epoch: [23]  [200/345]  eta: 0:01:48  lr: 0.000121  loss: 0.7677 (0.7623)  time: 0.7502  data: 0.0001  max mem: 14938
[16:42:17.121417] Epoch: [23]  [220/345]  eta: 0:01:33  lr: 0.000121  loss: 0.7674 (0.7628)  time: 0.7498  data: 0.0001  max mem: 14938
[16:42:32.119417] Epoch: [23]  [240/345]  eta: 0:01:18  lr: 0.000120  loss: 0.7632 (0.7628)  time: 0.7499  data: 0.0001  max mem: 14938
[16:42:47.109435] Epoch: [23]  [260/345]  eta: 0:01:03  lr: 0.000120  loss: 0.7659 (0.7630)  time: 0.7495  data: 0.0001  max mem: 14938
[16:43:02.094368] Epoch: [23]  [280/345]  eta: 0:00:48  lr: 0.000120  loss: 0.7640 (0.7630)  time: 0.7492  data: 0.0001  max mem: 14938
[16:43:17.086338] Epoch: [23]  [300/345]  eta: 0:00:33  lr: 0.000120  loss: 0.7589 (0.7629)  time: 0.7496  data: 0.0001  max mem: 14938
[16:43:32.166304] Epoch: [23]  [320/345]  eta: 0:00:18  lr: 0.000120  loss: 0.7585 (0.7632)  time: 0.7540  data: 0.0001  max mem: 14938
[16:43:47.155421] Epoch: [23]  [340/345]  eta: 0:00:03  lr: 0.000120  loss: 0.7640 (0.7631)  time: 0.7494  data: 0.0001  max mem: 14938
[16:43:50.154322] Epoch: [23]  [344/345]  eta: 0:00:00  lr: 0.000120  loss: 0.7654 (0.7632)  time: 0.7495  data: 0.0001  max mem: 14938
[16:43:50.197555] Epoch: [23] Total time: 0:04:18 (0.7506 s / it)
[16:43:50.197966] Averaged stats: lr: 0.000120  loss: 0.7654 (0.7632)
[16:43:50.528652] Test:  [  0/345]  eta: 0:01:52  loss: 0.7638 (0.7638)  time: 0.3266  data: 0.1447  max mem: 14938
[16:43:52.368541] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7636 (0.7572)  time: 0.1969  data: 0.0132  max mem: 14938
[16:43:54.208681] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7538 (0.7567)  time: 0.1839  data: 0.0001  max mem: 14938
[16:43:56.053759] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7498 (0.7546)  time: 0.1842  data: 0.0001  max mem: 14938
[16:43:57.904001] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7499 (0.7542)  time: 0.1847  data: 0.0001  max mem: 14938
[16:43:59.757250] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7543 (0.7541)  time: 0.1851  data: 0.0001  max mem: 14938
[16:44:01.611825] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7543 (0.7536)  time: 0.1853  data: 0.0001  max mem: 14938
[16:44:03.469920] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7474 (0.7517)  time: 0.1856  data: 0.0001  max mem: 14938
[16:44:05.332225] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7453 (0.7513)  time: 0.1860  data: 0.0001  max mem: 14938
[16:44:07.196418] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7460 (0.7512)  time: 0.1863  data: 0.0001  max mem: 14938
[16:44:09.068353] Test:  [100/345]  eta: 0:00:45  loss: 0.7496 (0.7513)  time: 0.1867  data: 0.0001  max mem: 14938
[16:44:10.940481] Test:  [110/345]  eta: 0:00:43  loss: 0.7531 (0.7522)  time: 0.1871  data: 0.0001  max mem: 14938
[16:44:12.817432] Test:  [120/345]  eta: 0:00:42  loss: 0.7462 (0.7516)  time: 0.1874  data: 0.0001  max mem: 14938
[16:44:14.696612] Test:  [130/345]  eta: 0:00:40  loss: 0.7447 (0.7516)  time: 0.1878  data: 0.0001  max mem: 14938
[16:44:16.578566] Test:  [140/345]  eta: 0:00:38  loss: 0.7447 (0.7510)  time: 0.1880  data: 0.0001  max mem: 14938
[16:44:18.464939] Test:  [150/345]  eta: 0:00:36  loss: 0.7437 (0.7506)  time: 0.1884  data: 0.0001  max mem: 14938
[16:44:20.353914] Test:  [160/345]  eta: 0:00:34  loss: 0.7510 (0.7509)  time: 0.1887  data: 0.0001  max mem: 14938
[16:44:22.247865] Test:  [170/345]  eta: 0:00:32  loss: 0.7541 (0.7512)  time: 0.1891  data: 0.0001  max mem: 14938
[16:44:24.146620] Test:  [180/345]  eta: 0:00:30  loss: 0.7541 (0.7512)  time: 0.1896  data: 0.0001  max mem: 14938
[16:44:26.047486] Test:  [190/345]  eta: 0:00:29  loss: 0.7474 (0.7511)  time: 0.1899  data: 0.0001  max mem: 14938
[16:44:27.951271] Test:  [200/345]  eta: 0:00:27  loss: 0.7495 (0.7510)  time: 0.1902  data: 0.0001  max mem: 14938
[16:44:29.857750] Test:  [210/345]  eta: 0:00:25  loss: 0.7475 (0.7509)  time: 0.1905  data: 0.0001  max mem: 14938
[16:44:31.768619] Test:  [220/345]  eta: 0:00:23  loss: 0.7472 (0.7509)  time: 0.1908  data: 0.0001  max mem: 14938
[16:44:33.684187] Test:  [230/345]  eta: 0:00:21  loss: 0.7462 (0.7508)  time: 0.1913  data: 0.0001  max mem: 14938
[16:44:35.601235] Test:  [240/345]  eta: 0:00:19  loss: 0.7415 (0.7505)  time: 0.1916  data: 0.0001  max mem: 14938
[16:44:37.522407] Test:  [250/345]  eta: 0:00:17  loss: 0.7415 (0.7503)  time: 0.1919  data: 0.0001  max mem: 14938
[16:44:39.447167] Test:  [260/345]  eta: 0:00:16  loss: 0.7522 (0.7506)  time: 0.1922  data: 0.0001  max mem: 14938
[16:44:41.374248] Test:  [270/345]  eta: 0:00:14  loss: 0.7522 (0.7505)  time: 0.1925  data: 0.0001  max mem: 14938
[16:44:43.306309] Test:  [280/345]  eta: 0:00:12  loss: 0.7449 (0.7503)  time: 0.1929  data: 0.0001  max mem: 14938
[16:44:45.241881] Test:  [290/345]  eta: 0:00:10  loss: 0.7402 (0.7500)  time: 0.1933  data: 0.0001  max mem: 14938
[16:44:47.179068] Test:  [300/345]  eta: 0:00:08  loss: 0.7368 (0.7498)  time: 0.1936  data: 0.0001  max mem: 14938
[16:44:49.121651] Test:  [310/345]  eta: 0:00:06  loss: 0.7381 (0.7498)  time: 0.1939  data: 0.0001  max mem: 14938
[16:44:51.067004] Test:  [320/345]  eta: 0:00:04  loss: 0.7510 (0.7498)  time: 0.1943  data: 0.0001  max mem: 14938
[16:44:53.015841] Test:  [330/345]  eta: 0:00:02  loss: 0.7497 (0.7497)  time: 0.1947  data: 0.0001  max mem: 14938
[16:44:54.967820] Test:  [340/345]  eta: 0:00:00  loss: 0.7497 (0.7497)  time: 0.1950  data: 0.0001  max mem: 14938
[16:44:55.750204] Test:  [344/345]  eta: 0:00:00  loss: 0.7520 (0.7498)  time: 0.1951  data: 0.0001  max mem: 14938
[16:44:55.806587] Test: Total time: 0:01:05 (0.1902 s / it)
[16:45:06.040340] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8707 (0.8707)  time: 0.3193  data: 0.1393  max mem: 14938
[16:45:07.858416] Test:  [10/57]  eta: 0:00:09  loss: 0.9071 (0.9057)  time: 0.1942  data: 0.0127  max mem: 14938
[16:45:09.682858] Test:  [20/57]  eta: 0:00:06  loss: 0.9112 (0.8937)  time: 0.1821  data: 0.0001  max mem: 14938
[16:45:11.509466] Test:  [30/57]  eta: 0:00:05  loss: 0.7839 (0.8503)  time: 0.1825  data: 0.0001  max mem: 14938
[16:45:13.341105] Test:  [40/57]  eta: 0:00:03  loss: 0.7499 (0.8269)  time: 0.1829  data: 0.0001  max mem: 14938
[16:45:15.178121] Test:  [50/57]  eta: 0:00:01  loss: 0.7576 (0.8193)  time: 0.1834  data: 0.0001  max mem: 14938
[16:45:16.168741] Test:  [56/57]  eta: 0:00:00  loss: 0.7782 (0.8245)  time: 0.1780  data: 0.0001  max mem: 14938
[16:45:16.227396] Test: Total time: 0:00:10 (0.1843 s / it)
[16:45:17.931863] Dice score of the network on the train images: 0.793065, val images: 0.816255
[16:45:17.932093] saving best_prec_model_0 @ epoch 23
[16:45:18.973903] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[16:45:19.862836] Epoch: [24]  [  0/345]  eta: 0:05:06  lr: 0.000120  loss: 0.7666 (0.7666)  time: 0.8880  data: 0.1438  max mem: 14938
[16:45:34.743885] Epoch: [24]  [ 20/345]  eta: 0:04:04  lr: 0.000119  loss: 0.7682 (0.7716)  time: 0.7440  data: 0.0001  max mem: 14938
[16:45:49.686712] Epoch: [24]  [ 40/345]  eta: 0:03:48  lr: 0.000119  loss: 0.7595 (0.7672)  time: 0.7471  data: 0.0001  max mem: 14938
[16:46:04.668692] Epoch: [24]  [ 60/345]  eta: 0:03:33  lr: 0.000119  loss: 0.7664 (0.7672)  time: 0.7491  data: 0.0001  max mem: 14938
[16:46:19.656976] Epoch: [24]  [ 80/345]  eta: 0:03:18  lr: 0.000119  loss: 0.7591 (0.7654)  time: 0.7494  data: 0.0001  max mem: 14938
[16:46:34.665846] Epoch: [24]  [100/345]  eta: 0:03:03  lr: 0.000119  loss: 0.7705 (0.7662)  time: 0.7504  data: 0.0001  max mem: 14938
[16:46:49.691658] Epoch: [24]  [120/345]  eta: 0:02:48  lr: 0.000119  loss: 0.7518 (0.7647)  time: 0.7512  data: 0.0001  max mem: 14938
[16:47:04.730731] Epoch: [24]  [140/345]  eta: 0:02:33  lr: 0.000118  loss: 0.7507 (0.7635)  time: 0.7519  data: 0.0001  max mem: 14938
[16:47:19.768023] Epoch: [24]  [160/345]  eta: 0:02:18  lr: 0.000118  loss: 0.7582 (0.7633)  time: 0.7518  data: 0.0001  max mem: 14938
[16:47:34.795179] Epoch: [24]  [180/345]  eta: 0:02:03  lr: 0.000118  loss: 0.7668 (0.7638)  time: 0.7513  data: 0.0001  max mem: 14938
[16:47:49.804173] Epoch: [24]  [200/345]  eta: 0:01:48  lr: 0.000118  loss: 0.7572 (0.7634)  time: 0.7504  data: 0.0001  max mem: 14938
[16:48:04.808922] Epoch: [24]  [220/345]  eta: 0:01:33  lr: 0.000118  loss: 0.7586 (0.7629)  time: 0.7502  data: 0.0001  max mem: 14938
[16:48:19.798800] Epoch: [24]  [240/345]  eta: 0:01:18  lr: 0.000118  loss: 0.7526 (0.7621)  time: 0.7494  data: 0.0001  max mem: 14938
[16:48:34.783955] Epoch: [24]  [260/345]  eta: 0:01:03  lr: 0.000117  loss: 0.7544 (0.7615)  time: 0.7492  data: 0.0001  max mem: 14938
[16:48:49.768487] Epoch: [24]  [280/345]  eta: 0:00:48  lr: 0.000117  loss: 0.7541 (0.7613)  time: 0.7492  data: 0.0001  max mem: 14938
[16:49:04.746170] Epoch: [24]  [300/345]  eta: 0:00:33  lr: 0.000117  loss: 0.7549 (0.7611)  time: 0.7488  data: 0.0001  max mem: 14938
[16:49:19.746371] Epoch: [24]  [320/345]  eta: 0:00:18  lr: 0.000117  loss: 0.7532 (0.7610)  time: 0.7500  data: 0.0001  max mem: 14938
[16:49:34.717273] Epoch: [24]  [340/345]  eta: 0:00:03  lr: 0.000117  loss: 0.7529 (0.7610)  time: 0.7485  data: 0.0001  max mem: 14938
[16:49:37.716755] Epoch: [24]  [344/345]  eta: 0:00:00  lr: 0.000117  loss: 0.7527 (0.7610)  time: 0.7493  data: 0.0001  max mem: 14938
[16:49:37.780554] Epoch: [24] Total time: 0:04:18 (0.7502 s / it)
[16:49:37.780891] Averaged stats: lr: 0.000117  loss: 0.7527 (0.7610)
[16:49:38.112830] Test:  [  0/345]  eta: 0:01:52  loss: 0.7291 (0.7291)  time: 0.3259  data: 0.1437  max mem: 14938
[16:49:39.951783] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7326 (0.7336)  time: 0.1967  data: 0.0131  max mem: 14938
[16:49:41.792916] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7343 (0.7352)  time: 0.1839  data: 0.0001  max mem: 14938
[16:49:43.635811] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7255 (0.7330)  time: 0.1841  data: 0.0001  max mem: 14938
[16:49:45.480813] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7415 (0.7349)  time: 0.1843  data: 0.0001  max mem: 14938
[16:49:47.332782] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7367 (0.7335)  time: 0.1848  data: 0.0001  max mem: 14938
[16:49:49.187672] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7245 (0.7319)  time: 0.1853  data: 0.0001  max mem: 14938
[16:49:51.045452] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7292 (0.7322)  time: 0.1856  data: 0.0001  max mem: 14938
[16:49:52.906004] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7269 (0.7311)  time: 0.1859  data: 0.0001  max mem: 14938
[16:49:54.770575] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7252 (0.7316)  time: 0.1862  data: 0.0001  max mem: 14938
[16:49:56.640047] Test:  [100/345]  eta: 0:00:45  loss: 0.7371 (0.7313)  time: 0.1867  data: 0.0001  max mem: 14938
[16:49:58.511672] Test:  [110/345]  eta: 0:00:43  loss: 0.7320 (0.7319)  time: 0.1870  data: 0.0001  max mem: 14938
[16:50:00.385427] Test:  [120/345]  eta: 0:00:42  loss: 0.7320 (0.7323)  time: 0.1872  data: 0.0001  max mem: 14938
[16:50:02.264615] Test:  [130/345]  eta: 0:00:40  loss: 0.7286 (0.7322)  time: 0.1876  data: 0.0001  max mem: 14938
[16:50:04.146784] Test:  [140/345]  eta: 0:00:38  loss: 0.7271 (0.7315)  time: 0.1880  data: 0.0001  max mem: 14938
[16:50:06.033201] Test:  [150/345]  eta: 0:00:36  loss: 0.7196 (0.7313)  time: 0.1884  data: 0.0001  max mem: 14938
[16:50:07.921405] Test:  [160/345]  eta: 0:00:34  loss: 0.7315 (0.7314)  time: 0.1887  data: 0.0001  max mem: 14938
[16:50:09.814658] Test:  [170/345]  eta: 0:00:32  loss: 0.7249 (0.7312)  time: 0.1890  data: 0.0001  max mem: 14938
[16:50:11.712314] Test:  [180/345]  eta: 0:00:30  loss: 0.7280 (0.7312)  time: 0.1895  data: 0.0001  max mem: 14938
[16:50:13.611167] Test:  [190/345]  eta: 0:00:29  loss: 0.7295 (0.7313)  time: 0.1898  data: 0.0001  max mem: 14938
[16:50:15.514331] Test:  [200/345]  eta: 0:00:27  loss: 0.7295 (0.7310)  time: 0.1900  data: 0.0001  max mem: 14938
[16:50:17.421807] Test:  [210/345]  eta: 0:00:25  loss: 0.7299 (0.7309)  time: 0.1905  data: 0.0001  max mem: 14938
[16:50:19.332044] Test:  [220/345]  eta: 0:00:23  loss: 0.7216 (0.7307)  time: 0.1908  data: 0.0001  max mem: 14938
[16:50:21.246048] Test:  [230/345]  eta: 0:00:21  loss: 0.7216 (0.7302)  time: 0.1912  data: 0.0001  max mem: 14938
[16:50:23.164346] Test:  [240/345]  eta: 0:00:19  loss: 0.7245 (0.7301)  time: 0.1916  data: 0.0001  max mem: 14938
[16:50:25.086587] Test:  [250/345]  eta: 0:00:17  loss: 0.7238 (0.7299)  time: 0.1920  data: 0.0001  max mem: 14938
[16:50:27.010545] Test:  [260/345]  eta: 0:00:16  loss: 0.7248 (0.7297)  time: 0.1923  data: 0.0001  max mem: 14938
[16:50:28.939360] Test:  [270/345]  eta: 0:00:14  loss: 0.7305 (0.7300)  time: 0.1926  data: 0.0001  max mem: 14938
[16:50:30.870424] Test:  [280/345]  eta: 0:00:12  loss: 0.7343 (0.7300)  time: 0.1929  data: 0.0001  max mem: 14938
[16:50:32.804306] Test:  [290/345]  eta: 0:00:10  loss: 0.7311 (0.7299)  time: 0.1932  data: 0.0001  max mem: 14938
[16:50:34.741571] Test:  [300/345]  eta: 0:00:08  loss: 0.7209 (0.7296)  time: 0.1935  data: 0.0001  max mem: 14938
[16:50:36.681339] Test:  [310/345]  eta: 0:00:06  loss: 0.7245 (0.7296)  time: 0.1938  data: 0.0001  max mem: 14938
[16:50:38.627258] Test:  [320/345]  eta: 0:00:04  loss: 0.7234 (0.7294)  time: 0.1942  data: 0.0001  max mem: 14938
[16:50:40.574860] Test:  [330/345]  eta: 0:00:02  loss: 0.7308 (0.7297)  time: 0.1946  data: 0.0001  max mem: 14938
[16:50:42.526062] Test:  [340/345]  eta: 0:00:00  loss: 0.7357 (0.7300)  time: 0.1949  data: 0.0001  max mem: 14938
[16:50:43.307678] Test:  [344/345]  eta: 0:00:00  loss: 0.7308 (0.7299)  time: 0.1950  data: 0.0001  max mem: 14938
[16:50:43.367531] Test: Total time: 0:01:05 (0.1901 s / it)
[16:50:53.761785] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8577 (0.8577)  time: 0.3204  data: 0.1405  max mem: 14938
[16:50:55.578968] Test:  [10/57]  eta: 0:00:09  loss: 0.8705 (0.8739)  time: 0.1943  data: 0.0128  max mem: 14938
[16:50:57.402863] Test:  [20/57]  eta: 0:00:06  loss: 0.8851 (0.8607)  time: 0.1820  data: 0.0001  max mem: 14938
[16:50:59.229710] Test:  [30/57]  eta: 0:00:05  loss: 0.7531 (0.8221)  time: 0.1825  data: 0.0001  max mem: 14938
[16:51:01.059188] Test:  [40/57]  eta: 0:00:03  loss: 0.7433 (0.8021)  time: 0.1828  data: 0.0001  max mem: 14938
[16:51:02.894316] Test:  [50/57]  eta: 0:00:01  loss: 0.7410 (0.7960)  time: 0.1832  data: 0.0001  max mem: 14938
[16:51:03.884057] Test:  [56/57]  eta: 0:00:00  loss: 0.7587 (0.8011)  time: 0.1777  data: 0.0001  max mem: 14938
[16:51:03.942584] Test: Total time: 0:00:10 (0.1842 s / it)
[16:51:05.674128] Dice score of the network on the train images: 0.789790, val images: 0.817211
[16:51:05.678446] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[16:51:06.567449] Epoch: [25]  [  0/345]  eta: 0:05:06  lr: 0.000117  loss: 0.7421 (0.7421)  time: 0.8879  data: 0.1463  max mem: 14938
[16:51:21.443175] Epoch: [25]  [ 20/345]  eta: 0:04:03  lr: 0.000116  loss: 0.7598 (0.7583)  time: 0.7437  data: 0.0001  max mem: 14938
[16:51:36.379832] Epoch: [25]  [ 40/345]  eta: 0:03:48  lr: 0.000116  loss: 0.7479 (0.7548)  time: 0.7468  data: 0.0001  max mem: 14938
[16:51:51.343834] Epoch: [25]  [ 60/345]  eta: 0:03:33  lr: 0.000116  loss: 0.7555 (0.7554)  time: 0.7482  data: 0.0001  max mem: 14938
[16:52:06.333001] Epoch: [25]  [ 80/345]  eta: 0:03:18  lr: 0.000116  loss: 0.7516 (0.7551)  time: 0.7494  data: 0.0001  max mem: 14938
[16:52:21.338427] Epoch: [25]  [100/345]  eta: 0:03:03  lr: 0.000116  loss: 0.7563 (0.7560)  time: 0.7502  data: 0.0001  max mem: 14938
[16:52:36.365255] Epoch: [25]  [120/345]  eta: 0:02:48  lr: 0.000115  loss: 0.7520 (0.7561)  time: 0.7513  data: 0.0001  max mem: 14938
[16:52:51.371318] Epoch: [25]  [140/345]  eta: 0:02:33  lr: 0.000115  loss: 0.7447 (0.7546)  time: 0.7503  data: 0.0001  max mem: 14938
[16:53:06.390392] Epoch: [25]  [160/345]  eta: 0:02:18  lr: 0.000115  loss: 0.7490 (0.7544)  time: 0.7509  data: 0.0001  max mem: 14938
[16:53:21.404409] Epoch: [25]  [180/345]  eta: 0:02:03  lr: 0.000115  loss: 0.7608 (0.7550)  time: 0.7507  data: 0.0001  max mem: 14938
[16:53:36.420300] Epoch: [25]  [200/345]  eta: 0:01:48  lr: 0.000115  loss: 0.7430 (0.7538)  time: 0.7508  data: 0.0001  max mem: 14938
[16:53:51.403281] Epoch: [25]  [220/345]  eta: 0:01:33  lr: 0.000114  loss: 0.7498 (0.7536)  time: 0.7491  data: 0.0001  max mem: 14938
[16:54:06.385961] Epoch: [25]  [240/345]  eta: 0:01:18  lr: 0.000114  loss: 0.7446 (0.7533)  time: 0.7491  data: 0.0001  max mem: 14938
[16:54:21.360221] Epoch: [25]  [260/345]  eta: 0:01:03  lr: 0.000114  loss: 0.7608 (0.7540)  time: 0.7487  data: 0.0001  max mem: 14938
[16:54:36.327992] Epoch: [25]  [280/345]  eta: 0:00:48  lr: 0.000114  loss: 0.7536 (0.7539)  time: 0.7484  data: 0.0001  max mem: 14938
[16:54:51.299259] Epoch: [25]  [300/345]  eta: 0:00:33  lr: 0.000114  loss: 0.7506 (0.7537)  time: 0.7485  data: 0.0001  max mem: 14938
[16:55:06.283834] Epoch: [25]  [320/345]  eta: 0:00:18  lr: 0.000113  loss: 0.7641 (0.7545)  time: 0.7492  data: 0.0001  max mem: 14938
[16:55:21.253445] Epoch: [25]  [340/345]  eta: 0:00:03  lr: 0.000113  loss: 0.7571 (0.7546)  time: 0.7484  data: 0.0001  max mem: 14938
[16:55:24.246624] Epoch: [25]  [344/345]  eta: 0:00:00  lr: 0.000113  loss: 0.7588 (0.7547)  time: 0.7483  data: 0.0001  max mem: 14938
[16:55:24.311875] Epoch: [25] Total time: 0:04:18 (0.7497 s / it)
[16:55:24.312376] Averaged stats: lr: 0.000113  loss: 0.7588 (0.7547)
[16:55:24.646427] Test:  [  0/345]  eta: 0:01:54  loss: 0.7061 (0.7061)  time: 0.3304  data: 0.1483  max mem: 14938
[16:55:26.486027] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7266 (0.7324)  time: 0.1972  data: 0.0136  max mem: 14938
[16:55:28.328423] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7278 (0.7311)  time: 0.1840  data: 0.0001  max mem: 14938
[16:55:30.172610] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7285 (0.7302)  time: 0.1843  data: 0.0001  max mem: 14938
[16:55:32.021175] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7306 (0.7314)  time: 0.1846  data: 0.0001  max mem: 14938
[16:55:33.874214] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7309 (0.7317)  time: 0.1850  data: 0.0001  max mem: 14938
[16:55:35.729204] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7309 (0.7326)  time: 0.1853  data: 0.0001  max mem: 14938
[16:55:37.587118] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7380 (0.7329)  time: 0.1856  data: 0.0001  max mem: 14938
[16:55:39.448961] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7337 (0.7325)  time: 0.1859  data: 0.0001  max mem: 14938
[16:55:41.314520] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7310 (0.7325)  time: 0.1863  data: 0.0001  max mem: 14938
[16:55:43.183054] Test:  [100/345]  eta: 0:00:45  loss: 0.7285 (0.7319)  time: 0.1866  data: 0.0001  max mem: 14938
[16:55:45.054091] Test:  [110/345]  eta: 0:00:43  loss: 0.7303 (0.7329)  time: 0.1869  data: 0.0001  max mem: 14938
[16:55:46.929238] Test:  [120/345]  eta: 0:00:42  loss: 0.7408 (0.7332)  time: 0.1873  data: 0.0001  max mem: 14938
[16:55:48.808373] Test:  [130/345]  eta: 0:00:40  loss: 0.7297 (0.7327)  time: 0.1877  data: 0.0001  max mem: 14938
[16:55:50.691757] Test:  [140/345]  eta: 0:00:38  loss: 0.7297 (0.7329)  time: 0.1881  data: 0.0001  max mem: 14938
[16:55:52.577820] Test:  [150/345]  eta: 0:00:36  loss: 0.7343 (0.7332)  time: 0.1884  data: 0.0001  max mem: 14938
[16:55:54.466518] Test:  [160/345]  eta: 0:00:34  loss: 0.7302 (0.7333)  time: 0.1887  data: 0.0001  max mem: 14938
[16:55:56.361160] Test:  [170/345]  eta: 0:00:32  loss: 0.7302 (0.7331)  time: 0.1891  data: 0.0001  max mem: 14938
[16:55:58.260687] Test:  [180/345]  eta: 0:00:30  loss: 0.7323 (0.7332)  time: 0.1897  data: 0.0001  max mem: 14938
[16:56:00.162747] Test:  [190/345]  eta: 0:00:29  loss: 0.7347 (0.7336)  time: 0.1900  data: 0.0001  max mem: 14938
[16:56:02.066504] Test:  [200/345]  eta: 0:00:27  loss: 0.7376 (0.7336)  time: 0.1902  data: 0.0001  max mem: 14938
[16:56:03.975107] Test:  [210/345]  eta: 0:00:25  loss: 0.7319 (0.7334)  time: 0.1906  data: 0.0001  max mem: 14938
[16:56:05.885197] Test:  [220/345]  eta: 0:00:23  loss: 0.7300 (0.7334)  time: 0.1909  data: 0.0001  max mem: 14938
[16:56:07.799316] Test:  [230/345]  eta: 0:00:21  loss: 0.7325 (0.7340)  time: 0.1912  data: 0.0001  max mem: 14938
[16:56:09.719858] Test:  [240/345]  eta: 0:00:19  loss: 0.7435 (0.7341)  time: 0.1917  data: 0.0001  max mem: 14938
[16:56:11.640559] Test:  [250/345]  eta: 0:00:17  loss: 0.7359 (0.7342)  time: 0.1920  data: 0.0001  max mem: 14938
[16:56:13.566378] Test:  [260/345]  eta: 0:00:16  loss: 0.7342 (0.7342)  time: 0.1922  data: 0.0001  max mem: 14938
[16:56:15.496056] Test:  [270/345]  eta: 0:00:14  loss: 0.7268 (0.7339)  time: 0.1927  data: 0.0001  max mem: 14938
[16:56:17.428623] Test:  [280/345]  eta: 0:00:12  loss: 0.7242 (0.7338)  time: 0.1931  data: 0.0001  max mem: 14938
[16:56:19.363358] Test:  [290/345]  eta: 0:00:10  loss: 0.7255 (0.7338)  time: 0.1933  data: 0.0001  max mem: 14938
[16:56:21.302719] Test:  [300/345]  eta: 0:00:08  loss: 0.7255 (0.7335)  time: 0.1937  data: 0.0001  max mem: 14938
[16:56:23.243149] Test:  [310/345]  eta: 0:00:06  loss: 0.7292 (0.7335)  time: 0.1939  data: 0.0001  max mem: 14938
[16:56:25.189642] Test:  [320/345]  eta: 0:00:04  loss: 0.7292 (0.7336)  time: 0.1943  data: 0.0001  max mem: 14938
[16:56:27.137544] Test:  [330/345]  eta: 0:00:02  loss: 0.7291 (0.7335)  time: 0.1947  data: 0.0001  max mem: 14938
[16:56:29.087583] Test:  [340/345]  eta: 0:00:00  loss: 0.7382 (0.7338)  time: 0.1948  data: 0.0001  max mem: 14938
[16:56:29.868998] Test:  [344/345]  eta: 0:00:00  loss: 0.7382 (0.7338)  time: 0.1950  data: 0.0001  max mem: 14938
[16:56:29.927396] Test: Total time: 0:01:05 (0.1902 s / it)
[16:56:40.379710] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8775 (0.8775)  time: 0.3215  data: 0.1417  max mem: 14938
[16:56:42.197067] Test:  [10/57]  eta: 0:00:09  loss: 0.8959 (0.8957)  time: 0.1943  data: 0.0129  max mem: 14938
[16:56:44.017344] Test:  [20/57]  eta: 0:00:06  loss: 0.8975 (0.8809)  time: 0.1818  data: 0.0001  max mem: 14938
[16:56:45.844426] Test:  [30/57]  eta: 0:00:05  loss: 0.7624 (0.8370)  time: 0.1823  data: 0.0001  max mem: 14938
[16:56:47.676798] Test:  [40/57]  eta: 0:00:03  loss: 0.7491 (0.8146)  time: 0.1829  data: 0.0001  max mem: 14938
[16:56:49.513430] Test:  [50/57]  eta: 0:00:01  loss: 0.7553 (0.8078)  time: 0.1834  data: 0.0001  max mem: 14938
[16:56:50.502844] Test:  [56/57]  eta: 0:00:00  loss: 0.7672 (0.8139)  time: 0.1780  data: 0.0001  max mem: 14938
[16:56:50.557014] Test: Total time: 0:00:10 (0.1842 s / it)
[16:56:52.309625] Dice score of the network on the train images: 0.791560, val images: 0.815753
[16:56:52.314057] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[16:56:53.197116] Epoch: [26]  [  0/345]  eta: 0:05:04  lr: 0.000113  loss: 0.7351 (0.7351)  time: 0.8819  data: 0.1404  max mem: 14938
[16:57:08.086283] Epoch: [26]  [ 20/345]  eta: 0:04:04  lr: 0.000113  loss: 0.7598 (0.7618)  time: 0.7444  data: 0.0001  max mem: 14938
[16:57:23.030373] Epoch: [26]  [ 40/345]  eta: 0:03:48  lr: 0.000113  loss: 0.7531 (0.7575)  time: 0.7472  data: 0.0001  max mem: 14938
[16:57:37.992926] Epoch: [26]  [ 60/345]  eta: 0:03:33  lr: 0.000112  loss: 0.7530 (0.7559)  time: 0.7481  data: 0.0001  max mem: 14938
[16:57:52.981371] Epoch: [26]  [ 80/345]  eta: 0:03:18  lr: 0.000112  loss: 0.7394 (0.7529)  time: 0.7494  data: 0.0001  max mem: 14938
[16:58:08.122316] Epoch: [26]  [100/345]  eta: 0:03:03  lr: 0.000112  loss: 0.7425 (0.7512)  time: 0.7570  data: 0.0001  max mem: 14938
[16:58:23.154380] Epoch: [26]  [120/345]  eta: 0:02:48  lr: 0.000112  loss: 0.7502 (0.7513)  time: 0.7516  data: 0.0001  max mem: 14938
[16:58:38.189578] Epoch: [26]  [140/345]  eta: 0:02:33  lr: 0.000111  loss: 0.7414 (0.7500)  time: 0.7517  data: 0.0001  max mem: 14938
[16:58:53.244240] Epoch: [26]  [160/345]  eta: 0:02:18  lr: 0.000111  loss: 0.7487 (0.7503)  time: 0.7527  data: 0.0001  max mem: 14938
[16:59:08.277246] Epoch: [26]  [180/345]  eta: 0:02:03  lr: 0.000111  loss: 0.7613 (0.7514)  time: 0.7516  data: 0.0001  max mem: 14938
[16:59:23.299407] Epoch: [26]  [200/345]  eta: 0:01:48  lr: 0.000111  loss: 0.7584 (0.7523)  time: 0.7511  data: 0.0001  max mem: 14938
[16:59:38.314386] Epoch: [26]  [220/345]  eta: 0:01:33  lr: 0.000110  loss: 0.7524 (0.7526)  time: 0.7507  data: 0.0001  max mem: 14938
[16:59:53.320547] Epoch: [26]  [240/345]  eta: 0:01:18  lr: 0.000110  loss: 0.7516 (0.7526)  time: 0.7503  data: 0.0001  max mem: 14938
[17:00:08.332964] Epoch: [26]  [260/345]  eta: 0:01:03  lr: 0.000110  loss: 0.7445 (0.7521)  time: 0.7506  data: 0.0001  max mem: 14938
[17:00:23.333894] Epoch: [26]  [280/345]  eta: 0:00:48  lr: 0.000110  loss: 0.7460 (0.7518)  time: 0.7500  data: 0.0001  max mem: 14938
[17:00:38.309033] Epoch: [26]  [300/345]  eta: 0:00:33  lr: 0.000110  loss: 0.7554 (0.7520)  time: 0.7487  data: 0.0001  max mem: 14938
[17:00:53.289304] Epoch: [26]  [320/345]  eta: 0:00:18  lr: 0.000109  loss: 0.7525 (0.7522)  time: 0.7490  data: 0.0001  max mem: 14938
[17:01:08.287250] Epoch: [26]  [340/345]  eta: 0:00:03  lr: 0.000109  loss: 0.7462 (0.7520)  time: 0.7498  data: 0.0001  max mem: 14938
[17:01:11.284223] Epoch: [26]  [344/345]  eta: 0:00:00  lr: 0.000109  loss: 0.7453 (0.7519)  time: 0.7496  data: 0.0001  max mem: 14938
[17:01:11.345823] Epoch: [26] Total time: 0:04:19 (0.7508 s / it)
[17:01:11.346154] Averaged stats: lr: 0.000109  loss: 0.7453 (0.7519)
[17:01:11.686438] Test:  [  0/345]  eta: 0:01:55  loss: 0.7116 (0.7116)  time: 0.3360  data: 0.1541  max mem: 14938
[17:01:13.523813] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7236 (0.7277)  time: 0.1975  data: 0.0141  max mem: 14938
[17:01:15.365700] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7272 (0.7268)  time: 0.1839  data: 0.0001  max mem: 14938
[17:01:17.209683] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7221 (0.7257)  time: 0.1842  data: 0.0001  max mem: 14938
[17:01:19.057439] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7197 (0.7233)  time: 0.1845  data: 0.0001  max mem: 14938
[17:01:20.908458] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7227 (0.7249)  time: 0.1849  data: 0.0001  max mem: 14938
[17:01:22.764349] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7275 (0.7253)  time: 0.1853  data: 0.0001  max mem: 14938
[17:01:24.623470] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7249 (0.7261)  time: 0.1857  data: 0.0001  max mem: 14938
[17:01:26.483952] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7274 (0.7259)  time: 0.1859  data: 0.0001  max mem: 14938
[17:01:28.348094] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7208 (0.7258)  time: 0.1862  data: 0.0001  max mem: 14938
[17:01:30.214350] Test:  [100/345]  eta: 0:00:45  loss: 0.7234 (0.7259)  time: 0.1865  data: 0.0001  max mem: 14938
[17:01:32.086096] Test:  [110/345]  eta: 0:00:43  loss: 0.7259 (0.7259)  time: 0.1868  data: 0.0001  max mem: 14938
[17:01:33.963653] Test:  [120/345]  eta: 0:00:42  loss: 0.7220 (0.7253)  time: 0.1874  data: 0.0001  max mem: 14938
[17:01:35.841527] Test:  [130/345]  eta: 0:00:40  loss: 0.7224 (0.7256)  time: 0.1877  data: 0.0001  max mem: 14938
[17:01:37.722697] Test:  [140/345]  eta: 0:00:38  loss: 0.7278 (0.7259)  time: 0.1879  data: 0.0001  max mem: 14938
[17:01:39.607395] Test:  [150/345]  eta: 0:00:36  loss: 0.7293 (0.7262)  time: 0.1882  data: 0.0001  max mem: 14938
[17:01:41.493595] Test:  [160/345]  eta: 0:00:34  loss: 0.7166 (0.7253)  time: 0.1885  data: 0.0001  max mem: 14938
[17:01:43.385415] Test:  [170/345]  eta: 0:00:32  loss: 0.7193 (0.7252)  time: 0.1888  data: 0.0001  max mem: 14938
[17:01:45.281179] Test:  [180/345]  eta: 0:00:30  loss: 0.7251 (0.7253)  time: 0.1893  data: 0.0001  max mem: 14938
[17:01:47.182490] Test:  [190/345]  eta: 0:00:29  loss: 0.7251 (0.7253)  time: 0.1898  data: 0.0001  max mem: 14938
[17:01:49.085203] Test:  [200/345]  eta: 0:00:27  loss: 0.7229 (0.7251)  time: 0.1902  data: 0.0001  max mem: 14938
[17:01:50.991082] Test:  [210/345]  eta: 0:00:25  loss: 0.7194 (0.7253)  time: 0.1904  data: 0.0001  max mem: 14938
[17:01:52.898868] Test:  [220/345]  eta: 0:00:23  loss: 0.7238 (0.7253)  time: 0.1906  data: 0.0001  max mem: 14938
[17:01:54.812383] Test:  [230/345]  eta: 0:00:21  loss: 0.7222 (0.7250)  time: 0.1910  data: 0.0001  max mem: 14938
[17:01:56.731318] Test:  [240/345]  eta: 0:00:19  loss: 0.7202 (0.7250)  time: 0.1916  data: 0.0001  max mem: 14938
[17:01:58.652333] Test:  [250/345]  eta: 0:00:17  loss: 0.7185 (0.7248)  time: 0.1919  data: 0.0001  max mem: 14938
[17:02:00.575799] Test:  [260/345]  eta: 0:00:16  loss: 0.7201 (0.7248)  time: 0.1922  data: 0.0001  max mem: 14938
[17:02:02.504474] Test:  [270/345]  eta: 0:00:14  loss: 0.7228 (0.7248)  time: 0.1926  data: 0.0001  max mem: 14938
[17:02:04.436613] Test:  [280/345]  eta: 0:00:12  loss: 0.7200 (0.7247)  time: 0.1930  data: 0.0001  max mem: 14938
[17:02:06.371401] Test:  [290/345]  eta: 0:00:10  loss: 0.7194 (0.7246)  time: 0.1933  data: 0.0001  max mem: 14938
[17:02:08.311446] Test:  [300/345]  eta: 0:00:08  loss: 0.7195 (0.7247)  time: 0.1937  data: 0.0001  max mem: 14938
[17:02:10.252709] Test:  [310/345]  eta: 0:00:06  loss: 0.7262 (0.7248)  time: 0.1940  data: 0.0001  max mem: 14938
[17:02:12.197742] Test:  [320/345]  eta: 0:00:04  loss: 0.7235 (0.7248)  time: 0.1943  data: 0.0001  max mem: 14938
[17:02:14.145633] Test:  [330/345]  eta: 0:00:02  loss: 0.7211 (0.7247)  time: 0.1946  data: 0.0001  max mem: 14938
[17:02:16.095649] Test:  [340/345]  eta: 0:00:00  loss: 0.7214 (0.7248)  time: 0.1948  data: 0.0001  max mem: 14938
[17:02:16.877175] Test:  [344/345]  eta: 0:00:00  loss: 0.7244 (0.7248)  time: 0.1949  data: 0.0001  max mem: 14938
[17:02:16.935833] Test: Total time: 0:01:05 (0.1901 s / it)
[17:02:27.301990] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8405 (0.8405)  time: 0.3166  data: 0.1367  max mem: 14938
[17:02:29.118085] Test:  [10/57]  eta: 0:00:09  loss: 0.8803 (0.8853)  time: 0.1938  data: 0.0125  max mem: 14938
[17:02:30.940075] Test:  [20/57]  eta: 0:00:06  loss: 0.8815 (0.8745)  time: 0.1818  data: 0.0001  max mem: 14938
[17:02:32.765867] Test:  [30/57]  eta: 0:00:05  loss: 0.7607 (0.8353)  time: 0.1823  data: 0.0001  max mem: 14938
[17:02:34.595569] Test:  [40/57]  eta: 0:00:03  loss: 0.7557 (0.8145)  time: 0.1827  data: 0.0001  max mem: 14938
[17:02:36.430576] Test:  [50/57]  eta: 0:00:01  loss: 0.7495 (0.8062)  time: 0.1832  data: 0.0001  max mem: 14938
[17:02:37.419701] Test:  [56/57]  eta: 0:00:00  loss: 0.7560 (0.8114)  time: 0.1778  data: 0.0001  max mem: 14938
[17:02:37.476836] Test: Total time: 0:00:10 (0.1841 s / it)
[17:02:39.293418] Dice score of the network on the train images: 0.810723, val images: 0.812615
[17:02:39.297767] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[17:02:40.185189] Epoch: [27]  [  0/345]  eta: 0:05:05  lr: 0.000109  loss: 0.7640 (0.7640)  time: 0.8865  data: 0.1457  max mem: 14938
[17:02:55.074185] Epoch: [27]  [ 20/345]  eta: 0:04:04  lr: 0.000109  loss: 0.7424 (0.7508)  time: 0.7444  data: 0.0001  max mem: 14938
[17:03:10.009003] Epoch: [27]  [ 40/345]  eta: 0:03:48  lr: 0.000108  loss: 0.7508 (0.7495)  time: 0.7467  data: 0.0001  max mem: 14938
[17:03:24.975667] Epoch: [27]  [ 60/345]  eta: 0:03:33  lr: 0.000108  loss: 0.7480 (0.7490)  time: 0.7483  data: 0.0001  max mem: 14938
[17:03:39.946982] Epoch: [27]  [ 80/345]  eta: 0:03:18  lr: 0.000108  loss: 0.7816 (0.7584)  time: 0.7485  data: 0.0001  max mem: 14938
[17:03:54.943298] Epoch: [27]  [100/345]  eta: 0:03:03  lr: 0.000108  loss: 0.7591 (0.7589)  time: 0.7498  data: 0.0001  max mem: 14938
[17:04:10.079475] Epoch: [27]  [120/345]  eta: 0:02:48  lr: 0.000107  loss: 0.7437 (0.7578)  time: 0.7568  data: 0.0001  max mem: 14938
[17:04:25.092593] Epoch: [27]  [140/345]  eta: 0:02:33  lr: 0.000107  loss: 0.7485 (0.7576)  time: 0.7506  data: 0.0001  max mem: 14938
[17:04:40.111883] Epoch: [27]  [160/345]  eta: 0:02:18  lr: 0.000107  loss: 0.7598 (0.7585)  time: 0.7509  data: 0.0001  max mem: 14938
[17:04:55.132759] Epoch: [27]  [180/345]  eta: 0:02:03  lr: 0.000107  loss: 0.7493 (0.7578)  time: 0.7510  data: 0.0001  max mem: 14938
[17:05:10.141236] Epoch: [27]  [200/345]  eta: 0:01:48  lr: 0.000106  loss: 0.7429 (0.7568)  time: 0.7504  data: 0.0001  max mem: 14938
[17:05:25.138332] Epoch: [27]  [220/345]  eta: 0:01:33  lr: 0.000106  loss: 0.7483 (0.7563)  time: 0.7498  data: 0.0001  max mem: 14938
[17:05:40.151850] Epoch: [27]  [240/345]  eta: 0:01:18  lr: 0.000106  loss: 0.7615 (0.7568)  time: 0.7506  data: 0.0001  max mem: 14938
[17:05:55.159304] Epoch: [27]  [260/345]  eta: 0:01:03  lr: 0.000106  loss: 0.7621 (0.7572)  time: 0.7503  data: 0.0001  max mem: 14938
[17:06:10.169644] Epoch: [27]  [280/345]  eta: 0:00:48  lr: 0.000105  loss: 0.7538 (0.7570)  time: 0.7505  data: 0.0001  max mem: 14938
[17:06:25.163660] Epoch: [27]  [300/345]  eta: 0:00:33  lr: 0.000105  loss: 0.7390 (0.7561)  time: 0.7497  data: 0.0001  max mem: 14938
[17:06:40.156319] Epoch: [27]  [320/345]  eta: 0:00:18  lr: 0.000105  loss: 0.7375 (0.7551)  time: 0.7496  data: 0.0001  max mem: 14938
[17:06:55.156786] Epoch: [27]  [340/345]  eta: 0:00:03  lr: 0.000104  loss: 0.7428 (0.7546)  time: 0.7500  data: 0.0001  max mem: 14938
[17:06:58.156214] Epoch: [27]  [344/345]  eta: 0:00:00  lr: 0.000104  loss: 0.7438 (0.7545)  time: 0.7499  data: 0.0001  max mem: 14938
[17:06:58.219909] Epoch: [27] Total time: 0:04:18 (0.7505 s / it)
[17:06:58.220319] Averaged stats: lr: 0.000104  loss: 0.7438 (0.7545)
[17:06:58.555794] Test:  [  0/345]  eta: 0:01:53  loss: 0.7147 (0.7147)  time: 0.3296  data: 0.1475  max mem: 14938
[17:07:00.395282] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7229 (0.7223)  time: 0.1971  data: 0.0135  max mem: 14938
[17:07:02.235202] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7176 (0.7152)  time: 0.1839  data: 0.0001  max mem: 14938
[17:07:04.080938] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7135 (0.7152)  time: 0.1842  data: 0.0001  max mem: 14938
[17:07:05.928715] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7135 (0.7145)  time: 0.1846  data: 0.0001  max mem: 14938
[17:07:07.779374] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7121 (0.7148)  time: 0.1849  data: 0.0001  max mem: 14938
[17:07:09.634546] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7166 (0.7153)  time: 0.1852  data: 0.0001  max mem: 14938
[17:07:11.493554] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7166 (0.7153)  time: 0.1857  data: 0.0001  max mem: 14938
[17:07:13.353135] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7134 (0.7155)  time: 0.1859  data: 0.0001  max mem: 14938
[17:07:15.220042] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7156 (0.7155)  time: 0.1863  data: 0.0001  max mem: 14938
[17:07:17.089441] Test:  [100/345]  eta: 0:00:45  loss: 0.7156 (0.7162)  time: 0.1868  data: 0.0001  max mem: 14938
[17:07:18.959740] Test:  [110/345]  eta: 0:00:43  loss: 0.7184 (0.7163)  time: 0.1869  data: 0.0001  max mem: 14938
[17:07:20.833886] Test:  [120/345]  eta: 0:00:42  loss: 0.7196 (0.7166)  time: 0.1872  data: 0.0001  max mem: 14938
[17:07:22.711399] Test:  [130/345]  eta: 0:00:40  loss: 0.7204 (0.7171)  time: 0.1875  data: 0.0001  max mem: 14938
[17:07:24.594484] Test:  [140/345]  eta: 0:00:38  loss: 0.7163 (0.7171)  time: 0.1880  data: 0.0001  max mem: 14938
[17:07:26.480511] Test:  [150/345]  eta: 0:00:36  loss: 0.7143 (0.7169)  time: 0.1884  data: 0.0001  max mem: 14938
[17:07:28.369484] Test:  [160/345]  eta: 0:00:34  loss: 0.7132 (0.7169)  time: 0.1887  data: 0.0001  max mem: 14938
[17:07:30.261636] Test:  [170/345]  eta: 0:00:32  loss: 0.7124 (0.7168)  time: 0.1890  data: 0.0001  max mem: 14938
[17:07:32.158565] Test:  [180/345]  eta: 0:00:30  loss: 0.7117 (0.7166)  time: 0.1894  data: 0.0001  max mem: 14938
[17:07:34.059787] Test:  [190/345]  eta: 0:00:29  loss: 0.7101 (0.7164)  time: 0.1899  data: 0.0001  max mem: 14938
[17:07:35.962705] Test:  [200/345]  eta: 0:00:27  loss: 0.7076 (0.7162)  time: 0.1902  data: 0.0001  max mem: 14938
[17:07:37.870382] Test:  [210/345]  eta: 0:00:25  loss: 0.7148 (0.7163)  time: 0.1905  data: 0.0001  max mem: 14938
[17:07:39.781199] Test:  [220/345]  eta: 0:00:23  loss: 0.7181 (0.7167)  time: 0.1909  data: 0.0001  max mem: 14938
[17:07:41.696014] Test:  [230/345]  eta: 0:00:21  loss: 0.7186 (0.7169)  time: 0.1912  data: 0.0001  max mem: 14938
[17:07:43.612821] Test:  [240/345]  eta: 0:00:19  loss: 0.7186 (0.7170)  time: 0.1915  data: 0.0001  max mem: 14938
[17:07:45.535339] Test:  [250/345]  eta: 0:00:17  loss: 0.7197 (0.7171)  time: 0.1919  data: 0.0001  max mem: 14938
[17:07:47.458774] Test:  [260/345]  eta: 0:00:16  loss: 0.7152 (0.7168)  time: 0.1922  data: 0.0001  max mem: 14938
[17:07:49.387097] Test:  [270/345]  eta: 0:00:14  loss: 0.7120 (0.7168)  time: 0.1925  data: 0.0001  max mem: 14938
[17:07:51.319179] Test:  [280/345]  eta: 0:00:12  loss: 0.7132 (0.7169)  time: 0.1930  data: 0.0001  max mem: 14938
[17:07:53.255168] Test:  [290/345]  eta: 0:00:10  loss: 0.7132 (0.7168)  time: 0.1934  data: 0.0001  max mem: 14938
[17:07:55.193711] Test:  [300/345]  eta: 0:00:08  loss: 0.7168 (0.7169)  time: 0.1937  data: 0.0001  max mem: 14938
[17:07:57.135046] Test:  [310/345]  eta: 0:00:06  loss: 0.7176 (0.7171)  time: 0.1939  data: 0.0001  max mem: 14938
[17:07:59.079739] Test:  [320/345]  eta: 0:00:04  loss: 0.7184 (0.7171)  time: 0.1942  data: 0.0001  max mem: 14938
[17:08:01.026060] Test:  [330/345]  eta: 0:00:02  loss: 0.7134 (0.7171)  time: 0.1945  data: 0.0001  max mem: 14938
[17:08:02.976358] Test:  [340/345]  eta: 0:00:00  loss: 0.7124 (0.7169)  time: 0.1948  data: 0.0001  max mem: 14938
[17:08:03.757301] Test:  [344/345]  eta: 0:00:00  loss: 0.7093 (0.7168)  time: 0.1949  data: 0.0001  max mem: 14938
[17:08:03.815794] Test: Total time: 0:01:05 (0.1901 s / it)
[17:08:14.205146] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8554 (0.8554)  time: 0.3221  data: 0.1421  max mem: 14938
[17:08:16.019781] Test:  [10/57]  eta: 0:00:09  loss: 0.8661 (0.8779)  time: 0.1942  data: 0.0130  max mem: 14938
[17:08:17.843320] Test:  [20/57]  eta: 0:00:06  loss: 0.8843 (0.8684)  time: 0.1818  data: 0.0001  max mem: 14938
[17:08:19.670027] Test:  [30/57]  eta: 0:00:05  loss: 0.7669 (0.8311)  time: 0.1825  data: 0.0001  max mem: 14938
[17:08:21.500277] Test:  [40/57]  eta: 0:00:03  loss: 0.7553 (0.8115)  time: 0.1828  data: 0.0001  max mem: 14938
[17:08:23.335072] Test:  [50/57]  eta: 0:00:01  loss: 0.7510 (0.8054)  time: 0.1832  data: 0.0001  max mem: 14938
[17:08:24.324973] Test:  [56/57]  eta: 0:00:00  loss: 0.7667 (0.8109)  time: 0.1778  data: 0.0001  max mem: 14938
[17:08:24.384094] Test: Total time: 0:00:10 (0.1842 s / it)
[17:08:26.133753] Dice score of the network on the train images: 0.807876, val images: 0.811845
[17:08:26.137839] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[17:08:27.018404] Epoch: [28]  [  0/345]  eta: 0:05:03  lr: 0.000104  loss: 0.7336 (0.7336)  time: 0.8797  data: 0.1398  max mem: 14938
[17:08:41.909002] Epoch: [28]  [ 20/345]  eta: 0:04:04  lr: 0.000104  loss: 0.7428 (0.7423)  time: 0.7445  data: 0.0001  max mem: 14938
[17:08:56.866637] Epoch: [28]  [ 40/345]  eta: 0:03:48  lr: 0.000104  loss: 0.7477 (0.7455)  time: 0.7478  data: 0.0001  max mem: 14938
[17:09:11.849669] Epoch: [28]  [ 60/345]  eta: 0:03:33  lr: 0.000103  loss: 0.7468 (0.7471)  time: 0.7491  data: 0.0001  max mem: 14938
[17:09:26.822018] Epoch: [28]  [ 80/345]  eta: 0:03:18  lr: 0.000103  loss: 0.7472 (0.7475)  time: 0.7486  data: 0.0001  max mem: 14938
[17:09:41.809287] Epoch: [28]  [100/345]  eta: 0:03:03  lr: 0.000103  loss: 0.7442 (0.7475)  time: 0.7493  data: 0.0001  max mem: 14938
[17:09:56.816291] Epoch: [28]  [120/345]  eta: 0:02:48  lr: 0.000103  loss: 0.7469 (0.7470)  time: 0.7503  data: 0.0001  max mem: 14938
[17:10:11.829869] Epoch: [28]  [140/345]  eta: 0:02:33  lr: 0.000102  loss: 0.7457 (0.7469)  time: 0.7506  data: 0.0001  max mem: 14938

[17:10:26.850739] Epoch: [28]  [160/345]  eta: 0:02:18  lr: 0.000102  loss: 0.7494 (0.7472)  time: 0.7510  data: 0.0001  max mem: 14938
[17:10:41.859087] Epoch: [28]  [180/345]  eta: 0:02:03  lr: 0.000102  loss: 0.7659 (0.7497)  time: 0.7504  data: 0.0001  max mem: 14938

[17:10:56.855102] Epoch: [28]  [200/345]  eta: 0:01:48  lr: 0.000101  loss: 0.7522 (0.7496)  time: 0.7498  data: 0.0001  max mem: 14938
[17:11:11.843827] Epoch: [28]  [220/345]  eta: 0:01:33  lr: 0.000101  loss: 0.7470 (0.7493)  time: 0.7494  data: 0.0001  max mem: 14938
[17:11:26.818607] Epoch: [28]  [240/345]  eta: 0:01:18  lr: 0.000101  loss: 0.7443 (0.7492)  time: 0.7487  data: 0.0001  max mem: 14938
[17:11:33.825297] NaN detected in inputs
[17:11:33.829614] Loss is nan, stopping training
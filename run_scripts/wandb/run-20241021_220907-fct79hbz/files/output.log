Not using distributed mode
[22:09:09.259709] job dir: /root/seg_framework/MS-Mamba/run_scripts
[22:09:09.259846] Namespace(accum_iter=1,
model='SegFormer3D',
in_channels=1,
lr=None,
blr=0.001,
min_lr=0,
dist_on_itp=False,
warmup_epochs=20,
device='cuda:0',
seed=42,
layer_decay=0.75,
clip_grad=None,
num_workers=8,
pin_mem=True,
resume='',
mask_mode='concatenate to image',
world_size=1,
embed_dim=1,
local_rank=-1,
dist_url='env://',
nb_classes=2,
data_dir='/root/MSLesSeg24/data',
datalist=None,
preprocess=False,
dim=2,
loss='mask tp1 tp2',
distributed=False)
[22:09:09.259970] device  cuda:0
[22:09:09.261015] Starting for fold 0
[22:09:09.452172] Elements in data_dir_paths: 11052
[22:09:09.486823] Elements in data_dir_paths: 1803
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[22:09:11.082724] number of params: 59617303
[22:09:11.082961] model: Vivim2D(
  (encoder): mamba_block(
    (downsample_layers): SegformerEncoder(
      (patch_embeddings): ModuleList(
        (0): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(2, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (3): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (block): ModuleList(
        (0): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): Identity()
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.003703703870996833)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.007407407741993666)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.011111111380159855)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.014814815483987331)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.018518518656492233)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.02222222276031971)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.025925926864147186)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.029629630967974663)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03333333507180214)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03703703731298447)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04074074327945709)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04444444552063942)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.048148151487112045)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.051851850003004074)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0555555559694767)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.05925925821065903)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06296296417713165)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06666667014360428)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07037036865949631)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07407407462596893)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07777778059244156)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08148147910833359)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08518518507480621)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08888889104127884)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.09259259700775146)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0962962955236435)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.10000000149011612)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (layer_norm): ModuleList(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (2): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (3): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (decoder): SegformerDecodeHead(
    (linear_c): ModuleList(
      (0): SegformerMLP(
        (proj): Linear(in_features=64, out_features=768, bias=True)
      )
      (1): SegformerMLP(
        (proj): Linear(in_features=128, out_features=768, bias=True)
      )
      (2): SegformerMLP(
        (proj): Linear(in_features=320, out_features=768, bias=True)
      )
      (3): SegformerMLP(
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
    )
    (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Conv2d(768, 150, kernel_size=(1, 1), stride=(1, 1))
  )
  (out): Conv2d(768, 1, kernel_size=(1, 1), stride=(1, 1))
)
[22:09:11.086193] base lr: 1.00e-03
[22:09:11.086265] actual lr: 1.25e-04
[22:09:11.086330] accumulate grad iterations: 1
[22:09:11.086381] effective batch size: 32
[22:09:11.088067] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.000125
    maximize: False
    weight_decay: 0.01
)
[22:09:11.090051] Start training for 100 epochs
[22:09:11.090139] Number of samples in train dataloader:  345
[22:09:11.091955] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
[22:09:20.860321] Epoch: [0]  [  0/345]  eta: 0:56:09  lr: 0.000000  loss: 1.6965 (1.6965)  time: 9.7673  data: 0.4015  max mem: 15812
[22:09:32.471306] Epoch: [0]  [ 20/345]  eta: 0:05:30  lr: 0.000000  loss: 1.6955 (1.6956)  time: 0.5805  data: 0.0001  max mem: 15812
[22:09:44.146022] Epoch: [0]  [ 40/345]  eta: 0:04:05  lr: 0.000001  loss: 1.6917 (1.6936)  time: 0.5837  data: 0.0001  max mem: 15812
[22:09:55.874917] Epoch: [0]  [ 60/345]  eta: 0:03:29  lr: 0.000001  loss: 1.6874 (1.6916)  time: 0.5864  data: 0.0001  max mem: 15812
[22:10:07.653666] Epoch: [0]  [ 80/345]  eta: 0:03:05  lr: 0.000001  loss: 1.6850 (1.6901)  time: 0.5889  data: 0.0001  max mem: 15812
[22:10:19.494033] Epoch: [0]  [100/345]  eta: 0:02:45  lr: 0.000002  loss: 1.6822 (1.6885)  time: 0.5920  data: 0.0001  max mem: 15812
[22:10:31.392801] Epoch: [0]  [120/345]  eta: 0:02:29  lr: 0.000002  loss: 1.6764 (1.6865)  time: 0.5949  data: 0.0001  max mem: 15812
[22:10:43.317982] Epoch: [0]  [140/345]  eta: 0:02:14  lr: 0.000003  loss: 1.6697 (1.6843)  time: 0.5962  data: 0.0001  max mem: 15812
[22:10:55.260878] Epoch: [0]  [160/345]  eta: 0:01:59  lr: 0.000003  loss: 1.6670 (1.6822)  time: 0.5971  data: 0.0001  max mem: 15812
[22:11:07.215925] Epoch: [0]  [180/345]  eta: 0:01:45  lr: 0.000003  loss: 1.6606 (1.6798)  time: 0.5977  data: 0.0001  max mem: 15812

[22:11:19.197419] Epoch: [0]  [200/345]  eta: 0:01:32  lr: 0.000004  loss: 1.6525 (1.6772)  time: 0.5990  data: 0.0001  max mem: 15812
[22:11:31.190526] Epoch: [0]  [220/345]  eta: 0:01:19  lr: 0.000004  loss: 1.6446 (1.6743)  time: 0.5996  data: 0.0001  max mem: 15812
[22:11:43.203009] Epoch: [0]  [240/345]  eta: 0:01:06  lr: 0.000004  loss: 1.6343 (1.6711)  time: 0.6006  data: 0.0001  max mem: 15812
[22:11:55.224165] Epoch: [0]  [260/345]  eta: 0:00:53  lr: 0.000005  loss: 1.6247 (1.6675)  time: 0.6010  data: 0.0001  max mem: 15812

[22:12:07.243445] Epoch: [0]  [280/345]  eta: 0:00:40  lr: 0.000005  loss: 1.6127 (1.6636)  time: 0.6009  data: 0.0001  max mem: 15812

[22:12:19.267434] Epoch: [0]  [300/345]  eta: 0:00:28  lr: 0.000005  loss: 1.5994 (1.6594)  time: 0.6012  data: 0.0001  max mem: 15812

[22:12:31.305110] Epoch: [0]  [320/345]  eta: 0:00:15  lr: 0.000006  loss: 1.5854 (1.6548)  time: 0.6018  data: 0.0001  max mem: 15812
[22:12:43.341075] Epoch: [0]  [340/345]  eta: 0:00:03  lr: 0.000006  loss: 1.5717 (1.6499)  time: 0.6018  data: 0.0001  max mem: 15812
[22:12:45.748739] Epoch: [0]  [344/345]  eta: 0:00:00  lr: 0.000006  loss: 1.5658 (1.6489)  time: 0.6017  data: 0.0001  max mem: 15812
[22:12:45.817696] Epoch: [0] Total time: 0:03:34 (0.6224 s / it)
[22:12:45.818014] Averaged stats: lr: 0.000006  loss: 1.5658 (1.6489)
[22:12:46.427834] Test:  [  0/345]  eta: 0:03:28  loss: 1.5938 (1.5938)  time: 0.6047  data: 0.4374  max mem: 15812
[22:12:48.117537] Test:  [ 10/345]  eta: 0:01:09  loss: 1.5938 (1.5936)  time: 0.2085  data: 0.0398  max mem: 15812
[22:12:49.810643] Test:  [ 20/345]  eta: 0:01:01  loss: 1.5941 (1.5938)  time: 0.1691  data: 0.0001  max mem: 15812
[22:12:51.510070] Test:  [ 30/345]  eta: 0:00:57  loss: 1.5925 (1.5932)  time: 0.1696  data: 0.0001  max mem: 15812
[22:12:53.215943] Test:  [ 40/345]  eta: 0:00:54  loss: 1.5925 (1.5931)  time: 0.1702  data: 0.0001  max mem: 15812
[22:12:54.926594] Test:  [ 50/345]  eta: 0:00:52  loss: 1.5935 (1.5932)  time: 0.1708  data: 0.0001  max mem: 15812
[22:12:56.642169] Test:  [ 60/345]  eta: 0:00:50  loss: 1.5935 (1.5931)  time: 0.1712  data: 0.0001  max mem: 15812
[22:12:58.363306] Test:  [ 70/345]  eta: 0:00:48  loss: 1.5913 (1.5928)  time: 0.1717  data: 0.0001  max mem: 15812
[22:13:00.089752] Test:  [ 80/345]  eta: 0:00:46  loss: 1.5913 (1.5927)  time: 0.1723  data: 0.0001  max mem: 15812
[22:13:01.821481] Test:  [ 90/345]  eta: 0:00:44  loss: 1.5924 (1.5926)  time: 0.1729  data: 0.0001  max mem: 15812
[22:13:03.557539] Test:  [100/345]  eta: 0:00:43  loss: 1.5928 (1.5926)  time: 0.1733  data: 0.0001  max mem: 15812
[22:13:05.301164] Test:  [110/345]  eta: 0:00:41  loss: 1.5931 (1.5926)  time: 0.1739  data: 0.0001  max mem: 15812
[22:13:07.047425] Test:  [120/345]  eta: 0:00:39  loss: 1.5936 (1.5927)  time: 0.1744  data: 0.0001  max mem: 15812
[22:13:09.197536] Test:  [130/345]  eta: 0:00:38  loss: 1.5921 (1.5925)  time: 0.1948  data: 0.0001  max mem: 15812
[22:13:10.952349] Test:  [140/345]  eta: 0:00:36  loss: 1.5925 (1.5925)  time: 0.1952  data: 0.0001  max mem: 15812
[22:13:12.821761] Test:  [150/345]  eta: 0:00:34  loss: 1.5931 (1.5925)  time: 0.1812  data: 0.0001  max mem: 15812
[22:13:14.611316] Test:  [160/345]  eta: 0:00:33  loss: 1.5937 (1.5926)  time: 0.1829  data: 0.0001  max mem: 15812
[22:13:16.386091] Test:  [170/345]  eta: 0:00:31  loss: 1.5926 (1.5926)  time: 0.1782  data: 0.0001  max mem: 15812
[22:13:18.353891] Test:  [180/345]  eta: 0:00:29  loss: 1.5930 (1.5926)  time: 0.1871  data: 0.0001  max mem: 15812
[22:13:20.138102] Test:  [190/345]  eta: 0:00:27  loss: 1.5934 (1.5926)  time: 0.1875  data: 0.0001  max mem: 15812
[22:13:22.134416] Test:  [200/345]  eta: 0:00:26  loss: 1.5923 (1.5926)  time: 0.1890  data: 0.0001  max mem: 15812
[22:13:24.103977] Test:  [210/345]  eta: 0:00:24  loss: 1.5923 (1.5926)  time: 0.1982  data: 0.0001  max mem: 15812
[22:13:25.927883] Test:  [220/345]  eta: 0:00:22  loss: 1.5940 (1.5926)  time: 0.1896  data: 0.0001  max mem: 15812
[22:13:27.841985] Test:  [230/345]  eta: 0:00:20  loss: 1.5944 (1.5927)  time: 0.1868  data: 0.0001  max mem: 15812
[22:13:29.848104] Test:  [240/345]  eta: 0:00:19  loss: 1.5926 (1.5926)  time: 0.1959  data: 0.0001  max mem: 15812
[22:13:31.689253] Test:  [250/345]  eta: 0:00:17  loss: 1.5926 (1.5927)  time: 0.1923  data: 0.0001  max mem: 15812
[22:13:33.878282] Test:  [260/345]  eta: 0:00:15  loss: 1.5921 (1.5926)  time: 0.2014  data: 0.0001  max mem: 15812
[22:13:36.080687] Test:  [270/345]  eta: 0:00:13  loss: 1.5934 (1.5927)  time: 0.2195  data: 0.0001  max mem: 15812
[22:13:38.155171] Test:  [280/345]  eta: 0:00:12  loss: 1.5934 (1.5926)  time: 0.2138  data: 0.0001  max mem: 15812
[22:13:40.006994] Test:  [290/345]  eta: 0:00:10  loss: 1.5909 (1.5926)  time: 0.1963  data: 0.0001  max mem: 15812
[22:13:42.119682] Test:  [300/345]  eta: 0:00:08  loss: 1.5934 (1.5927)  time: 0.1982  data: 0.0001  max mem: 15812
[22:13:44.142331] Test:  [310/345]  eta: 0:00:06  loss: 1.5937 (1.5927)  time: 0.2067  data: 0.0001  max mem: 15812
[22:13:46.361284] Test:  [320/345]  eta: 0:00:04  loss: 1.5932 (1.5927)  time: 0.2120  data: 0.0001  max mem: 15812
[22:13:48.595512] Test:  [330/345]  eta: 0:00:02  loss: 1.5927 (1.5927)  time: 0.2226  data: 0.0001  max mem: 15812
[22:13:50.760422] Test:  [340/345]  eta: 0:00:00  loss: 1.5917 (1.5926)  time: 0.2199  data: 0.0001  max mem: 15812
[22:13:51.802717] Test:  [344/345]  eta: 0:00:00  loss: 1.5918 (1.5927)  time: 0.2165  data: 0.0001  max mem: 15812
[22:13:51.873680] Test: Total time: 0:01:06 (0.1915 s / it)
[22:14:08.883359] Test:  [ 0/57]  eta: 0:00:30  loss: 1.6031 (1.6031)  time: 0.5336  data: 0.3698  max mem: 15812
[22:14:10.544314] Test:  [10/57]  eta: 0:00:09  loss: 1.5973 (1.5980)  time: 0.1994  data: 0.0337  max mem: 15812
[22:14:12.213775] Test:  [20/57]  eta: 0:00:06  loss: 1.5978 (1.5966)  time: 0.1664  data: 0.0001  max mem: 15812
[22:14:13.889792] Test:  [30/57]  eta: 0:00:04  loss: 1.5949 (1.5923)  time: 0.1672  data: 0.0001  max mem: 15812
[22:14:15.573274] Test:  [40/57]  eta: 0:00:02  loss: 1.5840 (1.5894)  time: 0.1679  data: 0.0001  max mem: 15812
[22:14:17.261333] Test:  [50/57]  eta: 0:00:01  loss: 1.5840 (1.5885)  time: 0.1685  data: 0.0001  max mem: 15812
[22:14:18.944754] Test:  [56/57]  eta: 0:00:00  loss: 1.5873 (1.5884)  time: 0.2022  data: 0.0001  max mem: 15812
[22:14:19.019417] Test: Total time: 0:00:10 (0.1872 s / it)
[22:14:21.982093] Dice score of the network on the train images: 0.000000, val images: 0.000000
[22:14:21.982322] saving best_dice_model_0 @ epoch 0
[22:14:22.705244] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:14:23.756231] Epoch: [1]  [  0/345]  eta: 0:06:02  lr: 0.000006  loss: 1.5590 (1.5590)  time: 1.0501  data: 0.4513  max mem: 15812
[22:14:35.679227] Epoch: [1]  [ 20/345]  eta: 0:03:20  lr: 0.000007  loss: 1.5513 (1.5526)  time: 0.5961  data: 0.0001  max mem: 15812
[22:14:47.642115] Epoch: [1]  [ 40/345]  eta: 0:03:05  lr: 0.000007  loss: 1.5421 (1.5472)  time: 0.5981  data: 0.0001  max mem: 15812
[22:14:59.641370] Epoch: [1]  [ 60/345]  eta: 0:02:52  lr: 0.000007  loss: 1.5235 (1.5400)  time: 0.5999  data: 0.0001  max mem: 15812
[22:15:11.675245] Epoch: [1]  [ 80/345]  eta: 0:02:40  lr: 0.000008  loss: 1.5102 (1.5335)  time: 0.6017  data: 0.0001  max mem: 15812
[22:15:23.732203] Epoch: [1]  [100/345]  eta: 0:02:28  lr: 0.000008  loss: 1.4967 (1.5266)  time: 0.6028  data: 0.0001  max mem: 15812
[22:15:35.925639] Epoch: [1]  [120/345]  eta: 0:02:16  lr: 0.000008  loss: 1.4868 (1.5198)  time: 0.6096  data: 0.0001  max mem: 15812
[22:15:47.989943] Epoch: [1]  [140/345]  eta: 0:02:03  lr: 0.000009  loss: 1.4720 (1.5131)  time: 0.6032  data: 0.0001  max mem: 15812
[22:16:00.052595] Epoch: [1]  [160/345]  eta: 0:01:51  lr: 0.000009  loss: 1.4666 (1.5074)  time: 0.6031  data: 0.0001  max mem: 15812
[22:16:12.118517] Epoch: [1]  [180/345]  eta: 0:01:39  lr: 0.000010  loss: 1.4544 (1.5018)  time: 0.6033  data: 0.0001  max mem: 15812
[22:16:24.181920] Epoch: [1]  [200/345]  eta: 0:01:27  lr: 0.000010  loss: 1.4464 (1.4965)  time: 0.6031  data: 0.0001  max mem: 15812
[22:16:36.243010] Epoch: [1]  [220/345]  eta: 0:01:15  lr: 0.000010  loss: 1.4327 (1.4908)  time: 0.6030  data: 0.0001  max mem: 15812
[22:16:48.284756] Epoch: [1]  [240/345]  eta: 0:01:03  lr: 0.000011  loss: 1.4221 (1.4854)  time: 0.6020  data: 0.0001  max mem: 15812
[22:17:00.336732] Epoch: [1]  [260/345]  eta: 0:00:51  lr: 0.000011  loss: 1.4219 (1.4805)  time: 0.6026  data: 0.0001  max mem: 15812
[22:17:12.385422] Epoch: [1]  [280/345]  eta: 0:00:39  lr: 0.000011  loss: 1.4123 (1.4760)  time: 0.6024  data: 0.0001  max mem: 15812
[22:17:24.418628] Epoch: [1]  [300/345]  eta: 0:00:27  lr: 0.000012  loss: 1.4043 (1.4715)  time: 0.6016  data: 0.0001  max mem: 15812
[22:17:36.463644] Epoch: [1]  [320/345]  eta: 0:00:15  lr: 0.000012  loss: 1.4006 (1.4672)  time: 0.6022  data: 0.0001  max mem: 15812
[22:17:48.506956] Epoch: [1]  [340/345]  eta: 0:00:03  lr: 0.000012  loss: 1.3953 (1.4632)  time: 0.6021  data: 0.0001  max mem: 15812
[22:17:50.915788] Epoch: [1]  [344/345]  eta: 0:00:00  lr: 0.000012  loss: 1.3953 (1.4624)  time: 0.6021  data: 0.0001  max mem: 15812
[22:17:50.993820] Epoch: [1] Total time: 0:03:28 (0.6037 s / it)
[22:17:50.994637] Averaged stats: lr: 0.000012  loss: 1.3953 (1.4624)
[22:17:51.588933] Test:  [  0/345]  eta: 0:03:23  loss: 1.3930 (1.3930)  time: 0.5892  data: 0.4227  max mem: 15812
[22:17:53.278969] Test:  [ 10/345]  eta: 0:01:09  loss: 1.3928 (1.3932)  time: 0.2071  data: 0.0385  max mem: 15812
[22:17:54.973292] Test:  [ 20/345]  eta: 0:01:01  loss: 1.3929 (1.3935)  time: 0.1691  data: 0.0001  max mem: 15812
[22:17:56.671426] Test:  [ 30/345]  eta: 0:00:57  loss: 1.3929 (1.3933)  time: 0.1696  data: 0.0001  max mem: 15812
[22:17:58.376234] Test:  [ 40/345]  eta: 0:00:54  loss: 1.3927 (1.3930)  time: 0.1701  data: 0.0001  max mem: 15812
[22:18:00.086930] Test:  [ 50/345]  eta: 0:00:52  loss: 1.3925 (1.3929)  time: 0.1707  data: 0.0001  max mem: 15812
[22:18:01.802898] Test:  [ 60/345]  eta: 0:00:50  loss: 1.3926 (1.3929)  time: 0.1713  data: 0.0001  max mem: 15812
[22:18:03.523144] Test:  [ 70/345]  eta: 0:00:48  loss: 1.3935 (1.3931)  time: 0.1717  data: 0.0001  max mem: 15812
[22:18:05.248115] Test:  [ 80/345]  eta: 0:00:46  loss: 1.3934 (1.3931)  time: 0.1722  data: 0.0001  max mem: 15812
[22:18:06.979465] Test:  [ 90/345]  eta: 0:00:44  loss: 1.3921 (1.3930)  time: 0.1728  data: 0.0001  max mem: 15812
[22:18:08.716401] Test:  [100/345]  eta: 0:00:42  loss: 1.3919 (1.3929)  time: 0.1734  data: 0.0001  max mem: 15812
[22:18:10.458815] Test:  [110/345]  eta: 0:00:41  loss: 1.3930 (1.3929)  time: 0.1739  data: 0.0001  max mem: 15812
[22:18:12.206026] Test:  [120/345]  eta: 0:00:39  loss: 1.3930 (1.3929)  time: 0.1744  data: 0.0001  max mem: 15812
[22:18:13.957613] Test:  [130/345]  eta: 0:00:37  loss: 1.3929 (1.3929)  time: 0.1749  data: 0.0001  max mem: 15812
[22:18:15.714847] Test:  [140/345]  eta: 0:00:35  loss: 1.3933 (1.3930)  time: 0.1754  data: 0.0001  max mem: 15812
[22:18:17.476085] Test:  [150/345]  eta: 0:00:34  loss: 1.3927 (1.3929)  time: 0.1759  data: 0.0001  max mem: 15812
[22:18:19.561862] Test:  [160/345]  eta: 0:00:32  loss: 1.3926 (1.3929)  time: 0.1923  data: 0.0001  max mem: 15812
[22:18:21.342492] Test:  [170/345]  eta: 0:00:31  loss: 1.3929 (1.3929)  time: 0.1933  data: 0.0001  max mem: 15812
[22:18:23.117632] Test:  [180/345]  eta: 0:00:29  loss: 1.3933 (1.3930)  time: 0.1777  data: 0.0001  max mem: 15812
[22:18:24.898296] Test:  [190/345]  eta: 0:00:27  loss: 1.3930 (1.3930)  time: 0.1777  data: 0.0001  max mem: 15812
[22:18:26.872581] Test:  [200/345]  eta: 0:00:25  loss: 1.3933 (1.3930)  time: 0.1877  data: 0.0001  max mem: 15812
[22:18:28.678344] Test:  [210/345]  eta: 0:00:24  loss: 1.3935 (1.3930)  time: 0.1889  data: 0.0001  max mem: 15812
[22:18:30.474175] Test:  [220/345]  eta: 0:00:22  loss: 1.3933 (1.3930)  time: 0.1800  data: 0.0001  max mem: 15812
[22:18:32.275530] Test:  [230/345]  eta: 0:00:20  loss: 1.3929 (1.3930)  time: 0.1798  data: 0.0001  max mem: 15812
[22:18:34.301338] Test:  [240/345]  eta: 0:00:18  loss: 1.3929 (1.3929)  time: 0.1913  data: 0.0001  max mem: 15812
[22:18:36.112983] Test:  [250/345]  eta: 0:00:17  loss: 1.3929 (1.3930)  time: 0.1918  data: 0.0001  max mem: 15812
[22:18:38.139840] Test:  [260/345]  eta: 0:00:15  loss: 1.3925 (1.3929)  time: 0.1919  data: 0.0001  max mem: 15812
[22:18:39.971168] Test:  [270/345]  eta: 0:00:13  loss: 1.3925 (1.3929)  time: 0.1928  data: 0.0001  max mem: 15812
[22:18:41.798091] Test:  [280/345]  eta: 0:00:11  loss: 1.3926 (1.3929)  time: 0.1828  data: 0.0001  max mem: 15812
[22:18:43.878052] Test:  [290/345]  eta: 0:00:09  loss: 1.3927 (1.3929)  time: 0.1953  data: 0.0001  max mem: 15812
[22:18:45.713077] Test:  [300/345]  eta: 0:00:08  loss: 1.3926 (1.3929)  time: 0.1957  data: 0.0001  max mem: 15812

[22:18:47.743818] Test:  [310/345]  eta: 0:00:06  loss: 1.3926 (1.3929)  time: 0.1932  data: 0.0001  max mem: 15812
[22:18:50.041427] Test:  [320/345]  eta: 0:00:04  loss: 1.3925 (1.3929)  time: 0.2164  data: 0.0001  max mem: 15812
[22:18:52.369330] Test:  [330/345]  eta: 0:00:02  loss: 1.3925 (1.3929)  time: 0.2312  data: 0.0001  max mem: 15812
[22:18:54.792878] Test:  [340/345]  eta: 0:00:00  loss: 1.3931 (1.3929)  time: 0.2375  data: 0.0001  max mem: 15812
[22:18:56.043976] Test:  [344/345]  eta: 0:00:00  loss: 1.3934 (1.3929)  time: 0.2394  data: 0.0001  max mem: 15812
[22:18:56.123099] Test: Total time: 0:01:05 (0.1888 s / it)
[22:19:13.133799] Test:  [ 0/57]  eta: 0:00:32  loss: 1.4002 (1.4002)  time: 0.5711  data: 0.4074  max mem: 15812
[22:19:14.792533] Test:  [10/57]  eta: 0:00:09  loss: 1.3979 (1.3969)  time: 0.2026  data: 0.0371  max mem: 15812
[22:19:16.459454] Test:  [20/57]  eta: 0:00:06  loss: 1.3979 (1.3960)  time: 0.1662  data: 0.0001  max mem: 15812
[22:19:18.130592] Test:  [30/57]  eta: 0:00:04  loss: 1.3926 (1.3927)  time: 0.1668  data: 0.0001  max mem: 15812
[22:19:19.807242] Test:  [40/57]  eta: 0:00:03  loss: 1.3871 (1.3906)  time: 0.1673  data: 0.0001  max mem: 15812
[22:19:21.493175] Test:  [50/57]  eta: 0:00:01  loss: 1.3871 (1.3899)  time: 0.1681  data: 0.0001  max mem: 15812
[22:19:22.405347] Test:  [56/57]  eta: 0:00:00  loss: 1.3899 (1.3898)  time: 0.1634  data: 0.0000  max mem: 15812
[22:19:22.470216] Test: Total time: 0:00:09 (0.1738 s / it)
[22:19:25.386594] Dice score of the network on the train images: 0.000000, val images: 0.000000
[22:19:25.390311] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:19:26.355912] Epoch: [2]  [  0/345]  eta: 0:05:32  lr: 0.000013  loss: 1.3903 (1.3903)  time: 0.9647  data: 0.3678  max mem: 15812
[22:19:38.291916] Epoch: [2]  [ 20/345]  eta: 0:03:19  lr: 0.000013  loss: 1.3872 (1.3882)  time: 0.5967  data: 0.0001  max mem: 15812
[22:19:50.258696] Epoch: [2]  [ 40/345]  eta: 0:03:04  lr: 0.000013  loss: 1.3834 (1.3868)  time: 0.5983  data: 0.0001  max mem: 15812
[22:20:02.261332] Epoch: [2]  [ 60/345]  eta: 0:02:52  lr: 0.000014  loss: 1.3799 (1.3848)  time: 0.6001  data: 0.0001  max mem: 15812
[22:20:14.296920] Epoch: [2]  [ 80/345]  eta: 0:02:39  lr: 0.000014  loss: 1.3745 (1.3833)  time: 0.6017  data: 0.0001  max mem: 15812
[22:20:26.339109] Epoch: [2]  [100/345]  eta: 0:02:27  lr: 0.000014  loss: 1.3708 (1.3813)  time: 0.6021  data: 0.0001  max mem: 15812
[22:20:38.378420] Epoch: [2]  [120/345]  eta: 0:02:15  lr: 0.000015  loss: 1.3643 (1.3789)  time: 0.6019  data: 0.0001  max mem: 15812
[22:20:50.415122] Epoch: [2]  [140/345]  eta: 0:02:03  lr: 0.000015  loss: 1.3576 (1.3760)  time: 0.6018  data: 0.0001  max mem: 15812
[22:21:02.450200] Epoch: [2]  [160/345]  eta: 0:01:51  lr: 0.000015  loss: 1.3560 (1.3741)  time: 0.6017  data: 0.0001  max mem: 15812
[22:21:14.484301] Epoch: [2]  [180/345]  eta: 0:01:39  lr: 0.000016  loss: 1.3545 (1.3721)  time: 0.6017  data: 0.0001  max mem: 15812
[22:21:26.512998] Epoch: [2]  [200/345]  eta: 0:01:27  lr: 0.000016  loss: 1.3490 (1.3700)  time: 0.6014  data: 0.0001  max mem: 15812
[22:21:38.545823] Epoch: [2]  [220/345]  eta: 0:01:15  lr: 0.000016  loss: 1.3484 (1.3680)  time: 0.6016  data: 0.0001  max mem: 15812
[22:21:50.567645] Epoch: [2]  [240/345]  eta: 0:01:03  lr: 0.000017  loss: 1.3415 (1.3660)  time: 0.6010  data: 0.0001  max mem: 15812
[22:22:02.608450] Epoch: [2]  [260/345]  eta: 0:00:51  lr: 0.000017  loss: 1.3398 (1.3645)  time: 0.6020  data: 0.0001  max mem: 15812
[22:22:14.636615] Epoch: [2]  [280/345]  eta: 0:00:39  lr: 0.000018  loss: 1.3390 (1.3629)  time: 0.6014  data: 0.0001  max mem: 15812
[22:22:26.764335] Epoch: [2]  [300/345]  eta: 0:00:27  lr: 0.000018  loss: 1.3290 (1.3610)  time: 0.6063  data: 0.0001  max mem: 15812
[22:22:38.798544] Epoch: [2]  [320/345]  eta: 0:00:15  lr: 0.000018  loss: 1.3324 (1.3595)  time: 0.6017  data: 0.0001  max mem: 15812
[22:22:50.839463] Epoch: [2]  [340/345]  eta: 0:00:03  lr: 0.000019  loss: 1.3339 (1.3582)  time: 0.6020  data: 0.0001  max mem: 15812
[22:22:53.243215] Epoch: [2]  [344/345]  eta: 0:00:00  lr: 0.000019  loss: 1.3339 (1.3580)  time: 0.6018  data: 0.0001  max mem: 15812
[22:22:53.319986] Epoch: [2] Total time: 0:03:27 (0.6027 s / it)
[22:22:53.320302] Averaged stats: lr: 0.000019  loss: 1.3339 (1.3580)
[22:22:53.896033] Test:  [  0/345]  eta: 0:03:17  loss: 1.3401 (1.3401)  time: 0.5711  data: 0.4049  max mem: 15812
[22:22:55.585411] Test:  [ 10/345]  eta: 0:01:08  loss: 1.3420 (1.3417)  time: 0.2054  data: 0.0369  max mem: 15812
[22:22:57.277981] Test:  [ 20/345]  eta: 0:01:01  loss: 1.3420 (1.3415)  time: 0.1690  data: 0.0001  max mem: 15812
[22:22:58.974529] Test:  [ 30/345]  eta: 0:00:57  loss: 1.3429 (1.3418)  time: 0.1694  data: 0.0001  max mem: 15812
[22:23:00.678402] Test:  [ 40/345]  eta: 0:00:54  loss: 1.3433 (1.3423)  time: 0.1700  data: 0.0001  max mem: 15812
[22:23:02.386329] Test:  [ 50/345]  eta: 0:00:52  loss: 1.3433 (1.3425)  time: 0.1705  data: 0.0001  max mem: 15812
[22:23:04.099242] Test:  [ 60/345]  eta: 0:00:50  loss: 1.3426 (1.3427)  time: 0.1710  data: 0.0001  max mem: 15812
[22:23:05.816595] Test:  [ 70/345]  eta: 0:00:48  loss: 1.3423 (1.3426)  time: 0.1715  data: 0.0001  max mem: 15812
[22:23:07.540700] Test:  [ 80/345]  eta: 0:00:46  loss: 1.3432 (1.3427)  time: 0.1720  data: 0.0001  max mem: 15812
[22:23:09.269716] Test:  [ 90/345]  eta: 0:00:44  loss: 1.3439 (1.3428)  time: 0.1726  data: 0.0001  max mem: 15812
[22:23:11.003509] Test:  [100/345]  eta: 0:00:42  loss: 1.3436 (1.3428)  time: 0.1731  data: 0.0001  max mem: 15812
[22:23:12.742264] Test:  [110/345]  eta: 0:00:41  loss: 1.3429 (1.3428)  time: 0.1736  data: 0.0001  max mem: 15812
[22:23:14.486370] Test:  [120/345]  eta: 0:00:39  loss: 1.3423 (1.3427)  time: 0.1741  data: 0.0001  max mem: 15812
[22:23:16.235933] Test:  [130/345]  eta: 0:00:37  loss: 1.3427 (1.3429)  time: 0.1746  data: 0.0001  max mem: 15812
[22:23:17.991603] Test:  [140/345]  eta: 0:00:35  loss: 1.3431 (1.3428)  time: 0.1752  data: 0.0001  max mem: 15812
[22:23:19.751169] Test:  [150/345]  eta: 0:00:34  loss: 1.3428 (1.3428)  time: 0.1757  data: 0.0001  max mem: 15812
[22:23:21.515787] Test:  [160/345]  eta: 0:00:32  loss: 1.3429 (1.3429)  time: 0.1761  data: 0.0001  max mem: 15812
[22:23:23.287120] Test:  [170/345]  eta: 0:00:30  loss: 1.3426 (1.3428)  time: 0.1767  data: 0.0001  max mem: 15812
[22:23:25.062773] Test:  [180/345]  eta: 0:00:28  loss: 1.3412 (1.3427)  time: 0.1773  data: 0.0001  max mem: 15812
[22:23:26.844092] Test:  [190/345]  eta: 0:00:27  loss: 1.3423 (1.3427)  time: 0.1778  data: 0.0001  max mem: 15812
[22:23:28.630824] Test:  [200/345]  eta: 0:00:25  loss: 1.3428 (1.3428)  time: 0.1783  data: 0.0001  max mem: 15812
[22:23:30.422455] Test:  [210/345]  eta: 0:00:23  loss: 1.3427 (1.3427)  time: 0.1789  data: 0.0001  max mem: 15812
[22:23:32.216155] Test:  [220/345]  eta: 0:00:21  loss: 1.3418 (1.3427)  time: 0.1792  data: 0.0001  max mem: 15812
[22:23:34.016193] Test:  [230/345]  eta: 0:00:20  loss: 1.3439 (1.3428)  time: 0.1796  data: 0.0001  max mem: 15812
[22:23:35.819979] Test:  [240/345]  eta: 0:00:18  loss: 1.3441 (1.3428)  time: 0.1801  data: 0.0001  max mem: 15812
[22:23:37.631333] Test:  [250/345]  eta: 0:00:16  loss: 1.3432 (1.3428)  time: 0.1807  data: 0.0001  max mem: 15812
[22:23:39.447991] Test:  [260/345]  eta: 0:00:15  loss: 1.3429 (1.3428)  time: 0.1813  data: 0.0001  max mem: 15812
[22:23:41.269268] Test:  [270/345]  eta: 0:00:13  loss: 1.3437 (1.3429)  time: 0.1818  data: 0.0001  max mem: 15812
[22:23:43.095624] Test:  [280/345]  eta: 0:00:11  loss: 1.3448 (1.3429)  time: 0.1823  data: 0.0001  max mem: 15812
[22:23:44.927940] Test:  [290/345]  eta: 0:00:09  loss: 1.3436 (1.3429)  time: 0.1829  data: 0.0001  max mem: 15812
[22:23:46.765259] Test:  [300/345]  eta: 0:00:07  loss: 1.3427 (1.3429)  time: 0.1834  data: 0.0001  max mem: 15812
[22:23:48.607159] Test:  [310/345]  eta: 0:00:06  loss: 1.3421 (1.3429)  time: 0.1839  data: 0.0001  max mem: 15812
[22:23:50.454160] Test:  [320/345]  eta: 0:00:04  loss: 1.3428 (1.3429)  time: 0.1844  data: 0.0001  max mem: 15812
[22:23:52.308032] Test:  [330/345]  eta: 0:00:02  loss: 1.3440 (1.3429)  time: 0.1850  data: 0.0001  max mem: 15812
[22:23:54.165641] Test:  [340/345]  eta: 0:00:00  loss: 1.3432 (1.3429)  time: 0.1855  data: 0.0001  max mem: 15812
[22:23:54.910413] Test:  [344/345]  eta: 0:00:00  loss: 1.3435 (1.3429)  time: 0.1857  data: 0.0001  max mem: 15812
[22:23:54.985144] Test: Total time: 0:01:01 (0.1787 s / it)
[22:24:11.948953] Test:  [ 0/57]  eta: 0:00:32  loss: 1.3513 (1.3513)  time: 0.5682  data: 0.4042  max mem: 15812
[22:24:13.609907] Test:  [10/57]  eta: 0:00:09  loss: 1.3483 (1.3475)  time: 0.2026  data: 0.0368  max mem: 15812
[22:24:15.277131] Test:  [20/57]  eta: 0:00:06  loss: 1.3483 (1.3467)  time: 0.1663  data: 0.0001  max mem: 15812
[22:24:16.950242] Test:  [30/57]  eta: 0:00:04  loss: 1.3411 (1.3425)  time: 0.1670  data: 0.0001  max mem: 15812
[22:24:18.629868] Test:  [40/57]  eta: 0:00:03  loss: 1.3346 (1.3398)  time: 0.1676  data: 0.0001  max mem: 15812
[22:24:20.318432] Test:  [50/57]  eta: 0:00:01  loss: 1.3346 (1.3389)  time: 0.1684  data: 0.0001  max mem: 15812
[22:24:21.230682] Test:  [56/57]  eta: 0:00:00  loss: 1.3385 (1.3387)  time: 0.1636  data: 0.0001  max mem: 15812
[22:24:21.302742] Test: Total time: 0:00:09 (0.1741 s / it)
[22:24:24.197021] Dice score of the network on the train images: 0.000000, val images: 0.000000
[22:24:24.201106] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:24:25.195386] Epoch: [3]  [  0/345]  eta: 0:05:42  lr: 0.000019  loss: 1.3233 (1.3233)  time: 0.9932  data: 0.3961  max mem: 15812
[22:24:37.112790] Epoch: [3]  [ 20/345]  eta: 0:03:19  lr: 0.000019  loss: 1.3269 (1.3270)  time: 0.5958  data: 0.0001  max mem: 15812
[22:24:49.072248] Epoch: [3]  [ 40/345]  eta: 0:03:04  lr: 0.000019  loss: 1.3227 (1.3284)  time: 0.5979  data: 0.0001  max mem: 15812
[22:25:01.086344] Epoch: [3]  [ 60/345]  eta: 0:02:52  lr: 0.000020  loss: 1.3264 (1.3277)  time: 0.6007  data: 0.0001  max mem: 15812
[22:25:13.110131] Epoch: [3]  [ 80/345]  eta: 0:02:39  lr: 0.000020  loss: 1.3173 (1.3259)  time: 0.6011  data: 0.0001  max mem: 15812
[22:25:25.158710] Epoch: [3]  [100/345]  eta: 0:02:27  lr: 0.000021  loss: 1.3134 (1.3249)  time: 0.6024  data: 0.0001  max mem: 15812
[22:25:37.225792] Epoch: [3]  [120/345]  eta: 0:02:15  lr: 0.000021  loss: 1.3151 (1.3246)  time: 0.6033  data: 0.0001  max mem: 15812
[22:25:49.286842] Epoch: [3]  [140/345]  eta: 0:02:03  lr: 0.000021  loss: 1.3108 (1.3231)  time: 0.6030  data: 0.0001  max mem: 15812
[22:26:01.348756] Epoch: [3]  [160/345]  eta: 0:01:51  lr: 0.000022  loss: 1.3078 (1.3214)  time: 0.6030  data: 0.0001  max mem: 15812
[22:26:13.400709] Epoch: [3]  [180/345]  eta: 0:01:39  lr: 0.000022  loss: 1.3089 (1.3203)  time: 0.6026  data: 0.0001  max mem: 15812
[22:26:25.448534] Epoch: [3]  [200/345]  eta: 0:01:27  lr: 0.000022  loss: 1.3048 (1.3189)  time: 0.6024  data: 0.0001  max mem: 15812
[22:26:37.501236] Epoch: [3]  [220/345]  eta: 0:01:15  lr: 0.000023  loss: 1.3027 (1.3180)  time: 0.6026  data: 0.0001  max mem: 15812
[22:26:49.546442] Epoch: [3]  [240/345]  eta: 0:01:03  lr: 0.000023  loss: 1.2995 (1.3170)  time: 0.6022  data: 0.0001  max mem: 15812
[22:27:01.584586] Epoch: [3]  [260/345]  eta: 0:00:51  lr: 0.000023  loss: 1.2994 (1.3157)  time: 0.6019  data: 0.0001  max mem: 15812
[22:27:13.620145] Epoch: [3]  [280/345]  eta: 0:00:39  lr: 0.000024  loss: 1.2979 (1.3145)  time: 0.6017  data: 0.0001  max mem: 15812
[22:27:25.649851] Epoch: [3]  [300/345]  eta: 0:00:27  lr: 0.000024  loss: 1.2990 (1.3137)  time: 0.6014  data: 0.0001  max mem: 15812
[22:27:37.657009] Epoch: [3]  [320/345]  eta: 0:00:15  lr: 0.000025  loss: 1.2966 (1.3129)  time: 0.6003  data: 0.0001  max mem: 15812
[22:27:49.665771] Epoch: [3]  [340/345]  eta: 0:00:03  lr: 0.000025  loss: 1.2924 (1.3118)  time: 0.6004  data: 0.0001  max mem: 15812
[22:27:52.067610] Epoch: [3]  [344/345]  eta: 0:00:00  lr: 0.000025  loss: 1.2921 (1.3117)  time: 0.6003  data: 0.0001  max mem: 15812
[22:27:52.148713] Epoch: [3] Total time: 0:03:27 (0.6027 s / it)
[22:27:52.149055] Averaged stats: lr: 0.000025  loss: 1.2921 (1.3117)
[22:27:52.742125] Test:  [  0/345]  eta: 0:03:22  loss: 1.2997 (1.2997)  time: 0.5874  data: 0.4214  max mem: 15812
[22:27:54.428740] Test:  [ 10/345]  eta: 0:01:09  loss: 1.2923 (1.2929)  time: 0.2067  data: 0.0384  max mem: 15812
[22:27:56.122339] Test:  [ 20/345]  eta: 0:01:01  loss: 1.2912 (1.2919)  time: 0.1689  data: 0.0001  max mem: 15812
[22:27:57.820207] Test:  [ 30/345]  eta: 0:00:57  loss: 1.2905 (1.2920)  time: 0.1695  data: 0.0001  max mem: 15812
[22:27:59.523246] Test:  [ 40/345]  eta: 0:00:54  loss: 1.2918 (1.2918)  time: 0.1700  data: 0.0001  max mem: 15812
[22:28:01.231411] Test:  [ 50/345]  eta: 0:00:52  loss: 1.2920 (1.2918)  time: 0.1705  data: 0.0001  max mem: 15812
[22:28:02.945757] Test:  [ 60/345]  eta: 0:00:50  loss: 1.2907 (1.2916)  time: 0.1711  data: 0.0001  max mem: 15812
[22:28:04.665584] Test:  [ 70/345]  eta: 0:00:48  loss: 1.2919 (1.2917)  time: 0.1716  data: 0.0001  max mem: 15812
[22:28:06.390221] Test:  [ 80/345]  eta: 0:00:46  loss: 1.2919 (1.2916)  time: 0.1722  data: 0.0001  max mem: 15812
[22:28:08.119879] Test:  [ 90/345]  eta: 0:00:44  loss: 1.2893 (1.2913)  time: 0.1727  data: 0.0001  max mem: 15812
[22:28:09.854237] Test:  [100/345]  eta: 0:00:42  loss: 1.2891 (1.2912)  time: 0.1731  data: 0.0001  max mem: 15812
[22:28:11.593995] Test:  [110/345]  eta: 0:00:41  loss: 1.2892 (1.2912)  time: 0.1736  data: 0.0001  max mem: 15812
[22:28:13.338369] Test:  [120/345]  eta: 0:00:39  loss: 1.2916 (1.2912)  time: 0.1741  data: 0.0001  max mem: 15812
[22:28:15.088310] Test:  [130/345]  eta: 0:00:37  loss: 1.2900 (1.2910)  time: 0.1747  data: 0.0001  max mem: 15812
[22:28:16.843517] Test:  [140/345]  eta: 0:00:35  loss: 1.2900 (1.2910)  time: 0.1752  data: 0.0001  max mem: 15812
[22:28:18.603616] Test:  [150/345]  eta: 0:00:34  loss: 1.2918 (1.2910)  time: 0.1757  data: 0.0001  max mem: 15812
[22:28:20.368957] Test:  [160/345]  eta: 0:00:32  loss: 1.2914 (1.2911)  time: 0.1762  data: 0.0001  max mem: 15812
[22:28:22.139545] Test:  [170/345]  eta: 0:00:30  loss: 1.2905 (1.2910)  time: 0.1767  data: 0.0001  max mem: 15812
[22:28:23.914853] Test:  [180/345]  eta: 0:00:28  loss: 1.2898 (1.2909)  time: 0.1772  data: 0.0001  max mem: 15812
[22:28:25.696223] Test:  [190/345]  eta: 0:00:27  loss: 1.2907 (1.2909)  time: 0.1778  data: 0.0001  max mem: 15812
[22:28:27.484963] Test:  [200/345]  eta: 0:00:25  loss: 1.2920 (1.2911)  time: 0.1784  data: 0.0001  max mem: 15812
[22:28:29.274889] Test:  [210/345]  eta: 0:00:23  loss: 1.2920 (1.2910)  time: 0.1789  data: 0.0001  max mem: 15812
[22:28:31.073055] Test:  [220/345]  eta: 0:00:22  loss: 1.2896 (1.2910)  time: 0.1793  data: 0.0001  max mem: 15812
[22:28:32.874773] Test:  [230/345]  eta: 0:00:20  loss: 1.2896 (1.2909)  time: 0.1799  data: 0.0001  max mem: 15812
[22:28:34.679476] Test:  [240/345]  eta: 0:00:18  loss: 1.2870 (1.2908)  time: 0.1803  data: 0.0001  max mem: 15812
[22:28:36.490735] Test:  [250/345]  eta: 0:00:16  loss: 1.2915 (1.2909)  time: 0.1807  data: 0.0001  max mem: 15812
[22:28:38.307007] Test:  [260/345]  eta: 0:00:15  loss: 1.2918 (1.2909)  time: 0.1813  data: 0.0001  max mem: 15812
[22:28:40.128843] Test:  [270/345]  eta: 0:00:13  loss: 1.2911 (1.2909)  time: 0.1818  data: 0.0001  max mem: 15812
[22:28:41.956641] Test:  [280/345]  eta: 0:00:11  loss: 1.2900 (1.2909)  time: 0.1824  data: 0.0001  max mem: 15812
[22:28:43.789587] Test:  [290/345]  eta: 0:00:09  loss: 1.2916 (1.2909)  time: 0.1830  data: 0.0001  max mem: 15812
[22:28:45.626323] Test:  [300/345]  eta: 0:00:07  loss: 1.2929 (1.2909)  time: 0.1834  data: 0.0001  max mem: 15812
[22:28:47.469559] Test:  [310/345]  eta: 0:00:06  loss: 1.2929 (1.2909)  time: 0.1839  data: 0.0001  max mem: 15812
[22:28:49.317715] Test:  [320/345]  eta: 0:00:04  loss: 1.2897 (1.2909)  time: 0.1845  data: 0.0001  max mem: 15812
[22:28:51.172060] Test:  [330/345]  eta: 0:00:02  loss: 1.2893 (1.2909)  time: 0.1851  data: 0.0001  max mem: 15812
[22:28:53.031036] Test:  [340/345]  eta: 0:00:00  loss: 1.2906 (1.2909)  time: 0.1856  data: 0.0001  max mem: 15812
[22:28:53.775481] Test:  [344/345]  eta: 0:00:00  loss: 1.2894 (1.2909)  time: 0.1858  data: 0.0001  max mem: 15812
[22:28:53.848169] Test: Total time: 0:01:01 (0.1788 s / it)
[22:29:10.755342] Test:  [ 0/57]  eta: 0:00:30  loss: 1.3046 (1.3046)  time: 0.5374  data: 0.3730  max mem: 15812
[22:29:12.415799] Test:  [10/57]  eta: 0:00:09  loss: 1.2991 (1.2981)  time: 0.1997  data: 0.0340  max mem: 15812
[22:29:14.081819] Test:  [20/57]  eta: 0:00:06  loss: 1.2991 (1.2975)  time: 0.1663  data: 0.0001  max mem: 15812
[22:29:15.755062] Test:  [30/57]  eta: 0:00:04  loss: 1.2835 (1.2897)  time: 0.1669  data: 0.0001  max mem: 15812
[22:29:17.434673] Test:  [40/57]  eta: 0:00:02  loss: 1.2722 (1.2847)  time: 0.1676  data: 0.0001  max mem: 15812
[22:29:19.124327] Test:  [50/57]  eta: 0:00:01  loss: 1.2722 (1.2828)  time: 0.1684  data: 0.0001  max mem: 15812
[22:29:20.036893] Test:  [56/57]  eta: 0:00:00  loss: 1.2796 (1.2826)  time: 0.1637  data: 0.0000  max mem: 15812
[22:29:20.108175] Test: Total time: 0:00:09 (0.1735 s / it)
[22:29:23.056354] Dice score of the network on the train images: 0.000000, val images: 0.000000
[22:29:23.060485] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:29:24.062728] Epoch: [4]  [  0/345]  eta: 0:05:45  lr: 0.000025  loss: 1.2984 (1.2984)  time: 1.0013  data: 0.4041  max mem: 15812
[22:29:35.984832] Epoch: [4]  [ 20/345]  eta: 0:03:19  lr: 0.000025  loss: 1.2863 (1.2910)  time: 0.5961  data: 0.0001  max mem: 15812
[22:29:48.057728] Epoch: [4]  [ 40/345]  eta: 0:03:05  lr: 0.000026  loss: 1.2881 (1.2895)  time: 0.6036  data: 0.0001  max mem: 15812
[22:30:00.063674] Epoch: [4]  [ 60/345]  eta: 0:02:52  lr: 0.000026  loss: 1.2772 (1.2875)  time: 0.6002  data: 0.0001  max mem: 15812
[22:30:12.103910] Epoch: [4]  [ 80/345]  eta: 0:02:40  lr: 0.000026  loss: 1.2739 (1.2849)  time: 0.6020  data: 0.0001  max mem: 15812
[22:30:24.159722] Epoch: [4]  [100/345]  eta: 0:02:28  lr: 0.000027  loss: 1.2696 (1.2826)  time: 0.6028  data: 0.0001  max mem: 15812
[22:30:36.226833] Epoch: [4]  [120/345]  eta: 0:02:16  lr: 0.000027  loss: 1.2637 (1.2795)  time: 0.6033  data: 0.0001  max mem: 15812
[22:30:48.287845] Epoch: [4]  [140/345]  eta: 0:02:03  lr: 0.000028  loss: 1.2583 (1.2768)  time: 0.6030  data: 0.0001  max mem: 15812
[22:31:00.352821] Epoch: [4]  [160/345]  eta: 0:01:51  lr: 0.000028  loss: 1.2646 (1.2750)  time: 0.6032  data: 0.0001  max mem: 15812
[22:31:12.393530] Epoch: [4]  [180/345]  eta: 0:01:39  lr: 0.000028  loss: 1.2413 (1.2718)  time: 0.6020  data: 0.0001  max mem: 15812
[22:31:24.447177] Epoch: [4]  [200/345]  eta: 0:01:27  lr: 0.000029  loss: 1.2341 (1.2682)  time: 0.6026  data: 0.0001  max mem: 15812
[22:31:36.501041] Epoch: [4]  [220/345]  eta: 0:01:15  lr: 0.000029  loss: 1.2286 (1.2649)  time: 0.6027  data: 0.0001  max mem: 15812
[22:31:48.543049] Epoch: [4]  [240/345]  eta: 0:01:03  lr: 0.000029  loss: 1.2204 (1.2613)  time: 0.6021  data: 0.0001  max mem: 15812
[22:32:00.585137] Epoch: [4]  [260/345]  eta: 0:00:51  lr: 0.000030  loss: 1.2115 (1.2577)  time: 0.6021  data: 0.0001  max mem: 15812
[22:32:12.631123] Epoch: [4]  [280/345]  eta: 0:00:39  lr: 0.000030  loss: 1.2044 (1.2539)  time: 0.6023  data: 0.0001  max mem: 15812
[22:32:24.657264] Epoch: [4]  [300/345]  eta: 0:00:27  lr: 0.000030  loss: 1.1858 (1.2498)  time: 0.6013  data: 0.0001  max mem: 15812
[22:32:36.691456] Epoch: [4]  [320/345]  eta: 0:00:15  lr: 0.000031  loss: 1.1773 (1.2456)  time: 0.6017  data: 0.0001  max mem: 15812
[22:32:48.725669] Epoch: [4]  [340/345]  eta: 0:00:03  lr: 0.000031  loss: 1.1820 (1.2419)  time: 0.6017  data: 0.0001  max mem: 15812
[22:32:51.132464] Epoch: [4]  [344/345]  eta: 0:00:00  lr: 0.000031  loss: 1.1820 (1.2414)  time: 0.6015  data: 0.0001  max mem: 15812
[22:32:51.209755] Epoch: [4] Total time: 0:03:28 (0.6033 s / it)
[22:32:51.209981] Averaged stats: lr: 0.000031  loss: 1.1820 (1.2414)
[22:32:51.849854] Test:  [  0/345]  eta: 0:03:38  loss: 1.1801 (1.1801)  time: 0.6346  data: 0.4684  max mem: 15812
[22:32:53.536126] Test:  [ 10/345]  eta: 0:01:10  loss: 1.1646 (1.1601)  time: 0.2109  data: 0.0427  max mem: 15812
[22:32:55.228537] Test:  [ 20/345]  eta: 0:01:02  loss: 1.1543 (1.1587)  time: 0.1689  data: 0.0001  max mem: 15812
[22:32:56.926047] Test:  [ 30/345]  eta: 0:00:58  loss: 1.1517 (1.1527)  time: 0.1694  data: 0.0001  max mem: 15812
[22:32:58.629086] Test:  [ 40/345]  eta: 0:00:55  loss: 1.1517 (1.1549)  time: 0.1700  data: 0.0001  max mem: 15812
[22:33:00.337358] Test:  [ 50/345]  eta: 0:00:52  loss: 1.1531 (1.1529)  time: 0.1705  data: 0.0001  max mem: 15812
[22:33:02.050683] Test:  [ 60/345]  eta: 0:00:50  loss: 1.1495 (1.1532)  time: 0.1710  data: 0.0001  max mem: 15812
[22:33:03.769534] Test:  [ 70/345]  eta: 0:00:48  loss: 1.1492 (1.1525)  time: 0.1715  data: 0.0001  max mem: 15812
[22:33:05.492469] Test:  [ 80/345]  eta: 0:00:46  loss: 1.1447 (1.1517)  time: 0.1720  data: 0.0001  max mem: 15812
[22:33:07.221142] Test:  [ 90/345]  eta: 0:00:44  loss: 1.1490 (1.1519)  time: 0.1725  data: 0.0001  max mem: 15812
[22:33:08.954929] Test:  [100/345]  eta: 0:00:43  loss: 1.1510 (1.1513)  time: 0.1731  data: 0.0001  max mem: 15812
[22:33:10.695073] Test:  [110/345]  eta: 0:00:41  loss: 1.1510 (1.1519)  time: 0.1736  data: 0.0001  max mem: 15812
[22:33:12.440346] Test:  [120/345]  eta: 0:00:39  loss: 1.1470 (1.1513)  time: 0.1742  data: 0.0001  max mem: 15812
[22:33:14.189151] Test:  [130/345]  eta: 0:00:37  loss: 1.1426 (1.1500)  time: 0.1746  data: 0.0001  max mem: 15812
[22:33:15.943898] Test:  [140/345]  eta: 0:00:35  loss: 1.1484 (1.1504)  time: 0.1751  data: 0.0001  max mem: 15812
[22:33:17.704484] Test:  [150/345]  eta: 0:00:34  loss: 1.1543 (1.1508)  time: 0.1757  data: 0.0001  max mem: 15812
[22:33:19.468801] Test:  [160/345]  eta: 0:00:32  loss: 1.1485 (1.1495)  time: 0.1762  data: 0.0001  max mem: 15812
[22:33:21.237641] Test:  [170/345]  eta: 0:00:30  loss: 1.1447 (1.1500)  time: 0.1766  data: 0.0001  max mem: 15812
[22:33:23.013453] Test:  [180/345]  eta: 0:00:28  loss: 1.1452 (1.1496)  time: 0.1772  data: 0.0001  max mem: 15812
[22:33:24.794061] Test:  [190/345]  eta: 0:00:27  loss: 1.1437 (1.1493)  time: 0.1778  data: 0.0001  max mem: 15812
[22:33:26.580793] Test:  [200/345]  eta: 0:00:25  loss: 1.1529 (1.1497)  time: 0.1783  data: 0.0001  max mem: 15812
[22:33:28.370865] Test:  [210/345]  eta: 0:00:23  loss: 1.1613 (1.1504)  time: 0.1788  data: 0.0001  max mem: 15812
[22:33:30.169394] Test:  [220/345]  eta: 0:00:22  loss: 1.1636 (1.1509)  time: 0.1794  data: 0.0001  max mem: 15812
[22:33:31.971678] Test:  [230/345]  eta: 0:00:20  loss: 1.1518 (1.1509)  time: 0.1800  data: 0.0001  max mem: 15812
[22:33:33.777494] Test:  [240/345]  eta: 0:00:18  loss: 1.1587 (1.1514)  time: 0.1803  data: 0.0001  max mem: 15812
[22:33:35.587480] Test:  [250/345]  eta: 0:00:16  loss: 1.1612 (1.1515)  time: 0.1807  data: 0.0001  max mem: 15812
[22:33:37.404348] Test:  [260/345]  eta: 0:00:15  loss: 1.1575 (1.1516)  time: 0.1813  data: 0.0001  max mem: 15812
[22:33:39.225801] Test:  [270/345]  eta: 0:00:13  loss: 1.1539 (1.1517)  time: 0.1819  data: 0.0001  max mem: 15812
[22:33:41.054237] Test:  [280/345]  eta: 0:00:11  loss: 1.1557 (1.1523)  time: 0.1824  data: 0.0001  max mem: 15812
[22:33:42.887343] Test:  [290/345]  eta: 0:00:09  loss: 1.1685 (1.1528)  time: 0.1830  data: 0.0001  max mem: 15812
[22:33:44.723958] Test:  [300/345]  eta: 0:00:07  loss: 1.1530 (1.1528)  time: 0.1834  data: 0.0001  max mem: 15812
[22:33:46.564879] Test:  [310/345]  eta: 0:00:06  loss: 1.1505 (1.1530)  time: 0.1838  data: 0.0001  max mem: 15812
[22:33:48.410788] Test:  [320/345]  eta: 0:00:04  loss: 1.1505 (1.1528)  time: 0.1843  data: 0.0001  max mem: 15812
[22:33:50.264145] Test:  [330/345]  eta: 0:00:02  loss: 1.1542 (1.1529)  time: 0.1849  data: 0.0001  max mem: 15812
[22:33:52.121839] Test:  [340/345]  eta: 0:00:00  loss: 1.1585 (1.1529)  time: 0.1855  data: 0.0001  max mem: 15812
[22:33:52.866675] Test:  [344/345]  eta: 0:00:00  loss: 1.1542 (1.1527)  time: 0.1857  data: 0.0001  max mem: 15812
[22:33:52.939430] Test: Total time: 0:01:01 (0.1789 s / it)
[22:34:09.863609] Test:  [ 0/57]  eta: 0:00:31  loss: 1.2290 (1.2290)  time: 0.5553  data: 0.3909  max mem: 15812
[22:34:11.523077] Test:  [10/57]  eta: 0:00:09  loss: 1.2038 (1.1930)  time: 0.2013  data: 0.0356  max mem: 15812
[22:34:13.188376] Test:  [20/57]  eta: 0:00:06  loss: 1.2142 (1.1945)  time: 0.1662  data: 0.0001  max mem: 15812
[22:34:14.861068] Test:  [30/57]  eta: 0:00:04  loss: 1.1266 (1.1474)  time: 0.1668  data: 0.0001  max mem: 15812
[22:34:16.541707] Test:  [40/57]  eta: 0:00:02  loss: 1.0367 (1.1177)  time: 0.1676  data: 0.0001  max mem: 15812
[22:34:18.229866] Test:  [50/57]  eta: 0:00:01  loss: 1.0393 (1.1092)  time: 0.1683  data: 0.0001  max mem: 15812
[22:34:19.142078] Test:  [56/57]  eta: 0:00:00  loss: 1.0963 (1.1132)  time: 0.1636  data: 0.0000  max mem: 15812
[22:34:19.212556] Test: Total time: 0:00:09 (0.1738 s / it)
[22:34:22.135819] Dice score of the network on the train images: 0.502969, val images: 0.603499
[22:34:22.136033] saving best_prec_model_0 @ epoch 4
[22:34:22.862343] saving best_rec_model_0 @ epoch 4
[22:34:23.556147] saving best_dice_model_0 @ epoch 4
[22:34:24.713467] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:34:25.747251] Epoch: [5]  [  0/345]  eta: 0:05:56  lr: 0.000031  loss: 1.2064 (1.2064)  time: 1.0328  data: 0.4361  max mem: 15812
[22:34:37.664952] Epoch: [5]  [ 20/345]  eta: 0:03:20  lr: 0.000032  loss: 1.1559 (1.1633)  time: 0.5958  data: 0.0001  max mem: 15812
[22:34:49.605338] Epoch: [5]  [ 40/345]  eta: 0:03:05  lr: 0.000032  loss: 1.1516 (1.1596)  time: 0.5970  data: 0.0001  max mem: 15812
[22:35:01.571927] Epoch: [5]  [ 60/345]  eta: 0:02:52  lr: 0.000032  loss: 1.1607 (1.1604)  time: 0.5983  data: 0.0001  max mem: 15812
[22:35:13.575584] Epoch: [5]  [ 80/345]  eta: 0:02:39  lr: 0.000033  loss: 1.1413 (1.1571)  time: 0.6001  data: 0.0001  max mem: 15812
[22:35:25.580888] Epoch: [5]  [100/345]  eta: 0:02:27  lr: 0.000033  loss: 1.1394 (1.1537)  time: 0.6002  data: 0.0001  max mem: 15812
[22:35:37.615284] Epoch: [5]  [120/345]  eta: 0:02:15  lr: 0.000033  loss: 1.1146 (1.1487)  time: 0.6017  data: 0.0001  max mem: 15812
[22:35:49.663246] Epoch: [5]  [140/345]  eta: 0:02:03  lr: 0.000034  loss: 1.1218 (1.1449)  time: 0.6024  data: 0.0001  max mem: 15812
[22:36:01.711298] Epoch: [5]  [160/345]  eta: 0:01:51  lr: 0.000034  loss: 1.1248 (1.1421)  time: 0.6024  data: 0.0001  max mem: 15812
[22:36:13.773651] Epoch: [5]  [180/345]  eta: 0:01:39  lr: 0.000035  loss: 1.1252 (1.1398)  time: 0.6031  data: 0.0001  max mem: 15812
[22:36:25.832539] Epoch: [5]  [200/345]  eta: 0:01:27  lr: 0.000035  loss: 1.0981 (1.1364)  time: 0.6029  data: 0.0001  max mem: 15812
[22:36:37.891254] Epoch: [5]  [220/345]  eta: 0:01:15  lr: 0.000035  loss: 1.0898 (1.1322)  time: 0.6029  data: 0.0001  max mem: 15812
[22:36:49.933010] Epoch: [5]  [240/345]  eta: 0:01:03  lr: 0.000036  loss: 1.0832 (1.1285)  time: 0.6020  data: 0.0001  max mem: 15812
[22:37:01.988830] Epoch: [5]  [260/345]  eta: 0:00:51  lr: 0.000036  loss: 1.0778 (1.1251)  time: 0.6027  data: 0.0001  max mem: 15812
[22:37:14.043870] Epoch: [5]  [280/345]  eta: 0:00:39  lr: 0.000036  loss: 1.0883 (1.1230)  time: 0.6027  data: 0.0001  max mem: 15812
[22:37:26.094875] Epoch: [5]  [300/345]  eta: 0:00:27  lr: 0.000037  loss: 1.0667 (1.1191)  time: 0.6025  data: 0.0001  max mem: 15812
[22:37:38.145884] Epoch: [5]  [320/345]  eta: 0:00:15  lr: 0.000037  loss: 1.0668 (1.1165)  time: 0.6025  data: 0.0001  max mem: 15812
[22:37:50.175308] Epoch: [5]  [340/345]  eta: 0:00:03  lr: 0.000037  loss: 1.0671 (1.1136)  time: 0.6014  data: 0.0001  max mem: 15812
[22:37:52.580658] Epoch: [5]  [344/345]  eta: 0:00:00  lr: 0.000037  loss: 1.0671 (1.1129)  time: 0.6011  data: 0.0001  max mem: 15812
[22:37:52.652034] Epoch: [5] Total time: 0:03:27 (0.6027 s / it)
[22:37:52.652717] Averaged stats: lr: 0.000037  loss: 1.0671 (1.1129)
[22:37:53.256558] Test:  [  0/345]  eta: 0:03:26  loss: 0.9764 (0.9764)  time: 0.5983  data: 0.4318  max mem: 15812
[22:37:54.946218] Test:  [ 10/345]  eta: 0:01:09  loss: 1.0173 (1.0129)  time: 0.2079  data: 0.0393  max mem: 15812
[22:37:56.640512] Test:  [ 20/345]  eta: 0:01:01  loss: 1.0177 (1.0170)  time: 0.1691  data: 0.0001  max mem: 15812
[22:37:58.338957] Test:  [ 30/345]  eta: 0:00:57  loss: 1.0221 (1.0216)  time: 0.1696  data: 0.0001  max mem: 15812
[22:38:00.044315] Test:  [ 40/345]  eta: 0:00:54  loss: 1.0186 (1.0203)  time: 0.1701  data: 0.0001  max mem: 15812
[22:38:01.752426] Test:  [ 50/345]  eta: 0:00:52  loss: 1.0155 (1.0209)  time: 0.1706  data: 0.0001  max mem: 15812
[22:38:03.467379] Test:  [ 60/345]  eta: 0:00:50  loss: 1.0098 (1.0193)  time: 0.1711  data: 0.0001  max mem: 15812
[22:38:05.187083] Test:  [ 70/345]  eta: 0:00:48  loss: 1.0151 (1.0205)  time: 0.1717  data: 0.0001  max mem: 15812
[22:38:06.912068] Test:  [ 80/345]  eta: 0:00:46  loss: 1.0206 (1.0206)  time: 0.1722  data: 0.0001  max mem: 15812
[22:38:08.642718] Test:  [ 90/345]  eta: 0:00:44  loss: 1.0116 (1.0198)  time: 0.1727  data: 0.0001  max mem: 15812
[22:38:10.378472] Test:  [100/345]  eta: 0:00:42  loss: 1.0116 (1.0191)  time: 0.1733  data: 0.0001  max mem: 15812
[22:38:12.117571] Test:  [110/345]  eta: 0:00:41  loss: 1.0100 (1.0187)  time: 0.1737  data: 0.0001  max mem: 15812
[22:38:13.862340] Test:  [120/345]  eta: 0:00:39  loss: 1.0163 (1.0185)  time: 0.1741  data: 0.0001  max mem: 15812
[22:38:15.612180] Test:  [130/345]  eta: 0:00:37  loss: 1.0231 (1.0194)  time: 0.1747  data: 0.0001  max mem: 15812
[22:38:17.368770] Test:  [140/345]  eta: 0:00:35  loss: 1.0289 (1.0201)  time: 0.1753  data: 0.0001  max mem: 15812
[22:38:19.129857] Test:  [150/345]  eta: 0:00:34  loss: 1.0289 (1.0204)  time: 0.1758  data: 0.0001  max mem: 15812
[22:38:20.896185] Test:  [160/345]  eta: 0:00:32  loss: 1.0269 (1.0200)  time: 0.1763  data: 0.0001  max mem: 15812
[22:38:22.667443] Test:  [170/345]  eta: 0:00:30  loss: 1.0150 (1.0199)  time: 0.1768  data: 0.0001  max mem: 15812
[22:38:24.445539] Test:  [180/345]  eta: 0:00:28  loss: 1.0179 (1.0199)  time: 0.1773  data: 0.0001  max mem: 15812
[22:38:26.227191] Test:  [190/345]  eta: 0:00:27  loss: 1.0175 (1.0196)  time: 0.1778  data: 0.0001  max mem: 15812
[22:38:28.013924] Test:  [200/345]  eta: 0:00:25  loss: 1.0264 (1.0203)  time: 0.1784  data: 0.0001  max mem: 15812
[22:38:29.807130] Test:  [210/345]  eta: 0:00:23  loss: 1.0289 (1.0204)  time: 0.1789  data: 0.0001  max mem: 15812
[22:38:31.601791] Test:  [220/345]  eta: 0:00:22  loss: 1.0048 (1.0194)  time: 0.1793  data: 0.0001  max mem: 15812
[22:38:33.401330] Test:  [230/345]  eta: 0:00:20  loss: 1.0056 (1.0190)  time: 0.1797  data: 0.0001  max mem: 15812
[22:38:35.205342] Test:  [240/345]  eta: 0:00:18  loss: 1.0187 (1.0196)  time: 0.1801  data: 0.0001  max mem: 15812
[22:38:37.018328] Test:  [250/345]  eta: 0:00:16  loss: 1.0285 (1.0196)  time: 0.1808  data: 0.0001  max mem: 15812
[22:38:38.835565] Test:  [260/345]  eta: 0:00:15  loss: 1.0278 (1.0201)  time: 0.1814  data: 0.0001  max mem: 15812
[22:38:40.657678] Test:  [270/345]  eta: 0:00:13  loss: 1.0288 (1.0202)  time: 0.1819  data: 0.0001  max mem: 15812
[22:38:42.485096] Test:  [280/345]  eta: 0:00:11  loss: 1.0288 (1.0206)  time: 0.1824  data: 0.0001  max mem: 15812
[22:38:44.317939] Test:  [290/345]  eta: 0:00:09  loss: 1.0130 (1.0208)  time: 0.1830  data: 0.0001  max mem: 15812
[22:38:46.155235] Test:  [300/345]  eta: 0:00:07  loss: 1.0265 (1.0215)  time: 0.1834  data: 0.0001  max mem: 15812
[22:38:47.997990] Test:  [310/345]  eta: 0:00:06  loss: 1.0226 (1.0212)  time: 0.1839  data: 0.0001  max mem: 15812
[22:38:49.844094] Test:  [320/345]  eta: 0:00:04  loss: 1.0137 (1.0212)  time: 0.1844  data: 0.0001  max mem: 15812
[22:38:51.697705] Test:  [330/345]  eta: 0:00:02  loss: 1.0146 (1.0211)  time: 0.1849  data: 0.0001  max mem: 15812
[22:38:53.555731] Test:  [340/345]  eta: 0:00:00  loss: 1.0150 (1.0208)  time: 0.1855  data: 0.0001  max mem: 15812
[22:38:54.300320] Test:  [344/345]  eta: 0:00:00  loss: 1.0150 (1.0209)  time: 0.1857  data: 0.0001  max mem: 15812
[22:38:54.375160] Test: Total time: 0:01:01 (0.1789 s / it)
[22:39:11.146160] Test:  [ 0/57]  eta: 0:00:32  loss: 1.1075 (1.1075)  time: 0.5749  data: 0.4111  max mem: 15812
[22:39:12.806739] Test:  [10/57]  eta: 0:00:09  loss: 1.0810 (1.0735)  time: 0.2031  data: 0.0375  max mem: 15812
[22:39:14.472410] Test:  [20/57]  eta: 0:00:06  loss: 1.1084 (1.0805)  time: 0.1662  data: 0.0001  max mem: 15812
[22:39:16.145006] Test:  [30/57]  eta: 0:00:04  loss: 0.9622 (1.0238)  time: 0.1669  data: 0.0001  max mem: 15812
[22:39:17.825034] Test:  [40/57]  eta: 0:00:03  loss: 0.8897 (0.9896)  time: 0.1676  data: 0.0001  max mem: 15812
[22:39:19.511449] Test:  [50/57]  eta: 0:00:01  loss: 0.8897 (0.9798)  time: 0.1683  data: 0.0001  max mem: 15812
[22:39:20.423541] Test:  [56/57]  eta: 0:00:00  loss: 0.9672 (0.9849)  time: 0.1635  data: 0.0000  max mem: 15812
[22:39:20.490295] Test: Total time: 0:00:09 (0.1740 s / it)
[22:39:23.258570] Dice score of the network on the train images: 0.628921, val images: 0.692848
[22:39:23.258790] saving best_prec_model_0 @ epoch 5
[22:39:24.637467] saving best_dice_model_0 @ epoch 5
[22:39:25.754113] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:39:26.807957] Epoch: [6]  [  0/345]  eta: 0:06:03  lr: 0.000038  loss: 1.0380 (1.0380)  time: 1.0527  data: 0.4571  max mem: 15812
[22:39:38.728002] Epoch: [6]  [ 20/345]  eta: 0:03:20  lr: 0.000038  loss: 1.0474 (1.0480)  time: 0.5959  data: 0.0001  max mem: 15812
[22:39:50.701185] Epoch: [6]  [ 40/345]  eta: 0:03:05  lr: 0.000038  loss: 1.0443 (1.0500)  time: 0.5986  data: 0.0001  max mem: 15812
[22:40:02.822970] Epoch: [6]  [ 60/345]  eta: 0:02:53  lr: 0.000039  loss: 1.0401 (1.0452)  time: 0.6060  data: 0.0001  max mem: 15812
[22:40:14.862162] Epoch: [6]  [ 80/345]  eta: 0:02:40  lr: 0.000039  loss: 1.0381 (1.0424)  time: 0.6019  data: 0.0001  max mem: 15812
[22:40:26.905952] Epoch: [6]  [100/345]  eta: 0:02:28  lr: 0.000039  loss: 1.0325 (1.0421)  time: 0.6021  data: 0.0001  max mem: 15812
[22:40:38.975207] Epoch: [6]  [120/345]  eta: 0:02:16  lr: 0.000040  loss: 1.0221 (1.0394)  time: 0.6034  data: 0.0001  max mem: 15812
[22:40:51.034447] Epoch: [6]  [140/345]  eta: 0:02:03  lr: 0.000040  loss: 1.0414 (1.0394)  time: 0.6029  data: 0.0001  max mem: 15812
[22:41:03.075557] Epoch: [6]  [160/345]  eta: 0:01:51  lr: 0.000040  loss: 1.0073 (1.0356)  time: 0.6020  data: 0.0001  max mem: 15812
[22:41:15.113475] Epoch: [6]  [180/345]  eta: 0:01:39  lr: 0.000041  loss: 1.0094 (1.0330)  time: 0.6019  data: 0.0001  max mem: 15812
[22:41:27.146573] Epoch: [6]  [200/345]  eta: 0:01:27  lr: 0.000041  loss: 1.0048 (1.0307)  time: 0.6016  data: 0.0001  max mem: 15812
[22:41:39.177247] Epoch: [6]  [220/345]  eta: 0:01:15  lr: 0.000041  loss: 1.0120 (1.0292)  time: 0.6015  data: 0.0001  max mem: 15812
[22:41:51.206433] Epoch: [6]  [240/345]  eta: 0:01:03  lr: 0.000042  loss: 0.9974 (1.0265)  time: 0.6014  data: 0.0001  max mem: 15812
[22:42:03.227952] Epoch: [6]  [260/345]  eta: 0:00:51  lr: 0.000042  loss: 0.9850 (1.0246)  time: 0.6010  data: 0.0001  max mem: 15812
[22:42:15.254040] Epoch: [6]  [280/345]  eta: 0:00:39  lr: 0.000043  loss: 0.9730 (1.0211)  time: 0.6013  data: 0.0001  max mem: 15812
[22:42:27.274649] Epoch: [6]  [300/345]  eta: 0:00:27  lr: 0.000043  loss: 0.9781 (1.0189)  time: 0.6010  data: 0.0001  max mem: 15812
[22:42:39.306158] Epoch: [6]  [320/345]  eta: 0:00:15  lr: 0.000043  loss: 0.9743 (1.0168)  time: 0.6015  data: 0.0001  max mem: 15812
[22:42:51.345057] Epoch: [6]  [340/345]  eta: 0:00:03  lr: 0.000044  loss: 0.9710 (1.0148)  time: 0.6019  data: 0.0001  max mem: 15812
[22:42:53.747300] Epoch: [6]  [344/345]  eta: 0:00:00  lr: 0.000044  loss: 0.9847 (1.0147)  time: 0.6018  data: 0.0001  max mem: 15812
[22:42:53.830711] Epoch: [6] Total time: 0:03:28 (0.6031 s / it)
[22:42:53.831035] Averaged stats: lr: 0.000044  loss: 0.9847 (1.0147)
[22:42:54.458888] Test:  [  0/345]  eta: 0:03:34  loss: 0.9472 (0.9472)  time: 0.6226  data: 0.4564  max mem: 15812
[22:42:56.146829] Test:  [ 10/345]  eta: 0:01:10  loss: 0.9385 (0.9380)  time: 0.2100  data: 0.0416  max mem: 15812
[22:42:57.839098] Test:  [ 20/345]  eta: 0:01:01  loss: 0.9489 (0.9483)  time: 0.1689  data: 0.0001  max mem: 15812
[22:42:59.538150] Test:  [ 30/345]  eta: 0:00:57  loss: 0.9494 (0.9461)  time: 0.1695  data: 0.0001  max mem: 15812
[22:43:01.242778] Test:  [ 40/345]  eta: 0:00:55  loss: 0.9516 (0.9503)  time: 0.1701  data: 0.0001  max mem: 15812
[22:43:02.950477] Test:  [ 50/345]  eta: 0:00:52  loss: 0.9502 (0.9472)  time: 0.1706  data: 0.0001  max mem: 15812
[22:43:04.663444] Test:  [ 60/345]  eta: 0:00:50  loss: 0.9400 (0.9459)  time: 0.1710  data: 0.0001  max mem: 15812
[22:43:06.382465] Test:  [ 70/345]  eta: 0:00:48  loss: 0.9569 (0.9493)  time: 0.1715  data: 0.0001  max mem: 15812
[22:43:08.105761] Test:  [ 80/345]  eta: 0:00:46  loss: 0.9683 (0.9516)  time: 0.1721  data: 0.0001  max mem: 15812
[22:43:09.833267] Test:  [ 90/345]  eta: 0:00:44  loss: 0.9571 (0.9517)  time: 0.1725  data: 0.0001  max mem: 15812
[22:43:11.566766] Test:  [100/345]  eta: 0:00:42  loss: 0.9375 (0.9502)  time: 0.1730  data: 0.0001  max mem: 15812
[22:43:13.305929] Test:  [110/345]  eta: 0:00:41  loss: 0.9383 (0.9495)  time: 0.1736  data: 0.0001  max mem: 15812
[22:43:15.049951] Test:  [120/345]  eta: 0:00:39  loss: 0.9399 (0.9493)  time: 0.1741  data: 0.0001  max mem: 15812
[22:43:16.800118] Test:  [130/345]  eta: 0:00:37  loss: 0.9299 (0.9487)  time: 0.1747  data: 0.0001  max mem: 15812
[22:43:18.554599] Test:  [140/345]  eta: 0:00:35  loss: 0.9299 (0.9476)  time: 0.1752  data: 0.0001  max mem: 15812
[22:43:20.316352] Test:  [150/345]  eta: 0:00:34  loss: 0.9400 (0.9480)  time: 0.1758  data: 0.0001  max mem: 15812
[22:43:22.081884] Test:  [160/345]  eta: 0:00:32  loss: 0.9303 (0.9476)  time: 0.1763  data: 0.0001  max mem: 15812
[22:43:23.852330] Test:  [170/345]  eta: 0:00:30  loss: 0.9301 (0.9482)  time: 0.1767  data: 0.0001  max mem: 15812
[22:43:25.627507] Test:  [180/345]  eta: 0:00:28  loss: 0.9569 (0.9476)  time: 0.1772  data: 0.0001  max mem: 15812
[22:43:27.407714] Test:  [190/345]  eta: 0:00:27  loss: 0.9320 (0.9466)  time: 0.1777  data: 0.0001  max mem: 15812
[22:43:29.193866] Test:  [200/345]  eta: 0:00:25  loss: 0.9337 (0.9464)  time: 0.1783  data: 0.0001  max mem: 15812
[22:43:30.984287] Test:  [210/345]  eta: 0:00:23  loss: 0.9453 (0.9472)  time: 0.1788  data: 0.0001  max mem: 15812
[22:43:32.779306] Test:  [220/345]  eta: 0:00:22  loss: 0.9465 (0.9466)  time: 0.1792  data: 0.0001  max mem: 15812
[22:43:34.579198] Test:  [230/345]  eta: 0:00:20  loss: 0.9304 (0.9462)  time: 0.1797  data: 0.0001  max mem: 15812
[22:43:36.383251] Test:  [240/345]  eta: 0:00:18  loss: 0.9448 (0.9462)  time: 0.1801  data: 0.0001  max mem: 15812
[22:43:38.194318] Test:  [250/345]  eta: 0:00:16  loss: 0.9416 (0.9459)  time: 0.1807  data: 0.0001  max mem: 15812
[22:43:40.009360] Test:  [260/345]  eta: 0:00:15  loss: 0.9236 (0.9452)  time: 0.1812  data: 0.0001  max mem: 15812
[22:43:41.829550] Test:  [270/345]  eta: 0:00:13  loss: 0.9371 (0.9454)  time: 0.1817  data: 0.0001  max mem: 15812
[22:43:43.656708] Test:  [280/345]  eta: 0:00:11  loss: 0.9527 (0.9455)  time: 0.1823  data: 0.0001  max mem: 15812
[22:43:45.488954] Test:  [290/345]  eta: 0:00:09  loss: 0.9401 (0.9449)  time: 0.1829  data: 0.0001  max mem: 15812
[22:43:47.326862] Test:  [300/345]  eta: 0:00:07  loss: 0.9340 (0.9449)  time: 0.1834  data: 0.0001  max mem: 15812
[22:43:49.169739] Test:  [310/345]  eta: 0:00:06  loss: 0.9463 (0.9446)  time: 0.1840  data: 0.0001  max mem: 15812
[22:43:51.016758] Test:  [320/345]  eta: 0:00:04  loss: 0.9459 (0.9448)  time: 0.1844  data: 0.0001  max mem: 15812
[22:43:52.870103] Test:  [330/345]  eta: 0:00:02  loss: 0.9439 (0.9445)  time: 0.1849  data: 0.0001  max mem: 15812
[22:43:54.728807] Test:  [340/345]  eta: 0:00:00  loss: 0.9380 (0.9443)  time: 0.1855  data: 0.0001  max mem: 15812
[22:43:55.473461] Test:  [344/345]  eta: 0:00:00  loss: 0.9380 (0.9441)  time: 0.1858  data: 0.0001  max mem: 15812
[22:43:55.542470] Test: Total time: 0:01:01 (0.1789 s / it)
[22:44:12.249877] Test:  [ 0/57]  eta: 0:00:30  loss: 1.0479 (1.0479)  time: 0.5371  data: 0.3733  max mem: 15812
[22:44:13.909912] Test:  [10/57]  eta: 0:00:09  loss: 1.0102 (1.0052)  time: 0.1997  data: 0.0340  max mem: 15812
[22:44:15.577326] Test:  [20/57]  eta: 0:00:06  loss: 0.9887 (0.9979)  time: 0.1663  data: 0.0001  max mem: 15812
[22:44:17.249805] Test:  [30/57]  eta: 0:00:04  loss: 0.9086 (0.9485)  time: 0.1669  data: 0.0001  max mem: 15812
[22:44:18.928939] Test:  [40/57]  eta: 0:00:02  loss: 0.8279 (0.9184)  time: 0.1675  data: 0.0001  max mem: 15812
[22:44:20.616083] Test:  [50/57]  eta: 0:00:01  loss: 0.8279 (0.9090)  time: 0.1683  data: 0.0001  max mem: 15812
[22:44:21.528028] Test:  [56/57]  eta: 0:00:00  loss: 0.8814 (0.9133)  time: 0.1635  data: 0.0001  max mem: 15812
[22:44:21.598418] Test: Total time: 0:00:09 (0.1734 s / it)
[22:44:24.397456] Dice score of the network on the train images: 0.645407, val images: 0.716071
[22:44:24.397676] saving best_prec_model_0 @ epoch 6
[22:44:25.446787] saving best_dice_model_0 @ epoch 6
[22:44:26.527147] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:44:27.525667] Epoch: [7]  [  0/345]  eta: 0:05:44  lr: 0.000044  loss: 0.9420 (0.9420)  time: 0.9975  data: 0.4005  max mem: 15812
[22:44:39.434216] Epoch: [7]  [ 20/345]  eta: 0:03:19  lr: 0.000044  loss: 0.9614 (0.9699)  time: 0.5954  data: 0.0001  max mem: 15812
[22:44:51.381062] Epoch: [7]  [ 40/345]  eta: 0:03:04  lr: 0.000044  loss: 0.9676 (0.9712)  time: 0.5973  data: 0.0001  max mem: 15812
[22:45:03.368689] Epoch: [7]  [ 60/345]  eta: 0:02:52  lr: 0.000045  loss: 0.9613 (0.9675)  time: 0.5993  data: 0.0001  max mem: 15812
[22:45:15.381965] Epoch: [7]  [ 80/345]  eta: 0:02:39  lr: 0.000045  loss: 0.9555 (0.9646)  time: 0.6006  data: 0.0001  max mem: 15812
[22:45:27.424613] Epoch: [7]  [100/345]  eta: 0:02:27  lr: 0.000046  loss: 0.9629 (0.9669)  time: 0.6021  data: 0.0001  max mem: 15812
[22:45:39.467611] Epoch: [7]  [120/345]  eta: 0:02:15  lr: 0.000046  loss: 0.9638 (0.9665)  time: 0.6021  data: 0.0001  max mem: 15812
[22:45:51.528138] Epoch: [7]  [140/345]  eta: 0:02:03  lr: 0.000046  loss: 0.9562 (0.9646)  time: 0.6030  data: 0.0001  max mem: 15812
[22:46:03.578313] Epoch: [7]  [160/345]  eta: 0:01:51  lr: 0.000047  loss: 0.9503 (0.9633)  time: 0.6025  data: 0.0001  max mem: 15812
[22:46:15.635565] Epoch: [7]  [180/345]  eta: 0:01:39  lr: 0.000047  loss: 0.9505 (0.9631)  time: 0.6028  data: 0.0001  max mem: 15812
[22:46:27.669206] Epoch: [7]  [200/345]  eta: 0:01:27  lr: 0.000047  loss: 0.9459 (0.9613)  time: 0.6016  data: 0.0001  max mem: 15812
[22:46:39.706331] Epoch: [7]  [220/345]  eta: 0:01:15  lr: 0.000048  loss: 0.9381 (0.9598)  time: 0.6018  data: 0.0001  max mem: 15812
[22:46:51.760966] Epoch: [7]  [240/345]  eta: 0:01:03  lr: 0.000048  loss: 0.9401 (0.9575)  time: 0.6027  data: 0.0001  max mem: 15812
[22:47:03.800382] Epoch: [7]  [260/345]  eta: 0:00:51  lr: 0.000048  loss: 0.9227 (0.9551)  time: 0.6019  data: 0.0001  max mem: 15812
[22:47:15.827505] Epoch: [7]  [280/345]  eta: 0:00:39  lr: 0.000049  loss: 0.9278 (0.9536)  time: 0.6013  data: 0.0001  max mem: 15812
[22:47:27.868959] Epoch: [7]  [300/345]  eta: 0:00:27  lr: 0.000049  loss: 0.9237 (0.9517)  time: 0.6020  data: 0.0001  max mem: 15812
[22:47:39.911087] Epoch: [7]  [320/345]  eta: 0:00:15  lr: 0.000050  loss: 0.9296 (0.9508)  time: 0.6021  data: 0.0001  max mem: 15812
[22:47:51.928575] Epoch: [7]  [340/345]  eta: 0:00:03  lr: 0.000050  loss: 0.9209 (0.9495)  time: 0.6008  data: 0.0001  max mem: 15812
[22:47:54.331350] Epoch: [7]  [344/345]  eta: 0:00:00  lr: 0.000050  loss: 0.9190 (0.9490)  time: 0.6008  data: 0.0001  max mem: 15812
[22:47:54.407502] Epoch: [7] Total time: 0:03:27 (0.6026 s / it)
[22:47:54.407818] Averaged stats: lr: 0.000050  loss: 0.9190 (0.9490)
[22:47:54.989101] Test:  [  0/345]  eta: 0:03:18  loss: 0.8644 (0.8644)  time: 0.5762  data: 0.4101  max mem: 15812
[22:47:56.677004] Test:  [ 10/345]  eta: 0:01:08  loss: 0.8644 (0.8639)  time: 0.2058  data: 0.0374  max mem: 15812
[22:47:58.369918] Test:  [ 20/345]  eta: 0:01:01  loss: 0.8677 (0.8730)  time: 0.1690  data: 0.0001  max mem: 15812
[22:48:00.068728] Test:  [ 30/345]  eta: 0:00:57  loss: 0.8677 (0.8712)  time: 0.1695  data: 0.0001  max mem: 15812
[22:48:01.772006] Test:  [ 40/345]  eta: 0:00:54  loss: 0.8528 (0.8648)  time: 0.1700  data: 0.0001  max mem: 15812
[22:48:03.480230] Test:  [ 50/345]  eta: 0:00:52  loss: 0.8617 (0.8665)  time: 0.1705  data: 0.0001  max mem: 15812
[22:48:05.193661] Test:  [ 60/345]  eta: 0:00:50  loss: 0.8626 (0.8679)  time: 0.1710  data: 0.0001  max mem: 15812
[22:48:06.912299] Test:  [ 70/345]  eta: 0:00:48  loss: 0.8596 (0.8668)  time: 0.1715  data: 0.0001  max mem: 15812
[22:48:08.636481] Test:  [ 80/345]  eta: 0:00:46  loss: 0.8627 (0.8670)  time: 0.1721  data: 0.0001  max mem: 15812
[22:48:10.365765] Test:  [ 90/345]  eta: 0:00:44  loss: 0.8627 (0.8676)  time: 0.1726  data: 0.0001  max mem: 15812
[22:48:12.100177] Test:  [100/345]  eta: 0:00:42  loss: 0.8686 (0.8682)  time: 0.1731  data: 0.0001  max mem: 15812
[22:48:13.840300] Test:  [110/345]  eta: 0:00:41  loss: 0.8742 (0.8691)  time: 0.1737  data: 0.0001  max mem: 15812
[22:48:15.585913] Test:  [120/345]  eta: 0:00:39  loss: 0.8770 (0.8695)  time: 0.1742  data: 0.0001  max mem: 15812
[22:48:17.337503] Test:  [130/345]  eta: 0:00:37  loss: 0.8754 (0.8696)  time: 0.1748  data: 0.0001  max mem: 15812
[22:48:19.093439] Test:  [140/345]  eta: 0:00:35  loss: 0.8616 (0.8680)  time: 0.1753  data: 0.0001  max mem: 15812
[22:48:20.853813] Test:  [150/345]  eta: 0:00:34  loss: 0.8697 (0.8694)  time: 0.1758  data: 0.0001  max mem: 15812
[22:48:22.619326] Test:  [160/345]  eta: 0:00:32  loss: 0.8826 (0.8697)  time: 0.1762  data: 0.0001  max mem: 15812
[22:48:24.391864] Test:  [170/345]  eta: 0:00:30  loss: 0.8738 (0.8698)  time: 0.1768  data: 0.0001  max mem: 15812
[22:48:26.167575] Test:  [180/345]  eta: 0:00:28  loss: 0.8734 (0.8703)  time: 0.1774  data: 0.0001  max mem: 15812
[22:48:27.948238] Test:  [190/345]  eta: 0:00:27  loss: 0.8711 (0.8697)  time: 0.1778  data: 0.0001  max mem: 15812
[22:48:29.734593] Test:  [200/345]  eta: 0:00:25  loss: 0.8605 (0.8695)  time: 0.1783  data: 0.0001  max mem: 15812
[22:48:31.526734] Test:  [210/345]  eta: 0:00:23  loss: 0.8655 (0.8694)  time: 0.1789  data: 0.0001  max mem: 15812
[22:48:33.321253] Test:  [220/345]  eta: 0:00:21  loss: 0.8680 (0.8696)  time: 0.1793  data: 0.0001  max mem: 15812
[22:48:35.121872] Test:  [230/345]  eta: 0:00:20  loss: 0.8682 (0.8698)  time: 0.1797  data: 0.0001  max mem: 15812
[22:48:36.926637] Test:  [240/345]  eta: 0:00:18  loss: 0.8727 (0.8702)  time: 0.1802  data: 0.0001  max mem: 15812
[22:48:38.740009] Test:  [250/345]  eta: 0:00:16  loss: 0.8660 (0.8695)  time: 0.1808  data: 0.0001  max mem: 15812
[22:48:40.555720] Test:  [260/345]  eta: 0:00:15  loss: 0.8523 (0.8693)  time: 0.1814  data: 0.0001  max mem: 15812
[22:48:42.378028] Test:  [270/345]  eta: 0:00:13  loss: 0.8610 (0.8696)  time: 0.1818  data: 0.0001  max mem: 15812
[22:48:44.205205] Test:  [280/345]  eta: 0:00:11  loss: 0.8695 (0.8697)  time: 0.1824  data: 0.0001  max mem: 15812
[22:48:46.037905] Test:  [290/345]  eta: 0:00:09  loss: 0.8695 (0.8699)  time: 0.1829  data: 0.0001  max mem: 15812
[22:48:47.874709] Test:  [300/345]  eta: 0:00:07  loss: 0.8704 (0.8699)  time: 0.1834  data: 0.0001  max mem: 15812
[22:48:49.716990] Test:  [310/345]  eta: 0:00:06  loss: 0.8721 (0.8704)  time: 0.1839  data: 0.0001  max mem: 15812
[22:48:51.564921] Test:  [320/345]  eta: 0:00:04  loss: 0.8750 (0.8707)  time: 0.1845  data: 0.0001  max mem: 15812
[22:48:53.418246] Test:  [330/345]  eta: 0:00:02  loss: 0.8666 (0.8704)  time: 0.1850  data: 0.0001  max mem: 15812
[22:48:55.276325] Test:  [340/345]  eta: 0:00:00  loss: 0.8643 (0.8704)  time: 0.1855  data: 0.0001  max mem: 15812
[22:48:56.020497] Test:  [344/345]  eta: 0:00:00  loss: 0.8726 (0.8706)  time: 0.1857  data: 0.0001  max mem: 15812
[22:48:56.099052] Test: Total time: 0:01:01 (0.1788 s / it)
[22:49:12.922617] Test:  [ 0/57]  eta: 0:00:31  loss: 0.9706 (0.9706)  time: 0.5530  data: 0.3886  max mem: 15812
[22:49:14.582583] Test:  [10/57]  eta: 0:00:09  loss: 0.9269 (0.9352)  time: 0.2011  data: 0.0354  max mem: 15812
[22:49:16.249039] Test:  [20/57]  eta: 0:00:06  loss: 0.9263 (0.9272)  time: 0.1662  data: 0.0001  max mem: 15812
[22:49:17.921431] Test:  [30/57]  eta: 0:00:04  loss: 0.8262 (0.8841)  time: 0.1669  data: 0.0001  max mem: 15812
[22:49:19.600675] Test:  [40/57]  eta: 0:00:02  loss: 0.7872 (0.8581)  time: 0.1675  data: 0.0001  max mem: 15812
[22:49:21.287132] Test:  [50/57]  eta: 0:00:01  loss: 0.7872 (0.8499)  time: 0.1682  data: 0.0001  max mem: 15812
[22:49:22.199460] Test:  [56/57]  eta: 0:00:00  loss: 0.8209 (0.8544)  time: 0.1635  data: 0.0001  max mem: 15812
[22:49:22.273681] Test: Total time: 0:00:09 (0.1738 s / it)
[22:49:25.080181] Dice score of the network on the train images: 0.664180, val images: 0.738482
[22:49:25.080414] saving best_prec_model_0 @ epoch 7
[22:49:26.138063] saving best_dice_model_0 @ epoch 7
[22:49:27.449182] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:49:28.431882] Epoch: [8]  [  0/345]  eta: 0:05:38  lr: 0.000050  loss: 0.9413 (0.9413)  time: 0.9816  data: 0.3854  max mem: 15812
[22:49:40.334920] Epoch: [8]  [ 20/345]  eta: 0:03:19  lr: 0.000050  loss: 0.9119 (0.9224)  time: 0.5951  data: 0.0001  max mem: 15812
[22:49:52.278770] Epoch: [8]  [ 40/345]  eta: 0:03:04  lr: 0.000051  loss: 0.9130 (0.9216)  time: 0.5971  data: 0.0001  max mem: 15812
[22:50:04.280196] Epoch: [8]  [ 60/345]  eta: 0:02:52  lr: 0.000051  loss: 0.9262 (0.9238)  time: 0.6000  data: 0.0001  max mem: 15812
[22:50:16.312261] Epoch: [8]  [ 80/345]  eta: 0:02:39  lr: 0.000051  loss: 0.9151 (0.9210)  time: 0.6016  data: 0.0001  max mem: 15812
[22:50:28.367893] Epoch: [8]  [100/345]  eta: 0:02:27  lr: 0.000052  loss: 0.9082 (0.9207)  time: 0.6027  data: 0.0001  max mem: 15812
[22:50:40.434182] Epoch: [8]  [120/345]  eta: 0:02:15  lr: 0.000052  loss: 0.9107 (0.9198)  time: 0.6033  data: 0.0001  max mem: 15812
[22:50:52.493953] Epoch: [8]  [140/345]  eta: 0:02:03  lr: 0.000053  loss: 0.9291 (0.9207)  time: 0.6029  data: 0.0001  max mem: 15812
[22:51:04.560956] Epoch: [8]  [160/345]  eta: 0:01:51  lr: 0.000053  loss: 0.9096 (0.9196)  time: 0.6033  data: 0.0001  max mem: 15812

[22:51:16.624746] Epoch: [8]  [180/345]  eta: 0:01:39  lr: 0.000053  loss: 0.8996 (0.9186)  time: 0.6031  data: 0.0001  max mem: 15812

[22:51:28.669875] Epoch: [8]  [200/345]  eta: 0:01:27  lr: 0.000054  loss: 0.8869 (0.9161)  time: 0.6022  data: 0.0001  max mem: 15812
[22:51:40.708288] Epoch: [8]  [220/345]  eta: 0:01:15  lr: 0.000054  loss: 0.8851 (0.9140)  time: 0.6019  data: 0.0001  max mem: 15812
[22:51:52.737674] Epoch: [8]  [240/345]  eta: 0:01:03  lr: 0.000054  loss: 0.8941 (0.9127)  time: 0.6014  data: 0.0001  max mem: 15812
[22:52:04.773931] Epoch: [8]  [260/345]  eta: 0:00:51  lr: 0.000055  loss: 0.9021 (0.9125)  time: 0.6018  data: 0.0001  max mem: 15812
[22:52:16.821227] Epoch: [8]  [280/345]  eta: 0:00:39  lr: 0.000055  loss: 0.9023 (0.9120)  time: 0.6023  data: 0.0001  max mem: 15812
[22:52:28.868771] Epoch: [8]  [300/345]  eta: 0:00:27  lr: 0.000055  loss: 0.8897 (0.9108)  time: 0.6023  data: 0.0001  max mem: 15812
[22:52:40.904494] Epoch: [8]  [320/345]  eta: 0:00:15  lr: 0.000056  loss: 0.8844 (0.9097)  time: 0.6017  data: 0.0001  max mem: 15812
[22:52:53.005367] Epoch: [8]  [340/345]  eta: 0:00:03  lr: 0.000056  loss: 0.8790 (0.9081)  time: 0.6050  data: 0.0001  max mem: 15812
[22:52:55.408323] Epoch: [8]  [344/345]  eta: 0:00:00  lr: 0.000056  loss: 0.8790 (0.9078)  time: 0.6007  data: 0.0001  max mem: 15812
[22:52:55.476908] Epoch: [8] Total time: 0:03:28 (0.6030 s / it)
[22:52:55.477660] Averaged stats: lr: 0.000056  loss: 0.8790 (0.9078)
[22:52:56.061264] Test:  [  0/345]  eta: 0:03:19  loss: 0.8721 (0.8721)  time: 0.5787  data: 0.4123  max mem: 15812
[22:52:57.749464] Test:  [ 10/345]  eta: 0:01:09  loss: 0.8458 (0.8470)  time: 0.2060  data: 0.0376  max mem: 15812
[22:52:59.441178] Test:  [ 20/345]  eta: 0:01:01  loss: 0.8347 (0.8379)  time: 0.1689  data: 0.0001  max mem: 15812
[22:53:01.139457] Test:  [ 30/345]  eta: 0:00:57  loss: 0.8428 (0.8417)  time: 0.1694  data: 0.0001  max mem: 15812
[22:53:02.842573] Test:  [ 40/345]  eta: 0:00:54  loss: 0.8520 (0.8446)  time: 0.1700  data: 0.0001  max mem: 15812
[22:53:04.551033] Test:  [ 50/345]  eta: 0:00:52  loss: 0.8462 (0.8438)  time: 0.1705  data: 0.0001  max mem: 15812
[22:53:06.263898] Test:  [ 60/345]  eta: 0:00:50  loss: 0.8418 (0.8454)  time: 0.1710  data: 0.0001  max mem: 15812
[22:53:07.981058] Test:  [ 70/345]  eta: 0:00:48  loss: 0.8449 (0.8467)  time: 0.1714  data: 0.0001  max mem: 15812
[22:53:09.706921] Test:  [ 80/345]  eta: 0:00:46  loss: 0.8565 (0.8468)  time: 0.1721  data: 0.0001  max mem: 15812
[22:53:11.435735] Test:  [ 90/345]  eta: 0:00:44  loss: 0.8554 (0.8483)  time: 0.1727  data: 0.0001  max mem: 15812
[22:53:13.169674] Test:  [100/345]  eta: 0:00:42  loss: 0.8512 (0.8479)  time: 0.1731  data: 0.0001  max mem: 15812
[22:53:14.909148] Test:  [110/345]  eta: 0:00:41  loss: 0.8462 (0.8486)  time: 0.1736  data: 0.0001  max mem: 15812
[22:53:16.653514] Test:  [120/345]  eta: 0:00:39  loss: 0.8605 (0.8486)  time: 0.1741  data: 0.0001  max mem: 15812
[22:53:18.403090] Test:  [130/345]  eta: 0:00:37  loss: 0.8313 (0.8478)  time: 0.1746  data: 0.0001  max mem: 15812
[22:53:20.158515] Test:  [140/345]  eta: 0:00:35  loss: 0.8434 (0.8483)  time: 0.1752  data: 0.0001  max mem: 15812
[22:53:21.919410] Test:  [150/345]  eta: 0:00:34  loss: 0.8434 (0.8482)  time: 0.1758  data: 0.0001  max mem: 15812
[22:53:23.683921] Test:  [160/345]  eta: 0:00:32  loss: 0.8401 (0.8484)  time: 0.1762  data: 0.0001  max mem: 15812
[22:53:25.455248] Test:  [170/345]  eta: 0:00:30  loss: 0.8622 (0.8494)  time: 0.1767  data: 0.0001  max mem: 15812
[22:53:27.231396] Test:  [180/345]  eta: 0:00:28  loss: 0.8570 (0.8489)  time: 0.1773  data: 0.0001  max mem: 15812
[22:53:29.013170] Test:  [190/345]  eta: 0:00:27  loss: 0.8493 (0.8493)  time: 0.1778  data: 0.0001  max mem: 15812
[22:53:30.799064] Test:  [200/345]  eta: 0:00:25  loss: 0.8477 (0.8488)  time: 0.1783  data: 0.0001  max mem: 15812
[22:53:32.591828] Test:  [210/345]  eta: 0:00:23  loss: 0.8486 (0.8491)  time: 0.1789  data: 0.0001  max mem: 15812
[22:53:34.387399] Test:  [220/345]  eta: 0:00:21  loss: 0.8487 (0.8490)  time: 0.1794  data: 0.0001  max mem: 15812
[22:53:36.186313] Test:  [230/345]  eta: 0:00:20  loss: 0.8483 (0.8496)  time: 0.1797  data: 0.0001  max mem: 15812
[22:53:37.990440] Test:  [240/345]  eta: 0:00:18  loss: 0.8544 (0.8496)  time: 0.1801  data: 0.0001  max mem: 15812
[22:53:39.805051] Test:  [250/345]  eta: 0:00:16  loss: 0.8448 (0.8490)  time: 0.1809  data: 0.0001  max mem: 15812
[22:53:41.622352] Test:  [260/345]  eta: 0:00:15  loss: 0.8540 (0.8497)  time: 0.1815  data: 0.0001  max mem: 15812
[22:53:43.444706] Test:  [270/345]  eta: 0:00:13  loss: 0.8574 (0.8496)  time: 0.1819  data: 0.0001  max mem: 15812
[22:53:45.271475] Test:  [280/345]  eta: 0:00:11  loss: 0.8587 (0.8503)  time: 0.1824  data: 0.0001  max mem: 15812
[22:53:47.105106] Test:  [290/345]  eta: 0:00:09  loss: 0.8579 (0.8507)  time: 0.1830  data: 0.0001  max mem: 15812
[22:53:48.942035] Test:  [300/345]  eta: 0:00:07  loss: 0.8542 (0.8509)  time: 0.1835  data: 0.0001  max mem: 15812
[22:53:50.784512] Test:  [310/345]  eta: 0:00:06  loss: 0.8570 (0.8511)  time: 0.1839  data: 0.0001  max mem: 15812
[22:53:52.630662] Test:  [320/345]  eta: 0:00:04  loss: 0.8342 (0.8504)  time: 0.1844  data: 0.0001  max mem: 15812
[22:53:54.484641] Test:  [330/345]  eta: 0:00:02  loss: 0.8415 (0.8503)  time: 0.1849  data: 0.0001  max mem: 15812
[22:53:56.343021] Test:  [340/345]  eta: 0:00:00  loss: 0.8478 (0.8509)  time: 0.1856  data: 0.0001  max mem: 15812
[22:53:57.087747] Test:  [344/345]  eta: 0:00:00  loss: 0.8643 (0.8511)  time: 0.1858  data: 0.0001  max mem: 15812
[22:53:57.163035] Test: Total time: 0:01:01 (0.1788 s / it)
[22:54:13.924972] Test:  [ 0/57]  eta: 0:00:34  loss: 0.9213 (0.9213)  time: 0.5978  data: 0.4334  max mem: 15812
[22:54:15.586182] Test:  [10/57]  eta: 0:00:09  loss: 0.8960 (0.9126)  time: 0.2053  data: 0.0395  max mem: 15812
[22:54:17.251661] Test:  [20/57]  eta: 0:00:06  loss: 0.8960 (0.9064)  time: 0.1663  data: 0.0001  max mem: 15812
[22:54:18.925301] Test:  [30/57]  eta: 0:00:04  loss: 0.8252 (0.8681)  time: 0.1669  data: 0.0001  max mem: 15812
[22:54:20.603652] Test:  [40/57]  eta: 0:00:03  loss: 0.7753 (0.8450)  time: 0.1675  data: 0.0001  max mem: 15812
[22:54:22.290853] Test:  [50/57]  eta: 0:00:01  loss: 0.7753 (0.8382)  time: 0.1682  data: 0.0001  max mem: 15812
[22:54:23.202661] Test:  [56/57]  eta: 0:00:00  loss: 0.8201 (0.8442)  time: 0.1635  data: 0.0000  max mem: 15812
[22:54:23.280404] Test: Total time: 0:00:09 (0.1746 s / it)
[22:54:26.078535] Dice score of the network on the train images: 0.664804, val images: 0.746436
[22:54:26.078759] saving best_rec_model_0 @ epoch 8
[22:54:27.140470] saving best_dice_model_0 @ epoch 8
[22:54:28.159035] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:54:29.144800] Epoch: [9]  [  0/345]  eta: 0:05:39  lr: 0.000056  loss: 0.8734 (0.8734)  time: 0.9847  data: 0.3883  max mem: 15812
[22:54:41.065654] Epoch: [9]  [ 20/345]  eta: 0:03:19  lr: 0.000057  loss: 0.8663 (0.8767)  time: 0.5960  data: 0.0001  max mem: 15812
[22:54:53.042958] Epoch: [9]  [ 40/345]  eta: 0:03:05  lr: 0.000057  loss: 0.9155 (0.8892)  time: 0.5988  data: 0.0001  max mem: 15812
[22:55:05.049573] Epoch: [9]  [ 60/345]  eta: 0:02:52  lr: 0.000057  loss: 0.8954 (0.8910)  time: 0.6003  data: 0.0001  max mem: 15812
[22:55:17.087680] Epoch: [9]  [ 80/345]  eta: 0:02:40  lr: 0.000058  loss: 0.8701 (0.8875)  time: 0.6019  data: 0.0001  max mem: 15812
[22:55:29.155529] Epoch: [9]  [100/345]  eta: 0:02:27  lr: 0.000058  loss: 0.8791 (0.8867)  time: 0.6033  data: 0.0001  max mem: 15812
[22:55:41.230167] Epoch: [9]  [120/345]  eta: 0:02:15  lr: 0.000058  loss: 0.8755 (0.8866)  time: 0.6037  data: 0.0001  max mem: 15812
[22:55:53.301990] Epoch: [9]  [140/345]  eta: 0:02:03  lr: 0.000059  loss: 0.8818 (0.8858)  time: 0.6035  data: 0.0001  max mem: 15812
[22:56:05.360028] Epoch: [9]  [160/345]  eta: 0:01:51  lr: 0.000059  loss: 0.8800 (0.8846)  time: 0.6029  data: 0.0001  max mem: 15812
[22:56:17.402923] Epoch: [9]  [180/345]  eta: 0:01:39  lr: 0.000060  loss: 0.8891 (0.8853)  time: 0.6021  data: 0.0001  max mem: 15812
[22:56:29.470065] Epoch: [9]  [200/345]  eta: 0:01:27  lr: 0.000060  loss: 0.8780 (0.8841)  time: 0.6033  data: 0.0001  max mem: 15812
[22:56:41.532005] Epoch: [9]  [220/345]  eta: 0:01:15  lr: 0.000060  loss: 0.8835 (0.8840)  time: 0.6031  data: 0.0001  max mem: 15812
[22:56:53.581183] Epoch: [9]  [240/345]  eta: 0:01:03  lr: 0.000061  loss: 0.8556 (0.8824)  time: 0.6024  data: 0.0001  max mem: 15812
[22:57:05.628115] Epoch: [9]  [260/345]  eta: 0:00:51  lr: 0.000061  loss: 0.8914 (0.8832)  time: 0.6023  data: 0.0001  max mem: 15812
[22:57:17.685827] Epoch: [9]  [280/345]  eta: 0:00:39  lr: 0.000061  loss: 0.8819 (0.8830)  time: 0.6028  data: 0.0001  max mem: 15812
[22:57:29.737515] Epoch: [9]  [300/345]  eta: 0:00:27  lr: 0.000062  loss: 0.8762 (0.8824)  time: 0.6025  data: 0.0001  max mem: 15812
[22:57:41.784077] Epoch: [9]  [320/345]  eta: 0:00:15  lr: 0.000062  loss: 0.8798 (0.8822)  time: 0.6023  data: 0.0001  max mem: 15812
[22:57:53.830896] Epoch: [9]  [340/345]  eta: 0:00:03  lr: 0.000062  loss: 0.8714 (0.8817)  time: 0.6023  data: 0.0001  max mem: 15812
[22:57:56.235937] Epoch: [9]  [344/345]  eta: 0:00:00  lr: 0.000062  loss: 0.8545 (0.8816)  time: 0.6019  data: 0.0001  max mem: 15812
[22:57:56.316973] Epoch: [9] Total time: 0:03:28 (0.6034 s / it)
[22:57:56.317190] Averaged stats: lr: 0.000062  loss: 0.8545 (0.8816)
[22:57:56.946990] Test:  [  0/345]  eta: 0:03:35  loss: 0.8416 (0.8416)  time: 0.6247  data: 0.4581  max mem: 15812
[22:57:58.635269] Test:  [ 10/345]  eta: 0:01:10  loss: 0.8408 (0.8397)  time: 0.2102  data: 0.0417  max mem: 15812
[22:58:00.328886] Test:  [ 20/345]  eta: 0:01:01  loss: 0.8344 (0.8361)  time: 0.1690  data: 0.0001  max mem: 15812
[22:58:02.027706] Test:  [ 30/345]  eta: 0:00:57  loss: 0.8203 (0.8319)  time: 0.1696  data: 0.0001  max mem: 15812
[22:58:03.731598] Test:  [ 40/345]  eta: 0:00:55  loss: 0.8271 (0.8346)  time: 0.1701  data: 0.0001  max mem: 15812
[22:58:05.440136] Test:  [ 50/345]  eta: 0:00:52  loss: 0.8461 (0.8372)  time: 0.1706  data: 0.0001  max mem: 15812
[22:58:07.154147] Test:  [ 60/345]  eta: 0:00:50  loss: 0.8297 (0.8347)  time: 0.1711  data: 0.0001  max mem: 15812
[22:58:08.872006] Test:  [ 70/345]  eta: 0:00:48  loss: 0.8163 (0.8345)  time: 0.1715  data: 0.0001  max mem: 15812
[22:58:10.596182] Test:  [ 80/345]  eta: 0:00:46  loss: 0.8163 (0.8327)  time: 0.1720  data: 0.0001  max mem: 15812
[22:58:12.325980] Test:  [ 90/345]  eta: 0:00:44  loss: 0.8260 (0.8331)  time: 0.1726  data: 0.0001  max mem: 15812
[22:58:14.061525] Test:  [100/345]  eta: 0:00:43  loss: 0.8313 (0.8330)  time: 0.1732  data: 0.0001  max mem: 15812
[22:58:15.801330] Test:  [110/345]  eta: 0:00:41  loss: 0.8208 (0.8326)  time: 0.1737  data: 0.0001  max mem: 15812
[22:58:17.547741] Test:  [120/345]  eta: 0:00:39  loss: 0.8250 (0.8331)  time: 0.1743  data: 0.0001  max mem: 15812
[22:58:19.298710] Test:  [130/345]  eta: 0:00:37  loss: 0.8271 (0.8330)  time: 0.1748  data: 0.0001  max mem: 15812
[22:58:21.054471] Test:  [140/345]  eta: 0:00:35  loss: 0.8222 (0.8319)  time: 0.1753  data: 0.0001  max mem: 15812
[22:58:22.814506] Test:  [150/345]  eta: 0:00:34  loss: 0.8197 (0.8319)  time: 0.1757  data: 0.0001  max mem: 15812
[22:58:24.580516] Test:  [160/345]  eta: 0:00:32  loss: 0.8236 (0.8316)  time: 0.1762  data: 0.0001  max mem: 15812
[22:58:26.352287] Test:  [170/345]  eta: 0:00:30  loss: 0.8354 (0.8323)  time: 0.1768  data: 0.0001  max mem: 15812
[22:58:28.127462] Test:  [180/345]  eta: 0:00:28  loss: 0.8418 (0.8331)  time: 0.1773  data: 0.0001  max mem: 15812
[22:58:29.909347] Test:  [190/345]  eta: 0:00:27  loss: 0.8337 (0.8329)  time: 0.1778  data: 0.0001  max mem: 15812
[22:58:31.696829] Test:  [200/345]  eta: 0:00:25  loss: 0.8291 (0.8327)  time: 0.1784  data: 0.0001  max mem: 15812
[22:58:33.488430] Test:  [210/345]  eta: 0:00:23  loss: 0.8360 (0.8327)  time: 0.1789  data: 0.0001  max mem: 15812
[22:58:35.285816] Test:  [220/345]  eta: 0:00:22  loss: 0.8359 (0.8330)  time: 0.1794  data: 0.0001  max mem: 15812
[22:58:37.088337] Test:  [230/345]  eta: 0:00:20  loss: 0.8452 (0.8337)  time: 0.1799  data: 0.0001  max mem: 15812
[22:58:38.892791] Test:  [240/345]  eta: 0:00:18  loss: 0.8452 (0.8340)  time: 0.1803  data: 0.0001  max mem: 15812
[22:58:40.706136] Test:  [250/345]  eta: 0:00:16  loss: 0.8316 (0.8341)  time: 0.1808  data: 0.0001  max mem: 15812
[22:58:42.522355] Test:  [260/345]  eta: 0:00:15  loss: 0.8294 (0.8339)  time: 0.1814  data: 0.0001  max mem: 15812
[22:58:44.345108] Test:  [270/345]  eta: 0:00:13  loss: 0.8243 (0.8336)  time: 0.1819  data: 0.0001  max mem: 15812
[22:58:46.173123] Test:  [280/345]  eta: 0:00:11  loss: 0.8246 (0.8337)  time: 0.1825  data: 0.0001  max mem: 15812
[22:58:48.007502] Test:  [290/345]  eta: 0:00:09  loss: 0.8270 (0.8333)  time: 0.1831  data: 0.0001  max mem: 15812
[22:58:49.844681] Test:  [300/345]  eta: 0:00:07  loss: 0.8223 (0.8332)  time: 0.1835  data: 0.0001  max mem: 15812
[22:58:51.688151] Test:  [310/345]  eta: 0:00:06  loss: 0.8241 (0.8334)  time: 0.1840  data: 0.0001  max mem: 15812
[22:58:53.535285] Test:  [320/345]  eta: 0:00:04  loss: 0.8392 (0.8336)  time: 0.1845  data: 0.0001  max mem: 15812
[22:58:55.390675] Test:  [330/345]  eta: 0:00:02  loss: 0.8260 (0.8332)  time: 0.1851  data: 0.0001  max mem: 15812
[22:58:57.250239] Test:  [340/345]  eta: 0:00:00  loss: 0.8266 (0.8334)  time: 0.1857  data: 0.0001  max mem: 15812
[22:58:57.996534] Test:  [344/345]  eta: 0:00:00  loss: 0.8266 (0.8334)  time: 0.1859  data: 0.0001  max mem: 15812
[22:58:58.073131] Test: Total time: 0:01:01 (0.1790 s / it)
[22:59:14.863807] Test:  [ 0/57]  eta: 0:00:33  loss: 0.8843 (0.8843)  time: 0.5840  data: 0.4200  max mem: 15812
[22:59:16.523412] Test:  [10/57]  eta: 0:00:09  loss: 0.8807 (0.9002)  time: 0.2039  data: 0.0383  max mem: 15812
[22:59:18.190460] Test:  [20/57]  eta: 0:00:06  loss: 0.8938 (0.8962)  time: 0.1663  data: 0.0001  max mem: 15812
[22:59:19.865617] Test:  [30/57]  eta: 0:00:04  loss: 0.8119 (0.8592)  time: 0.1671  data: 0.0001  max mem: 15812
[22:59:21.546645] Test:  [40/57]  eta: 0:00:03  loss: 0.7716 (0.8372)  time: 0.1677  data: 0.0001  max mem: 15812
[22:59:23.233980] Test:  [50/57]  eta: 0:00:01  loss: 0.7716 (0.8306)  time: 0.1684  data: 0.0001  max mem: 15812
[22:59:24.146730] Test:  [56/57]  eta: 0:00:00  loss: 0.8114 (0.8366)  time: 0.1636  data: 0.0001  max mem: 15812
[22:59:24.203036] Test: Total time: 0:00:09 (0.1741 s / it)
[22:59:27.016882] Dice score of the network on the train images: 0.676496, val images: 0.749081
[22:59:27.017095] saving best_prec_model_0 @ epoch 9
[22:59:28.075896] saving best_dice_model_0 @ epoch 9
[22:59:29.172481] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:59:30.159728] Epoch: [10]  [  0/345]  eta: 0:05:40  lr: 0.000063  loss: 0.8890 (0.8890)  time: 0.9860  data: 0.3885  max mem: 15812
[22:59:42.093051] Epoch: [10]  [ 20/345]  eta: 0:03:19  lr: 0.000063  loss: 0.8726 (0.8718)  time: 0.5965  data: 0.0001  max mem: 15812
[22:59:54.072285] Epoch: [10]  [ 40/345]  eta: 0:03:05  lr: 0.000063  loss: 0.8612 (0.8663)  time: 0.5989  data: 0.0001  max mem: 15812
[23:00:06.073056] Epoch: [10]  [ 60/345]  eta: 0:02:52  lr: 0.000064  loss: 0.8656 (0.8662)  time: 0.6000  data: 0.0001  max mem: 15812
[23:00:18.112327] Epoch: [10]  [ 80/345]  eta: 0:02:40  lr: 0.000064  loss: 0.8675 (0.8671)  time: 0.6019  data: 0.0001  max mem: 15812
[23:00:30.170485] Epoch: [10]  [100/345]  eta: 0:02:27  lr: 0.000064  loss: 0.8669 (0.8670)  time: 0.6029  data: 0.0001  max mem: 15812
[23:00:42.237443] Epoch: [10]  [120/345]  eta: 0:02:15  lr: 0.000065  loss: 0.8611 (0.8656)  time: 0.6033  data: 0.0001  max mem: 15812
[23:00:54.302065] Epoch: [10]  [140/345]  eta: 0:02:03  lr: 0.000065  loss: 0.8389 (0.8630)  time: 0.6032  data: 0.0001  max mem: 15812
[23:01:06.369556] Epoch: [10]  [160/345]  eta: 0:01:51  lr: 0.000065  loss: 0.8572 (0.8628)  time: 0.6033  data: 0.0001  max mem: 15812
[23:01:18.436659] Epoch: [10]  [180/345]  eta: 0:01:39  lr: 0.000066  loss: 0.8465 (0.8625)  time: 0.6033  data: 0.0001  max mem: 15812
[23:01:30.487068] Epoch: [10]  [200/345]  eta: 0:01:27  lr: 0.000066  loss: 0.8480 (0.8615)  time: 0.6025  data: 0.0001  max mem: 15812
[23:01:42.556517] Epoch: [10]  [220/345]  eta: 0:01:15  lr: 0.000066  loss: 0.8403 (0.8601)  time: 0.6034  data: 0.0001  max mem: 15812
[23:01:54.614104] Epoch: [10]  [240/345]  eta: 0:01:03  lr: 0.000067  loss: 0.8511 (0.8598)  time: 0.6028  data: 0.0001  max mem: 15812
[23:02:06.675535] Epoch: [10]  [260/345]  eta: 0:00:51  lr: 0.000067  loss: 0.8567 (0.8599)  time: 0.6030  data: 0.0001  max mem: 15812
[23:02:18.727549] Epoch: [10]  [280/345]  eta: 0:00:39  lr: 0.000068  loss: 0.8416 (0.8589)  time: 0.6026  data: 0.0001  max mem: 15812
[23:02:30.780204] Epoch: [10]  [300/345]  eta: 0:00:27  lr: 0.000068  loss: 0.8328 (0.8579)  time: 0.6026  data: 0.0001  max mem: 15812
[23:02:42.833116] Epoch: [10]  [320/345]  eta: 0:00:15  lr: 0.000068  loss: 0.8396 (0.8574)  time: 0.6026  data: 0.0001  max mem: 15812
[23:02:54.879316] Epoch: [10]  [340/345]  eta: 0:00:03  lr: 0.000069  loss: 0.8353 (0.8561)  time: 0.6023  data: 0.0001  max mem: 15812
[23:02:57.289318] Epoch: [10]  [344/345]  eta: 0:00:00  lr: 0.000069  loss: 0.8353 (0.8558)  time: 0.6022  data: 0.0001  max mem: 15812
[23:02:57.361767] Epoch: [10] Total time: 0:03:28 (0.6034 s / it)
[23:02:57.362099] Averaged stats: lr: 0.000069  loss: 0.8353 (0.8558)
[23:02:57.957140] Test:  [  0/345]  eta: 0:03:23  loss: 0.7711 (0.7711)  time: 0.5895  data: 0.4227  max mem: 15812
[23:02:59.645945] Test:  [ 10/345]  eta: 0:01:09  loss: 0.8159 (0.8129)  time: 0.2070  data: 0.0385  max mem: 15812
[23:03:01.339812] Test:  [ 20/345]  eta: 0:01:01  loss: 0.8159 (0.8142)  time: 0.1691  data: 0.0001  max mem: 15812
[23:03:03.037672] Test:  [ 30/345]  eta: 0:00:57  loss: 0.8139 (0.8166)  time: 0.1695  data: 0.0001  max mem: 15812
[23:03:04.742032] Test:  [ 40/345]  eta: 0:00:54  loss: 0.8293 (0.8196)  time: 0.1700  data: 0.0001  max mem: 15812
[23:03:06.450655] Test:  [ 50/345]  eta: 0:00:52  loss: 0.8170 (0.8178)  time: 0.1706  data: 0.0001  max mem: 15812
[23:03:08.166285] Test:  [ 60/345]  eta: 0:00:50  loss: 0.8064 (0.8192)  time: 0.1712  data: 0.0001  max mem: 15812
[23:03:09.886186] Test:  [ 70/345]  eta: 0:00:48  loss: 0.8184 (0.8194)  time: 0.1717  data: 0.0001  max mem: 15812
[23:03:11.611171] Test:  [ 80/345]  eta: 0:00:46  loss: 0.8063 (0.8179)  time: 0.1722  data: 0.0001  max mem: 15812
[23:03:13.341723] Test:  [ 90/345]  eta: 0:00:44  loss: 0.8063 (0.8181)  time: 0.1727  data: 0.0001  max mem: 15812
[23:03:15.076746] Test:  [100/345]  eta: 0:00:42  loss: 0.8240 (0.8190)  time: 0.1732  data: 0.0001  max mem: 15812
[23:03:16.817596] Test:  [110/345]  eta: 0:00:41  loss: 0.8280 (0.8194)  time: 0.1737  data: 0.0001  max mem: 15812
[23:03:18.563311] Test:  [120/345]  eta: 0:00:39  loss: 0.8180 (0.8188)  time: 0.1743  data: 0.0001  max mem: 15812
[23:03:20.315137] Test:  [130/345]  eta: 0:00:37  loss: 0.8144 (0.8184)  time: 0.1748  data: 0.0001  max mem: 15812
[23:03:22.070090] Test:  [140/345]  eta: 0:00:35  loss: 0.8077 (0.8178)  time: 0.1753  data: 0.0001  max mem: 15812
[23:03:23.831568] Test:  [150/345]  eta: 0:00:34  loss: 0.8077 (0.8174)  time: 0.1758  data: 0.0001  max mem: 15812
[23:03:25.598662] Test:  [160/345]  eta: 0:00:32  loss: 0.8219 (0.8180)  time: 0.1764  data: 0.0001  max mem: 15812
[23:03:27.370054] Test:  [170/345]  eta: 0:00:30  loss: 0.8206 (0.8178)  time: 0.1769  data: 0.0001  max mem: 15812
[23:03:29.147730] Test:  [180/345]  eta: 0:00:28  loss: 0.8188 (0.8178)  time: 0.1774  data: 0.0001  max mem: 15812
[23:03:30.929513] Test:  [190/345]  eta: 0:00:27  loss: 0.8279 (0.8188)  time: 0.1779  data: 0.0001  max mem: 15812
[23:03:32.717153] Test:  [200/345]  eta: 0:00:25  loss: 0.8332 (0.8195)  time: 0.1784  data: 0.0001  max mem: 15812
[23:03:34.508150] Test:  [210/345]  eta: 0:00:23  loss: 0.8218 (0.8196)  time: 0.1789  data: 0.0001  max mem: 15812
[23:03:36.308922] Test:  [220/345]  eta: 0:00:22  loss: 0.8198 (0.8200)  time: 0.1795  data: 0.0001  max mem: 15812
[23:03:38.112016] Test:  [230/345]  eta: 0:00:20  loss: 0.8175 (0.8208)  time: 0.1801  data: 0.0001  max mem: 15812
[23:03:39.917091] Test:  [240/345]  eta: 0:00:18  loss: 0.8140 (0.8202)  time: 0.1804  data: 0.0001  max mem: 15812
[23:03:41.732344] Test:  [250/345]  eta: 0:00:16  loss: 0.8169 (0.8203)  time: 0.1810  data: 0.0001  max mem: 15812
[23:03:43.549857] Test:  [260/345]  eta: 0:00:15  loss: 0.8170 (0.8203)  time: 0.1816  data: 0.0001  max mem: 15812
[23:03:45.373184] Test:  [270/345]  eta: 0:00:13  loss: 0.8161 (0.8201)  time: 0.1820  data: 0.0001  max mem: 15812
[23:03:47.200962] Test:  [280/345]  eta: 0:00:11  loss: 0.8137 (0.8201)  time: 0.1825  data: 0.0001  max mem: 15812
[23:03:49.034875] Test:  [290/345]  eta: 0:00:09  loss: 0.8137 (0.8202)  time: 0.1830  data: 0.0001  max mem: 15812
[23:03:50.872794] Test:  [300/345]  eta: 0:00:07  loss: 0.7992 (0.8201)  time: 0.1835  data: 0.0001  max mem: 15812
[23:03:52.716245] Test:  [310/345]  eta: 0:00:06  loss: 0.7989 (0.8194)  time: 0.1840  data: 0.0001  max mem: 15812
[23:03:54.564824] Test:  [320/345]  eta: 0:00:04  loss: 0.8116 (0.8196)  time: 0.1845  data: 0.0001  max mem: 15812
[23:03:56.421031] Test:  [330/345]  eta: 0:00:02  loss: 0.8188 (0.8195)  time: 0.1852  data: 0.0001  max mem: 15812
[23:03:58.280851] Test:  [340/345]  eta: 0:00:00  loss: 0.8161 (0.8197)  time: 0.1857  data: 0.0001  max mem: 15812
[23:03:59.026800] Test:  [344/345]  eta: 0:00:00  loss: 0.8255 (0.8197)  time: 0.1859  data: 0.0001  max mem: 15812
[23:03:59.104829] Test: Total time: 0:01:01 (0.1790 s / it)
[23:04:15.919853] Test:  [ 0/57]  eta: 0:00:30  loss: 0.9118 (0.9118)  time: 0.5320  data: 0.3681  max mem: 15812
[23:04:17.580799] Test:  [10/57]  eta: 0:00:09  loss: 0.9118 (0.9119)  time: 0.1993  data: 0.0335  max mem: 15812
[23:04:19.249011] Test:  [20/57]  eta: 0:00:06  loss: 0.9138 (0.9070)  time: 0.1664  data: 0.0001  max mem: 15812
[23:04:20.922840] Test:  [30/57]  eta: 0:00:04  loss: 0.8115 (0.8662)  time: 0.1670  data: 0.0001  max mem: 15812
[23:04:22.602964] Test:  [40/57]  eta: 0:00:02  loss: 0.7701 (0.8417)  time: 0.1676  data: 0.0001  max mem: 15812
[23:04:24.290721] Test:  [50/57]  eta: 0:00:01  loss: 0.7701 (0.8338)  time: 0.1683  data: 0.0001  max mem: 15812
[23:04:25.202854] Test:  [56/57]  eta: 0:00:00  loss: 0.8049 (0.8386)  time: 0.1636  data: 0.0000  max mem: 15812
[23:04:25.277388] Test: Total time: 0:00:09 (0.1735 s / it)
[23:04:28.049405] Dice score of the network on the train images: 0.703243, val images: 0.754346
[23:04:28.049625] saving best_prec_model_0 @ epoch 10
[23:04:29.477954] saving best_dice_model_0 @ epoch 10
[23:04:30.485790] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:04:31.470866] Epoch: [11]  [  0/345]  eta: 0:05:39  lr: 0.000069  loss: 0.8833 (0.8833)  time: 0.9838  data: 0.3859  max mem: 15812
[23:04:43.517837] Epoch: [11]  [ 20/345]  eta: 0:03:21  lr: 0.000069  loss: 0.8515 (0.8505)  time: 0.6023  data: 0.0001  max mem: 15812
[23:04:55.485839] Epoch: [11]  [ 40/345]  eta: 0:03:05  lr: 0.000069  loss: 0.8304 (0.8436)  time: 0.5984  data: 0.0001  max mem: 15812
[23:05:07.482872] Epoch: [11]  [ 60/345]  eta: 0:02:52  lr: 0.000070  loss: 0.8306 (0.8409)  time: 0.5998  data: 0.0001  max mem: 15812
[23:05:19.524095] Epoch: [11]  [ 80/345]  eta: 0:02:40  lr: 0.000070  loss: 0.8418 (0.8411)  time: 0.6020  data: 0.0001  max mem: 15812
[23:05:31.572654] Epoch: [11]  [100/345]  eta: 0:02:28  lr: 0.000071  loss: 0.8355 (0.8414)  time: 0.6024  data: 0.0001  max mem: 15812
[23:05:43.636158] Epoch: [11]  [120/345]  eta: 0:02:16  lr: 0.000071  loss: 0.8374 (0.8422)  time: 0.6031  data: 0.0001  max mem: 15812

[23:05:55.703304] Epoch: [11]  [140/345]  eta: 0:02:03  lr: 0.000071  loss: 0.8408 (0.8419)  time: 0.6033  data: 0.0001  max mem: 15812
[23:06:07.755899] Epoch: [11]  [160/345]  eta: 0:01:51  lr: 0.000072  loss: 0.8386 (0.8408)  time: 0.6026  data: 0.0001  max mem: 15812
[23:06:19.812494] Epoch: [11]  [180/345]  eta: 0:01:39  lr: 0.000072  loss: 0.8205 (0.8398)  time: 0.6028  data: 0.0001  max mem: 15812
[23:06:31.865402] Epoch: [11]  [200/345]  eta: 0:01:27  lr: 0.000072  loss: 0.8242 (0.8390)  time: 0.6026  data: 0.0001  max mem: 15812
[23:06:43.923248] Epoch: [11]  [220/345]  eta: 0:01:15  lr: 0.000073  loss: 0.8381 (0.8390)  time: 0.6028  data: 0.0001  max mem: 15812
[23:06:55.940267] Epoch: [11]  [240/345]  eta: 0:01:03  lr: 0.000073  loss: 0.8185 (0.8378)  time: 0.6008  data: 0.0001  max mem: 15812
[23:07:07.950939] Epoch: [11]  [260/345]  eta: 0:00:51  lr: 0.000073  loss: 0.8309 (0.8375)  time: 0.6005  data: 0.0001  max mem: 15812
[23:07:19.995519] Epoch: [11]  [280/345]  eta: 0:00:39  lr: 0.000074  loss: 0.8430 (0.8380)  time: 0.6022  data: 0.0001  max mem: 15812
[23:07:32.031244] Epoch: [11]  [300/345]  eta: 0:00:27  lr: 0.000074  loss: 0.8220 (0.8371)  time: 0.6017  data: 0.0001  max mem: 15812
[23:07:44.054675] Epoch: [11]  [320/345]  eta: 0:00:15  lr: 0.000075  loss: 0.8196 (0.8367)  time: 0.6011  data: 0.0001  max mem: 15812
[23:07:56.076684] Epoch: [11]  [340/345]  eta: 0:00:03  lr: 0.000075  loss: 0.8272 (0.8362)  time: 0.6011  data: 0.0001  max mem: 15812
[23:07:58.479952] Epoch: [11]  [344/345]  eta: 0:00:00  lr: 0.000075  loss: 0.8272 (0.8361)  time: 0.6010  data: 0.0001  max mem: 15812
[23:07:58.556886] Epoch: [11] Total time: 0:03:28 (0.6031 s / it)
[23:07:58.557085] Averaged stats: lr: 0.000075  loss: 0.8272 (0.8361)
[23:07:59.181829] Test:  [  0/345]  eta: 0:03:33  loss: 0.8096 (0.8096)  time: 0.6197  data: 0.4534  max mem: 15812
[23:08:00.869821] Test:  [ 10/345]  eta: 0:01:10  loss: 0.8070 (0.8057)  time: 0.2097  data: 0.0413  max mem: 15812
[23:08:02.563169] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7984 (0.7984)  time: 0.1690  data: 0.0001  max mem: 15812
[23:08:04.262379] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7791 (0.7956)  time: 0.1696  data: 0.0001  max mem: 15812
[23:08:05.966615] Test:  [ 40/345]  eta: 0:00:55  loss: 0.8043 (0.8014)  time: 0.1701  data: 0.0001  max mem: 15812
[23:08:07.674952] Test:  [ 50/345]  eta: 0:00:52  loss: 0.8121 (0.8031)  time: 0.1706  data: 0.0001  max mem: 15812
[23:08:09.388687] Test:  [ 60/345]  eta: 0:00:50  loss: 0.8030 (0.8018)  time: 0.1710  data: 0.0001  max mem: 15812
[23:08:11.107331] Test:  [ 70/345]  eta: 0:00:48  loss: 0.8049 (0.8024)  time: 0.1716  data: 0.0001  max mem: 15812
[23:08:12.831770] Test:  [ 80/345]  eta: 0:00:46  loss: 0.8054 (0.8026)  time: 0.1721  data: 0.0001  max mem: 15812
[23:08:14.561998] Test:  [ 90/345]  eta: 0:00:44  loss: 0.8035 (0.8035)  time: 0.1727  data: 0.0001  max mem: 15812
[23:08:16.295459] Test:  [100/345]  eta: 0:00:43  loss: 0.8035 (0.8034)  time: 0.1731  data: 0.0001  max mem: 15812
[23:08:18.035624] Test:  [110/345]  eta: 0:00:41  loss: 0.8083 (0.8039)  time: 0.1736  data: 0.0001  max mem: 15812
[23:08:19.781993] Test:  [120/345]  eta: 0:00:39  loss: 0.7974 (0.8033)  time: 0.1743  data: 0.0001  max mem: 15812
[23:08:21.532670] Test:  [130/345]  eta: 0:00:37  loss: 0.7948 (0.8033)  time: 0.1748  data: 0.0001  max mem: 15812
[23:08:23.287462] Test:  [140/345]  eta: 0:00:35  loss: 0.7966 (0.8037)  time: 0.1752  data: 0.0001  max mem: 15812
[23:08:25.048139] Test:  [150/345]  eta: 0:00:34  loss: 0.8038 (0.8040)  time: 0.1757  data: 0.0001  max mem: 15812
[23:08:26.814597] Test:  [160/345]  eta: 0:00:32  loss: 0.8015 (0.8036)  time: 0.1763  data: 0.0001  max mem: 15812
[23:08:28.586007] Test:  [170/345]  eta: 0:00:30  loss: 0.7931 (0.8032)  time: 0.1768  data: 0.0001  max mem: 15812
[23:08:30.363031] Test:  [180/345]  eta: 0:00:28  loss: 0.8019 (0.8039)  time: 0.1774  data: 0.0001  max mem: 15812
[23:08:32.144703] Test:  [190/345]  eta: 0:00:27  loss: 0.8138 (0.8043)  time: 0.1779  data: 0.0001  max mem: 15812
[23:08:33.931156] Test:  [200/345]  eta: 0:00:25  loss: 0.8048 (0.8039)  time: 0.1783  data: 0.0001  max mem: 15812
[23:08:35.725096] Test:  [210/345]  eta: 0:00:23  loss: 0.7973 (0.8039)  time: 0.1790  data: 0.0001  max mem: 15812
[23:08:37.523207] Test:  [220/345]  eta: 0:00:22  loss: 0.8063 (0.8036)  time: 0.1795  data: 0.0001  max mem: 15812
[23:08:39.326737] Test:  [230/345]  eta: 0:00:20  loss: 0.8020 (0.8033)  time: 0.1800  data: 0.0001  max mem: 15812
[23:08:41.135370] Test:  [240/345]  eta: 0:00:18  loss: 0.7930 (0.8032)  time: 0.1805  data: 0.0001  max mem: 15812
[23:08:42.947652] Test:  [250/345]  eta: 0:00:16  loss: 0.7882 (0.8024)  time: 0.1810  data: 0.0001  max mem: 15812
[23:08:44.764349] Test:  [260/345]  eta: 0:00:15  loss: 0.7882 (0.8024)  time: 0.1814  data: 0.0001  max mem: 15812
[23:08:46.586356] Test:  [270/345]  eta: 0:00:13  loss: 0.7977 (0.8023)  time: 0.1819  data: 0.0001  max mem: 15812
[23:08:48.414297] Test:  [280/345]  eta: 0:00:11  loss: 0.8092 (0.8026)  time: 0.1824  data: 0.0001  max mem: 15812
[23:08:50.247144] Test:  [290/345]  eta: 0:00:09  loss: 0.8101 (0.8029)  time: 0.1830  data: 0.0001  max mem: 15812
[23:08:52.083977] Test:  [300/345]  eta: 0:00:07  loss: 0.8021 (0.8028)  time: 0.1834  data: 0.0001  max mem: 15812
[23:08:53.927311] Test:  [310/345]  eta: 0:00:06  loss: 0.7990 (0.8027)  time: 0.1839  data: 0.0001  max mem: 15812
[23:08:55.774827] Test:  [320/345]  eta: 0:00:04  loss: 0.8017 (0.8030)  time: 0.1845  data: 0.0001  max mem: 15812
[23:08:57.629576] Test:  [330/345]  eta: 0:00:02  loss: 0.7967 (0.8028)  time: 0.1851  data: 0.0001  max mem: 15812
[23:08:59.488339] Test:  [340/345]  eta: 0:00:00  loss: 0.7970 (0.8029)  time: 0.1856  data: 0.0001  max mem: 15812
[23:09:00.232668] Test:  [344/345]  eta: 0:00:00  loss: 0.7949 (0.8028)  time: 0.1858  data: 0.0001  max mem: 15812
[23:09:00.297508] Test: Total time: 0:01:01 (0.1789 s / it)
[23:09:17.227460] Test:  [ 0/57]  eta: 0:00:31  loss: 0.8476 (0.8476)  time: 0.5584  data: 0.3935  max mem: 15812
[23:09:18.888879] Test:  [10/57]  eta: 0:00:09  loss: 0.8560 (0.8797)  time: 0.2017  data: 0.0359  max mem: 15812
[23:09:20.557489] Test:  [20/57]  eta: 0:00:06  loss: 0.8665 (0.8754)  time: 0.1664  data: 0.0001  max mem: 15812
[23:09:22.232929] Test:  [30/57]  eta: 0:00:04  loss: 0.7931 (0.8429)  time: 0.1671  data: 0.0001  max mem: 15812
[23:09:23.913437] Test:  [40/57]  eta: 0:00:03  loss: 0.7656 (0.8235)  time: 0.1677  data: 0.0001  max mem: 15812
[23:09:25.600473] Test:  [50/57]  eta: 0:00:01  loss: 0.7571 (0.8170)  time: 0.1683  data: 0.0001  max mem: 15812
[23:09:26.512498] Test:  [56/57]  eta: 0:00:00  loss: 0.7972 (0.8223)  time: 0.1635  data: 0.0000  max mem: 15812
[23:09:26.573153] Test: Total time: 0:00:09 (0.1738 s / it)
[23:09:29.381363] Dice score of the network on the train images: 0.685894, val images: 0.742001
[23:09:29.385837] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:09:30.401915] Epoch: [12]  [  0/345]  eta: 0:05:50  lr: 0.000075  loss: 0.8844 (0.8844)  time: 1.0150  data: 0.4166  max mem: 15812
[23:09:42.344391] Epoch: [12]  [ 20/345]  eta: 0:03:20  lr: 0.000075  loss: 0.8279 (0.8373)  time: 0.5971  data: 0.0001  max mem: 15812
[23:09:54.331030] Epoch: [12]  [ 40/345]  eta: 0:03:05  lr: 0.000076  loss: 0.8290 (0.8331)  time: 0.5993  data: 0.0001  max mem: 15812
[23:10:06.339472] Epoch: [12]  [ 60/345]  eta: 0:02:52  lr: 0.000076  loss: 0.8244 (0.8315)  time: 0.6004  data: 0.0001  max mem: 15812
[23:10:18.368355] Epoch: [12]  [ 80/345]  eta: 0:02:40  lr: 0.000076  loss: 0.8209 (0.8305)  time: 0.6014  data: 0.0001  max mem: 15812
[23:10:30.428939] Epoch: [12]  [100/345]  eta: 0:02:28  lr: 0.000077  loss: 0.7996 (0.8258)  time: 0.6030  data: 0.0001  max mem: 15812
[23:10:42.499827] Epoch: [12]  [120/345]  eta: 0:02:15  lr: 0.000077  loss: 0.8196 (0.8250)  time: 0.6035  data: 0.0001  max mem: 15812
[23:10:54.566867] Epoch: [12]  [140/345]  eta: 0:02:03  lr: 0.000078  loss: 0.8264 (0.8252)  time: 0.6033  data: 0.0001  max mem: 15812
[23:11:06.630324] Epoch: [12]  [160/345]  eta: 0:01:51  lr: 0.000078  loss: 0.8245 (0.8251)  time: 0.6031  data: 0.0001  max mem: 15812
[23:11:18.697476] Epoch: [12]  [180/345]  eta: 0:01:39  lr: 0.000078  loss: 0.8275 (0.8257)  time: 0.6033  data: 0.0001  max mem: 15812
[23:11:30.758744] Epoch: [12]  [200/345]  eta: 0:01:27  lr: 0.000079  loss: 0.8167 (0.8251)  time: 0.6030  data: 0.0001  max mem: 15812
[23:11:42.786795] Epoch: [12]  [220/345]  eta: 0:01:15  lr: 0.000079  loss: 0.8212 (0.8246)  time: 0.6014  data: 0.0001  max mem: 15812
[23:11:54.816216] Epoch: [12]  [240/345]  eta: 0:01:03  lr: 0.000079  loss: 0.8054 (0.8234)  time: 0.6014  data: 0.0001  max mem: 15812
[23:12:06.851938] Epoch: [12]  [260/345]  eta: 0:00:51  lr: 0.000080  loss: 0.8194 (0.8230)  time: 0.6018  data: 0.0001  max mem: 15812
[23:12:18.886755] Epoch: [12]  [280/345]  eta: 0:00:39  lr: 0.000080  loss: 0.8190 (0.8229)  time: 0.6017  data: 0.0001  max mem: 15812
[23:12:30.931872] Epoch: [12]  [300/345]  eta: 0:00:27  lr: 0.000080  loss: 0.8269 (0.8228)  time: 0.6022  data: 0.0001  max mem: 15812
[23:12:43.079068] Epoch: [12]  [320/345]  eta: 0:00:15  lr: 0.000081  loss: 0.8330 (0.8235)  time: 0.6073  data: 0.0001  max mem: 15812
[23:12:55.093412] Epoch: [12]  [340/345]  eta: 0:00:03  lr: 0.000081  loss: 0.8420 (0.8243)  time: 0.6007  data: 0.0001  max mem: 15812
[23:12:57.496745] Epoch: [12]  [344/345]  eta: 0:00:00  lr: 0.000081  loss: 0.8371 (0.8244)  time: 0.6005  data: 0.0001  max mem: 15812
[23:12:57.578996] Epoch: [12] Total time: 0:03:28 (0.6035 s / it)
[23:12:57.579331] Averaged stats: lr: 0.000081  loss: 0.8371 (0.8244)
[23:12:58.175818] Test:  [  0/345]  eta: 0:03:24  loss: 0.8286 (0.8286)  time: 0.5929  data: 0.4259  max mem: 15812
[23:12:59.864866] Test:  [ 10/345]  eta: 0:01:09  loss: 0.7927 (0.7986)  time: 0.2074  data: 0.0388  max mem: 15812
[23:13:01.557748] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7854 (0.7890)  time: 0.1690  data: 0.0001  max mem: 15812
[23:13:03.255956] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7752 (0.7882)  time: 0.1695  data: 0.0001  max mem: 15812
[23:13:04.960635] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7823 (0.7879)  time: 0.1701  data: 0.0001  max mem: 15812
[23:13:06.669292] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7792 (0.7865)  time: 0.1706  data: 0.0001  max mem: 15812
[23:13:08.383861] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7858 (0.7882)  time: 0.1711  data: 0.0001  max mem: 15812
[23:13:10.101352] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7926 (0.7880)  time: 0.1715  data: 0.0001  max mem: 15812
[23:13:11.826310] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7903 (0.7885)  time: 0.1721  data: 0.0001  max mem: 15812
[23:13:13.556549] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7891 (0.7886)  time: 0.1727  data: 0.0001  max mem: 15812
[23:13:15.290846] Test:  [100/345]  eta: 0:00:42  loss: 0.7787 (0.7874)  time: 0.1732  data: 0.0001  max mem: 15812
[23:13:17.031408] Test:  [110/345]  eta: 0:00:41  loss: 0.7823 (0.7874)  time: 0.1737  data: 0.0001  max mem: 15812
[23:13:18.776005] Test:  [120/345]  eta: 0:00:39  loss: 0.7901 (0.7874)  time: 0.1742  data: 0.0001  max mem: 15812
[23:13:20.527488] Test:  [130/345]  eta: 0:00:37  loss: 0.7857 (0.7869)  time: 0.1747  data: 0.0001  max mem: 15812
[23:13:22.284740] Test:  [140/345]  eta: 0:00:35  loss: 0.7827 (0.7871)  time: 0.1754  data: 0.0001  max mem: 15812
[23:13:24.046123] Test:  [150/345]  eta: 0:00:34  loss: 0.7871 (0.7872)  time: 0.1759  data: 0.0001  max mem: 15812
[23:13:25.812284] Test:  [160/345]  eta: 0:00:32  loss: 0.7845 (0.7870)  time: 0.1763  data: 0.0001  max mem: 15812
[23:13:27.583713] Test:  [170/345]  eta: 0:00:30  loss: 0.7807 (0.7866)  time: 0.1768  data: 0.0001  max mem: 15812
[23:13:29.361540] Test:  [180/345]  eta: 0:00:28  loss: 0.7758 (0.7860)  time: 0.1774  data: 0.0001  max mem: 15812
[23:13:31.143616] Test:  [190/345]  eta: 0:00:27  loss: 0.7732 (0.7854)  time: 0.1779  data: 0.0001  max mem: 15812
[23:13:32.930944] Test:  [200/345]  eta: 0:00:25  loss: 0.7797 (0.7854)  time: 0.1784  data: 0.0001  max mem: 15812
[23:13:34.725508] Test:  [210/345]  eta: 0:00:23  loss: 0.7790 (0.7851)  time: 0.1790  data: 0.0001  max mem: 15812
[23:13:36.520971] Test:  [220/345]  eta: 0:00:22  loss: 0.7785 (0.7853)  time: 0.1794  data: 0.0001  max mem: 15812
[23:13:38.321126] Test:  [230/345]  eta: 0:00:20  loss: 0.7830 (0.7853)  time: 0.1797  data: 0.0001  max mem: 15812
[23:13:40.126029] Test:  [240/345]  eta: 0:00:18  loss: 0.7799 (0.7851)  time: 0.1802  data: 0.0001  max mem: 15812
[23:13:41.940584] Test:  [250/345]  eta: 0:00:16  loss: 0.7825 (0.7855)  time: 0.1809  data: 0.0001  max mem: 15812
[23:13:43.758649] Test:  [260/345]  eta: 0:00:15  loss: 0.7825 (0.7852)  time: 0.1816  data: 0.0001  max mem: 15812
[23:13:45.581527] Test:  [270/345]  eta: 0:00:13  loss: 0.7805 (0.7852)  time: 0.1820  data: 0.0001  max mem: 15812
[23:13:47.408850] Test:  [280/345]  eta: 0:00:11  loss: 0.7959 (0.7858)  time: 0.1824  data: 0.0001  max mem: 15812
[23:13:49.243728] Test:  [290/345]  eta: 0:00:09  loss: 0.7979 (0.7859)  time: 0.1830  data: 0.0001  max mem: 15812
[23:13:51.081989] Test:  [300/345]  eta: 0:00:07  loss: 0.7860 (0.7860)  time: 0.1836  data: 0.0001  max mem: 15812
[23:13:52.925467] Test:  [310/345]  eta: 0:00:06  loss: 0.7806 (0.7858)  time: 0.1840  data: 0.0001  max mem: 15812
[23:13:54.773051] Test:  [320/345]  eta: 0:00:04  loss: 0.7797 (0.7856)  time: 0.1845  data: 0.0001  max mem: 15812
[23:13:56.628272] Test:  [330/345]  eta: 0:00:02  loss: 0.7764 (0.7855)  time: 0.1851  data: 0.0001  max mem: 15812
[23:13:58.486746] Test:  [340/345]  eta: 0:00:00  loss: 0.7793 (0.7856)  time: 0.1856  data: 0.0001  max mem: 15812
[23:13:59.231923] Test:  [344/345]  eta: 0:00:00  loss: 0.7833 (0.7858)  time: 0.1858  data: 0.0001  max mem: 15812
[23:13:59.305643] Test: Total time: 0:01:01 (0.1789 s / it)
[23:14:16.040813] Test:  [ 0/57]  eta: 0:00:31  loss: 0.8954 (0.8954)  time: 0.5545  data: 0.3901  max mem: 15812
[23:14:17.703055] Test:  [10/57]  eta: 0:00:09  loss: 0.8957 (0.8941)  time: 0.2014  data: 0.0355  max mem: 15812
[23:14:19.373418] Test:  [20/57]  eta: 0:00:06  loss: 0.8957 (0.8851)  time: 0.1666  data: 0.0001  max mem: 15812
[23:14:21.047771] Test:  [30/57]  eta: 0:00:04  loss: 0.7945 (0.8444)  time: 0.1672  data: 0.0001  max mem: 15812
[23:14:22.729894] Test:  [40/57]  eta: 0:00:03  loss: 0.7541 (0.8224)  time: 0.1678  data: 0.0001  max mem: 15812
[23:14:24.418829] Test:  [50/57]  eta: 0:00:01  loss: 0.7507 (0.8143)  time: 0.1685  data: 0.0001  max mem: 15812
[23:14:25.330573] Test:  [56/57]  eta: 0:00:00  loss: 0.7841 (0.8200)  time: 0.1636  data: 0.0000  max mem: 15812
[23:14:25.406234] Test: Total time: 0:00:09 (0.1740 s / it)
[23:14:28.233619] Dice score of the network on the train images: 0.691063, val images: 0.767227
[23:14:28.233837] saving best_dice_model_0 @ epoch 12
[23:14:29.625242] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:14:30.617781] Epoch: [13]  [  0/345]  eta: 0:05:42  lr: 0.000081  loss: 0.8328 (0.8328)  time: 0.9913  data: 0.3934  max mem: 15812
[23:14:42.544243] Epoch: [13]  [ 20/345]  eta: 0:03:19  lr: 0.000082  loss: 0.8175 (0.8133)  time: 0.5963  data: 0.0001  max mem: 15812
[23:14:54.517452] Epoch: [13]  [ 40/345]  eta: 0:03:05  lr: 0.000082  loss: 0.8102 (0.8134)  time: 0.5986  data: 0.0001  max mem: 15812
[23:15:06.496376] Epoch: [13]  [ 60/345]  eta: 0:02:52  lr: 0.000082  loss: 0.8073 (0.8135)  time: 0.5989  data: 0.0001  max mem: 15812
[23:15:18.528080] Epoch: [13]  [ 80/345]  eta: 0:02:39  lr: 0.000083  loss: 0.8038 (0.8126)  time: 0.6015  data: 0.0001  max mem: 15812
[23:15:30.570469] Epoch: [13]  [100/345]  eta: 0:02:27  lr: 0.000083  loss: 0.8125 (0.8146)  time: 0.6021  data: 0.0001  max mem: 15812
[23:15:42.632310] Epoch: [13]  [120/345]  eta: 0:02:15  lr: 0.000083  loss: 0.8109 (0.8148)  time: 0.6030  data: 0.0001  max mem: 15812
[23:15:54.697983] Epoch: [13]  [140/345]  eta: 0:02:03  lr: 0.000084  loss: 0.8214 (0.8163)  time: 0.6032  data: 0.0001  max mem: 15812
[23:16:06.757306] Epoch: [13]  [160/345]  eta: 0:01:51  lr: 0.000084  loss: 0.8260 (0.8169)  time: 0.6029  data: 0.0001  max mem: 15812
[23:16:18.809178] Epoch: [13]  [180/345]  eta: 0:01:39  lr: 0.000085  loss: 0.8223 (0.8173)  time: 0.6025  data: 0.0001  max mem: 15812
[23:16:30.864834] Epoch: [13]  [200/345]  eta: 0:01:27  lr: 0.000085  loss: 0.8167 (0.8178)  time: 0.6027  data: 0.0001  max mem: 15812
[23:16:42.912089] Epoch: [13]  [220/345]  eta: 0:01:15  lr: 0.000085  loss: 0.8202 (0.8179)  time: 0.6023  data: 0.0001  max mem: 15812
[23:16:54.961902] Epoch: [13]  [240/345]  eta: 0:01:03  lr: 0.000086  loss: 0.8220 (0.8186)  time: 0.6024  data: 0.0001  max mem: 15812
[23:17:07.002383] Epoch: [13]  [260/345]  eta: 0:00:51  lr: 0.000086  loss: 0.8233 (0.8189)  time: 0.6020  data: 0.0001  max mem: 15812
[23:17:19.044234] Epoch: [13]  [280/345]  eta: 0:00:39  lr: 0.000086  loss: 0.8053 (0.8180)  time: 0.6020  data: 0.0001  max mem: 15812
[23:17:31.080397] Epoch: [13]  [300/345]  eta: 0:00:27  lr: 0.000087  loss: 0.8137 (0.8174)  time: 0.6018  data: 0.0001  max mem: 15812
[23:17:43.120012] Epoch: [13]  [320/345]  eta: 0:00:15  lr: 0.000087  loss: 0.8063 (0.8171)  time: 0.6019  data: 0.0001  max mem: 15812
[23:17:55.165749] Epoch: [13]  [340/345]  eta: 0:00:03  lr: 0.000087  loss: 0.7912 (0.8163)  time: 0.6022  data: 0.0001  max mem: 15812
[23:17:57.571187] Epoch: [13]  [344/345]  eta: 0:00:00  lr: 0.000087  loss: 0.8055 (0.8162)  time: 0.6018  data: 0.0001  max mem: 15812
[23:17:57.645415] Epoch: [13] Total time: 0:03:28 (0.6030 s / it)
[23:17:57.645995] Averaged stats: lr: 0.000087  loss: 0.8055 (0.8162)
[23:17:58.235240] Test:  [  0/345]  eta: 0:03:21  loss: 0.7700 (0.7700)  time: 0.5835  data: 0.4167  max mem: 15812
[23:17:59.924043] Test:  [ 10/345]  eta: 0:01:09  loss: 0.7704 (0.7750)  time: 0.2065  data: 0.0380  max mem: 15812
[23:18:01.619179] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7704 (0.7760)  time: 0.1691  data: 0.0001  max mem: 15812
[23:18:03.318727] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7695 (0.7770)  time: 0.1697  data: 0.0001  max mem: 15812
[23:18:05.023939] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7696 (0.7758)  time: 0.1702  data: 0.0001  max mem: 15812
[23:18:06.732361] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7724 (0.7782)  time: 0.1706  data: 0.0001  max mem: 15812
[23:18:08.448110] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7768 (0.7772)  time: 0.1711  data: 0.0001  max mem: 15812
[23:18:10.167502] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7694 (0.7764)  time: 0.1717  data: 0.0001  max mem: 15812
[23:18:11.893503] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7800 (0.7783)  time: 0.1722  data: 0.0001  max mem: 15812
[23:18:13.623760] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7800 (0.7766)  time: 0.1728  data: 0.0001  max mem: 15812
[23:18:15.359038] Test:  [100/345]  eta: 0:00:42  loss: 0.7673 (0.7762)  time: 0.1732  data: 0.0001  max mem: 15812
[23:18:17.098730] Test:  [110/345]  eta: 0:00:41  loss: 0.7734 (0.7764)  time: 0.1737  data: 0.0001  max mem: 15812
[23:18:18.846156] Test:  [120/345]  eta: 0:00:39  loss: 0.7783 (0.7768)  time: 0.1743  data: 0.0001  max mem: 15812
[23:18:20.596419] Test:  [130/345]  eta: 0:00:37  loss: 0.7829 (0.7772)  time: 0.1748  data: 0.0001  max mem: 15812
[23:18:22.353376] Test:  [140/345]  eta: 0:00:35  loss: 0.7858 (0.7774)  time: 0.1753  data: 0.0001  max mem: 15812
[23:18:24.114811] Test:  [150/345]  eta: 0:00:34  loss: 0.7852 (0.7776)  time: 0.1759  data: 0.0001  max mem: 15812
[23:18:25.880727] Test:  [160/345]  eta: 0:00:32  loss: 0.7790 (0.7774)  time: 0.1763  data: 0.0001  max mem: 15812
[23:18:27.652216] Test:  [170/345]  eta: 0:00:30  loss: 0.7727 (0.7771)  time: 0.1768  data: 0.0001  max mem: 15812
[23:18:29.428605] Test:  [180/345]  eta: 0:00:28  loss: 0.7727 (0.7768)  time: 0.1773  data: 0.0001  max mem: 15812
[23:18:31.209880] Test:  [190/345]  eta: 0:00:27  loss: 0.7770 (0.7771)  time: 0.1778  data: 0.0001  max mem: 15812
[23:18:32.996879] Test:  [200/345]  eta: 0:00:25  loss: 0.7787 (0.7770)  time: 0.1784  data: 0.0001  max mem: 15812
[23:18:34.790174] Test:  [210/345]  eta: 0:00:23  loss: 0.7708 (0.7770)  time: 0.1790  data: 0.0001  max mem: 15812
[23:18:36.589474] Test:  [220/345]  eta: 0:00:22  loss: 0.7721 (0.7768)  time: 0.1796  data: 0.0001  max mem: 15812
[23:18:38.392940] Test:  [230/345]  eta: 0:00:20  loss: 0.7737 (0.7769)  time: 0.1801  data: 0.0001  max mem: 15812
[23:18:40.201143] Test:  [240/345]  eta: 0:00:18  loss: 0.7755 (0.7770)  time: 0.1805  data: 0.0001  max mem: 15812
[23:18:42.012697] Test:  [250/345]  eta: 0:00:16  loss: 0.7677 (0.7766)  time: 0.1809  data: 0.0001  max mem: 15812
[23:18:43.829878] Test:  [260/345]  eta: 0:00:15  loss: 0.7694 (0.7770)  time: 0.1814  data: 0.0001  max mem: 15812
[23:18:45.651643] Test:  [270/345]  eta: 0:00:13  loss: 0.7785 (0.7770)  time: 0.1819  data: 0.0001  max mem: 15812
[23:18:47.479462] Test:  [280/345]  eta: 0:00:11  loss: 0.7732 (0.7770)  time: 0.1824  data: 0.0001  max mem: 15812
[23:18:49.312534] Test:  [290/345]  eta: 0:00:09  loss: 0.7757 (0.7768)  time: 0.1830  data: 0.0001  max mem: 15812
[23:18:51.150496] Test:  [300/345]  eta: 0:00:07  loss: 0.7770 (0.7769)  time: 0.1835  data: 0.0001  max mem: 15812
[23:18:52.994070] Test:  [310/345]  eta: 0:00:06  loss: 0.7754 (0.7768)  time: 0.1840  data: 0.0001  max mem: 15812
[23:18:54.841605] Test:  [320/345]  eta: 0:00:04  loss: 0.7798 (0.7770)  time: 0.1845  data: 0.0001  max mem: 15812
[23:18:56.696726] Test:  [330/345]  eta: 0:00:02  loss: 0.7729 (0.7767)  time: 0.1851  data: 0.0001  max mem: 15812
[23:18:58.557405] Test:  [340/345]  eta: 0:00:00  loss: 0.7708 (0.7766)  time: 0.1857  data: 0.0001  max mem: 15812
[23:18:59.302229] Test:  [344/345]  eta: 0:00:00  loss: 0.7708 (0.7765)  time: 0.1859  data: 0.0001  max mem: 15812
[23:18:59.372374] Test: Total time: 0:01:01 (0.1789 s / it)
[23:19:16.025669] Test:  [ 0/57]  eta: 0:00:30  loss: 0.8625 (0.8625)  time: 0.5369  data: 0.3725  max mem: 15812
[23:19:17.687656] Test:  [10/57]  eta: 0:00:09  loss: 0.8599 (0.8735)  time: 0.1998  data: 0.0339  max mem: 15812
[23:19:19.355197] Test:  [20/57]  eta: 0:00:06  loss: 0.8585 (0.8647)  time: 0.1664  data: 0.0001  max mem: 15812
[23:19:21.030322] Test:  [30/57]  eta: 0:00:04  loss: 0.7657 (0.8300)  time: 0.1671  data: 0.0001  max mem: 15812
[23:19:22.711320] Test:  [40/57]  eta: 0:00:02  loss: 0.7595 (0.8111)  time: 0.1677  data: 0.0001  max mem: 15812
[23:19:24.399981] Test:  [50/57]  eta: 0:00:01  loss: 0.7425 (0.8045)  time: 0.1684  data: 0.0001  max mem: 15812
[23:19:25.311829] Test:  [56/57]  eta: 0:00:00  loss: 0.7839 (0.8097)  time: 0.1636  data: 0.0000  max mem: 15812
[23:19:25.386390] Test: Total time: 0:00:09 (0.1737 s / it)
[23:19:28.170860] Dice score of the network on the train images: 0.693152, val images: 0.758408
[23:19:28.175343] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:19:29.211134] Epoch: [14]  [  0/345]  eta: 0:05:56  lr: 0.000087  loss: 0.8076 (0.8076)  time: 1.0347  data: 0.4372  max mem: 15812
[23:19:41.142489] Epoch: [14]  [ 20/345]  eta: 0:03:20  lr: 0.000088  loss: 0.7930 (0.8013)  time: 0.5965  data: 0.0001  max mem: 15812
[23:19:53.119500] Epoch: [14]  [ 40/345]  eta: 0:03:05  lr: 0.000088  loss: 0.8073 (0.8057)  time: 0.5988  data: 0.0001  max mem: 15812
[23:20:05.121669] Epoch: [14]  [ 60/345]  eta: 0:02:52  lr: 0.000089  loss: 0.8011 (0.8064)  time: 0.6001  data: 0.0001  max mem: 15812
[23:20:17.132449] Epoch: [14]  [ 80/345]  eta: 0:02:40  lr: 0.000089  loss: 0.7997 (0.8048)  time: 0.6005  data: 0.0001  max mem: 15812
[23:20:29.165278] Epoch: [14]  [100/345]  eta: 0:02:27  lr: 0.000089  loss: 0.8043 (0.8057)  time: 0.6016  data: 0.0001  max mem: 15812
[23:20:41.212201] Epoch: [14]  [120/345]  eta: 0:02:15  lr: 0.000090  loss: 0.8107 (0.8071)  time: 0.6023  data: 0.0001  max mem: 15812
[23:20:53.249794] Epoch: [14]  [140/345]  eta: 0:02:03  lr: 0.000090  loss: 0.8045 (0.8071)  time: 0.6018  data: 0.0001  max mem: 15812
[23:21:05.287340] Epoch: [14]  [160/345]  eta: 0:01:51  lr: 0.000090  loss: 0.8046 (0.8066)  time: 0.6018  data: 0.0001  max mem: 15812
[23:21:17.322681] Epoch: [14]  [180/345]  eta: 0:01:39  lr: 0.000091  loss: 0.8121 (0.8075)  time: 0.6017  data: 0.0001  max mem: 15812
[23:21:29.357275] Epoch: [14]  [200/345]  eta: 0:01:27  lr: 0.000091  loss: 0.8160 (0.8083)  time: 0.6017  data: 0.0001  max mem: 15812
[23:21:41.385673] Epoch: [14]  [220/345]  eta: 0:01:15  lr: 0.000091  loss: 0.8074 (0.8084)  time: 0.6014  data: 0.0001  max mem: 15812
[23:21:53.412721] Epoch: [14]  [240/345]  eta: 0:01:03  lr: 0.000092  loss: 0.8463 (0.8114)  time: 0.6013  data: 0.0001  max mem: 15812
[23:22:05.437377] Epoch: [14]  [260/345]  eta: 0:00:51  lr: 0.000092  loss: 0.8102 (0.8120)  time: 0.6012  data: 0.0001  max mem: 15812
[23:22:17.456007] Epoch: [14]  [280/345]  eta: 0:00:39  lr: 0.000093  loss: 0.8107 (0.8121)  time: 0.6009  data: 0.0001  max mem: 15812
[23:22:29.473728] Epoch: [14]  [300/345]  eta: 0:00:27  lr: 0.000093  loss: 0.8216 (0.8127)  time: 0.6008  data: 0.0001  max mem: 15812
[23:22:41.492765] Epoch: [14]  [320/345]  eta: 0:00:15  lr: 0.000093  loss: 0.8049 (0.8123)  time: 0.6009  data: 0.0001  max mem: 15812
[23:22:53.510012] Epoch: [14]  [340/345]  eta: 0:00:03  lr: 0.000094  loss: 0.8172 (0.8126)  time: 0.6008  data: 0.0001  max mem: 15812
[23:22:55.912296] Epoch: [14]  [344/345]  eta: 0:00:00  lr: 0.000094  loss: 0.8172 (0.8125)  time: 0.6007  data: 0.0001  max mem: 15812
[23:22:55.991535] Epoch: [14] Total time: 0:03:27 (0.6024 s / it)
[23:22:55.991755] Averaged stats: lr: 0.000094  loss: 0.8172 (0.8125)
[23:22:56.622754] Test:  [  0/345]  eta: 0:03:35  loss: 0.7686 (0.7686)  time: 0.6261  data: 0.4591  max mem: 15812
[23:22:58.312930] Test:  [ 10/345]  eta: 0:01:10  loss: 0.7686 (0.7696)  time: 0.2105  data: 0.0418  max mem: 15812
[23:23:00.007251] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7739 (0.7751)  time: 0.1691  data: 0.0001  max mem: 15812
[23:23:01.708185] Test:  [ 30/345]  eta: 0:00:58  loss: 0.7739 (0.7724)  time: 0.1697  data: 0.0001  max mem: 15812
[23:23:03.411217] Test:  [ 40/345]  eta: 0:00:55  loss: 0.7664 (0.7718)  time: 0.1701  data: 0.0001  max mem: 15812
[23:23:05.120363] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7631 (0.7703)  time: 0.1705  data: 0.0001  max mem: 15812
[23:23:06.836137] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7665 (0.7713)  time: 0.1712  data: 0.0001  max mem: 15812
[23:23:08.558201] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7703 (0.7712)  time: 0.1718  data: 0.0001  max mem: 15812
[23:23:10.284985] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7709 (0.7723)  time: 0.1724  data: 0.0001  max mem: 15812
[23:23:12.014006] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7753 (0.7723)  time: 0.1727  data: 0.0001  max mem: 15812
[23:23:13.750707] Test:  [100/345]  eta: 0:00:43  loss: 0.7710 (0.7731)  time: 0.1732  data: 0.0001  max mem: 15812
[23:23:15.490640] Test:  [110/345]  eta: 0:00:41  loss: 0.7726 (0.7734)  time: 0.1738  data: 0.0001  max mem: 15812
[23:23:17.237320] Test:  [120/345]  eta: 0:00:39  loss: 0.7742 (0.7735)  time: 0.1743  data: 0.0001  max mem: 15812
[23:23:18.988776] Test:  [130/345]  eta: 0:00:37  loss: 0.7727 (0.7737)  time: 0.1748  data: 0.0001  max mem: 15812
[23:23:20.743740] Test:  [140/345]  eta: 0:00:35  loss: 0.7749 (0.7737)  time: 0.1752  data: 0.0001  max mem: 15812
[23:23:22.505006] Test:  [150/345]  eta: 0:00:34  loss: 0.7682 (0.7730)  time: 0.1758  data: 0.0001  max mem: 15812
[23:23:24.271942] Test:  [160/345]  eta: 0:00:32  loss: 0.7715 (0.7735)  time: 0.1763  data: 0.0001  max mem: 15812
[23:23:26.043732] Test:  [170/345]  eta: 0:00:30  loss: 0.7746 (0.7731)  time: 0.1769  data: 0.0001  max mem: 15812
[23:23:27.820455] Test:  [180/345]  eta: 0:00:28  loss: 0.7687 (0.7730)  time: 0.1774  data: 0.0001  max mem: 15812
[23:23:29.602085] Test:  [190/345]  eta: 0:00:27  loss: 0.7732 (0.7733)  time: 0.1779  data: 0.0001  max mem: 15812
[23:23:31.391172] Test:  [200/345]  eta: 0:00:25  loss: 0.7829 (0.7733)  time: 0.1785  data: 0.0001  max mem: 15812
[23:23:33.181684] Test:  [210/345]  eta: 0:00:23  loss: 0.7630 (0.7732)  time: 0.1789  data: 0.0001  max mem: 15812
[23:23:34.980730] Test:  [220/345]  eta: 0:00:22  loss: 0.7696 (0.7733)  time: 0.1794  data: 0.0001  max mem: 15812
[23:23:36.783715] Test:  [230/345]  eta: 0:00:20  loss: 0.7696 (0.7726)  time: 0.1800  data: 0.0001  max mem: 15812
[23:23:38.588605] Test:  [240/345]  eta: 0:00:18  loss: 0.7699 (0.7732)  time: 0.1803  data: 0.0001  max mem: 15812
[23:23:40.401723] Test:  [250/345]  eta: 0:00:16  loss: 0.7767 (0.7732)  time: 0.1808  data: 0.0001  max mem: 15812
[23:23:42.218258] Test:  [260/345]  eta: 0:00:15  loss: 0.7767 (0.7734)  time: 0.1814  data: 0.0001  max mem: 15812
[23:23:44.041951] Test:  [270/345]  eta: 0:00:13  loss: 0.7781 (0.7737)  time: 0.1820  data: 0.0001  max mem: 15812
[23:23:45.869113] Test:  [280/345]  eta: 0:00:11  loss: 0.7766 (0.7740)  time: 0.1825  data: 0.0001  max mem: 15812
[23:23:47.703482] Test:  [290/345]  eta: 0:00:09  loss: 0.7622 (0.7738)  time: 0.1830  data: 0.0001  max mem: 15812
[23:23:49.542790] Test:  [300/345]  eta: 0:00:08  loss: 0.7720 (0.7738)  time: 0.1836  data: 0.0001  max mem: 15812
[23:23:51.385861] Test:  [310/345]  eta: 0:00:06  loss: 0.7766 (0.7739)  time: 0.1841  data: 0.0001  max mem: 15812
[23:23:53.234738] Test:  [320/345]  eta: 0:00:04  loss: 0.7855 (0.7742)  time: 0.1845  data: 0.0001  max mem: 15812
[23:23:55.089633] Test:  [330/345]  eta: 0:00:02  loss: 0.7813 (0.7743)  time: 0.1851  data: 0.0001  max mem: 15812
[23:23:56.950032] Test:  [340/345]  eta: 0:00:00  loss: 0.7773 (0.7740)  time: 0.1857  data: 0.0001  max mem: 15812
[23:23:57.695306] Test:  [344/345]  eta: 0:00:00  loss: 0.7776 (0.7741)  time: 0.1859  data: 0.0001  max mem: 15812
[23:23:57.766408] Test: Total time: 0:01:01 (0.1790 s / it)
[23:24:14.487716] Test:  [ 0/57]  eta: 0:00:30  loss: 0.9087 (0.9087)  time: 0.5437  data: 0.3796  max mem: 15812
[23:24:16.150083] Test:  [10/57]  eta: 0:00:09  loss: 0.8957 (0.8867)  time: 0.2005  data: 0.0346  max mem: 15812
[23:24:17.819115] Test:  [20/57]  eta: 0:00:06  loss: 0.8957 (0.8753)  time: 0.1665  data: 0.0001  max mem: 15812
[23:24:19.494619] Test:  [30/57]  eta: 0:00:04  loss: 0.7788 (0.8418)  time: 0.1672  data: 0.0001  max mem: 15812
[23:24:21.174613] Test:  [40/57]  eta: 0:00:02  loss: 0.7730 (0.8233)  time: 0.1677  data: 0.0001  max mem: 15812
[23:24:22.862243] Test:  [50/57]  eta: 0:00:01  loss: 0.7702 (0.8177)  time: 0.1683  data: 0.0001  max mem: 15812
[23:24:23.774450] Test:  [56/57]  eta: 0:00:00  loss: 0.7764 (0.8219)  time: 0.1636  data: 0.0000  max mem: 15812
[23:24:23.838702] Test: Total time: 0:00:09 (0.1736 s / it)
[23:24:26.629992] Dice score of the network on the train images: 0.703991, val images: 0.748791
[23:24:26.634882] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:24:27.631157] Epoch: [15]  [  0/345]  eta: 0:05:43  lr: 0.000094  loss: 0.7951 (0.7951)  time: 0.9952  data: 0.3995  max mem: 15812
[23:24:39.561945] Epoch: [15]  [ 20/345]  eta: 0:03:20  lr: 0.000094  loss: 0.7944 (0.7960)  time: 0.5965  data: 0.0001  max mem: 15812
[23:24:51.546455] Epoch: [15]  [ 40/345]  eta: 0:03:05  lr: 0.000094  loss: 0.7989 (0.8013)  time: 0.5992  data: 0.0001  max mem: 15812
[23:25:03.672275] Epoch: [15]  [ 60/345]  eta: 0:02:53  lr: 0.000095  loss: 0.7954 (0.8013)  time: 0.6062  data: 0.0001  max mem: 15812
[23:25:15.710808] Epoch: [15]  [ 80/345]  eta: 0:02:40  lr: 0.000095  loss: 0.7978 (0.8020)  time: 0.6019  data: 0.0001  max mem: 15812
[23:25:27.771758] Epoch: [15]  [100/345]  eta: 0:02:28  lr: 0.000096  loss: 0.7886 (0.7999)  time: 0.6030  data: 0.0001  max mem: 15812
[23:25:39.824252] Epoch: [15]  [120/345]  eta: 0:02:16  lr: 0.000096  loss: 0.7965 (0.8001)  time: 0.6026  data: 0.0001  max mem: 15812
[23:25:51.886273] Epoch: [15]  [140/345]  eta: 0:02:03  lr: 0.000096  loss: 0.8024 (0.8003)  time: 0.6031  data: 0.0001  max mem: 15812
[23:26:03.948526] Epoch: [15]  [160/345]  eta: 0:01:51  lr: 0.000097  loss: 0.7957 (0.8005)  time: 0.6031  data: 0.0001  max mem: 15812
[23:26:16.004930] Epoch: [15]  [180/345]  eta: 0:01:39  lr: 0.000097  loss: 0.8016 (0.8025)  time: 0.6028  data: 0.0001  max mem: 15812
[23:26:28.058701] Epoch: [15]  [200/345]  eta: 0:01:27  lr: 0.000097  loss: 0.8056 (0.8028)  time: 0.6026  data: 0.0001  max mem: 15812
[23:26:40.114774] Epoch: [15]  [220/345]  eta: 0:01:15  lr: 0.000098  loss: 0.8086 (0.8033)  time: 0.6028  data: 0.0001  max mem: 15812
[23:26:52.163057] Epoch: [15]  [240/345]  eta: 0:01:03  lr: 0.000098  loss: 0.8048 (0.8035)  time: 0.6024  data: 0.0001  max mem: 15812
[23:27:04.209765] Epoch: [15]  [260/345]  eta: 0:00:51  lr: 0.000098  loss: 0.7922 (0.8032)  time: 0.6023  data: 0.0001  max mem: 15812
[23:27:16.231096] Epoch: [15]  [280/345]  eta: 0:00:39  lr: 0.000099  loss: 0.7879 (0.8028)  time: 0.6010  data: 0.0001  max mem: 15812
[23:27:28.273199] Epoch: [15]  [300/345]  eta: 0:00:27  lr: 0.000099  loss: 0.7915 (0.8022)  time: 0.6021  data: 0.0001  max mem: 15812
[23:27:40.304588] Epoch: [15]  [320/345]  eta: 0:00:15  lr: 0.000100  loss: 0.8004 (0.8023)  time: 0.6015  data: 0.0001  max mem: 15812
[23:27:52.319763] Epoch: [15]  [340/345]  eta: 0:00:03  lr: 0.000100  loss: 0.8015 (0.8023)  time: 0.6007  data: 0.0001  max mem: 15812
[23:27:54.723901] Epoch: [15]  [344/345]  eta: 0:00:00  lr: 0.000100  loss: 0.8019 (0.8024)  time: 0.6007  data: 0.0001  max mem: 15812
[23:27:54.794534] Epoch: [15] Total time: 0:03:28 (0.6034 s / it)
[23:27:54.794922] Averaged stats: lr: 0.000100  loss: 0.8019 (0.8024)
[23:27:55.434691] Test:  [  0/345]  eta: 0:03:39  loss: 0.7845 (0.7845)  time: 0.6362  data: 0.4696  max mem: 15812
[23:27:57.124115] Test:  [ 10/345]  eta: 0:01:10  loss: 0.7786 (0.7815)  time: 0.2114  data: 0.0428  max mem: 15812
[23:27:58.817147] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7731 (0.7768)  time: 0.1691  data: 0.0001  max mem: 15812
[23:28:00.515286] Test:  [ 30/345]  eta: 0:00:58  loss: 0.7726 (0.7769)  time: 0.1695  data: 0.0001  max mem: 15812
[23:28:02.219135] Test:  [ 40/345]  eta: 0:00:55  loss: 0.7728 (0.7758)  time: 0.1700  data: 0.0001  max mem: 15812
[23:28:03.927925] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7801 (0.7780)  time: 0.1706  data: 0.0001  max mem: 15812
[23:28:05.641653] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7801 (0.7781)  time: 0.1711  data: 0.0001  max mem: 15812
[23:28:07.360608] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7721 (0.7775)  time: 0.1716  data: 0.0001  max mem: 15812
[23:28:09.084744] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7706 (0.7763)  time: 0.1721  data: 0.0001  max mem: 15812
[23:28:10.816547] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7731 (0.7759)  time: 0.1727  data: 0.0001  max mem: 15812
[23:28:12.551481] Test:  [100/345]  eta: 0:00:43  loss: 0.7739 (0.7763)  time: 0.1733  data: 0.0001  max mem: 15812
[23:28:14.292085] Test:  [110/345]  eta: 0:00:41  loss: 0.7865 (0.7772)  time: 0.1737  data: 0.0001  max mem: 15812
[23:28:16.037301] Test:  [120/345]  eta: 0:00:39  loss: 0.7791 (0.7771)  time: 0.1742  data: 0.0001  max mem: 15812
[23:28:17.787664] Test:  [130/345]  eta: 0:00:37  loss: 0.7780 (0.7776)  time: 0.1747  data: 0.0001  max mem: 15812
[23:28:19.543744] Test:  [140/345]  eta: 0:00:35  loss: 0.7809 (0.7779)  time: 0.1753  data: 0.0001  max mem: 15812
[23:28:21.305539] Test:  [150/345]  eta: 0:00:34  loss: 0.7826 (0.7779)  time: 0.1758  data: 0.0001  max mem: 15812
[23:28:23.071178] Test:  [160/345]  eta: 0:00:32  loss: 0.7826 (0.7782)  time: 0.1763  data: 0.0001  max mem: 15812
[23:28:24.841468] Test:  [170/345]  eta: 0:00:30  loss: 0.7787 (0.7779)  time: 0.1767  data: 0.0001  max mem: 15812
[23:28:26.619012] Test:  [180/345]  eta: 0:00:28  loss: 0.7772 (0.7779)  time: 0.1773  data: 0.0001  max mem: 15812
[23:28:28.400386] Test:  [190/345]  eta: 0:00:27  loss: 0.7782 (0.7780)  time: 0.1779  data: 0.0001  max mem: 15812
[23:28:30.186445] Test:  [200/345]  eta: 0:00:25  loss: 0.7773 (0.7779)  time: 0.1783  data: 0.0001  max mem: 15812
[23:28:31.980941] Test:  [210/345]  eta: 0:00:23  loss: 0.7710 (0.7776)  time: 0.1790  data: 0.0001  max mem: 15812
[23:28:33.778888] Test:  [220/345]  eta: 0:00:22  loss: 0.7716 (0.7777)  time: 0.1796  data: 0.0001  max mem: 15812
[23:28:35.583225] Test:  [230/345]  eta: 0:00:20  loss: 0.7829 (0.7778)  time: 0.1801  data: 0.0001  max mem: 15812
[23:28:37.389876] Test:  [240/345]  eta: 0:00:18  loss: 0.7771 (0.7778)  time: 0.1805  data: 0.0001  max mem: 15812
[23:28:39.202079] Test:  [250/345]  eta: 0:00:16  loss: 0.7759 (0.7780)  time: 0.1809  data: 0.0001  max mem: 15812
[23:28:41.019620] Test:  [260/345]  eta: 0:00:15  loss: 0.7652 (0.7775)  time: 0.1814  data: 0.0001  max mem: 15812
[23:28:42.842296] Test:  [270/345]  eta: 0:00:13  loss: 0.7724 (0.7777)  time: 0.1819  data: 0.0001  max mem: 15812
[23:28:44.668870] Test:  [280/345]  eta: 0:00:11  loss: 0.7827 (0.7780)  time: 0.1824  data: 0.0001  max mem: 15812
[23:28:46.503038] Test:  [290/345]  eta: 0:00:09  loss: 0.7808 (0.7784)  time: 0.1830  data: 0.0001  max mem: 15812
[23:28:48.341177] Test:  [300/345]  eta: 0:00:08  loss: 0.7746 (0.7782)  time: 0.1835  data: 0.0001  max mem: 15812
[23:28:50.184656] Test:  [310/345]  eta: 0:00:06  loss: 0.7727 (0.7780)  time: 0.1840  data: 0.0001  max mem: 15812
[23:28:52.032937] Test:  [320/345]  eta: 0:00:04  loss: 0.7729 (0.7778)  time: 0.1845  data: 0.0001  max mem: 15812
[23:28:53.886644] Test:  [330/345]  eta: 0:00:02  loss: 0.7729 (0.7774)  time: 0.1850  data: 0.0001  max mem: 15812
[23:28:55.744992] Test:  [340/345]  eta: 0:00:00  loss: 0.7765 (0.7776)  time: 0.1855  data: 0.0001  max mem: 15812
[23:28:56.490168] Test:  [344/345]  eta: 0:00:00  loss: 0.7752 (0.7775)  time: 0.1857  data: 0.0001  max mem: 15812
[23:28:56.558172] Test: Total time: 0:01:01 (0.1790 s / it)
[23:29:13.385273] Test:  [ 0/57]  eta: 0:00:30  loss: 0.8288 (0.8288)  time: 0.5320  data: 0.3682  max mem: 15812
[23:29:15.047751] Test:  [10/57]  eta: 0:00:09  loss: 0.8767 (0.8842)  time: 0.1994  data: 0.0336  max mem: 15812
[23:29:16.715456] Test:  [20/57]  eta: 0:00:06  loss: 0.8882 (0.8805)  time: 0.1664  data: 0.0001  max mem: 15812
[23:29:18.389118] Test:  [30/57]  eta: 0:00:04  loss: 0.7928 (0.8483)  time: 0.1670  data: 0.0001  max mem: 15812
[23:29:20.069711] Test:  [40/57]  eta: 0:00:02  loss: 0.7797 (0.8316)  time: 0.1677  data: 0.0001  max mem: 15812
[23:29:21.757467] Test:  [50/57]  eta: 0:00:01  loss: 0.7751 (0.8273)  time: 0.1684  data: 0.0001  max mem: 15812
[23:29:22.669613] Test:  [56/57]  eta: 0:00:00  loss: 0.7867 (0.8312)  time: 0.1636  data: 0.0000  max mem: 15812
[23:29:22.734994] Test: Total time: 0:00:09 (0.1734 s / it)
[23:29:25.557486] Dice score of the network on the train images: 0.713790, val images: 0.727061
[23:29:25.557702] saving best_prec_model_0 @ epoch 15
[23:29:26.586995] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:29:27.579307] Epoch: [16]  [  0/345]  eta: 0:05:42  lr: 0.000100  loss: 0.7844 (0.7844)  time: 0.9914  data: 0.3948  max mem: 15812
[23:29:39.486115] Epoch: [16]  [ 20/345]  eta: 0:03:19  lr: 0.000100  loss: 0.8025 (0.8003)  time: 0.5953  data: 0.0001  max mem: 15812

[23:29:51.435653] Epoch: [16]  [ 40/345]  eta: 0:03:04  lr: 0.000101  loss: 0.8076 (0.8040)  time: 0.5974  data: 0.0001  max mem: 15812
[23:30:03.429564] Epoch: [16]  [ 60/345]  eta: 0:02:52  lr: 0.000101  loss: 0.8214 (0.8122)  time: 0.5997  data: 0.0001  max mem: 15812
[23:30:15.456194] Epoch: [16]  [ 80/345]  eta: 0:02:39  lr: 0.000101  loss: 0.7989 (0.8089)  time: 0.6013  data: 0.0001  max mem: 15812

[23:30:27.498675] Epoch: [16]  [100/345]  eta: 0:02:27  lr: 0.000102  loss: 0.7910 (0.8054)  time: 0.6021  data: 0.0001  max mem: 15812
[23:30:39.548114] Epoch: [16]  [120/345]  eta: 0:02:15  lr: 0.000102  loss: 0.8007 (0.8057)  time: 0.6024  data: 0.0001  max mem: 15812
[23:30:51.599899] Epoch: [16]  [140/345]  eta: 0:02:03  lr: 0.000103  loss: 0.8088 (0.8060)  time: 0.6025  data: 0.0001  max mem: 15812
[23:31:03.643545] Epoch: [16]  [160/345]  eta: 0:01:51  lr: 0.000103  loss: 0.8089 (0.8064)  time: 0.6021  data: 0.0001  max mem: 15812
[23:31:15.693736] Epoch: [16]  [180/345]  eta: 0:01:39  lr: 0.000103  loss: 0.7874 (0.8047)  time: 0.6025  data: 0.0001  max mem: 15812
[23:31:27.750735] Epoch: [16]  [200/345]  eta: 0:01:27  lr: 0.000104  loss: 0.7893 (0.8035)  time: 0.6028  data: 0.0001  max mem: 15812
[23:31:39.808696] Epoch: [16]  [220/345]  eta: 0:01:15  lr: 0.000104  loss: 0.8082 (0.8044)  time: 0.6029  data: 0.0001  max mem: 15812
[23:31:51.844876] Epoch: [16]  [240/345]  eta: 0:01:03  lr: 0.000104  loss: 0.8040 (0.8044)  time: 0.6018  data: 0.0001  max mem: 15812
[23:32:03.954004] Epoch: [16]  [260/345]  eta: 0:00:51  lr: 0.000105  loss: 0.8113 (0.8051)  time: 0.6054  data: 0.0001  max mem: 15812
[23:32:15.998661] Epoch: [16]  [280/345]  eta: 0:00:39  lr: 0.000105  loss: 0.8004 (0.8046)  time: 0.6022  data: 0.0001  max mem: 15812
[23:32:28.050866] Epoch: [16]  [300/345]  eta: 0:00:27  lr: 0.000105  loss: 0.7937 (0.8042)  time: 0.6026  data: 0.0001  max mem: 15812
[23:32:40.071128] Epoch: [16]  [320/345]  eta: 0:00:15  lr: 0.000106  loss: 0.7970 (0.8034)  time: 0.6010  data: 0.0001  max mem: 15812
[23:32:52.107595] Epoch: [16]  [340/345]  eta: 0:00:03  lr: 0.000106  loss: 0.8016 (0.8037)  time: 0.6018  data: 0.0001  max mem: 15812
[23:32:54.513325] Epoch: [16]  [344/345]  eta: 0:00:00  lr: 0.000106  loss: 0.8159 (0.8041)  time: 0.6015  data: 0.0001  max mem: 15812
[23:32:54.584160] Epoch: [16] Total time: 0:03:27 (0.6029 s / it)
[23:32:54.584395] Averaged stats: lr: 0.000106  loss: 0.8159 (0.8041)
[23:32:55.167502] Test:  [  0/345]  eta: 0:03:19  loss: 0.7777 (0.7777)  time: 0.5780  data: 0.4109  max mem: 15812
[23:32:56.856488] Test:  [ 10/345]  eta: 0:01:09  loss: 0.8067 (0.8066)  time: 0.2060  data: 0.0374  max mem: 15812
[23:32:58.551445] Test:  [ 20/345]  eta: 0:01:01  loss: 0.8105 (0.8099)  time: 0.1691  data: 0.0001  max mem: 15812
[23:33:00.250927] Test:  [ 30/345]  eta: 0:00:57  loss: 0.8078 (0.8093)  time: 0.1697  data: 0.0001  max mem: 15812
[23:33:01.955761] Test:  [ 40/345]  eta: 0:00:54  loss: 0.8078 (0.8113)  time: 0.1702  data: 0.0001  max mem: 15812
[23:33:03.666520] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7983 (0.8100)  time: 0.1707  data: 0.0001  max mem: 15812
[23:33:05.382411] Test:  [ 60/345]  eta: 0:00:50  loss: 0.8017 (0.8098)  time: 0.1713  data: 0.0001  max mem: 15812
[23:33:07.102853] Test:  [ 70/345]  eta: 0:00:48  loss: 0.8014 (0.8079)  time: 0.1718  data: 0.0001  max mem: 15812
[23:33:08.827730] Test:  [ 80/345]  eta: 0:00:46  loss: 0.8014 (0.8080)  time: 0.1722  data: 0.0001  max mem: 15812
[23:33:10.559710] Test:  [ 90/345]  eta: 0:00:44  loss: 0.8037 (0.8076)  time: 0.1728  data: 0.0001  max mem: 15812
[23:33:12.296528] Test:  [100/345]  eta: 0:00:42  loss: 0.7939 (0.8061)  time: 0.1734  data: 0.0001  max mem: 15812
[23:33:14.038698] Test:  [110/345]  eta: 0:00:41  loss: 0.7932 (0.8058)  time: 0.1739  data: 0.0001  max mem: 15812
[23:33:15.784271] Test:  [120/345]  eta: 0:00:39  loss: 0.8039 (0.8060)  time: 0.1743  data: 0.0001  max mem: 15812
[23:33:17.535093] Test:  [130/345]  eta: 0:00:37  loss: 0.8039 (0.8056)  time: 0.1748  data: 0.0001  max mem: 15812
[23:33:19.291371] Test:  [140/345]  eta: 0:00:35  loss: 0.8043 (0.8061)  time: 0.1753  data: 0.0001  max mem: 15812
[23:33:21.053316] Test:  [150/345]  eta: 0:00:34  loss: 0.8162 (0.8073)  time: 0.1758  data: 0.0001  max mem: 15812
[23:33:22.821584] Test:  [160/345]  eta: 0:00:32  loss: 0.8138 (0.8074)  time: 0.1764  data: 0.0001  max mem: 15812
[23:33:24.594496] Test:  [170/345]  eta: 0:00:30  loss: 0.8054 (0.8068)  time: 0.1769  data: 0.0001  max mem: 15812
[23:33:26.372277] Test:  [180/345]  eta: 0:00:28  loss: 0.8006 (0.8064)  time: 0.1775  data: 0.0001  max mem: 15812
[23:33:28.155521] Test:  [190/345]  eta: 0:00:27  loss: 0.7965 (0.8057)  time: 0.1780  data: 0.0001  max mem: 15812
[23:33:29.943597] Test:  [200/345]  eta: 0:00:25  loss: 0.7946 (0.8053)  time: 0.1785  data: 0.0001  max mem: 15812
[23:33:31.737631] Test:  [210/345]  eta: 0:00:23  loss: 0.8017 (0.8055)  time: 0.1790  data: 0.0001  max mem: 15812
[23:33:33.534150] Test:  [220/345]  eta: 0:00:22  loss: 0.8045 (0.8056)  time: 0.1795  data: 0.0001  max mem: 15812
[23:33:35.333942] Test:  [230/345]  eta: 0:00:20  loss: 0.8011 (0.8057)  time: 0.1798  data: 0.0001  max mem: 15812
[23:33:37.142262] Test:  [240/345]  eta: 0:00:18  loss: 0.7992 (0.8057)  time: 0.1803  data: 0.0001  max mem: 15812
[23:33:38.956559] Test:  [250/345]  eta: 0:00:16  loss: 0.8025 (0.8061)  time: 0.1811  data: 0.0001  max mem: 15812
[23:33:40.775491] Test:  [260/345]  eta: 0:00:15  loss: 0.8092 (0.8061)  time: 0.1816  data: 0.0001  max mem: 15812
[23:33:42.599351] Test:  [270/345]  eta: 0:00:13  loss: 0.8020 (0.8060)  time: 0.1821  data: 0.0001  max mem: 15812
[23:33:44.428576] Test:  [280/345]  eta: 0:00:11  loss: 0.7996 (0.8055)  time: 0.1826  data: 0.0001  max mem: 15812
[23:33:46.263058] Test:  [290/345]  eta: 0:00:09  loss: 0.7964 (0.8056)  time: 0.1831  data: 0.0001  max mem: 15812
[23:33:48.102888] Test:  [300/345]  eta: 0:00:07  loss: 0.8022 (0.8058)  time: 0.1837  data: 0.0001  max mem: 15812
[23:33:49.947582] Test:  [310/345]  eta: 0:00:06  loss: 0.8052 (0.8058)  time: 0.1842  data: 0.0001  max mem: 15812
[23:33:51.797712] Test:  [320/345]  eta: 0:00:04  loss: 0.8031 (0.8058)  time: 0.1847  data: 0.0001  max mem: 15812
[23:33:53.651776] Test:  [330/345]  eta: 0:00:02  loss: 0.8034 (0.8060)  time: 0.1852  data: 0.0001  max mem: 15812
[23:33:55.510530] Test:  [340/345]  eta: 0:00:00  loss: 0.8052 (0.8059)  time: 0.1856  data: 0.0001  max mem: 15812
[23:33:56.256084] Test:  [344/345]  eta: 0:00:00  loss: 0.8068 (0.8059)  time: 0.1858  data: 0.0001  max mem: 15812
[23:33:56.335256] Test: Total time: 0:01:01 (0.1790 s / it)
[23:34:13.121402] Test:  [ 0/57]  eta: 0:00:31  loss: 0.9186 (0.9186)  time: 0.5560  data: 0.3917  max mem: 15812
[23:34:14.784628] Test:  [10/57]  eta: 0:00:09  loss: 0.8846 (0.9093)  time: 0.2017  data: 0.0357  max mem: 15812
[23:34:16.454611] Test:  [20/57]  eta: 0:00:06  loss: 0.9049 (0.9057)  time: 0.1666  data: 0.0001  max mem: 15812
[23:34:18.129869] Test:  [30/57]  eta: 0:00:04  loss: 0.8096 (0.8707)  time: 0.1672  data: 0.0001  max mem: 15812
[23:34:19.811535] Test:  [40/57]  eta: 0:00:03  loss: 0.7893 (0.8507)  time: 0.1678  data: 0.0001  max mem: 15812
[23:34:21.498803] Test:  [50/57]  eta: 0:00:01  loss: 0.7917 (0.8449)  time: 0.1684  data: 0.0001  max mem: 15812
[23:34:22.410669] Test:  [56/57]  eta: 0:00:00  loss: 0.8271 (0.8518)  time: 0.1636  data: 0.0001  max mem: 15812
[23:34:22.484168] Test: Total time: 0:00:09 (0.1740 s / it)
[23:34:25.290101] Dice score of the network on the train images: 0.691890, val images: 0.710907
[23:34:25.294159] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:34:26.279028] Epoch: [17]  [  0/345]  eta: 0:05:39  lr: 0.000106  loss: 0.8213 (0.8213)  time: 0.9840  data: 0.3878  max mem: 15812
[23:34:38.206363] Epoch: [17]  [ 20/345]  eta: 0:03:19  lr: 0.000107  loss: 0.8176 (0.8198)  time: 0.5963  data: 0.0001  max mem: 15812
[23:34:50.178126] Epoch: [17]  [ 40/345]  eta: 0:03:05  lr: 0.000107  loss: 0.8037 (0.8142)  time: 0.5985  data: 0.0001  max mem: 15812
[23:35:02.173097] Epoch: [17]  [ 60/345]  eta: 0:02:52  lr: 0.000107  loss: 0.8021 (0.8104)  time: 0.5997  data: 0.0001  max mem: 15812
[23:35:14.194436] Epoch: [17]  [ 80/345]  eta: 0:02:39  lr: 0.000108  loss: 0.7864 (0.8056)  time: 0.6010  data: 0.0001  max mem: 15812
[23:35:26.245810] Epoch: [17]  [100/345]  eta: 0:02:27  lr: 0.000108  loss: 0.7873 (0.8023)  time: 0.6025  data: 0.0001  max mem: 15812
[23:35:38.305189] Epoch: [17]  [120/345]  eta: 0:02:15  lr: 0.000108  loss: 0.7946 (0.8007)  time: 0.6029  data: 0.0001  max mem: 15812
[23:35:50.362310] Epoch: [17]  [140/345]  eta: 0:02:03  lr: 0.000109  loss: 0.7868 (0.7989)  time: 0.6028  data: 0.0001  max mem: 15812
[23:36:02.417427] Epoch: [17]  [160/345]  eta: 0:01:51  lr: 0.000109  loss: 0.7843 (0.7971)  time: 0.6027  data: 0.0001  max mem: 15812
[23:36:14.467953] Epoch: [17]  [180/345]  eta: 0:01:39  lr: 0.000110  loss: 0.7916 (0.7971)  time: 0.6025  data: 0.0001  max mem: 15812
[23:36:26.519485] Epoch: [17]  [200/345]  eta: 0:01:27  lr: 0.000110  loss: 0.8005 (0.7972)  time: 0.6025  data: 0.0001  max mem: 15812
[23:36:38.550726] Epoch: [17]  [220/345]  eta: 0:01:15  lr: 0.000110  loss: 0.7882 (0.7968)  time: 0.6015  data: 0.0001  max mem: 15812
[23:36:50.576353] Epoch: [17]  [240/345]  eta: 0:01:03  lr: 0.000111  loss: 0.7997 (0.7975)  time: 0.6012  data: 0.0001  max mem: 15812
[23:37:02.590643] Epoch: [17]  [260/345]  eta: 0:00:51  lr: 0.000111  loss: 0.7977 (0.7974)  time: 0.6007  data: 0.0001  max mem: 15812
[23:37:14.603678] Epoch: [17]  [280/345]  eta: 0:00:39  lr: 0.000111  loss: 0.7852 (0.7974)  time: 0.6006  data: 0.0001  max mem: 15812
[23:37:26.643463] Epoch: [17]  [300/345]  eta: 0:00:27  lr: 0.000112  loss: 0.8157 (0.7994)  time: 0.6019  data: 0.0001  max mem: 15812
[23:37:38.671185] Epoch: [17]  [320/345]  eta: 0:00:15  lr: 0.000112  loss: 0.7944 (0.7998)  time: 0.6013  data: 0.0001  max mem: 15812
[23:37:50.702378] Epoch: [17]  [340/345]  eta: 0:00:03  lr: 0.000112  loss: 0.7899 (0.7993)  time: 0.6015  data: 0.0001  max mem: 15812
[23:37:53.107979] Epoch: [17]  [344/345]  eta: 0:00:00  lr: 0.000112  loss: 0.7806 (0.7992)  time: 0.6016  data: 0.0001  max mem: 15812
[23:37:53.189104] Epoch: [17] Total time: 0:03:27 (0.6026 s / it)
[23:37:53.189440] Averaged stats: lr: 0.000112  loss: 0.7806 (0.7992)
[23:37:53.769115] Test:  [  0/345]  eta: 0:03:18  loss: 0.7706 (0.7706)  time: 0.5744  data: 0.4070  max mem: 15812
[23:37:55.458419] Test:  [ 10/345]  eta: 0:01:08  loss: 0.7597 (0.7588)  time: 0.2057  data: 0.0371  max mem: 15812
[23:37:57.152364] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7520 (0.7528)  time: 0.1691  data: 0.0001  max mem: 15812
[23:37:58.852309] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7565 (0.7600)  time: 0.1696  data: 0.0001  max mem: 15812
[23:38:00.557450] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7676 (0.7597)  time: 0.1702  data: 0.0001  max mem: 15812
[23:38:02.266182] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7681 (0.7628)  time: 0.1706  data: 0.0001  max mem: 15812
[23:38:03.980338] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7619 (0.7616)  time: 0.1711  data: 0.0001  max mem: 15812
[23:38:05.699887] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7554 (0.7613)  time: 0.1716  data: 0.0001  max mem: 15812
[23:38:07.427170] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7524 (0.7606)  time: 0.1723  data: 0.0001  max mem: 15812
[23:38:09.158550] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7585 (0.7606)  time: 0.1729  data: 0.0001  max mem: 15812
[23:38:10.893976] Test:  [100/345]  eta: 0:00:42  loss: 0.7585 (0.7602)  time: 0.1733  data: 0.0001  max mem: 15812
[23:38:12.635298] Test:  [110/345]  eta: 0:00:41  loss: 0.7489 (0.7592)  time: 0.1738  data: 0.0001  max mem: 15812
[23:38:14.380819] Test:  [120/345]  eta: 0:00:39  loss: 0.7489 (0.7596)  time: 0.1743  data: 0.0001  max mem: 15812
[23:38:16.132489] Test:  [130/345]  eta: 0:00:37  loss: 0.7613 (0.7599)  time: 0.1748  data: 0.0001  max mem: 15812
[23:38:17.888694] Test:  [140/345]  eta: 0:00:35  loss: 0.7607 (0.7597)  time: 0.1753  data: 0.0001  max mem: 15812
[23:38:19.650080] Test:  [150/345]  eta: 0:00:34  loss: 0.7513 (0.7593)  time: 0.1758  data: 0.0001  max mem: 15812
[23:38:21.416665] Test:  [160/345]  eta: 0:00:32  loss: 0.7564 (0.7596)  time: 0.1763  data: 0.0001  max mem: 15812
[23:38:23.188356] Test:  [170/345]  eta: 0:00:30  loss: 0.7638 (0.7595)  time: 0.1768  data: 0.0001  max mem: 15812
[23:38:24.965248] Test:  [180/345]  eta: 0:00:28  loss: 0.7601 (0.7599)  time: 0.1774  data: 0.0001  max mem: 15812
[23:38:26.748178] Test:  [190/345]  eta: 0:00:27  loss: 0.7630 (0.7603)  time: 0.1779  data: 0.0001  max mem: 15812
[23:38:28.535399] Test:  [200/345]  eta: 0:00:25  loss: 0.7665 (0.7605)  time: 0.1784  data: 0.0001  max mem: 15812
[23:38:30.328868] Test:  [210/345]  eta: 0:00:23  loss: 0.7664 (0.7609)  time: 0.1790  data: 0.0001  max mem: 15812
[23:38:32.130447] Test:  [220/345]  eta: 0:00:22  loss: 0.7619 (0.7607)  time: 0.1797  data: 0.0001  max mem: 15812
[23:38:33.933456] Test:  [230/345]  eta: 0:00:20  loss: 0.7589 (0.7605)  time: 0.1802  data: 0.0001  max mem: 15812
[23:38:35.737476] Test:  [240/345]  eta: 0:00:18  loss: 0.7585 (0.7604)  time: 0.1803  data: 0.0001  max mem: 15812
[23:38:37.552131] Test:  [250/345]  eta: 0:00:16  loss: 0.7613 (0.7606)  time: 0.1809  data: 0.0001  max mem: 15812
[23:38:39.369233] Test:  [260/345]  eta: 0:00:15  loss: 0.7666 (0.7607)  time: 0.1815  data: 0.0001  max mem: 15812
[23:38:41.192686] Test:  [270/345]  eta: 0:00:13  loss: 0.7645 (0.7609)  time: 0.1819  data: 0.0001  max mem: 15812
[23:38:43.021018] Test:  [280/345]  eta: 0:00:11  loss: 0.7609 (0.7609)  time: 0.1825  data: 0.0001  max mem: 15812
[23:38:44.855410] Test:  [290/345]  eta: 0:00:09  loss: 0.7480 (0.7605)  time: 0.1831  data: 0.0001  max mem: 15812
[23:38:46.694561] Test:  [300/345]  eta: 0:00:07  loss: 0.7601 (0.7606)  time: 0.1836  data: 0.0001  max mem: 15812
[23:38:48.539490] Test:  [310/345]  eta: 0:00:06  loss: 0.7630 (0.7609)  time: 0.1841  data: 0.0001  max mem: 15812
[23:38:50.388522] Test:  [320/345]  eta: 0:00:04  loss: 0.7572 (0.7607)  time: 0.1846  data: 0.0001  max mem: 15812
[23:38:52.242561] Test:  [330/345]  eta: 0:00:02  loss: 0.7494 (0.7604)  time: 0.1851  data: 0.0001  max mem: 15812
[23:38:54.102751] Test:  [340/345]  eta: 0:00:00  loss: 0.7563 (0.7602)  time: 0.1857  data: 0.0001  max mem: 15812
[23:38:54.849183] Test:  [344/345]  eta: 0:00:00  loss: 0.7564 (0.7601)  time: 0.1859  data: 0.0001  max mem: 15812
[23:38:54.917237] Test: Total time: 0:01:01 (0.1789 s / it)
[23:39:11.759232] Test:  [ 0/57]  eta: 0:00:31  loss: 0.8827 (0.8827)  time: 0.5480  data: 0.3836  max mem: 15812
[23:39:13.421813] Test:  [10/57]  eta: 0:00:09  loss: 0.8827 (0.8843)  time: 0.2009  data: 0.0350  max mem: 15812
[23:39:15.092588] Test:  [20/57]  eta: 0:00:06  loss: 0.8802 (0.8764)  time: 0.1666  data: 0.0001  max mem: 15812
[23:39:16.768046] Test:  [30/57]  eta: 0:00:04  loss: 0.7767 (0.8385)  time: 0.1672  data: 0.0001  max mem: 15812
[23:39:18.449973] Test:  [40/57]  eta: 0:00:03  loss: 0.7546 (0.8192)  time: 0.1678  data: 0.0001  max mem: 15812
[23:39:20.136078] Test:  [50/57]  eta: 0:00:01  loss: 0.7449 (0.8109)  time: 0.1683  data: 0.0001  max mem: 15812
[23:39:21.047133] Test:  [56/57]  eta: 0:00:00  loss: 0.7689 (0.8165)  time: 0.1635  data: 0.0001  max mem: 15812
[23:39:21.119100] Test: Total time: 0:00:09 (0.1738 s / it)
[23:39:23.946462] Dice score of the network on the train images: 0.709681, val images: 0.762348
[23:39:23.950653] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:39:24.923670] Epoch: [18]  [  0/345]  eta: 0:05:35  lr: 0.000113  loss: 0.7781 (0.7781)  time: 0.9722  data: 0.3752  max mem: 15812
[23:39:36.856456] Epoch: [18]  [ 20/345]  eta: 0:03:19  lr: 0.000113  loss: 0.7823 (0.7846)  time: 0.5966  data: 0.0001  max mem: 15812
[23:39:48.836875] Epoch: [18]  [ 40/345]  eta: 0:03:05  lr: 0.000113  loss: 0.7858 (0.7875)  time: 0.5990  data: 0.0001  max mem: 15812
[23:40:00.839811] Epoch: [18]  [ 60/345]  eta: 0:02:52  lr: 0.000114  loss: 0.7860 (0.7869)  time: 0.6001  data: 0.0001  max mem: 15812
[23:40:12.863194] Epoch: [18]  [ 80/345]  eta: 0:02:40  lr: 0.000114  loss: 0.7780 (0.7847)  time: 0.6011  data: 0.0001  max mem: 15812
[23:40:24.924409] Epoch: [18]  [100/345]  eta: 0:02:27  lr: 0.000114  loss: 0.7834 (0.7841)  time: 0.6030  data: 0.0001  max mem: 15812
[23:40:36.987470] Epoch: [18]  [120/345]  eta: 0:02:15  lr: 0.000115  loss: 0.7934 (0.7848)  time: 0.6031  data: 0.0001  max mem: 15812
[23:40:49.043476] Epoch: [18]  [140/345]  eta: 0:02:03  lr: 0.000115  loss: 0.7903 (0.7853)  time: 0.6028  data: 0.0001  max mem: 15812
[23:41:01.081685] Epoch: [18]  [160/345]  eta: 0:01:51  lr: 0.000115  loss: 0.7731 (0.7847)  time: 0.6019  data: 0.0001  max mem: 15812
[23:41:13.131370] Epoch: [18]  [180/345]  eta: 0:01:39  lr: 0.000116  loss: 0.7775 (0.7843)  time: 0.6024  data: 0.0001  max mem: 15812
[23:41:25.184051] Epoch: [18]  [200/345]  eta: 0:01:27  lr: 0.000116  loss: 0.7758 (0.7837)  time: 0.6026  data: 0.0001  max mem: 15812
[23:41:37.238020] Epoch: [18]  [220/345]  eta: 0:01:15  lr: 0.000116  loss: 0.7948 (0.7841)  time: 0.6027  data: 0.0001  max mem: 15812
[23:41:49.266962] Epoch: [18]  [240/345]  eta: 0:01:03  lr: 0.000117  loss: 0.7847 (0.7840)  time: 0.6014  data: 0.0001  max mem: 15812
[23:42:01.307763] Epoch: [18]  [260/345]  eta: 0:00:51  lr: 0.000117  loss: 0.7772 (0.7837)  time: 0.6020  data: 0.0001  max mem: 15812
[23:42:13.351079] Epoch: [18]  [280/345]  eta: 0:00:39  lr: 0.000118  loss: 0.7832 (0.7842)  time: 0.6021  data: 0.0001  max mem: 15812
[23:42:25.397777] Epoch: [18]  [300/345]  eta: 0:00:27  lr: 0.000118  loss: 0.7830 (0.7842)  time: 0.6023  data: 0.0001  max mem: 15812
[23:42:37.431899] Epoch: [18]  [320/345]  eta: 0:00:15  lr: 0.000118  loss: 0.7766 (0.7842)  time: 0.6017  data: 0.0001  max mem: 15812
[23:42:49.531626] Epoch: [18]  [340/345]  eta: 0:00:03  lr: 0.000119  loss: 0.7969 (0.7847)  time: 0.6049  data: 0.0001  max mem: 15812
[23:42:51.939426] Epoch: [18]  [344/345]  eta: 0:00:00  lr: 0.000119  loss: 0.7969 (0.7846)  time: 0.6052  data: 0.0001  max mem: 15812
[23:42:52.017801] Epoch: [18] Total time: 0:03:28 (0.6031 s / it)
[23:42:52.018034] Averaged stats: lr: 0.000119  loss: 0.7969 (0.7846)
[23:42:52.603248] Test:  [  0/345]  eta: 0:03:20  loss: 0.7455 (0.7455)  time: 0.5800  data: 0.4140  max mem: 15812
[23:42:54.289844] Test:  [ 10/345]  eta: 0:01:09  loss: 0.7434 (0.7439)  time: 0.2060  data: 0.0377  max mem: 15812
[23:42:55.982060] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7541 (0.7549)  time: 0.1689  data: 0.0001  max mem: 15812
[23:42:57.679145] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7583 (0.7552)  time: 0.1694  data: 0.0001  max mem: 15812
[23:42:59.381533] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7518 (0.7537)  time: 0.1699  data: 0.0001  max mem: 15812
[23:43:01.088497] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7526 (0.7537)  time: 0.1704  data: 0.0001  max mem: 15812
[23:43:02.801043] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7526 (0.7532)  time: 0.1709  data: 0.0001  max mem: 15812
[23:43:04.518923] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7413 (0.7511)  time: 0.1715  data: 0.0001  max mem: 15812
[23:43:06.242950] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7464 (0.7514)  time: 0.1720  data: 0.0001  max mem: 15812
[23:43:07.972109] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7481 (0.7512)  time: 0.1726  data: 0.0001  max mem: 15812
[23:43:09.705116] Test:  [100/345]  eta: 0:00:42  loss: 0.7425 (0.7507)  time: 0.1730  data: 0.0001  max mem: 15812
[23:43:11.444038] Test:  [110/345]  eta: 0:00:41  loss: 0.7486 (0.7511)  time: 0.1735  data: 0.0001  max mem: 15812
[23:43:13.187991] Test:  [120/345]  eta: 0:00:39  loss: 0.7457 (0.7510)  time: 0.1741  data: 0.0001  max mem: 15812
[23:43:14.935934] Test:  [130/345]  eta: 0:00:37  loss: 0.7539 (0.7513)  time: 0.1745  data: 0.0001  max mem: 15812
[23:43:16.690850] Test:  [140/345]  eta: 0:00:35  loss: 0.7523 (0.7509)  time: 0.1751  data: 0.0001  max mem: 15812
[23:43:18.449594] Test:  [150/345]  eta: 0:00:34  loss: 0.7420 (0.7506)  time: 0.1756  data: 0.0001  max mem: 15812
[23:43:20.214127] Test:  [160/345]  eta: 0:00:32  loss: 0.7464 (0.7511)  time: 0.1761  data: 0.0001  max mem: 15812
[23:43:21.984403] Test:  [170/345]  eta: 0:00:30  loss: 0.7561 (0.7515)  time: 0.1767  data: 0.0001  max mem: 15812
[23:43:23.758920] Test:  [180/345]  eta: 0:00:28  loss: 0.7563 (0.7517)  time: 0.1772  data: 0.0001  max mem: 15812
[23:43:25.539685] Test:  [190/345]  eta: 0:00:27  loss: 0.7563 (0.7522)  time: 0.1777  data: 0.0001  max mem: 15812
[23:43:27.326158] Test:  [200/345]  eta: 0:00:25  loss: 0.7560 (0.7524)  time: 0.1783  data: 0.0001  max mem: 15812
[23:43:29.117047] Test:  [210/345]  eta: 0:00:23  loss: 0.7482 (0.7519)  time: 0.1788  data: 0.0001  max mem: 15812
[23:43:30.912524] Test:  [220/345]  eta: 0:00:21  loss: 0.7423 (0.7516)  time: 0.1793  data: 0.0001  max mem: 15812
[23:43:32.713201] Test:  [230/345]  eta: 0:00:20  loss: 0.7489 (0.7517)  time: 0.1798  data: 0.0001  max mem: 15812
[23:43:34.517820] Test:  [240/345]  eta: 0:00:18  loss: 0.7489 (0.7518)  time: 0.1802  data: 0.0001  max mem: 15812
[23:43:36.329709] Test:  [250/345]  eta: 0:00:16  loss: 0.7479 (0.7515)  time: 0.1808  data: 0.0001  max mem: 15812
[23:43:38.145435] Test:  [260/345]  eta: 0:00:15  loss: 0.7562 (0.7521)  time: 0.1813  data: 0.0001  max mem: 15812
[23:43:39.966127] Test:  [270/345]  eta: 0:00:13  loss: 0.7529 (0.7521)  time: 0.1818  data: 0.0001  max mem: 15812
[23:43:41.795026] Test:  [280/345]  eta: 0:00:11  loss: 0.7434 (0.7521)  time: 0.1824  data: 0.0001  max mem: 15812
[23:43:43.628369] Test:  [290/345]  eta: 0:00:09  loss: 0.7526 (0.7525)  time: 0.1830  data: 0.0001  max mem: 15812
[23:43:45.464876] Test:  [300/345]  eta: 0:00:07  loss: 0.7526 (0.7523)  time: 0.1834  data: 0.0001  max mem: 15812
[23:43:47.306755] Test:  [310/345]  eta: 0:00:06  loss: 0.7437 (0.7521)  time: 0.1839  data: 0.0001  max mem: 15812
[23:43:49.153388] Test:  [320/345]  eta: 0:00:04  loss: 0.7529 (0.7520)  time: 0.1844  data: 0.0001  max mem: 15812
[23:43:51.005403] Test:  [330/345]  eta: 0:00:02  loss: 0.7547 (0.7522)  time: 0.1849  data: 0.0001  max mem: 15812
[23:43:52.863126] Test:  [340/345]  eta: 0:00:00  loss: 0.7531 (0.7521)  time: 0.1854  data: 0.0001  max mem: 15812
[23:43:53.607511] Test:  [344/345]  eta: 0:00:00  loss: 0.7452 (0.7520)  time: 0.1856  data: 0.0001  max mem: 15812
[23:43:53.681606] Test: Total time: 0:01:01 (0.1787 s / it)
[23:44:10.583577] Test:  [ 0/57]  eta: 0:00:30  loss: 0.8441 (0.8441)  time: 0.5419  data: 0.3779  max mem: 15812
[23:44:12.244496] Test:  [10/57]  eta: 0:00:09  loss: 0.8706 (0.8672)  time: 0.2002  data: 0.0344  max mem: 15812
[23:44:13.913834] Test:  [20/57]  eta: 0:00:06  loss: 0.8732 (0.8679)  time: 0.1664  data: 0.0001  max mem: 15812
[23:44:15.589174] Test:  [30/57]  eta: 0:00:04  loss: 0.7815 (0.8304)  time: 0.1672  data: 0.0001  max mem: 15812
[23:44:17.270888] Test:  [40/57]  eta: 0:00:02  loss: 0.7610 (0.8114)  time: 0.1678  data: 0.0001  max mem: 15812
[23:44:18.957455] Test:  [50/57]  eta: 0:00:01  loss: 0.7576 (0.8061)  time: 0.1684  data: 0.0001  max mem: 15812
[23:44:19.868684] Test:  [56/57]  eta: 0:00:00  loss: 0.7750 (0.8118)  time: 0.1635  data: 0.0001  max mem: 15812
[23:44:19.939218] Test: Total time: 0:00:09 (0.1737 s / it)
[23:44:22.759231] Dice score of the network on the train images: 0.703361, val images: 0.757070
[23:44:22.764102] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:44:23.776695] Epoch: [19]  [  0/345]  eta: 0:05:49  lr: 0.000119  loss: 0.7692 (0.7692)  time: 1.0117  data: 0.4140  max mem: 15812
[23:44:35.708382] Epoch: [19]  [ 20/345]  eta: 0:03:20  lr: 0.000119  loss: 0.7797 (0.7844)  time: 0.5965  data: 0.0001  max mem: 15812
[23:44:47.676783] Epoch: [19]  [ 40/345]  eta: 0:03:05  lr: 0.000119  loss: 0.7855 (0.7852)  time: 0.5984  data: 0.0001  max mem: 15812
[23:44:59.677003] Epoch: [19]  [ 60/345]  eta: 0:02:52  lr: 0.000120  loss: 0.7744 (0.7834)  time: 0.6000  data: 0.0001  max mem: 15812
[23:45:11.699200] Epoch: [19]  [ 80/345]  eta: 0:02:40  lr: 0.000120  loss: 0.7775 (0.7833)  time: 0.6011  data: 0.0001  max mem: 15812
[23:45:23.748632] Epoch: [19]  [100/345]  eta: 0:02:27  lr: 0.000121  loss: 0.7862 (0.7851)  time: 0.6024  data: 0.0001  max mem: 15812
[23:45:35.785974] Epoch: [19]  [120/345]  eta: 0:02:15  lr: 0.000121  loss: 0.7825 (0.7852)  time: 0.6018  data: 0.0001  max mem: 15812
[23:45:47.824385] Epoch: [19]  [140/345]  eta: 0:02:03  lr: 0.000121  loss: 0.7852 (0.7853)  time: 0.6019  data: 0.0001  max mem: 15812
[23:45:59.859167] Epoch: [19]  [160/345]  eta: 0:01:51  lr: 0.000122  loss: 0.7823 (0.7857)  time: 0.6017  data: 0.0001  max mem: 15812
[23:46:11.910498] Epoch: [19]  [180/345]  eta: 0:01:39  lr: 0.000122  loss: 0.7767 (0.7855)  time: 0.6025  data: 0.0001  max mem: 15812
[23:46:23.944496] Epoch: [19]  [200/345]  eta: 0:01:27  lr: 0.000122  loss: 0.7713 (0.7842)  time: 0.6017  data: 0.0001  max mem: 15812
[23:46:35.975432] Epoch: [19]  [220/345]  eta: 0:01:15  lr: 0.000123  loss: 0.7742 (0.7837)  time: 0.6015  data: 0.0001  max mem: 15812
[23:46:48.012458] Epoch: [19]  [240/345]  eta: 0:01:03  lr: 0.000123  loss: 0.7819 (0.7843)  time: 0.6018  data: 0.0001  max mem: 15812
[23:47:00.036982] Epoch: [19]  [260/345]  eta: 0:00:51  lr: 0.000123  loss: 0.7783 (0.7844)  time: 0.6012  data: 0.0001  max mem: 15812
[23:47:12.069737] Epoch: [19]  [280/345]  eta: 0:00:39  lr: 0.000124  loss: 0.7822 (0.7844)  time: 0.6016  data: 0.0001  max mem: 15812
[23:47:24.106961] Epoch: [19]  [300/345]  eta: 0:00:27  lr: 0.000124  loss: 0.7820 (0.7845)  time: 0.6018  data: 0.0001  max mem: 15812
[23:47:36.146122] Epoch: [19]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.7738 (0.7838)  time: 0.6019  data: 0.0001  max mem: 15812
[23:47:48.167002] Epoch: [19]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.7668 (0.7833)  time: 0.6010  data: 0.0001  max mem: 15812
[23:47:50.576173] Epoch: [19]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.7707 (0.7833)  time: 0.6013  data: 0.0001  max mem: 15812
[23:47:50.652509] Epoch: [19] Total time: 0:03:27 (0.6026 s / it)
[23:47:50.653342] Averaged stats: lr: 0.000125  loss: 0.7707 (0.7833)
[23:47:51.260151] Test:  [  0/345]  eta: 0:03:27  loss: 0.7425 (0.7425)  time: 0.6006  data: 0.4349  max mem: 15812
[23:47:52.948006] Test:  [ 10/345]  eta: 0:01:09  loss: 0.7350 (0.7391)  time: 0.2079  data: 0.0396  max mem: 15812
[23:47:54.639701] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7378 (0.7414)  time: 0.1689  data: 0.0001  max mem: 15812
[23:47:56.336779] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7403 (0.7393)  time: 0.1694  data: 0.0001  max mem: 15812
[23:47:58.040108] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7429 (0.7430)  time: 0.1700  data: 0.0001  max mem: 15812
[23:47:59.747648] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7454 (0.7430)  time: 0.1705  data: 0.0001  max mem: 15812
[23:48:01.461466] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7442 (0.7436)  time: 0.1710  data: 0.0001  max mem: 15812
[23:48:03.180855] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7415 (0.7426)  time: 0.1716  data: 0.0001  max mem: 15812
[23:48:04.906026] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7409 (0.7432)  time: 0.1722  data: 0.0001  max mem: 15812
[23:48:06.635669] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7465 (0.7436)  time: 0.1727  data: 0.0001  max mem: 15812
[23:48:08.369921] Test:  [100/345]  eta: 0:00:42  loss: 0.7443 (0.7436)  time: 0.1731  data: 0.0001  max mem: 15812
[23:48:10.108102] Test:  [110/345]  eta: 0:00:41  loss: 0.7415 (0.7432)  time: 0.1736  data: 0.0001  max mem: 15812
[23:48:11.852568] Test:  [120/345]  eta: 0:00:39  loss: 0.7357 (0.7431)  time: 0.1741  data: 0.0001  max mem: 15812
[23:48:13.603110] Test:  [130/345]  eta: 0:00:37  loss: 0.7428 (0.7432)  time: 0.1747  data: 0.0001  max mem: 15812
[23:48:15.358449] Test:  [140/345]  eta: 0:00:35  loss: 0.7483 (0.7439)  time: 0.1752  data: 0.0001  max mem: 15812
[23:48:17.118781] Test:  [150/345]  eta: 0:00:34  loss: 0.7517 (0.7443)  time: 0.1757  data: 0.0001  max mem: 15812
[23:48:18.884815] Test:  [160/345]  eta: 0:00:32  loss: 0.7496 (0.7445)  time: 0.1763  data: 0.0001  max mem: 15812
[23:48:20.654608] Test:  [170/345]  eta: 0:00:30  loss: 0.7444 (0.7446)  time: 0.1767  data: 0.0001  max mem: 15812
[23:48:22.431389] Test:  [180/345]  eta: 0:00:28  loss: 0.7359 (0.7442)  time: 0.1773  data: 0.0001  max mem: 15812
[23:48:24.213059] Test:  [190/345]  eta: 0:00:27  loss: 0.7449 (0.7443)  time: 0.1779  data: 0.0001  max mem: 15812
[23:48:26.001055] Test:  [200/345]  eta: 0:00:25  loss: 0.7530 (0.7448)  time: 0.1784  data: 0.0001  max mem: 15812
[23:48:27.792131] Test:  [210/345]  eta: 0:00:23  loss: 0.7472 (0.7447)  time: 0.1789  data: 0.0001  max mem: 15812
[23:48:29.592333] Test:  [220/345]  eta: 0:00:22  loss: 0.7472 (0.7449)  time: 0.1795  data: 0.0001  max mem: 15812
[23:48:31.394364] Test:  [230/345]  eta: 0:00:20  loss: 0.7515 (0.7449)  time: 0.1800  data: 0.0001  max mem: 15812
[23:48:33.199989] Test:  [240/345]  eta: 0:00:18  loss: 0.7359 (0.7450)  time: 0.1803  data: 0.0001  max mem: 15812
[23:48:35.011722] Test:  [250/345]  eta: 0:00:16  loss: 0.7363 (0.7450)  time: 0.1808  data: 0.0001  max mem: 15812
[23:48:36.828041] Test:  [260/345]  eta: 0:00:15  loss: 0.7381 (0.7448)  time: 0.1813  data: 0.0001  max mem: 15812
[23:48:38.651087] Test:  [270/345]  eta: 0:00:13  loss: 0.7407 (0.7447)  time: 0.1819  data: 0.0001  max mem: 15812
[23:48:40.478465] Test:  [280/345]  eta: 0:00:11  loss: 0.7496 (0.7451)  time: 0.1825  data: 0.0001  max mem: 15812
[23:48:42.311375] Test:  [290/345]  eta: 0:00:09  loss: 0.7469 (0.7452)  time: 0.1829  data: 0.0001  max mem: 15812
[23:48:44.147683] Test:  [300/345]  eta: 0:00:07  loss: 0.7469 (0.7454)  time: 0.1834  data: 0.0001  max mem: 15812
[23:48:45.988900] Test:  [310/345]  eta: 0:00:06  loss: 0.7496 (0.7455)  time: 0.1838  data: 0.0001  max mem: 15812
[23:48:47.835820] Test:  [320/345]  eta: 0:00:04  loss: 0.7398 (0.7453)  time: 0.1843  data: 0.0001  max mem: 15812
[23:48:49.688710] Test:  [330/345]  eta: 0:00:02  loss: 0.7329 (0.7449)  time: 0.1849  data: 0.0001  max mem: 15812
[23:48:51.546944] Test:  [340/345]  eta: 0:00:00  loss: 0.7404 (0.7452)  time: 0.1855  data: 0.0001  max mem: 15812
[23:48:52.291182] Test:  [344/345]  eta: 0:00:00  loss: 0.7418 (0.7451)  time: 0.1857  data: 0.0001  max mem: 15812
[23:48:52.358180] Test: Total time: 0:01:01 (0.1788 s / it)
[23:49:09.249155] Test:  [ 0/57]  eta: 0:00:31  loss: 0.9004 (0.9004)  time: 0.5516  data: 0.3871  max mem: 15812
[23:49:10.911354] Test:  [10/57]  eta: 0:00:09  loss: 0.8995 (0.8800)  time: 0.2012  data: 0.0353  max mem: 15812
[23:49:12.581596] Test:  [20/57]  eta: 0:00:06  loss: 0.8824 (0.8716)  time: 0.1665  data: 0.0001  max mem: 15812
[23:49:14.256225] Test:  [30/57]  eta: 0:00:04  loss: 0.7658 (0.8319)  time: 0.1672  data: 0.0001  max mem: 15812
[23:49:15.937235] Test:  [40/57]  eta: 0:00:03  loss: 0.7494 (0.8119)  time: 0.1677  data: 0.0001  max mem: 15812
[23:49:17.624345] Test:  [50/57]  eta: 0:00:01  loss: 0.7436 (0.8051)  time: 0.1683  data: 0.0001  max mem: 15812
[23:49:18.536651] Test:  [56/57]  eta: 0:00:00  loss: 0.7588 (0.8110)  time: 0.1636  data: 0.0000  max mem: 15812
[23:49:18.616301] Test: Total time: 0:00:09 (0.1740 s / it)
[23:49:21.400498] Dice score of the network on the train images: 0.710629, val images: 0.762120
[23:49:21.404483] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:49:22.384454] Epoch: [20]  [  0/345]  eta: 0:05:37  lr: 0.000125  loss: 0.7545 (0.7545)  time: 0.9790  data: 0.3815  max mem: 15812
[23:49:34.313875] Epoch: [20]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.7585 (0.7621)  time: 0.5964  data: 0.0001  max mem: 15812
[23:49:46.293294] Epoch: [20]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.7729 (0.7689)  time: 0.5989  data: 0.0001  max mem: 15812
[23:49:58.299631] Epoch: [20]  [ 60/345]  eta: 0:02:52  lr: 0.000125  loss: 0.7654 (0.7698)  time: 0.6003  data: 0.0001  max mem: 15812
[23:50:10.320572] Epoch: [20]  [ 80/345]  eta: 0:02:40  lr: 0.000125  loss: 0.7621 (0.7687)  time: 0.6010  data: 0.0001  max mem: 15812
[23:50:22.377693] Epoch: [20]  [100/345]  eta: 0:02:27  lr: 0.000125  loss: 0.7806 (0.7738)  time: 0.6028  data: 0.0001  max mem: 15812
[23:50:34.447384] Epoch: [20]  [120/345]  eta: 0:02:15  lr: 0.000125  loss: 0.7974 (0.7782)  time: 0.6034  data: 0.0001  max mem: 15812
[23:50:46.488875] Epoch: [20]  [140/345]  eta: 0:02:03  lr: 0.000125  loss: 0.7893 (0.7804)  time: 0.6020  data: 0.0001  max mem: 15812
[23:50:58.523194] Epoch: [20]  [160/345]  eta: 0:01:51  lr: 0.000125  loss: 0.7890 (0.7816)  time: 0.6017  data: 0.0001  max mem: 15812
[23:51:10.556026] Epoch: [20]  [180/345]  eta: 0:01:39  lr: 0.000125  loss: 0.7780 (0.7815)  time: 0.6016  data: 0.0001  max mem: 15812
[23:51:22.584503] Epoch: [20]  [200/345]  eta: 0:01:27  lr: 0.000125  loss: 0.7748 (0.7807)  time: 0.6014  data: 0.0001  max mem: 15812
[23:51:34.608673] Epoch: [20]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.7752 (0.7803)  time: 0.6012  data: 0.0001  max mem: 15812
[23:51:46.630397] Epoch: [20]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.7774 (0.7799)  time: 0.6010  data: 0.0001  max mem: 15812
[23:51:58.664198] Epoch: [20]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.7735 (0.7793)  time: 0.6016  data: 0.0001  max mem: 15812
[23:52:10.681797] Epoch: [20]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.7678 (0.7788)  time: 0.6008  data: 0.0001  max mem: 15812
[23:52:22.703781] Epoch: [20]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.7755 (0.7791)  time: 0.6011  data: 0.0001  max mem: 15812
[23:52:34.720566] Epoch: [20]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.7726 (0.7789)  time: 0.6008  data: 0.0001  max mem: 15812
[23:52:46.735987] Epoch: [20]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.7763 (0.7789)  time: 0.6007  data: 0.0001  max mem: 15812
[23:52:49.141196] Epoch: [20]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.7763 (0.7788)  time: 0.6008  data: 0.0001  max mem: 15812
[23:52:49.210763] Epoch: [20] Total time: 0:03:27 (0.6023 s / it)
[23:52:49.211098] Averaged stats: lr: 0.000125  loss: 0.7763 (0.7788)
[23:52:49.827842] Test:  [  0/345]  eta: 0:03:30  loss: 0.7424 (0.7424)  time: 0.6112  data: 0.4451  max mem: 15812
[23:52:51.515931] Test:  [ 10/345]  eta: 0:01:09  loss: 0.7338 (0.7336)  time: 0.2089  data: 0.0406  max mem: 15812
[23:52:53.209612] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7322 (0.7328)  time: 0.1690  data: 0.0001  max mem: 15812
[23:52:54.908180] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7339 (0.7344)  time: 0.1695  data: 0.0001  max mem: 15812
[23:52:56.611683] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7381 (0.7352)  time: 0.1700  data: 0.0001  max mem: 15812
[23:52:58.320238] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7381 (0.7357)  time: 0.1705  data: 0.0001  max mem: 15812
[23:53:00.033602] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7389 (0.7374)  time: 0.1710  data: 0.0001  max mem: 15812
[23:53:01.751699] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7351 (0.7370)  time: 0.1715  data: 0.0001  max mem: 15812
[23:53:03.475511] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7343 (0.7363)  time: 0.1720  data: 0.0001  max mem: 15812
[23:53:05.204835] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7343 (0.7371)  time: 0.1726  data: 0.0001  max mem: 15812
[23:53:06.939278] Test:  [100/345]  eta: 0:00:42  loss: 0.7352 (0.7370)  time: 0.1731  data: 0.0001  max mem: 15812
[23:53:08.678703] Test:  [110/345]  eta: 0:00:41  loss: 0.7340 (0.7369)  time: 0.1736  data: 0.0001  max mem: 15812
[23:53:10.422942] Test:  [120/345]  eta: 0:00:39  loss: 0.7321 (0.7366)  time: 0.1741  data: 0.0001  max mem: 15812
[23:53:12.172009] Test:  [130/345]  eta: 0:00:37  loss: 0.7322 (0.7369)  time: 0.1746  data: 0.0001  max mem: 15812
[23:53:13.927175] Test:  [140/345]  eta: 0:00:35  loss: 0.7404 (0.7370)  time: 0.1751  data: 0.0001  max mem: 15812
[23:53:15.686616] Test:  [150/345]  eta: 0:00:34  loss: 0.7435 (0.7376)  time: 0.1757  data: 0.0001  max mem: 15812
[23:53:17.451407] Test:  [160/345]  eta: 0:00:32  loss: 0.7302 (0.7375)  time: 0.1761  data: 0.0001  max mem: 15812
[23:53:19.220788] Test:  [170/345]  eta: 0:00:30  loss: 0.7304 (0.7374)  time: 0.1766  data: 0.0001  max mem: 15812
[23:53:20.996272] Test:  [180/345]  eta: 0:00:28  loss: 0.7340 (0.7371)  time: 0.1772  data: 0.0001  max mem: 15812
[23:53:22.778247] Test:  [190/345]  eta: 0:00:27  loss: 0.7269 (0.7366)  time: 0.1778  data: 0.0001  max mem: 15812
[23:53:24.565718] Test:  [200/345]  eta: 0:00:25  loss: 0.7261 (0.7363)  time: 0.1784  data: 0.0001  max mem: 15812
[23:53:26.355842] Test:  [210/345]  eta: 0:00:23  loss: 0.7347 (0.7369)  time: 0.1788  data: 0.0001  max mem: 15812
[23:53:28.154763] Test:  [220/345]  eta: 0:00:22  loss: 0.7364 (0.7367)  time: 0.1794  data: 0.0001  max mem: 15812
[23:53:29.957266] Test:  [230/345]  eta: 0:00:20  loss: 0.7369 (0.7369)  time: 0.1800  data: 0.0001  max mem: 15812
[23:53:31.762313] Test:  [240/345]  eta: 0:00:18  loss: 0.7376 (0.7370)  time: 0.1803  data: 0.0001  max mem: 15812
[23:53:33.573691] Test:  [250/345]  eta: 0:00:16  loss: 0.7376 (0.7372)  time: 0.1808  data: 0.0001  max mem: 15812
[23:53:35.388851] Test:  [260/345]  eta: 0:00:15  loss: 0.7316 (0.7369)  time: 0.1813  data: 0.0001  max mem: 15812
[23:53:37.211256] Test:  [270/345]  eta: 0:00:13  loss: 0.7361 (0.7372)  time: 0.1818  data: 0.0001  max mem: 15812
[23:53:39.040740] Test:  [280/345]  eta: 0:00:11  loss: 0.7456 (0.7375)  time: 0.1825  data: 0.0001  max mem: 15812
[23:53:40.873729] Test:  [290/345]  eta: 0:00:09  loss: 0.7386 (0.7377)  time: 0.1831  data: 0.0001  max mem: 15812
[23:53:42.711221] Test:  [300/345]  eta: 0:00:07  loss: 0.7332 (0.7375)  time: 0.1835  data: 0.0001  max mem: 15812
[23:53:44.554708] Test:  [310/345]  eta: 0:00:06  loss: 0.7295 (0.7372)  time: 0.1840  data: 0.0001  max mem: 15812
[23:53:46.402327] Test:  [320/345]  eta: 0:00:04  loss: 0.7388 (0.7376)  time: 0.1845  data: 0.0001  max mem: 15812
[23:53:48.255719] Test:  [330/345]  eta: 0:00:02  loss: 0.7403 (0.7376)  time: 0.1850  data: 0.0001  max mem: 15812
[23:53:50.112874] Test:  [340/345]  eta: 0:00:00  loss: 0.7388 (0.7377)  time: 0.1855  data: 0.0001  max mem: 15812
[23:53:50.858097] Test:  [344/345]  eta: 0:00:00  loss: 0.7388 (0.7378)  time: 0.1857  data: 0.0001  max mem: 15812
[23:53:50.929915] Test: Total time: 0:01:01 (0.1789 s / it)
[23:54:07.741511] Test:  [ 0/57]  eta: 0:00:34  loss: 0.8360 (0.8360)  time: 0.6013  data: 0.4369  max mem: 15812
[23:54:09.402892] Test:  [10/57]  eta: 0:00:09  loss: 0.8588 (0.8692)  time: 0.2056  data: 0.0398  max mem: 15812
[23:54:11.072627] Test:  [20/57]  eta: 0:00:06  loss: 0.8588 (0.8583)  time: 0.1665  data: 0.0001  max mem: 15812
[23:54:12.748771] Test:  [30/57]  eta: 0:00:04  loss: 0.7578 (0.8194)  time: 0.1672  data: 0.0001  max mem: 15812
[23:54:14.429910] Test:  [40/57]  eta: 0:00:03  loss: 0.7416 (0.7998)  time: 0.1678  data: 0.0001  max mem: 15812
[23:54:16.117198] Test:  [50/57]  eta: 0:00:01  loss: 0.7281 (0.7932)  time: 0.1684  data: 0.0001  max mem: 15812
[23:54:17.030379] Test:  [56/57]  eta: 0:00:00  loss: 0.7594 (0.7991)  time: 0.1636  data: 0.0000  max mem: 15812
[23:54:17.095846] Test: Total time: 0:00:09 (0.1747 s / it)
[23:54:19.921985] Dice score of the network on the train images: 0.713029, val images: 0.767367
[23:54:19.922221] saving best_dice_model_0 @ epoch 20
[23:54:21.136069] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:54:22.129913] Epoch: [21]  [  0/345]  eta: 0:05:42  lr: 0.000125  loss: 0.7811 (0.7811)  time: 0.9925  data: 0.3974  max mem: 15812
[23:54:34.046442] Epoch: [21]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.7591 (0.7617)  time: 0.5958  data: 0.0001  max mem: 15812
[23:54:46.013245] Epoch: [21]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.7689 (0.7662)  time: 0.5983  data: 0.0001  max mem: 15812
[23:54:58.006160] Epoch: [21]  [ 60/345]  eta: 0:02:52  lr: 0.000125  loss: 0.7648 (0.7672)  time: 0.5996  data: 0.0001  max mem: 15812
[23:55:10.020534] Epoch: [21]  [ 80/345]  eta: 0:02:39  lr: 0.000125  loss: 0.7916 (0.7729)  time: 0.6007  data: 0.0001  max mem: 15812
[23:55:22.065770] Epoch: [21]  [100/345]  eta: 0:02:27  lr: 0.000125  loss: 0.7787 (0.7740)  time: 0.6022  data: 0.0001  max mem: 15812
[23:55:34.122144] Epoch: [21]  [120/345]  eta: 0:02:15  lr: 0.000125  loss: 0.7869 (0.7757)  time: 0.6028  data: 0.0001  max mem: 15812
[23:55:46.177967] Epoch: [21]  [140/345]  eta: 0:02:03  lr: 0.000125  loss: 0.7951 (0.7781)  time: 0.6027  data: 0.0001  max mem: 15812
[23:55:58.237089] Epoch: [21]  [160/345]  eta: 0:01:51  lr: 0.000125  loss: 0.7790 (0.7784)  time: 0.6029  data: 0.0001  max mem: 15812
[23:56:10.286856] Epoch: [21]  [180/345]  eta: 0:01:39  lr: 0.000125  loss: 0.7885 (0.7795)  time: 0.6024  data: 0.0001  max mem: 15812
[23:56:22.345455] Epoch: [21]  [200/345]  eta: 0:01:27  lr: 0.000125  loss: 0.7670 (0.7788)  time: 0.6029  data: 0.0001  max mem: 15812
[23:56:34.390650] Epoch: [21]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.7698 (0.7781)  time: 0.6022  data: 0.0001  max mem: 15812
[23:56:46.431757] Epoch: [21]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.7662 (0.7778)  time: 0.6020  data: 0.0001  max mem: 15812
[23:56:58.477750] Epoch: [21]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.7681 (0.7775)  time: 0.6023  data: 0.0001  max mem: 15812
[23:57:10.520040] Epoch: [21]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.7647 (0.7768)  time: 0.6021  data: 0.0001  max mem: 15812
[23:57:22.563656] Epoch: [21]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.7687 (0.7761)  time: 0.6021  data: 0.0001  max mem: 15812
[23:57:34.611987] Epoch: [21]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.7724 (0.7759)  time: 0.6024  data: 0.0001  max mem: 15812
[23:57:46.641412] Epoch: [21]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.7691 (0.7755)  time: 0.6014  data: 0.0001  max mem: 15812
[23:57:49.050483] Epoch: [21]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.7691 (0.7755)  time: 0.6016  data: 0.0001  max mem: 15812
[23:57:49.122466] Epoch: [21] Total time: 0:03:27 (0.6029 s / it)
[23:57:49.122783] Averaged stats: lr: 0.000125  loss: 0.7691 (0.7755)
[23:57:49.706706] Test:  [  0/345]  eta: 0:03:19  loss: 0.7193 (0.7193)  time: 0.5789  data: 0.4133  max mem: 15812
[23:57:51.393749] Test:  [ 10/345]  eta: 0:01:08  loss: 0.7311 (0.7337)  time: 0.2059  data: 0.0376  max mem: 15812
[23:57:53.086586] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7334 (0.7348)  time: 0.1689  data: 0.0001  max mem: 15812
[23:57:54.784731] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7389 (0.7381)  time: 0.1695  data: 0.0001  max mem: 15812
[23:57:56.487581] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7457 (0.7391)  time: 0.1700  data: 0.0001  max mem: 15812
[23:57:58.194736] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7409 (0.7391)  time: 0.1704  data: 0.0001  max mem: 15812
[23:57:59.907837] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7325 (0.7383)  time: 0.1710  data: 0.0001  max mem: 15812
[23:58:01.626685] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7307 (0.7383)  time: 0.1715  data: 0.0001  max mem: 15812
[23:58:03.350729] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7393 (0.7388)  time: 0.1721  data: 0.0001  max mem: 15812
[23:58:05.080536] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7418 (0.7389)  time: 0.1726  data: 0.0001  max mem: 15812
[23:58:06.814158] Test:  [100/345]  eta: 0:00:42  loss: 0.7353 (0.7390)  time: 0.1731  data: 0.0001  max mem: 15812
[23:58:08.554127] Test:  [110/345]  eta: 0:00:41  loss: 0.7402 (0.7401)  time: 0.1736  data: 0.0001  max mem: 15812
[23:58:10.298009] Test:  [120/345]  eta: 0:00:39  loss: 0.7462 (0.7405)  time: 0.1741  data: 0.0001  max mem: 15812
[23:58:12.047712] Test:  [130/345]  eta: 0:00:37  loss: 0.7425 (0.7406)  time: 0.1746  data: 0.0001  max mem: 15812
[23:58:13.801199] Test:  [140/345]  eta: 0:00:35  loss: 0.7428 (0.7410)  time: 0.1751  data: 0.0001  max mem: 15812
[23:58:15.560616] Test:  [150/345]  eta: 0:00:34  loss: 0.7432 (0.7411)  time: 0.1756  data: 0.0001  max mem: 15812
[23:58:17.326337] Test:  [160/345]  eta: 0:00:32  loss: 0.7401 (0.7412)  time: 0.1762  data: 0.0001  max mem: 15812
[23:58:19.096450] Test:  [170/345]  eta: 0:00:30  loss: 0.7372 (0.7410)  time: 0.1767  data: 0.0001  max mem: 15812
[23:58:20.872241] Test:  [180/345]  eta: 0:00:28  loss: 0.7332 (0.7407)  time: 0.1772  data: 0.0001  max mem: 15812
[23:58:22.655073] Test:  [190/345]  eta: 0:00:27  loss: 0.7310 (0.7403)  time: 0.1779  data: 0.0001  max mem: 15812
[23:58:24.440907] Test:  [200/345]  eta: 0:00:25  loss: 0.7316 (0.7403)  time: 0.1784  data: 0.0001  max mem: 15812
[23:58:26.235224] Test:  [210/345]  eta: 0:00:23  loss: 0.7315 (0.7400)  time: 0.1789  data: 0.0001  max mem: 15812
[23:58:28.035172] Test:  [220/345]  eta: 0:00:21  loss: 0.7381 (0.7400)  time: 0.1797  data: 0.0001  max mem: 15812
[23:58:29.837399] Test:  [230/345]  eta: 0:00:20  loss: 0.7381 (0.7398)  time: 0.1800  data: 0.0001  max mem: 15812
[23:58:31.643834] Test:  [240/345]  eta: 0:00:18  loss: 0.7324 (0.7397)  time: 0.1804  data: 0.0001  max mem: 15812
[23:58:33.454173] Test:  [250/345]  eta: 0:00:16  loss: 0.7378 (0.7399)  time: 0.1808  data: 0.0001  max mem: 15812
[23:58:35.269844] Test:  [260/345]  eta: 0:00:15  loss: 0.7378 (0.7397)  time: 0.1812  data: 0.0001  max mem: 15812
[23:58:37.092380] Test:  [270/345]  eta: 0:00:13  loss: 0.7334 (0.7399)  time: 0.1818  data: 0.0001  max mem: 15812
[23:58:38.919641] Test:  [280/345]  eta: 0:00:11  loss: 0.7375 (0.7399)  time: 0.1824  data: 0.0001  max mem: 15812
[23:58:40.750988] Test:  [290/345]  eta: 0:00:09  loss: 0.7345 (0.7396)  time: 0.1829  data: 0.0001  max mem: 15812
[23:58:42.588199] Test:  [300/345]  eta: 0:00:07  loss: 0.7336 (0.7395)  time: 0.1834  data: 0.0001  max mem: 15812
[23:58:44.431030] Test:  [310/345]  eta: 0:00:06  loss: 0.7401 (0.7399)  time: 0.1839  data: 0.0001  max mem: 15812
[23:58:46.278166] Test:  [320/345]  eta: 0:00:04  loss: 0.7371 (0.7398)  time: 0.1844  data: 0.0001  max mem: 15812
[23:58:48.131924] Test:  [330/345]  eta: 0:00:02  loss: 0.7289 (0.7395)  time: 0.1850  data: 0.0001  max mem: 15812
[23:58:49.989484] Test:  [340/345]  eta: 0:00:00  loss: 0.7336 (0.7394)  time: 0.1855  data: 0.0001  max mem: 15812
[23:58:50.734315] Test:  [344/345]  eta: 0:00:00  loss: 0.7361 (0.7394)  time: 0.1857  data: 0.0001  max mem: 15812
[23:58:50.809355] Test: Total time: 0:01:01 (0.1788 s / it)
[23:59:07.505304] Test:  [ 0/57]  eta: 0:00:31  loss: 0.8391 (0.8391)  time: 0.5491  data: 0.3850  max mem: 15812
[23:59:09.166068] Test:  [10/57]  eta: 0:00:09  loss: 0.8852 (0.8756)  time: 0.2008  data: 0.0351  max mem: 15812
[23:59:10.836351] Test:  [20/57]  eta: 0:00:06  loss: 0.8934 (0.8705)  time: 0.1665  data: 0.0001  max mem: 15812
[23:59:12.511728] Test:  [30/57]  eta: 0:00:04  loss: 0.7674 (0.8330)  time: 0.1672  data: 0.0001  max mem: 15812
[23:59:14.192183] Test:  [40/57]  eta: 0:00:02  loss: 0.7598 (0.8135)  time: 0.1677  data: 0.0001  max mem: 15812
[23:59:15.880074] Test:  [50/57]  eta: 0:00:01  loss: 0.7420 (0.8057)  time: 0.1684  data: 0.0001  max mem: 15812
[23:59:16.791433] Test:  [56/57]  eta: 0:00:00  loss: 0.7594 (0.8107)  time: 0.1635  data: 0.0000  max mem: 15812
[23:59:16.860423] Test: Total time: 0:00:09 (0.1738 s / it)
[23:59:19.657469] Dice score of the network on the train images: 0.714145, val images: 0.757083
[23:59:19.661748] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:59:20.652167] Epoch: [22]  [  0/345]  eta: 0:05:41  lr: 0.000125  loss: 0.7616 (0.7616)  time: 0.9894  data: 0.3922  max mem: 15812
[23:59:32.576677] Epoch: [22]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.7644 (0.7664)  time: 0.5962  data: 0.0001  max mem: 15812
[23:59:44.554249] Epoch: [22]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.7563 (0.7650)  time: 0.5988  data: 0.0001  max mem: 15812
[23:59:56.545641] Epoch: [22]  [ 60/345]  eta: 0:02:52  lr: 0.000125  loss: 0.7587 (0.7637)  time: 0.5995  data: 0.0001  max mem: 15812
[00:00:08.570157] Epoch: [22]  [ 80/345]  eta: 0:02:39  lr: 0.000125  loss: 0.7610 (0.7635)  time: 0.6012  data: 0.0001  max mem: 15812
[00:00:20.625718] Epoch: [22]  [100/345]  eta: 0:02:27  lr: 0.000125  loss: 0.7638 (0.7642)  time: 0.6027  data: 0.0001  max mem: 15812
[00:00:32.693603] Epoch: [22]  [120/345]  eta: 0:02:15  lr: 0.000125  loss: 0.7560 (0.7632)  time: 0.6033  data: 0.0001  max mem: 15812
[00:00:44.762100] Epoch: [22]  [140/345]  eta: 0:02:03  lr: 0.000125  loss: 0.7741 (0.7653)  time: 0.6034  data: 0.0001  max mem: 15812
[00:00:56.830283] Epoch: [22]  [160/345]  eta: 0:01:51  lr: 0.000125  loss: 0.7601 (0.7651)  time: 0.6034  data: 0.0001  max mem: 15812
[00:01:08.885941] Epoch: [22]  [180/345]  eta: 0:01:39  lr: 0.000125  loss: 0.7869 (0.7677)  time: 0.6027  data: 0.0001  max mem: 15812
[00:01:20.943233] Epoch: [22]  [200/345]  eta: 0:01:27  lr: 0.000125  loss: 0.7802 (0.7695)  time: 0.6028  data: 0.0001  max mem: 15812
[00:01:33.115459] Epoch: [22]  [220/345]  eta: 0:01:15  lr: 0.000125  loss: 0.7679 (0.7695)  time: 0.6086  data: 0.0001  max mem: 15812
[00:01:45.149301] Epoch: [22]  [240/345]  eta: 0:01:03  lr: 0.000125  loss: 0.7784 (0.7703)  time: 0.6016  data: 0.0001  max mem: 15812
[00:01:57.201232] Epoch: [22]  [260/345]  eta: 0:00:51  lr: 0.000125  loss: 0.7972 (0.7724)  time: 0.6026  data: 0.0001  max mem: 15812
[00:02:09.254520] Epoch: [22]  [280/345]  eta: 0:00:39  lr: 0.000125  loss: 0.8351 (0.7767)  time: 0.6026  data: 0.0001  max mem: 15812
[00:02:21.302415] Epoch: [22]  [300/345]  eta: 0:00:27  lr: 0.000125  loss: 0.8063 (0.7791)  time: 0.6024  data: 0.0001  max mem: 15812
[00:02:33.345360] Epoch: [22]  [320/345]  eta: 0:00:15  lr: 0.000125  loss: 0.7871 (0.7799)  time: 0.6021  data: 0.0001  max mem: 15812
[00:02:45.387744] Epoch: [22]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.7784 (0.7798)  time: 0.6021  data: 0.0001  max mem: 15812
[00:02:47.794663] Epoch: [22]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.7764 (0.7796)  time: 0.6019  data: 0.0001  max mem: 15812
[00:02:47.872834] Epoch: [22] Total time: 0:03:28 (0.6035 s / it)
[00:02:47.873221] Averaged stats: lr: 0.000125  loss: 0.7764 (0.7796)
[00:02:48.449140] Test:  [  0/345]  eta: 0:03:16  loss: 0.7310 (0.7310)  time: 0.5704  data: 0.4041  max mem: 15812
[00:02:50.136712] Test:  [ 10/345]  eta: 0:01:08  loss: 0.7391 (0.7419)  time: 0.2052  data: 0.0368  max mem: 15812
[00:02:51.829834] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7420 (0.7426)  time: 0.1690  data: 0.0001  max mem: 15812
[00:02:53.528533] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7470 (0.7440)  time: 0.1695  data: 0.0001  max mem: 15812
[00:02:55.232231] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7493 (0.7458)  time: 0.1701  data: 0.0001  max mem: 15812
[00:02:56.939330] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7440 (0.7449)  time: 0.1705  data: 0.0001  max mem: 15812
[00:02:58.653959] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7440 (0.7457)  time: 0.1710  data: 0.0001  max mem: 15812
[00:03:00.372481] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7469 (0.7454)  time: 0.1716  data: 0.0001  max mem: 15812
[00:03:02.096664] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7469 (0.7454)  time: 0.1721  data: 0.0001  max mem: 15812
[00:03:03.825501] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7385 (0.7452)  time: 0.1726  data: 0.0001  max mem: 15812
[00:03:05.559167] Test:  [100/345]  eta: 0:00:42  loss: 0.7447 (0.7457)  time: 0.1731  data: 0.0001  max mem: 15812
[00:03:07.298629] Test:  [110/345]  eta: 0:00:41  loss: 0.7464 (0.7457)  time: 0.1736  data: 0.0001  max mem: 15812
[00:03:09.042528] Test:  [120/345]  eta: 0:00:39  loss: 0.7454 (0.7456)  time: 0.1741  data: 0.0001  max mem: 15812
[00:03:10.792619] Test:  [130/345]  eta: 0:00:37  loss: 0.7460 (0.7462)  time: 0.1746  data: 0.0001  max mem: 15812
[00:03:12.546711] Test:  [140/345]  eta: 0:00:35  loss: 0.7473 (0.7465)  time: 0.1751  data: 0.0001  max mem: 15812
[00:03:14.306634] Test:  [150/345]  eta: 0:00:34  loss: 0.7420 (0.7460)  time: 0.1756  data: 0.0001  max mem: 15812
[00:03:16.072338] Test:  [160/345]  eta: 0:00:32  loss: 0.7420 (0.7464)  time: 0.1762  data: 0.0001  max mem: 15812
[00:03:17.842276] Test:  [170/345]  eta: 0:00:30  loss: 0.7385 (0.7461)  time: 0.1767  data: 0.0001  max mem: 15812
[00:03:19.618273] Test:  [180/345]  eta: 0:00:28  loss: 0.7432 (0.7463)  time: 0.1772  data: 0.0001  max mem: 15812
[00:03:21.400434] Test:  [190/345]  eta: 0:00:27  loss: 0.7484 (0.7465)  time: 0.1779  data: 0.0001  max mem: 15812
[00:03:23.186728] Test:  [200/345]  eta: 0:00:25  loss: 0.7412 (0.7464)  time: 0.1784  data: 0.0001  max mem: 15812
[00:03:24.979021] Test:  [210/345]  eta: 0:00:23  loss: 0.7411 (0.7462)  time: 0.1789  data: 0.0001  max mem: 15812
[00:03:26.779183] Test:  [220/345]  eta: 0:00:21  loss: 0.7465 (0.7465)  time: 0.1796  data: 0.0001  max mem: 15812
[00:03:28.582391] Test:  [230/345]  eta: 0:00:20  loss: 0.7518 (0.7464)  time: 0.1801  data: 0.0001  max mem: 15812
[00:03:30.388088] Test:  [240/345]  eta: 0:00:18  loss: 0.7438 (0.7465)  time: 0.1804  data: 0.0001  max mem: 15812
[00:03:32.198350] Test:  [250/345]  eta: 0:00:16  loss: 0.7537 (0.7470)  time: 0.1807  data: 0.0001  max mem: 15812
[00:03:34.015069] Test:  [260/345]  eta: 0:00:15  loss: 0.7523 (0.7470)  time: 0.1813  data: 0.0001  max mem: 15812
[00:03:35.837378] Test:  [270/345]  eta: 0:00:13  loss: 0.7467 (0.7472)  time: 0.1819  data: 0.0001  max mem: 15812
[00:03:37.665554] Test:  [280/345]  eta: 0:00:11  loss: 0.7526 (0.7474)  time: 0.1825  data: 0.0001  max mem: 15812
[00:03:39.498560] Test:  [290/345]  eta: 0:00:09  loss: 0.7493 (0.7475)  time: 0.1830  data: 0.0001  max mem: 15812
[00:03:41.335007] Test:  [300/345]  eta: 0:00:07  loss: 0.7460 (0.7475)  time: 0.1834  data: 0.0001  max mem: 15812
[00:03:43.176914] Test:  [310/345]  eta: 0:00:06  loss: 0.7476 (0.7475)  time: 0.1839  data: 0.0001  max mem: 15812
[00:03:45.024306] Test:  [320/345]  eta: 0:00:04  loss: 0.7443 (0.7476)  time: 0.1844  data: 0.0001  max mem: 15812
[00:03:46.881039] Test:  [330/345]  eta: 0:00:02  loss: 0.7480 (0.7481)  time: 0.1851  data: 0.0001  max mem: 15812
[00:03:48.739080] Test:  [340/345]  eta: 0:00:00  loss: 0.7555 (0.7482)  time: 0.1856  data: 0.0001  max mem: 15812
[00:03:49.485009] Test:  [344/345]  eta: 0:00:00  loss: 0.7545 (0.7482)  time: 0.1859  data: 0.0001  max mem: 15812
[00:03:49.553766] Test: Total time: 0:01:01 (0.1788 s / it)
[00:04:06.416313] Test:  [ 0/57]  eta: 0:00:33  loss: 0.8534 (0.8534)  time: 0.5797  data: 0.4156  max mem: 15812
[00:04:08.078133] Test:  [10/57]  eta: 0:00:09  loss: 0.8600 (0.8612)  time: 0.2037  data: 0.0379  max mem: 15812
[00:04:09.749128] Test:  [20/57]  eta: 0:00:06  loss: 0.8600 (0.8510)  time: 0.1666  data: 0.0001  max mem: 15812
[00:04:11.424604] Test:  [30/57]  eta: 0:00:04  loss: 0.7685 (0.8176)  time: 0.1673  data: 0.0001  max mem: 15812
[00:04:13.106642] Test:  [40/57]  eta: 0:00:03  loss: 0.7400 (0.7997)  time: 0.1678  data: 0.0001  max mem: 15812
[00:04:14.794944] Test:  [50/57]  eta: 0:00:01  loss: 0.7377 (0.7937)  time: 0.1685  data: 0.0001  max mem: 15812
[00:04:15.707335] Test:  [56/57]  eta: 0:00:00  loss: 0.7749 (0.8000)  time: 0.1637  data: 0.0000  max mem: 15812
[00:04:15.774912] Test: Total time: 0:00:09 (0.1744 s / it)
[00:04:18.571267] Dice score of the network on the train images: 0.697401, val images: 0.757575
[00:04:18.571477] saving best_rec_model_0 @ epoch 22
[00:04:19.621998] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[00:04:20.612022] Epoch: [23]  [  0/345]  eta: 0:05:41  lr: 0.000125  loss: 0.7646 (0.7646)  time: 0.9888  data: 0.3915  max mem: 15812
[00:04:32.529998] Epoch: [23]  [ 20/345]  eta: 0:03:19  lr: 0.000125  loss: 0.7660 (0.7666)  time: 0.5958  data: 0.0001  max mem: 15812

[00:04:44.509736] Epoch: [23]  [ 40/345]  eta: 0:03:05  lr: 0.000125  loss: 0.7861 (0.7782)  time: 0.5989  data: 0.0001  max mem: 15812

[00:04:56.512002] Epoch: [23]  [ 60/345]  eta: 0:02:52  lr: 0.000125  loss: 0.7730 (0.7765)  time: 0.6001  data: 0.0001  max mem: 15812

[00:05:08.538285] Epoch: [23]  [ 80/345]  eta: 0:02:40  lr: 0.000124  loss: 0.7630 (0.7738)  time: 0.6013  data: 0.0001  max mem: 15812
[00:05:20.583152] Epoch: [23]  [100/345]  eta: 0:02:27  lr: 0.000124  loss: 0.7685 (0.7727)  time: 0.6022  data: 0.0001  max mem: 15812
[00:05:32.658934] Epoch: [23]  [120/345]  eta: 0:02:15  lr: 0.000124  loss: 0.7675 (0.7726)  time: 0.6037  data: 0.0001  max mem: 15812
[00:05:44.729205] Epoch: [23]  [140/345]  eta: 0:02:03  lr: 0.000124  loss: 0.7600 (0.7706)  time: 0.6035  data: 0.0001  max mem: 15812
[00:05:56.774970] Epoch: [23]  [160/345]  eta: 0:01:51  lr: 0.000124  loss: 0.7721 (0.7706)  time: 0.6022  data: 0.0001  max mem: 15812
[00:06:08.819835] Epoch: [23]  [180/345]  eta: 0:01:39  lr: 0.000124  loss: 0.7504 (0.7694)  time: 0.6022  data: 0.0001  max mem: 15812
[00:06:20.863816] Epoch: [23]  [200/345]  eta: 0:01:27  lr: 0.000124  loss: 0.7648 (0.7692)  time: 0.6021  data: 0.0001  max mem: 15812
[00:06:32.915169] Epoch: [23]  [220/345]  eta: 0:01:15  lr: 0.000124  loss: 0.7731 (0.7697)  time: 0.6025  data: 0.0001  max mem: 15812
[00:06:44.948898] Epoch: [23]  [240/345]  eta: 0:01:03  lr: 0.000124  loss: 0.7798 (0.7703)  time: 0.6016  data: 0.0001  max mem: 15812
[00:06:56.984086] Epoch: [23]  [260/345]  eta: 0:00:51  lr: 0.000124  loss: 0.7813 (0.7708)  time: 0.6017  data: 0.0001  max mem: 15812
[00:07:09.021099] Epoch: [23]  [280/345]  eta: 0:00:39  lr: 0.000124  loss: 0.7662 (0.7705)  time: 0.6018  data: 0.0001  max mem: 15812
[00:07:21.067319] Epoch: [23]  [300/345]  eta: 0:00:27  lr: 0.000124  loss: 0.7560 (0.7697)  time: 0.6023  data: 0.0001  max mem: 15812
[00:07:33.118022] Epoch: [23]  [320/345]  eta: 0:00:15  lr: 0.000124  loss: 0.7645 (0.7699)  time: 0.6025  data: 0.0001  max mem: 15812
[00:07:45.158966] Epoch: [23]  [340/345]  eta: 0:00:03  lr: 0.000124  loss: 0.7670 (0.7696)  time: 0.6020  data: 0.0001  max mem: 15812
[00:07:47.567022] Epoch: [23]  [344/345]  eta: 0:00:00  lr: 0.000124  loss: 0.7592 (0.7694)  time: 0.6021  data: 0.0001  max mem: 15812
[00:07:47.640312] Epoch: [23] Total time: 0:03:28 (0.6030 s / it)
[00:07:47.640497] Averaged stats: lr: 0.000124  loss: 0.7592 (0.7694)
[00:07:48.258735] Test:  [  0/345]  eta: 0:03:31  loss: 0.7434 (0.7434)  time: 0.6129  data: 0.4465  max mem: 15812
[00:07:49.946084] Test:  [ 10/345]  eta: 0:01:10  loss: 0.7434 (0.7378)  time: 0.2090  data: 0.0407  max mem: 15812
[00:07:51.638643] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7354 (0.7381)  time: 0.1689  data: 0.0001  max mem: 15812
[00:07:53.336753] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7349 (0.7370)  time: 0.1695  data: 0.0001  max mem: 15812
[00:07:55.040486] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7382 (0.7380)  time: 0.1700  data: 0.0001  max mem: 15812
[00:07:56.748272] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7430 (0.7391)  time: 0.1705  data: 0.0001  max mem: 15812
[00:07:58.462296] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7388 (0.7388)  time: 0.1710  data: 0.0001  max mem: 15812
[00:08:00.181888] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7349 (0.7378)  time: 0.1716  data: 0.0001  max mem: 15812
[00:08:01.906515] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7353 (0.7378)  time: 0.1721  data: 0.0001  max mem: 15812
[00:08:03.637444] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7353 (0.7378)  time: 0.1727  data: 0.0001  max mem: 15812
[00:08:05.370821] Test:  [100/345]  eta: 0:00:42  loss: 0.7369 (0.7380)  time: 0.1732  data: 0.0001  max mem: 15812
[00:08:07.109624] Test:  [110/345]  eta: 0:00:41  loss: 0.7389 (0.7382)  time: 0.1736  data: 0.0001  max mem: 15812
[00:08:08.854492] Test:  [120/345]  eta: 0:00:39  loss: 0.7401 (0.7382)  time: 0.1741  data: 0.0001  max mem: 15812
[00:08:10.605602] Test:  [130/345]  eta: 0:00:37  loss: 0.7370 (0.7381)  time: 0.1747  data: 0.0001  max mem: 15812
[00:08:12.361700] Test:  [140/345]  eta: 0:00:35  loss: 0.7289 (0.7375)  time: 0.1753  data: 0.0001  max mem: 15812
[00:08:14.122722] Test:  [150/345]  eta: 0:00:34  loss: 0.7280 (0.7373)  time: 0.1758  data: 0.0001  max mem: 15812
[00:08:15.887765] Test:  [160/345]  eta: 0:00:32  loss: 0.7396 (0.7377)  time: 0.1762  data: 0.0001  max mem: 15812
[00:08:17.659320] Test:  [170/345]  eta: 0:00:30  loss: 0.7437 (0.7379)  time: 0.1768  data: 0.0001  max mem: 15812
[00:08:19.437081] Test:  [180/345]  eta: 0:00:28  loss: 0.7400 (0.7380)  time: 0.1774  data: 0.0001  max mem: 15812
[00:08:21.219484] Test:  [190/345]  eta: 0:00:27  loss: 0.7368 (0.7378)  time: 0.1779  data: 0.0001  max mem: 15812
[00:08:23.004420] Test:  [200/345]  eta: 0:00:25  loss: 0.7335 (0.7379)  time: 0.1783  data: 0.0001  max mem: 15812
[00:08:24.799513] Test:  [210/345]  eta: 0:00:23  loss: 0.7352 (0.7379)  time: 0.1789  data: 0.0001  max mem: 15812
[00:08:26.599777] Test:  [220/345]  eta: 0:00:22  loss: 0.7352 (0.7379)  time: 0.1797  data: 0.0001  max mem: 15812
[00:08:28.404826] Test:  [230/345]  eta: 0:00:20  loss: 0.7372 (0.7379)  time: 0.1801  data: 0.0001  max mem: 15812
[00:08:30.210136] Test:  [240/345]  eta: 0:00:18  loss: 0.7327 (0.7376)  time: 0.1804  data: 0.0001  max mem: 15812
[00:08:32.021870] Test:  [250/345]  eta: 0:00:16  loss: 0.7313 (0.7375)  time: 0.1808  data: 0.0001  max mem: 15812
[00:08:33.837280] Test:  [260/345]  eta: 0:00:15  loss: 0.7370 (0.7377)  time: 0.1813  data: 0.0001  max mem: 15812
[00:08:35.660916] Test:  [270/345]  eta: 0:00:13  loss: 0.7398 (0.7376)  time: 0.1819  data: 0.0001  max mem: 15812
[00:08:37.489034] Test:  [280/345]  eta: 0:00:11  loss: 0.7276 (0.7373)  time: 0.1825  data: 0.0001  max mem: 15812
[00:08:39.321832] Test:  [290/345]  eta: 0:00:09  loss: 0.7258 (0.7370)  time: 0.1830  data: 0.0001  max mem: 15812
[00:08:41.160144] Test:  [300/345]  eta: 0:00:07  loss: 0.7355 (0.7370)  time: 0.1835  data: 0.0001  max mem: 15812
[00:08:43.002241] Test:  [310/345]  eta: 0:00:06  loss: 0.7355 (0.7371)  time: 0.1840  data: 0.0001  max mem: 15812
[00:08:44.851141] Test:  [320/345]  eta: 0:00:04  loss: 0.7343 (0.7368)  time: 0.1845  data: 0.0001  max mem: 15812
[00:08:46.704804] Test:  [330/345]  eta: 0:00:02  loss: 0.7314 (0.7367)  time: 0.1851  data: 0.0001  max mem: 15812
[00:08:48.563343] Test:  [340/345]  eta: 0:00:00  loss: 0.7326 (0.7366)  time: 0.1856  data: 0.0001  max mem: 15812
[00:08:49.307235] Test:  [344/345]  eta: 0:00:00  loss: 0.7350 (0.7367)  time: 0.1857  data: 0.0001  max mem: 15812
[00:08:49.379438] Test: Total time: 0:01:01 (0.1789 s / it)
[00:09:06.280960] Test:  [ 0/57]  eta: 0:00:31  loss: 0.8366 (0.8366)  time: 0.5455  data: 0.3814  max mem: 15812
[00:09:07.944944] Test:  [10/57]  eta: 0:00:09  loss: 0.8609 (0.8684)  time: 0.2008  data: 0.0348  max mem: 15812
[00:09:09.615597] Test:  [20/57]  eta: 0:00:06  loss: 0.8755 (0.8589)  time: 0.1667  data: 0.0001  max mem: 15812
[00:09:11.290983] Test:  [30/57]  eta: 0:00:04  loss: 0.7588 (0.8229)  time: 0.1672  data: 0.0001  max mem: 15812
[00:09:12.973493] Test:  [40/57]  eta: 0:00:02  loss: 0.7476 (0.8041)  time: 0.1678  data: 0.0001  max mem: 15812
[00:09:14.661308] Test:  [50/57]  eta: 0:00:01  loss: 0.7455 (0.7965)  time: 0.1685  data: 0.0001  max mem: 15812
[00:09:15.574362] Test:  [56/57]  eta: 0:00:00  loss: 0.7681 (0.8018)  time: 0.1637  data: 0.0001  max mem: 15812
[00:09:15.648508] Test: Total time: 0:00:09 (0.1739 s / it)
[00:09:18.509729] Dice score of the network on the train images: 0.706885, val images: 0.762858
[00:09:18.513794] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[00:09:19.555885] Epoch: [24]  [  0/345]  eta: 0:05:59  lr: 0.000124  loss: 0.7598 (0.7598)  time: 1.0412  data: 0.4436  max mem: 15812
[00:09:31.473202] Epoch: [24]  [ 20/345]  eta: 0:03:20  lr: 0.000124  loss: 0.7704 (0.7682)  time: 0.5958  data: 0.0001  max mem: 15812
[00:09:43.444253] Epoch: [24]  [ 40/345]  eta: 0:03:05  lr: 0.000124  loss: 0.7608 (0.7671)  time: 0.5985  data: 0.0001  max mem: 15812
[00:09:55.454523] Epoch: [24]  [ 60/345]  eta: 0:02:52  lr: 0.000124  loss: 0.7539 (0.7641)  time: 0.6005  data: 0.0001  max mem: 15812
[00:10:07.485227] Epoch: [24]  [ 80/345]  eta: 0:02:40  lr: 0.000124  loss: 0.7580 (0.7636)  time: 0.6015  data: 0.0001  max mem: 15812
[00:10:19.532166] Epoch: [24]  [100/345]  eta: 0:02:28  lr: 0.000124  loss: 0.7593 (0.7636)  time: 0.6023  data: 0.0001  max mem: 15812
[00:10:31.603466] Epoch: [24]  [120/345]  eta: 0:02:15  lr: 0.000124  loss: 0.7529 (0.7616)  time: 0.6035  data: 0.0001  max mem: 15812
[00:10:43.671050] Epoch: [24]  [140/345]  eta: 0:02:03  lr: 0.000124  loss: 0.7505 (0.7608)  time: 0.6033  data: 0.0001  max mem: 15812
[00:10:55.736384] Epoch: [24]  [160/345]  eta: 0:01:51  lr: 0.000124  loss: 0.7550 (0.7603)  time: 0.6032  data: 0.0001  max mem: 15812
[00:11:07.798883] Epoch: [24]  [180/345]  eta: 0:01:39  lr: 0.000124  loss: 0.7591 (0.7605)  time: 0.6031  data: 0.0001  max mem: 15812
[00:11:19.859777] Epoch: [24]  [200/345]  eta: 0:01:27  lr: 0.000124  loss: 0.7541 (0.7603)  time: 0.6030  data: 0.0001  max mem: 15812
[00:11:31.915289] Epoch: [24]  [220/345]  eta: 0:01:15  lr: 0.000124  loss: 0.7573 (0.7604)  time: 0.6027  data: 0.0001  max mem: 15812
[00:11:43.963108] Epoch: [24]  [240/345]  eta: 0:01:03  lr: 0.000124  loss: 0.7653 (0.7608)  time: 0.6024  data: 0.0001  max mem: 15812
[00:11:56.005073] Epoch: [24]  [260/345]  eta: 0:00:51  lr: 0.000124  loss: 0.7512 (0.7603)  time: 0.6021  data: 0.0001  max mem: 15812
[00:12:08.058732] Epoch: [24]  [280/345]  eta: 0:00:39  lr: 0.000124  loss: 0.7578 (0.7602)  time: 0.6026  data: 0.0001  max mem: 15812
[00:12:20.111087] Epoch: [24]  [300/345]  eta: 0:00:27  lr: 0.000124  loss: 0.7639 (0.7605)  time: 0.6026  data: 0.0001  max mem: 15812
[00:12:32.162635] Epoch: [24]  [320/345]  eta: 0:00:15  lr: 0.000124  loss: 0.7631 (0.7607)  time: 0.6025  data: 0.0001  max mem: 15812
[00:12:44.182445] Epoch: [24]  [340/345]  eta: 0:00:03  lr: 0.000124  loss: 0.7573 (0.7607)  time: 0.6009  data: 0.0001  max mem: 15812
[00:12:46.585024] Epoch: [24]  [344/345]  eta: 0:00:00  lr: 0.000124  loss: 0.7537 (0.7606)  time: 0.6008  data: 0.0001  max mem: 15812
[00:12:46.664715] Epoch: [24] Total time: 0:03:28 (0.6033 s / it)
[00:12:46.664952] Averaged stats: lr: 0.000124  loss: 0.7537 (0.7606)
[00:12:47.286025] Test:  [  0/345]  eta: 0:03:32  loss: 0.7442 (0.7442)  time: 0.6159  data: 0.4494  max mem: 15812
[00:12:48.974024] Test:  [ 10/345]  eta: 0:01:10  loss: 0.7316 (0.7339)  time: 0.2094  data: 0.0409  max mem: 15812
[00:12:50.665986] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7334 (0.7354)  time: 0.1689  data: 0.0001  max mem: 15812
[00:12:52.363433] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7281 (0.7319)  time: 0.1694  data: 0.0001  max mem: 15812
[00:12:54.065161] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7289 (0.7348)  time: 0.1699  data: 0.0001  max mem: 15812
[00:12:55.773339] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7317 (0.7338)  time: 0.1704  data: 0.0001  max mem: 15812
[00:12:57.485608] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7297 (0.7332)  time: 0.1710  data: 0.0001  max mem: 15812
[00:12:59.204539] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7297 (0.7331)  time: 0.1715  data: 0.0001  max mem: 15812
[00:13:00.926926] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7229 (0.7316)  time: 0.1720  data: 0.0001  max mem: 15812
[00:13:02.656090] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7256 (0.7320)  time: 0.1725  data: 0.0001  max mem: 15812
[00:13:04.389835] Test:  [100/345]  eta: 0:00:42  loss: 0.7337 (0.7317)  time: 0.1731  data: 0.0001  max mem: 15812
[00:13:06.128394] Test:  [110/345]  eta: 0:00:41  loss: 0.7285 (0.7320)  time: 0.1736  data: 0.0001  max mem: 15812
[00:13:07.873024] Test:  [120/345]  eta: 0:00:39  loss: 0.7363 (0.7326)  time: 0.1741  data: 0.0001  max mem: 15812
[00:13:09.622392] Test:  [130/345]  eta: 0:00:37  loss: 0.7357 (0.7327)  time: 0.1746  data: 0.0001  max mem: 15812
[00:13:11.376127] Test:  [140/345]  eta: 0:00:35  loss: 0.7265 (0.7322)  time: 0.1751  data: 0.0001  max mem: 15812
[00:13:13.136466] Test:  [150/345]  eta: 0:00:34  loss: 0.7262 (0.7319)  time: 0.1756  data: 0.0001  max mem: 15812
[00:13:14.901826] Test:  [160/345]  eta: 0:00:32  loss: 0.7331 (0.7322)  time: 0.1762  data: 0.0001  max mem: 15812
[00:13:16.672541] Test:  [170/345]  eta: 0:00:30  loss: 0.7302 (0.7320)  time: 0.1767  data: 0.0001  max mem: 15812
[00:13:18.447549] Test:  [180/345]  eta: 0:00:28  loss: 0.7279 (0.7319)  time: 0.1772  data: 0.0001  max mem: 15812
[00:13:20.229290] Test:  [190/345]  eta: 0:00:27  loss: 0.7308 (0.7322)  time: 0.1778  data: 0.0001  max mem: 15812
[00:13:22.014725] Test:  [200/345]  eta: 0:00:25  loss: 0.7297 (0.7319)  time: 0.1783  data: 0.0001  max mem: 15812
[00:13:23.805137] Test:  [210/345]  eta: 0:00:23  loss: 0.7297 (0.7320)  time: 0.1787  data: 0.0001  max mem: 15812
[00:13:25.604910] Test:  [220/345]  eta: 0:00:22  loss: 0.7307 (0.7318)  time: 0.1794  data: 0.0001  max mem: 15812
[00:13:27.407576] Test:  [230/345]  eta: 0:00:20  loss: 0.7253 (0.7314)  time: 0.1801  data: 0.0001  max mem: 15812
[00:13:29.213725] Test:  [240/345]  eta: 0:00:18  loss: 0.7247 (0.7314)  time: 0.1804  data: 0.0001  max mem: 15812
[00:13:31.023580] Test:  [250/345]  eta: 0:00:16  loss: 0.7228 (0.7313)  time: 0.1807  data: 0.0001  max mem: 15812
[00:13:32.838933] Test:  [260/345]  eta: 0:00:15  loss: 0.7224 (0.7309)  time: 0.1812  data: 0.0001  max mem: 15812
[00:13:34.660443] Test:  [270/345]  eta: 0:00:13  loss: 0.7313 (0.7312)  time: 0.1818  data: 0.0001  max mem: 15812
[00:13:36.487436] Test:  [280/345]  eta: 0:00:11  loss: 0.7316 (0.7311)  time: 0.1824  data: 0.0001  max mem: 15812
[00:13:38.319332] Test:  [290/345]  eta: 0:00:09  loss: 0.7279 (0.7312)  time: 0.1829  data: 0.0001  max mem: 15812
[00:13:40.156518] Test:  [300/345]  eta: 0:00:07  loss: 0.7218 (0.7308)  time: 0.1834  data: 0.0001  max mem: 15812
[00:13:41.998412] Test:  [310/345]  eta: 0:00:06  loss: 0.7212 (0.7308)  time: 0.1839  data: 0.0001  max mem: 15812
[00:13:43.845650] Test:  [320/345]  eta: 0:00:04  loss: 0.7255 (0.7307)  time: 0.1844  data: 0.0001  max mem: 15812
[00:13:45.699825] Test:  [330/345]  eta: 0:00:02  loss: 0.7296 (0.7309)  time: 0.1850  data: 0.0001  max mem: 15812
[00:13:47.558265] Test:  [340/345]  eta: 0:00:00  loss: 0.7387 (0.7311)  time: 0.1856  data: 0.0001  max mem: 15812
[00:13:48.302716] Test:  [344/345]  eta: 0:00:00  loss: 0.7359 (0.7312)  time: 0.1857  data: 0.0001  max mem: 15812
[00:13:48.368726] Test: Total time: 0:01:01 (0.1788 s / it)
[00:14:05.141202] Test:  [ 0/57]  eta: 0:00:33  loss: 0.8537 (0.8537)  time: 0.5853  data: 0.4216  max mem: 15812
[00:14:06.802863] Test:  [10/57]  eta: 0:00:09  loss: 0.8537 (0.8749)  time: 0.2042  data: 0.0384  max mem: 15812
[00:14:08.473455] Test:  [20/57]  eta: 0:00:06  loss: 0.8781 (0.8656)  time: 0.1665  data: 0.0001  max mem: 15812
[00:14:10.149846] Test:  [30/57]  eta: 0:00:04  loss: 0.7552 (0.8253)  time: 0.1673  data: 0.0001  max mem: 15812
[00:14:11.830549] Test:  [40/57]  eta: 0:00:03  loss: 0.7448 (0.8052)  time: 0.1678  data: 0.0001  max mem: 15812
[00:14:13.519142] Test:  [50/57]  eta: 0:00:01  loss: 0.7428 (0.7981)  time: 0.1684  data: 0.0001  max mem: 15812
[00:14:14.431352] Test:  [56/57]  eta: 0:00:00  loss: 0.7616 (0.8034)  time: 0.1636  data: 0.0000  max mem: 15812
[00:14:14.500978] Test: Total time: 0:00:09 (0.1745 s / it)
[00:14:17.307464] Dice score of the network on the train images: 0.714274, val images: 0.767402
[00:14:17.307685] saving best_dice_model_0 @ epoch 24
[00:14:18.435357] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[00:14:19.456915] Epoch: [25]  [  0/345]  eta: 0:05:52  lr: 0.000124  loss: 0.7526 (0.7526)  time: 1.0206  data: 0.4246  max mem: 15812
[00:14:31.373471] Epoch: [25]  [ 20/345]  eta: 0:03:20  lr: 0.000124  loss: 0.7584 (0.7570)  time: 0.5958  data: 0.0001  max mem: 15812
[00:14:43.334426] Epoch: [25]  [ 40/345]  eta: 0:03:05  lr: 0.000124  loss: 0.7458 (0.7540)  time: 0.5980  data: 0.0001  max mem: 15812
[00:14:55.337374] Epoch: [25]  [ 60/345]  eta: 0:02:52  lr: 0.000124  loss: 0.7665 (0.7580)  time: 0.6001  data: 0.0001  max mem: 15812
[00:15:07.359690] Epoch: [25]  [ 80/345]  eta: 0:02:40  lr: 0.000124  loss: 0.7618 (0.7583)  time: 0.6011  data: 0.0001  max mem: 15812
[00:15:19.403529] Epoch: [25]  [100/345]  eta: 0:02:27  lr: 0.000124  loss: 0.7509 (0.7578)  time: 0.6021  data: 0.0001  max mem: 15812
[00:15:31.437166] Epoch: [25]  [120/345]  eta: 0:02:15  lr: 0.000124  loss: 0.7538 (0.7573)  time: 0.6016  data: 0.0001  max mem: 15812
[00:15:43.478521] Epoch: [25]  [140/345]  eta: 0:02:03  lr: 0.000124  loss: 0.7498 (0.7563)  time: 0.6020  data: 0.0001  max mem: 15812
[00:15:55.538255] Epoch: [25]  [160/345]  eta: 0:01:51  lr: 0.000124  loss: 0.7427 (0.7554)  time: 0.6029  data: 0.0001  max mem: 15812
[00:16:07.593724] Epoch: [25]  [180/345]  eta: 0:01:39  lr: 0.000124  loss: 0.7620 (0.7556)  time: 0.6027  data: 0.0001  max mem: 15812
[00:16:19.640730] Epoch: [25]  [200/345]  eta: 0:01:27  lr: 0.000124  loss: 0.7390 (0.7540)  time: 0.6023  data: 0.0001  max mem: 15812
[00:16:31.672043] Epoch: [25]  [220/345]  eta: 0:01:15  lr: 0.000123  loss: 0.7450 (0.7537)  time: 0.6015  data: 0.0001  max mem: 15812
[00:16:43.708947] Epoch: [25]  [240/345]  eta: 0:01:03  lr: 0.000123  loss: 0.7491 (0.7534)  time: 0.6018  data: 0.0001  max mem: 15812
[00:16:55.755967] Epoch: [25]  [260/345]  eta: 0:00:51  lr: 0.000123  loss: 0.7584 (0.7538)  time: 0.6023  data: 0.0001  max mem: 15812
[00:17:07.801328] Epoch: [25]  [280/345]  eta: 0:00:39  lr: 0.000123  loss: 0.7526 (0.7537)  time: 0.6022  data: 0.0001  max mem: 15812
[00:17:19.820157] Epoch: [25]  [300/345]  eta: 0:00:27  lr: 0.000123  loss: 0.7449 (0.7532)  time: 0.6009  data: 0.0001  max mem: 15812
[00:17:31.835911] Epoch: [25]  [320/345]  eta: 0:00:15  lr: 0.000123  loss: 0.7440 (0.7531)  time: 0.6007  data: 0.0001  max mem: 15812
[00:17:43.849485] Epoch: [25]  [340/345]  eta: 0:00:03  lr: 0.000123  loss: 0.7451 (0.7527)  time: 0.6006  data: 0.0001  max mem: 15812
[00:17:46.252038] Epoch: [25]  [344/345]  eta: 0:00:00  lr: 0.000123  loss: 0.7484 (0.7528)  time: 0.6005  data: 0.0001  max mem: 15812
[00:17:46.319441] Epoch: [25] Total time: 0:03:27 (0.6026 s / it)
[00:17:46.319566] Averaged stats: lr: 0.000123  loss: 0.7484 (0.7528)
[00:17:46.919646] Test:  [  0/345]  eta: 0:03:25  loss: 0.7031 (0.7031)  time: 0.5963  data: 0.4301  max mem: 15812
[00:17:48.608397] Test:  [ 10/345]  eta: 0:01:09  loss: 0.7194 (0.7256)  time: 0.2077  data: 0.0392  max mem: 15812
[00:17:50.300583] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7194 (0.7228)  time: 0.1690  data: 0.0001  max mem: 15812
[00:17:52.000059] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7147 (0.7221)  time: 0.1694  data: 0.0001  max mem: 15812
[00:17:53.702826] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7234 (0.7230)  time: 0.1700  data: 0.0001  max mem: 15812
[00:17:55.410093] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7253 (0.7216)  time: 0.1704  data: 0.0001  max mem: 15812
[00:17:57.122068] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7141 (0.7217)  time: 0.1709  data: 0.0001  max mem: 15812
[00:17:58.841501] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7246 (0.7221)  time: 0.1715  data: 0.0001  max mem: 15812
[00:18:00.565333] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7203 (0.7216)  time: 0.1721  data: 0.0001  max mem: 15812
[00:18:02.293288] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7189 (0.7218)  time: 0.1725  data: 0.0001  max mem: 15812
[00:18:04.026665] Test:  [100/345]  eta: 0:00:42  loss: 0.7189 (0.7214)  time: 0.1730  data: 0.0001  max mem: 15812
[00:18:05.766401] Test:  [110/345]  eta: 0:00:41  loss: 0.7247 (0.7227)  time: 0.1736  data: 0.0001  max mem: 15812
[00:18:07.510143] Test:  [120/345]  eta: 0:00:39  loss: 0.7304 (0.7232)  time: 0.1741  data: 0.0001  max mem: 15812
[00:18:09.260830] Test:  [130/345]  eta: 0:00:37  loss: 0.7203 (0.7227)  time: 0.1747  data: 0.0001  max mem: 15812
[00:18:11.014736] Test:  [140/345]  eta: 0:00:35  loss: 0.7177 (0.7231)  time: 0.1752  data: 0.0001  max mem: 15812
[00:18:12.774974] Test:  [150/345]  eta: 0:00:34  loss: 0.7251 (0.7232)  time: 0.1756  data: 0.0001  max mem: 15812
[00:18:14.542636] Test:  [160/345]  eta: 0:00:32  loss: 0.7215 (0.7233)  time: 0.1763  data: 0.0001  max mem: 15812
[00:18:16.312926] Test:  [170/345]  eta: 0:00:30  loss: 0.7177 (0.7232)  time: 0.1768  data: 0.0001  max mem: 15812
[00:18:18.088361] Test:  [180/345]  eta: 0:00:28  loss: 0.7193 (0.7232)  time: 0.1772  data: 0.0001  max mem: 15812
[00:18:19.868659] Test:  [190/345]  eta: 0:00:27  loss: 0.7230 (0.7233)  time: 0.1777  data: 0.0001  max mem: 15812
[00:18:21.654762] Test:  [200/345]  eta: 0:00:25  loss: 0.7233 (0.7236)  time: 0.1783  data: 0.0001  max mem: 15812
[00:18:23.447891] Test:  [210/345]  eta: 0:00:23  loss: 0.7169 (0.7232)  time: 0.1789  data: 0.0001  max mem: 15812
[00:18:25.249004] Test:  [220/345]  eta: 0:00:22  loss: 0.7156 (0.7233)  time: 0.1797  data: 0.0001  max mem: 15812
[00:18:27.052323] Test:  [230/345]  eta: 0:00:20  loss: 0.7270 (0.7237)  time: 0.1802  data: 0.0001  max mem: 15812
[00:18:28.857568] Test:  [240/345]  eta: 0:00:18  loss: 0.7287 (0.7239)  time: 0.1804  data: 0.0001  max mem: 15812
[00:18:30.668559] Test:  [250/345]  eta: 0:00:16  loss: 0.7268 (0.7241)  time: 0.1808  data: 0.0001  max mem: 15812
[00:18:32.486270] Test:  [260/345]  eta: 0:00:15  loss: 0.7262 (0.7242)  time: 0.1814  data: 0.0001  max mem: 15812
[00:18:34.309829] Test:  [270/345]  eta: 0:00:13  loss: 0.7229 (0.7240)  time: 0.1820  data: 0.0001  max mem: 15812
[00:18:36.137108] Test:  [280/345]  eta: 0:00:11  loss: 0.7202 (0.7239)  time: 0.1825  data: 0.0001  max mem: 15812
[00:18:37.968888] Test:  [290/345]  eta: 0:00:09  loss: 0.7208 (0.7240)  time: 0.1829  data: 0.0001  max mem: 15812
[00:18:39.807065] Test:  [300/345]  eta: 0:00:07  loss: 0.7208 (0.7238)  time: 0.1834  data: 0.0001  max mem: 15812
[00:18:41.651383] Test:  [310/345]  eta: 0:00:06  loss: 0.7218 (0.7237)  time: 0.1841  data: 0.0001  max mem: 15812
[00:18:43.498158] Test:  [320/345]  eta: 0:00:04  loss: 0.7218 (0.7238)  time: 0.1845  data: 0.0001  max mem: 15812
[00:18:45.352407] Test:  [330/345]  eta: 0:00:02  loss: 0.7222 (0.7237)  time: 0.1850  data: 0.0001  max mem: 15812
[00:18:47.211435] Test:  [340/345]  eta: 0:00:00  loss: 0.7267 (0.7239)  time: 0.1856  data: 0.0001  max mem: 15812
[00:18:47.956101] Test:  [344/345]  eta: 0:00:00  loss: 0.7267 (0.7239)  time: 0.1858  data: 0.0001  max mem: 15812
[00:18:48.029405] Test: Total time: 0:01:01 (0.1789 s / it)
[00:19:04.802415] Test:  [ 0/57]  eta: 0:00:32  loss: 0.8389 (0.8389)  time: 0.5638  data: 0.3996  max mem: 15812
[00:19:06.463968] Test:  [10/57]  eta: 0:00:09  loss: 0.8547 (0.8648)  time: 0.2022  data: 0.0364  max mem: 15812
[00:19:08.132712] Test:  [20/57]  eta: 0:00:06  loss: 0.8652 (0.8599)  time: 0.1664  data: 0.0001  max mem: 15812
[00:19:09.808237] Test:  [30/57]  eta: 0:00:04  loss: 0.7665 (0.8225)  time: 0.1672  data: 0.0001  max mem: 15812
[00:19:11.490701] Test:  [40/57]  eta: 0:00:03  loss: 0.7392 (0.8031)  time: 0.1678  data: 0.0001  max mem: 15812
[00:19:13.179246] Test:  [50/57]  eta: 0:00:01  loss: 0.7345 (0.7946)  time: 0.1685  data: 0.0001  max mem: 15812
[00:19:14.090412] Test:  [56/57]  eta: 0:00:00  loss: 0.7568 (0.7995)  time: 0.1636  data: 0.0001  max mem: 15812
[00:19:14.161581] Test: Total time: 0:00:09 (0.1741 s / it)
[00:19:16.980095] Dice score of the network on the train images: 0.715870, val images: 0.773029
[00:19:16.980339] saving best_dice_model_0 @ epoch 25
[00:19:17.999265] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[00:19:19.062625] Epoch: [26]  [  0/345]  eta: 0:06:06  lr: 0.000123  loss: 0.7291 (0.7291)  time: 1.0622  data: 0.4651  max mem: 15812
[00:19:30.981750] Epoch: [26]  [ 20/345]  eta: 0:03:20  lr: 0.000123  loss: 0.7538 (0.7568)  time: 0.5959  data: 0.0001  max mem: 15812
[00:19:42.943955] Epoch: [26]  [ 40/345]  eta: 0:03:05  lr: 0.000123  loss: 0.7649 (0.7616)  time: 0.5981  data: 0.0001  max mem: 15812
[00:19:54.942452] Epoch: [26]  [ 60/345]  eta: 0:02:52  lr: 0.000123  loss: 0.7771 (0.7659)  time: 0.5999  data: 0.0001  max mem: 15812
[00:20:06.965801] Epoch: [26]  [ 80/345]  eta: 0:02:40  lr: 0.000123  loss: 0.7496 (0.7633)  time: 0.6011  data: 0.0001  max mem: 15812
[00:20:18.999107] Epoch: [26]  [100/345]  eta: 0:02:27  lr: 0.000123  loss: 0.7402 (0.7604)  time: 0.6016  data: 0.0001  max mem: 15812
[00:20:31.045795] Epoch: [26]  [120/345]  eta: 0:02:15  lr: 0.000123  loss: 0.7470 (0.7597)  time: 0.6023  data: 0.0001  max mem: 15812
[00:20:43.087344] Epoch: [26]  [140/345]  eta: 0:02:03  lr: 0.000123  loss: 0.7471 (0.7582)  time: 0.6020  data: 0.0001  max mem: 15812
[00:20:55.138388] Epoch: [26]  [160/345]  eta: 0:01:51  lr: 0.000123  loss: 0.7488 (0.7576)  time: 0.6025  data: 0.0001  max mem: 15812
[00:21:07.223422] Epoch: [26]  [180/345]  eta: 0:01:39  lr: 0.000123  loss: 0.7621 (0.7584)  time: 0.6042  data: 0.0001  max mem: 15812
[00:21:19.253813] Epoch: [26]  [200/345]  eta: 0:01:27  lr: 0.000123  loss: 0.7599 (0.7587)  time: 0.6015  data: 0.0001  max mem: 15812
[00:21:31.270286] Epoch: [26]  [220/345]  eta: 0:01:15  lr: 0.000123  loss: 0.7578 (0.7584)  time: 0.6008  data: 0.0001  max mem: 15812
[00:21:43.289969] Epoch: [26]  [240/345]  eta: 0:01:03  lr: 0.000123  loss: 0.7493 (0.7580)  time: 0.6009  data: 0.0001  max mem: 15812
[00:21:55.304725] Epoch: [26]  [260/345]  eta: 0:00:51  lr: 0.000123  loss: 0.7451 (0.7571)  time: 0.6007  data: 0.0001  max mem: 15812
[00:22:07.316251] Epoch: [26]  [280/345]  eta: 0:00:39  lr: 0.000123  loss: 0.7523 (0.7566)  time: 0.6005  data: 0.0001  max mem: 15812
[00:22:19.336671] Epoch: [26]  [300/345]  eta: 0:00:27  lr: 0.000123  loss: 0.7575 (0.7567)  time: 0.6010  data: 0.0001  max mem: 15812
[00:22:31.361434] Epoch: [26]  [320/345]  eta: 0:00:15  lr: 0.000123  loss: 0.7536 (0.7563)  time: 0.6012  data: 0.0001  max mem: 15812
[00:22:43.386871] Epoch: [26]  [340/345]  eta: 0:00:03  lr: 0.000123  loss: 0.7476 (0.7558)  time: 0.6012  data: 0.0001  max mem: 15812
[00:22:45.789435] Epoch: [26]  [344/345]  eta: 0:00:00  lr: 0.000123  loss: 0.7476 (0.7557)  time: 0.6011  data: 0.0001  max mem: 15812
[00:22:45.864547] Epoch: [26] Total time: 0:03:27 (0.6025 s / it)
[00:22:45.864840] Averaged stats: lr: 0.000123  loss: 0.7476 (0.7557)
[00:22:46.479070] Test:  [  0/345]  eta: 0:03:29  loss: 0.7213 (0.7213)  time: 0.6066  data: 0.4403  max mem: 15812
[00:22:48.167691] Test:  [ 10/345]  eta: 0:01:09  loss: 0.7296 (0.7322)  time: 0.2086  data: 0.0401  max mem: 15812
[00:22:49.860461] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7305 (0.7327)  time: 0.1690  data: 0.0001  max mem: 15812
[00:22:51.558372] Test:  [ 30/345]  eta: 0:00:57  loss: 0.7259 (0.7298)  time: 0.1695  data: 0.0001  max mem: 15812
[00:22:53.262375] Test:  [ 40/345]  eta: 0:00:54  loss: 0.7197 (0.7269)  time: 0.1700  data: 0.0001  max mem: 15812
[00:22:54.970096] Test:  [ 50/345]  eta: 0:00:52  loss: 0.7235 (0.7276)  time: 0.1705  data: 0.0001  max mem: 15812
[00:22:56.683131] Test:  [ 60/345]  eta: 0:00:50  loss: 0.7249 (0.7269)  time: 0.1710  data: 0.0001  max mem: 15812
[00:22:58.401777] Test:  [ 70/345]  eta: 0:00:48  loss: 0.7227 (0.7273)  time: 0.1715  data: 0.0001  max mem: 15812
[00:23:00.126431] Test:  [ 80/345]  eta: 0:00:46  loss: 0.7191 (0.7269)  time: 0.1721  data: 0.0001  max mem: 15812
[00:23:01.855905] Test:  [ 90/345]  eta: 0:00:44  loss: 0.7191 (0.7267)  time: 0.1726  data: 0.0001  max mem: 15812
[00:23:03.590510] Test:  [100/345]  eta: 0:00:42  loss: 0.7307 (0.7272)  time: 0.1731  data: 0.0001  max mem: 15812
[00:23:05.330914] Test:  [110/345]  eta: 0:00:41  loss: 0.7320 (0.7275)  time: 0.1737  data: 0.0001  max mem: 15812
[00:23:07.075518] Test:  [120/345]  eta: 0:00:39  loss: 0.7240 (0.7270)  time: 0.1742  data: 0.0001  max mem: 15812
[00:23:08.826435] Test:  [130/345]  eta: 0:00:37  loss: 0.7238 (0.7272)  time: 0.1747  data: 0.0001  max mem: 15812
[00:23:10.581034] Test:  [140/345]  eta: 0:00:35  loss: 0.7206 (0.7268)  time: 0.1752  data: 0.0001  max mem: 15812
[00:23:12.341806] Test:  [150/345]  eta: 0:00:34  loss: 0.7262 (0.7273)  time: 0.1757  data: 0.0001  max mem: 15812
[00:23:14.106650] Test:  [160/345]  eta: 0:00:32  loss: 0.7175 (0.7263)  time: 0.1762  data: 0.0001  max mem: 15812
[00:23:15.877840] Test:  [170/345]  eta: 0:00:30  loss: 0.7175 (0.7261)  time: 0.1767  data: 0.0001  max mem: 15812
[00:23:17.653609] Test:  [180/345]  eta: 0:00:28  loss: 0.7233 (0.7263)  time: 0.1773  data: 0.0001  max mem: 15812
[00:23:19.433821] Test:  [190/345]  eta: 0:00:27  loss: 0.7227 (0.7261)  time: 0.1777  data: 0.0001  max mem: 15812
[00:23:21.220536] Test:  [200/345]  eta: 0:00:25  loss: 0.7220 (0.7260)  time: 0.1783  data: 0.0001  max mem: 15812
[00:23:23.012157] Test:  [210/345]  eta: 0:00:23  loss: 0.7220 (0.7261)  time: 0.1789  data: 0.0001  max mem: 15812
[00:23:24.812472] Test:  [220/345]  eta: 0:00:22  loss: 0.7243 (0.7262)  time: 0.1795  data: 0.0001  max mem: 15812
[00:23:26.615151] Test:  [230/345]  eta: 0:00:20  loss: 0.7243 (0.7260)  time: 0.1801  data: 0.0001  max mem: 15812
[00:23:28.420962] Test:  [240/345]  eta: 0:00:18  loss: 0.7234 (0.7261)  time: 0.1804  data: 0.0001  max mem: 15812
[00:23:30.234283] Test:  [250/345]  eta: 0:00:16  loss: 0.7218 (0.7261)  time: 0.1809  data: 0.0001  max mem: 15812
[00:23:32.049802] Test:  [260/345]  eta: 0:00:15  loss: 0.7207 (0.7260)  time: 0.1814  data: 0.0001  max mem: 15812
[00:23:33.871509] Test:  [270/345]  eta: 0:00:13  loss: 0.7243 (0.7260)  time: 0.1818  data: 0.0001  max mem: 15812
[00:23:35.698698] Test:  [280/345]  eta: 0:00:11  loss: 0.7244 (0.7259)  time: 0.1824  data: 0.0001  max mem: 15812
[00:23:37.532162] Test:  [290/345]  eta: 0:00:09  loss: 0.7198 (0.7259)  time: 0.1830  data: 0.0001  max mem: 15812
[00:23:39.369619] Test:  [300/345]  eta: 0:00:07  loss: 0.7217 (0.7260)  time: 0.1835  data: 0.0001  max mem: 15812
[00:23:41.214753] Test:  [310/345]  eta: 0:00:06  loss: 0.7263 (0.7261)  time: 0.1841  data: 0.0001  max mem: 15812
[00:23:43.062522] Test:  [320/345]  eta: 0:00:04  loss: 0.7259 (0.7262)  time: 0.1846  data: 0.0001  max mem: 15812
[00:23:44.916651] Test:  [330/345]  eta: 0:00:02  loss: 0.7253 (0.7259)  time: 0.1850  data: 0.0001  max mem: 15812
[00:23:46.775686] Test:  [340/345]  eta: 0:00:00  loss: 0.7153 (0.7258)  time: 0.1856  data: 0.0001  max mem: 15812
[00:23:47.520493] Test:  [344/345]  eta: 0:00:00  loss: 0.7180 (0.7257)  time: 0.1858  data: 0.0001  max mem: 15812
[00:23:47.585528] Test: Total time: 0:01:01 (0.1789 s / it)
[00:24:04.420752] Test:  [ 0/57]  eta: 0:00:30  loss: 0.8742 (0.8742)  time: 0.5432  data: 0.3786  max mem: 15812
[00:24:06.082414] Test:  [10/57]  eta: 0:00:09  loss: 0.8894 (0.8829)  time: 0.2004  data: 0.0345  max mem: 15812
[00:24:07.753029] Test:  [20/57]  eta: 0:00:06  loss: 0.8894 (0.8745)  time: 0.1665  data: 0.0001  max mem: 15812
[00:24:09.430838] Test:  [30/57]  eta: 0:00:04  loss: 0.7721 (0.8338)  time: 0.1674  data: 0.0001  max mem: 15812
[00:24:11.114399] Test:  [40/57]  eta: 0:00:02  loss: 0.7461 (0.8125)  time: 0.1680  data: 0.0001  max mem: 15812
[00:24:12.804371] Test:  [50/57]  eta: 0:00:01  loss: 0.7400 (0.8044)  time: 0.1686  data: 0.0001  max mem: 15812
[00:24:13.716808] Test:  [56/57]  eta: 0:00:00  loss: 0.7591 (0.8104)  time: 0.1637  data: 0.0001  max mem: 15812
[00:24:13.787296] Test: Total time: 0:00:09 (0.1739 s / it)
[00:24:16.581703] Dice score of the network on the train images: 0.721299, val images: 0.769087
[00:24:16.586014] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[00:24:17.591256] Epoch: [27]  [  0/345]  eta: 0:05:46  lr: 0.000123  loss: 0.7718 (0.7718)  time: 1.0044  data: 0.4068  max mem: 15812
[00:24:29.498050] Epoch: [27]  [ 20/345]  eta: 0:03:19  lr: 0.000123  loss: 0.7468 (0.7514)  time: 0.5953  data: 0.0001  max mem: 15812
[00:24:41.449604] Epoch: [27]  [ 40/345]  eta: 0:03:04  lr: 0.000123  loss: 0.7424 (0.7485)  time: 0.5975  data: 0.0001  max mem: 15812
[00:24:53.435203] Epoch: [27]  [ 60/345]  eta: 0:02:52  lr: 0.000123  loss: 0.7401 (0.7472)  time: 0.5992  data: 0.0001  max mem: 15812
[00:25:05.457002] Epoch: [27]  [ 80/345]  eta: 0:02:39  lr: 0.000122  loss: 0.7451 (0.7479)  time: 0.6010  data: 0.0001  max mem: 15812
[00:25:17.494505] Epoch: [27]  [100/345]  eta: 0:02:27  lr: 0.000122  loss: 0.7488 (0.7478)  time: 0.6018  data: 0.0001  max mem: 15812
[00:25:29.545275] Epoch: [27]  [120/345]  eta: 0:02:15  lr: 0.000122  loss: 0.7389 (0.7471)  time: 0.6025  data: 0.0001  max mem: 15812
[00:25:41.598141] Epoch: [27]  [140/345]  eta: 0:02:03  lr: 0.000122  loss: 0.7400 (0.7462)  time: 0.6026  data: 0.0001  max mem: 15812
[00:25:53.769667] Epoch: [27]  [160/345]  eta: 0:01:51  lr: 0.000122  loss: 0.7460 (0.7464)  time: 0.6085  data: 0.0001  max mem: 15812
[00:26:05.812890] Epoch: [27]  [180/345]  eta: 0:01:39  lr: 0.000122  loss: 0.7461 (0.7467)  time: 0.6021  data: 0.0001  max mem: 15812
[00:26:16.818154] NaN detected in inputs
[00:26:16.822469] Loss is nan, stopping training
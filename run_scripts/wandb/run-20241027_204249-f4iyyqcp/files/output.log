Not using distributed mode
[20:42:51.715302] job dir: /root/seg_framework/MS-Mamba/run_scripts
[20:42:51.715452] Namespace(accum_iter=1,
model='SegFormer3D',
in_channels=1,
lr=None,
blr=0.001,
min_lr=0,
dist_on_itp=False,
warmup_epochs=20,
device='cuda:0',
seed=42,
layer_decay=0.75,
clip_grad=None,
num_workers=1,
pin_mem=True,
resume='',
mask_mode='concatenate to image',
world_size=1,
embed_dim=1,
local_rank=-1,
dist_url='env://',
nb_classes=2,
data_dir='/root/MSLesSeg24/data',
datalist=None,
preprocess=False,
dim=2,
loss='mask tp1 tp2',
distributed=False)
[20:42:51.715566] device  cuda:0
[20:42:51.716405] Random seed set as 42
[20:42:51.716704] Starting for fold 0
[20:42:51.907154] Elements in data_dir_paths: 11052
[20:42:51.941395] Elements in data_dir_paths: 1803
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/fold_0/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[20:42:53.939897] number of params: 59617303
[20:42:53.940141] model: Vivim2D(
  (encoder): mamba_block(
    (downsample_layers): SegformerEncoder(
      (patch_embeddings): ModuleList(
        (0): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(2, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (3): SegformerOverlapPatchEmbeddings(
          (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (block): ModuleList(
        (0): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): Identity()
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.003703703870996833)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=64, out_features=64, bias=True)
                (key): Linear(in_features=64, out_features=64, bias=True)
                (value): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=64, out_features=64, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.007407407741993666)
            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=256, out_features=64, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.011111111380159855)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.014814815483987331)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.018518518656492233)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=128, out_features=128, bias=True)
                (key): Linear(in_features=128, out_features=128, bias=True)
                (value): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=128, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.02222222276031971)
            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=512, out_features=128, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.025925926864147186)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.029629630967974663)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03333333507180214)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.03703703731298447)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04074074327945709)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.04444444552063942)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.048148151487112045)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.051851850003004074)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0555555559694767)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.05925925821065903)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06296296417713165)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.06666667014360428)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07037036865949631)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07407407462596893)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.07777778059244156)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08148147910833359)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08518518507480621)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SegformerLayer(
            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=320, out_features=320, bias=True)
                (key): Linear(in_features=320, out_features=320, bias=True)
                (value): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=320, out_features=320, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.08888889104127884)
            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=1280, out_features=320, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleList(
          (0): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.09259259700775146)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.0962962955236435)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SegformerLayer(
            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SegformerAttention(
              (self): SegformerEfficientSelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): SegformerSelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (drop_path): SegformerDropPath(p=0.10000000149011612)
            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): SegformerMixFFN(
              (dense1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): SegformerDWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (intermediate_act_fn): GELUActivation()
              (dense2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (layer_norm): ModuleList(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (act): SiLU()
              (x_proj): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_b): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_b): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_b): Linear(in_features=4, out_features=128, bias=True)
              (conv1d_s): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
              (x_proj_s): Linear(in_features=128, out_features=36, bias=False)
              (dt_proj_s): Linear(in_features=4, out_features=128, bias=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=128, out_features=512, bias=False)
              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (act): SiLU()
              (x_proj): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
              (conv1d_s): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
              (x_proj_s): Linear(in_features=256, out_features=40, bias=False)
              (dt_proj_s): Linear(in_features=8, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=128, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (2): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=320, out_features=1280, bias=False)
              (conv1d): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (act): SiLU()
              (x_proj): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_b): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_b): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_b): Linear(in_features=20, out_features=640, bias=True)
              (conv1d_s): Conv1d(640, 640, kernel_size=(4,), stride=(1,), padding=(3,), groups=640)
              (x_proj_s): Linear(in_features=640, out_features=52, bias=False)
              (dt_proj_s): Linear(in_features=20, out_features=640, bias=True)
              (out_proj): Linear(in_features=640, out_features=320, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (3): Sequential(
        (0): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MambaLayer(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mamba): Mamba(
              (in_proj): Linear(in_features=512, out_features=2048, bias=False)
              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (act): SiLU()
              (x_proj): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_b): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_b): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=32, out_features=1024, bias=True)
              (conv1d_s): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
              (x_proj_s): Linear(in_features=1024, out_features=64, bias=False)
              (dt_proj_s): Linear(in_features=32, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=512, bias=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (decoder): SegformerDecodeHead(
    (linear_c): ModuleList(
      (0): SegformerMLP(
        (proj): Linear(in_features=64, out_features=768, bias=True)
      )
      (1): SegformerMLP(
        (proj): Linear(in_features=128, out_features=768, bias=True)
      )
      (2): SegformerMLP(
        (proj): Linear(in_features=320, out_features=768, bias=True)
      )
      (3): SegformerMLP(
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
    )
    (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Conv2d(768, 150, kernel_size=(1, 1), stride=(1, 1))
  )
  (out): Conv2d(768, 1, kernel_size=(1, 1), stride=(1, 1))
)
[20:42:53.943100] base lr: 1.00e-03
[20:42:53.943160] actual lr: 1.25e-04
[20:42:53.943215] accumulate grad iterations: 1
[20:42:53.943266] effective batch size: 32
[20:42:53.944678] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.000125
    maximize: False
    weight_decay: 0.01
)
[20:42:53.946740] Start training for 50 epochs
[20:42:53.948463] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[20:42:55.561935] Epoch: [0]  [  0/345]  eta: 0:09:16  lr: 0.000000  loss: 1.6965 (1.6965)  time: 1.6124  data: 0.1924  max mem: 14473
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/val_ft
/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
[20:43:09.951529] Epoch: [0]  [ 20/345]  eta: 0:04:07  lr: 0.000000  loss: 1.6955 (1.6956)  time: 0.7194  data: 0.0001  max mem: 14938
[20:43:24.399703] Epoch: [0]  [ 40/345]  eta: 0:03:46  lr: 0.000001  loss: 1.6917 (1.6936)  time: 0.7224  data: 0.0001  max mem: 14938
[20:43:39.060321] Epoch: [0]  [ 60/345]  eta: 0:03:30  lr: 0.000001  loss: 1.6874 (1.6916)  time: 0.7330  data: 0.0001  max mem: 14938
[20:43:53.712055] Epoch: [0]  [ 80/345]  eta: 0:03:15  lr: 0.000001  loss: 1.6850 (1.6901)  time: 0.7325  data: 0.0001  max mem: 14938
[20:44:08.441678] Epoch: [0]  [100/345]  eta: 0:03:00  lr: 0.000002  loss: 1.6822 (1.6885)  time: 0.7364  data: 0.0001  max mem: 14938
[20:44:23.216506] Epoch: [0]  [120/345]  eta: 0:02:45  lr: 0.000002  loss: 1.6764 (1.6865)  time: 0.7387  data: 0.0001  max mem: 14938
[20:44:38.017927] Epoch: [0]  [140/345]  eta: 0:02:31  lr: 0.000003  loss: 1.6697 (1.6843)  time: 0.7400  data: 0.0001  max mem: 14938
[20:44:52.860591] Epoch: [0]  [160/345]  eta: 0:02:16  lr: 0.000003  loss: 1.6670 (1.6822)  time: 0.7421  data: 0.0001  max mem: 14938

[20:45:07.738761] Epoch: [0]  [180/345]  eta: 0:02:01  lr: 0.000003  loss: 1.6606 (1.6798)  time: 0.7439  data: 0.0001  max mem: 14938
[20:45:22.637105] Epoch: [0]  [200/345]  eta: 0:01:47  lr: 0.000004  loss: 1.6525 (1.6772)  time: 0.7449  data: 0.0001  max mem: 14938
[20:45:37.538672] Epoch: [0]  [220/345]  eta: 0:01:32  lr: 0.000004  loss: 1.6446 (1.6743)  time: 0.7450  data: 0.0001  max mem: 14938
[20:45:52.452742] Epoch: [0]  [240/345]  eta: 0:01:17  lr: 0.000004  loss: 1.6343 (1.6711)  time: 0.7457  data: 0.0001  max mem: 14938
[20:46:07.377812] Epoch: [0]  [260/345]  eta: 0:01:02  lr: 0.000005  loss: 1.6247 (1.6675)  time: 0.7462  data: 0.0001  max mem: 14938
[20:46:22.306339] Epoch: [0]  [280/345]  eta: 0:00:48  lr: 0.000005  loss: 1.6127 (1.6636)  time: 0.7464  data: 0.0001  max mem: 14938
[20:46:37.230372] Epoch: [0]  [300/345]  eta: 0:00:33  lr: 0.000005  loss: 1.5994 (1.6594)  time: 0.7462  data: 0.0001  max mem: 14938
[20:46:52.158473] Epoch: [0]  [320/345]  eta: 0:00:18  lr: 0.000006  loss: 1.5854 (1.6548)  time: 0.7464  data: 0.0001  max mem: 14938
[20:47:07.095221] Epoch: [0]  [340/345]  eta: 0:00:03  lr: 0.000006  loss: 1.5717 (1.6499)  time: 0.7468  data: 0.0001  max mem: 14938
[20:47:10.084934] Epoch: [0]  [344/345]  eta: 0:00:00  lr: 0.000006  loss: 1.5658 (1.6489)  time: 0.7470  data: 0.0001  max mem: 14938
[20:47:10.144711] Epoch: [0] Total time: 0:04:16 (0.7426 s / it)
[20:47:10.145195] Averaged stats: lr: 0.000006  loss: 1.5658 (1.6489)
[20:47:10.487763] Test:  [  0/345]  eta: 0:01:57  loss: 1.5937 (1.5937)  time: 0.3392  data: 0.1574  max mem: 14938
[20:47:12.324727] Test:  [ 10/345]  eta: 0:01:06  loss: 1.5937 (1.5936)  time: 0.1978  data: 0.0144  max mem: 14938
[20:47:14.163314] Test:  [ 20/345]  eta: 0:01:02  loss: 1.5941 (1.5938)  time: 0.1837  data: 0.0001  max mem: 14938
[20:47:16.007505] Test:  [ 30/345]  eta: 0:00:59  loss: 1.5925 (1.5932)  time: 0.1841  data: 0.0001  max mem: 14938
[20:47:17.856058] Test:  [ 40/345]  eta: 0:00:57  loss: 1.5925 (1.5930)  time: 0.1846  data: 0.0001  max mem: 14938
[20:47:19.707055] Test:  [ 50/345]  eta: 0:00:55  loss: 1.5935 (1.5932)  time: 0.1849  data: 0.0001  max mem: 14938
[20:47:21.562991] Test:  [ 60/345]  eta: 0:00:53  loss: 1.5935 (1.5931)  time: 0.1853  data: 0.0001  max mem: 14938
[20:47:23.422464] Test:  [ 70/345]  eta: 0:00:51  loss: 1.5913 (1.5928)  time: 0.1857  data: 0.0001  max mem: 14938
[20:47:25.284378] Test:  [ 80/345]  eta: 0:00:49  loss: 1.5913 (1.5927)  time: 0.1860  data: 0.0001  max mem: 14938
[20:47:27.151528] Test:  [ 90/345]  eta: 0:00:47  loss: 1.5924 (1.5926)  time: 0.1864  data: 0.0001  max mem: 14938
[20:47:29.021006] Test:  [100/345]  eta: 0:00:45  loss: 1.5927 (1.5926)  time: 0.1868  data: 0.0001  max mem: 14938
[20:47:30.894297] Test:  [110/345]  eta: 0:00:43  loss: 1.5931 (1.5926)  time: 0.1871  data: 0.0001  max mem: 14938
[20:47:32.769413] Test:  [120/345]  eta: 0:00:42  loss: 1.5936 (1.5927)  time: 0.1874  data: 0.0001  max mem: 14938
[20:47:34.648153] Test:  [130/345]  eta: 0:00:40  loss: 1.5920 (1.5925)  time: 0.1876  data: 0.0001  max mem: 14938
[20:47:36.533135] Test:  [140/345]  eta: 0:00:38  loss: 1.5925 (1.5925)  time: 0.1881  data: 0.0001  max mem: 14938
[20:47:38.420921] Test:  [150/345]  eta: 0:00:36  loss: 1.5931 (1.5925)  time: 0.1886  data: 0.0001  max mem: 14938
[20:47:40.312358] Test:  [160/345]  eta: 0:00:34  loss: 1.5937 (1.5926)  time: 0.1889  data: 0.0001  max mem: 14938
[20:47:42.208239] Test:  [170/345]  eta: 0:00:32  loss: 1.5926 (1.5926)  time: 0.1893  data: 0.0001  max mem: 14938
[20:47:44.103682] Test:  [180/345]  eta: 0:00:30  loss: 1.5930 (1.5925)  time: 0.1895  data: 0.0001  max mem: 14938
[20:47:46.003270] Test:  [190/345]  eta: 0:00:29  loss: 1.5934 (1.5926)  time: 0.1897  data: 0.0001  max mem: 14938

[20:47:47.907148] Test:  [200/345]  eta: 0:00:27  loss: 1.5923 (1.5926)  time: 0.1901  data: 0.0001  max mem: 14938
[20:47:50.224573] Test:  [210/345]  eta: 0:00:25  loss: 1.5923 (1.5926)  time: 0.2110  data: 0.0001  max mem: 14938
[20:47:52.145448] Test:  [220/345]  eta: 0:00:23  loss: 1.5940 (1.5926)  time: 0.2119  data: 0.0001  max mem: 14938
[20:47:54.194823] Test:  [230/345]  eta: 0:00:21  loss: 1.5944 (1.5927)  time: 0.1985  data: 0.0001  max mem: 14938
[20:47:56.114027] Test:  [240/345]  eta: 0:00:20  loss: 1.5926 (1.5926)  time: 0.1984  data: 0.0001  max mem: 14938
[20:47:58.212157] Test:  [250/345]  eta: 0:00:18  loss: 1.5926 (1.5926)  time: 0.2008  data: 0.0001  max mem: 14938
[20:48:00.310486] Test:  [260/345]  eta: 0:00:16  loss: 1.5921 (1.5926)  time: 0.2098  data: 0.0001  max mem: 14938
[20:48:02.403014] Test:  [270/345]  eta: 0:00:14  loss: 1.5934 (1.5927)  time: 0.2095  data: 0.0001  max mem: 14938
[20:48:04.366813] Test:  [280/345]  eta: 0:00:12  loss: 1.5934 (1.5926)  time: 0.2028  data: 0.0001  max mem: 14938
[20:48:06.490683] Test:  [290/345]  eta: 0:00:10  loss: 1.5909 (1.5926)  time: 0.2043  data: 0.0001  max mem: 14938
[20:48:08.686133] Test:  [300/345]  eta: 0:00:08  loss: 1.5934 (1.5927)  time: 0.2159  data: 0.0001  max mem: 14938
[20:48:10.809740] Test:  [310/345]  eta: 0:00:06  loss: 1.5937 (1.5927)  time: 0.2159  data: 0.0001  max mem: 14938
[20:48:12.909593] Test:  [320/345]  eta: 0:00:04  loss: 1.5932 (1.5927)  time: 0.2111  data: 0.0001  max mem: 14938
[20:48:15.049882] Test:  [330/345]  eta: 0:00:02  loss: 1.5927 (1.5927)  time: 0.2119  data: 0.0001  max mem: 14938
[20:48:17.082342] Test:  [340/345]  eta: 0:00:00  loss: 1.5917 (1.5926)  time: 0.2086  data: 0.0001  max mem: 14938
[20:48:18.109232] Test:  [344/345]  eta: 0:00:00  loss: 1.5918 (1.5926)  time: 0.2199  data: 0.0001  max mem: 14938
[20:48:18.169318] Test: Total time: 0:01:08 (0.1972 s / it)
[20:48:29.851526] Test:  [ 0/57]  eta: 0:00:18  loss: 1.6031 (1.6031)  time: 0.3287  data: 0.1496  max mem: 14938
[20:48:31.664520] Test:  [10/57]  eta: 0:00:09  loss: 1.5973 (1.5979)  time: 0.1946  data: 0.0137  max mem: 14938
[20:48:33.482131] Test:  [20/57]  eta: 0:00:06  loss: 1.5978 (1.5966)  time: 0.1815  data: 0.0001  max mem: 14938
[20:48:35.304856] Test:  [30/57]  eta: 0:00:05  loss: 1.5949 (1.5923)  time: 0.1820  data: 0.0001  max mem: 14938
[20:48:37.131556] Test:  [40/57]  eta: 0:00:03  loss: 1.5839 (1.5894)  time: 0.1824  data: 0.0001  max mem: 14938
[20:48:38.965270] Test:  [50/57]  eta: 0:00:01  loss: 1.5839 (1.5885)  time: 0.1830  data: 0.0001  max mem: 14938
[20:48:39.980078] Test:  [56/57]  eta: 0:00:00  loss: 1.5872 (1.5884)  time: 0.1790  data: 0.0001  max mem: 14938
[20:48:40.039299] Test: Total time: 0:00:10 (0.1845 s / it)
[20:48:42.076259] Dice score of the network on the train images: 0.000000, val images: 0.000000
[20:48:42.076503] saving best_dice_model_0 @ epoch 0
[20:48:43.145152] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[20:48:44.033856] Epoch: [1]  [  0/345]  eta: 0:05:06  lr: 0.000006  loss: 1.5589 (1.5589)  time: 0.8875  data: 0.1453  max mem: 14938
[20:48:58.873915] Epoch: [1]  [ 20/345]  eta: 0:04:03  lr: 0.000007  loss: 1.5513 (1.5526)  time: 0.7420  data: 0.0001  max mem: 14938
[20:49:13.790145] Epoch: [1]  [ 40/345]  eta: 0:03:47  lr: 0.000007  loss: 1.5420 (1.5472)  time: 0.7458  data: 0.0001  max mem: 14938
[20:49:28.733748] Epoch: [1]  [ 60/345]  eta: 0:03:32  lr: 0.000007  loss: 1.5235 (1.5400)  time: 0.7471  data: 0.0001  max mem: 14938
[20:49:43.706142] Epoch: [1]  [ 80/345]  eta: 0:03:18  lr: 0.000008  loss: 1.5102 (1.5334)  time: 0.7486  data: 0.0001  max mem: 14938
[20:49:58.701958] Epoch: [1]  [100/345]  eta: 0:03:03  lr: 0.000008  loss: 1.4967 (1.5266)  time: 0.7497  data: 0.0001  max mem: 14938
[20:50:13.830693] Epoch: [1]  [120/345]  eta: 0:02:48  lr: 0.000008  loss: 1.4868 (1.5198)  time: 0.7564  data: 0.0001  max mem: 14938
[20:50:28.822824] Epoch: [1]  [140/345]  eta: 0:02:33  lr: 0.000009  loss: 1.4719 (1.5131)  time: 0.7496  data: 0.0001  max mem: 14938
[20:50:43.814600] Epoch: [1]  [160/345]  eta: 0:02:18  lr: 0.000009  loss: 1.4666 (1.5074)  time: 0.7495  data: 0.0001  max mem: 14938
[20:50:58.791403] Epoch: [1]  [180/345]  eta: 0:02:03  lr: 0.000010  loss: 1.4543 (1.5018)  time: 0.7488  data: 0.0001  max mem: 14938
[20:51:13.775737] Epoch: [1]  [200/345]  eta: 0:01:48  lr: 0.000010  loss: 1.4464 (1.4965)  time: 0.7492  data: 0.0001  max mem: 14938
[20:51:28.741903] Epoch: [1]  [220/345]  eta: 0:01:33  lr: 0.000010  loss: 1.4327 (1.4908)  time: 0.7483  data: 0.0001  max mem: 14938
[20:51:43.708130] Epoch: [1]  [240/345]  eta: 0:01:18  lr: 0.000011  loss: 1.4221 (1.4853)  time: 0.7483  data: 0.0001  max mem: 14938
[20:51:58.663642] Epoch: [1]  [260/345]  eta: 0:01:03  lr: 0.000011  loss: 1.4219 (1.4805)  time: 0.7477  data: 0.0001  max mem: 14938
[20:52:13.619925] Epoch: [1]  [280/345]  eta: 0:00:48  lr: 0.000011  loss: 1.4123 (1.4760)  time: 0.7478  data: 0.0001  max mem: 14938
[20:52:28.573441] Epoch: [1]  [300/345]  eta: 0:00:33  lr: 0.000012  loss: 1.4043 (1.4714)  time: 0.7476  data: 0.0001  max mem: 14938
[20:52:43.519198] Epoch: [1]  [320/345]  eta: 0:00:18  lr: 0.000012  loss: 1.4006 (1.4672)  time: 0.7472  data: 0.0001  max mem: 14938
[20:52:58.472285] Epoch: [1]  [340/345]  eta: 0:00:03  lr: 0.000012  loss: 1.3953 (1.4632)  time: 0.7476  data: 0.0001  max mem: 14938
[20:53:01.462012] Epoch: [1]  [344/345]  eta: 0:00:00  lr: 0.000012  loss: 1.3953 (1.4624)  time: 0.7475  data: 0.0001  max mem: 14938
[20:53:01.527489] Epoch: [1] Total time: 0:04:18 (0.7489 s / it)
[20:53:01.527728] Averaged stats: lr: 0.000012  loss: 1.3953 (1.4624)
[20:53:01.871027] Test:  [  0/345]  eta: 0:01:57  loss: 1.3930 (1.3930)  time: 0.3402  data: 0.1586  max mem: 14938
[20:53:03.706036] Test:  [ 10/345]  eta: 0:01:06  loss: 1.3928 (1.3932)  time: 0.1977  data: 0.0145  max mem: 14938
[20:53:05.544407] Test:  [ 20/345]  eta: 0:01:02  loss: 1.3929 (1.3935)  time: 0.1836  data: 0.0001  max mem: 14938
[20:53:07.385325] Test:  [ 30/345]  eta: 0:00:59  loss: 1.3929 (1.3933)  time: 0.1839  data: 0.0001  max mem: 14938
[20:53:09.229683] Test:  [ 40/345]  eta: 0:00:57  loss: 1.3927 (1.3930)  time: 0.1842  data: 0.0001  max mem: 14938
[20:53:11.077918] Test:  [ 50/345]  eta: 0:00:55  loss: 1.3925 (1.3929)  time: 0.1846  data: 0.0001  max mem: 14938
[20:53:12.929121] Test:  [ 60/345]  eta: 0:00:53  loss: 1.3926 (1.3929)  time: 0.1849  data: 0.0001  max mem: 14938
[20:53:14.783405] Test:  [ 70/345]  eta: 0:00:51  loss: 1.3935 (1.3931)  time: 0.1852  data: 0.0001  max mem: 14938
[20:53:16.639624] Test:  [ 80/345]  eta: 0:00:49  loss: 1.3934 (1.3931)  time: 0.1855  data: 0.0001  max mem: 14938
[20:53:18.501486] Test:  [ 90/345]  eta: 0:00:47  loss: 1.3921 (1.3930)  time: 0.1859  data: 0.0001  max mem: 14938
[20:53:20.366670] Test:  [100/345]  eta: 0:00:45  loss: 1.3919 (1.3929)  time: 0.1863  data: 0.0001  max mem: 14938
[20:53:22.234215] Test:  [110/345]  eta: 0:00:43  loss: 1.3929 (1.3929)  time: 0.1866  data: 0.0001  max mem: 14938
[20:53:24.105586] Test:  [120/345]  eta: 0:00:41  loss: 1.3929 (1.3929)  time: 0.1869  data: 0.0001  max mem: 14938
[20:53:25.980543] Test:  [130/345]  eta: 0:00:40  loss: 1.3929 (1.3929)  time: 0.1873  data: 0.0001  max mem: 14938
[20:53:27.859791] Test:  [140/345]  eta: 0:00:38  loss: 1.3933 (1.3929)  time: 0.1877  data: 0.0001  max mem: 14938
[20:53:29.743442] Test:  [150/345]  eta: 0:00:36  loss: 1.3927 (1.3929)  time: 0.1881  data: 0.0001  max mem: 14938
[20:53:31.627255] Test:  [160/345]  eta: 0:00:34  loss: 1.3926 (1.3929)  time: 0.1883  data: 0.0001  max mem: 14938
[20:53:33.517461] Test:  [170/345]  eta: 0:00:32  loss: 1.3929 (1.3929)  time: 0.1887  data: 0.0001  max mem: 14938
[20:53:35.410619] Test:  [180/345]  eta: 0:00:30  loss: 1.3933 (1.3929)  time: 0.1891  data: 0.0001  max mem: 14938
[20:53:37.307799] Test:  [190/345]  eta: 0:00:29  loss: 1.3930 (1.3929)  time: 0.1895  data: 0.0001  max mem: 14938
[20:53:39.208592] Test:  [200/345]  eta: 0:00:27  loss: 1.3932 (1.3929)  time: 0.1898  data: 0.0001  max mem: 14938
[20:53:41.112873] Test:  [210/345]  eta: 0:00:25  loss: 1.3935 (1.3930)  time: 0.1902  data: 0.0001  max mem: 14938
[20:53:43.019821] Test:  [220/345]  eta: 0:00:23  loss: 1.3933 (1.3930)  time: 0.1905  data: 0.0001  max mem: 14938
[20:53:44.932317] Test:  [230/345]  eta: 0:00:21  loss: 1.3929 (1.3930)  time: 0.1909  data: 0.0001  max mem: 14938
[20:53:46.846432] Test:  [240/345]  eta: 0:00:19  loss: 1.3929 (1.3929)  time: 0.1913  data: 0.0001  max mem: 14938
[20:53:48.765794] Test:  [250/345]  eta: 0:00:17  loss: 1.3929 (1.3929)  time: 0.1916  data: 0.0001  max mem: 14938
[20:53:50.687619] Test:  [260/345]  eta: 0:00:16  loss: 1.3925 (1.3929)  time: 0.1920  data: 0.0001  max mem: 14938
[20:53:52.615317] Test:  [270/345]  eta: 0:00:14  loss: 1.3925 (1.3929)  time: 0.1924  data: 0.0001  max mem: 14938
[20:53:54.542993] Test:  [280/345]  eta: 0:00:12  loss: 1.3926 (1.3929)  time: 0.1927  data: 0.0001  max mem: 14938
[20:53:56.475895] Test:  [290/345]  eta: 0:00:10  loss: 1.3927 (1.3929)  time: 0.1930  data: 0.0001  max mem: 14938
[20:53:58.411808] Test:  [300/345]  eta: 0:00:08  loss: 1.3926 (1.3929)  time: 0.1934  data: 0.0001  max mem: 14938
[20:54:00.351386] Test:  [310/345]  eta: 0:00:06  loss: 1.3926 (1.3929)  time: 0.1937  data: 0.0001  max mem: 14938
[20:54:02.294295] Test:  [320/345]  eta: 0:00:04  loss: 1.3924 (1.3929)  time: 0.1941  data: 0.0001  max mem: 14938
[20:54:04.238100] Test:  [330/345]  eta: 0:00:02  loss: 1.3924 (1.3929)  time: 0.1943  data: 0.0001  max mem: 14938
[20:54:06.185503] Test:  [340/345]  eta: 0:00:00  loss: 1.3931 (1.3929)  time: 0.1945  data: 0.0001  max mem: 14938
[20:54:06.966494] Test:  [344/345]  eta: 0:00:00  loss: 1.3934 (1.3929)  time: 0.1947  data: 0.0001  max mem: 14938
[20:54:07.026438] Test: Total time: 0:01:05 (0.1898 s / it)
[20:54:18.627227] Test:  [ 0/57]  eta: 0:00:18  loss: 1.4002 (1.4002)  time: 0.3235  data: 0.1444  max mem: 14938
[20:54:20.440646] Test:  [10/57]  eta: 0:00:09  loss: 1.3979 (1.3969)  time: 0.1942  data: 0.0132  max mem: 14938
[20:54:22.259227] Test:  [20/57]  eta: 0:00:06  loss: 1.3979 (1.3960)  time: 0.1815  data: 0.0001  max mem: 14938
[20:54:24.081519] Test:  [30/57]  eta: 0:00:05  loss: 1.3925 (1.3927)  time: 0.1820  data: 0.0001  max mem: 14938
[20:54:25.907686] Test:  [40/57]  eta: 0:00:03  loss: 1.3871 (1.3906)  time: 0.1824  data: 0.0001  max mem: 14938
[20:54:27.740114] Test:  [50/57]  eta: 0:00:01  loss: 1.3871 (1.3899)  time: 0.1829  data: 0.0001  max mem: 14938
[20:54:28.727975] Test:  [56/57]  eta: 0:00:00  loss: 1.3898 (1.3898)  time: 0.1774  data: 0.0001  max mem: 14938
[20:54:28.787597] Test: Total time: 0:00:10 (0.1839 s / it)
[20:54:30.832119] Dice score of the network on the train images: 0.000000, val images: 0.000000
[20:54:30.836527] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[20:54:31.721493] Epoch: [2]  [  0/345]  eta: 0:05:04  lr: 0.000013  loss: 1.3903 (1.3903)  time: 0.8839  data: 0.1443  max mem: 14938
[20:54:46.573178] Epoch: [2]  [ 20/345]  eta: 0:04:03  lr: 0.000013  loss: 1.3872 (1.3882)  time: 0.7425  data: 0.0001  max mem: 14938
[20:55:01.506463] Epoch: [2]  [ 40/345]  eta: 0:03:48  lr: 0.000013  loss: 1.3834 (1.3868)  time: 0.7466  data: 0.0001  max mem: 14938
[20:55:16.470789] Epoch: [2]  [ 60/345]  eta: 0:03:33  lr: 0.000014  loss: 1.3799 (1.3847)  time: 0.7482  data: 0.0001  max mem: 14938
[20:55:31.463257] Epoch: [2]  [ 80/345]  eta: 0:03:18  lr: 0.000014  loss: 1.3745 (1.3833)  time: 0.7496  data: 0.0001  max mem: 14938
[20:55:46.482701] Epoch: [2]  [100/345]  eta: 0:03:03  lr: 0.000014  loss: 1.3708 (1.3813)  time: 0.7509  data: 0.0001  max mem: 14938
[20:56:01.626205] Epoch: [2]  [120/345]  eta: 0:02:48  lr: 0.000015  loss: 1.3643 (1.3789)  time: 0.7571  data: 0.0001  max mem: 14938
[20:56:16.637159] Epoch: [2]  [140/345]  eta: 0:02:33  lr: 0.000015  loss: 1.3576 (1.3760)  time: 0.7505  data: 0.0001  max mem: 14938
[20:56:31.640783] Epoch: [2]  [160/345]  eta: 0:02:18  lr: 0.000015  loss: 1.3560 (1.3741)  time: 0.7501  data: 0.0001  max mem: 14938
[20:56:46.632944] Epoch: [2]  [180/345]  eta: 0:02:03  lr: 0.000016  loss: 1.3545 (1.3721)  time: 0.7496  data: 0.0000  max mem: 14938
[20:57:01.614453] Epoch: [2]  [200/345]  eta: 0:01:48  lr: 0.000016  loss: 1.3490 (1.3699)  time: 0.7490  data: 0.0001  max mem: 14938
[20:57:16.591037] Epoch: [2]  [220/345]  eta: 0:01:33  lr: 0.000016  loss: 1.3484 (1.3680)  time: 0.7488  data: 0.0001  max mem: 14938
[20:57:31.577907] Epoch: [2]  [240/345]  eta: 0:01:18  lr: 0.000017  loss: 1.3415 (1.3660)  time: 0.7493  data: 0.0001  max mem: 14938
[20:57:46.558320] Epoch: [2]  [260/345]  eta: 0:01:03  lr: 0.000017  loss: 1.3398 (1.3645)  time: 0.7490  data: 0.0001  max mem: 14938
[20:58:01.537546] Epoch: [2]  [280/345]  eta: 0:00:48  lr: 0.000018  loss: 1.3390 (1.3629)  time: 0.7489  data: 0.0001  max mem: 14938
[20:58:16.490833] Epoch: [2]  [300/345]  eta: 0:00:33  lr: 0.000018  loss: 1.3290 (1.3610)  time: 0.7476  data: 0.0001  max mem: 14938
[20:58:31.440034] Epoch: [2]  [320/345]  eta: 0:00:18  lr: 0.000018  loss: 1.3324 (1.3595)  time: 0.7474  data: 0.0001  max mem: 14938
[20:58:46.387695] Epoch: [2]  [340/345]  eta: 0:00:03  lr: 0.000019  loss: 1.3339 (1.3582)  time: 0.7473  data: 0.0001  max mem: 14938
[20:58:49.378804] Epoch: [2]  [344/345]  eta: 0:00:00  lr: 0.000019  loss: 1.3339 (1.3580)  time: 0.7474  data: 0.0001  max mem: 14938
[20:58:49.441261] Epoch: [2] Total time: 0:04:18 (0.7496 s / it)
[20:58:49.441554] Averaged stats: lr: 0.000019  loss: 1.3339 (1.3580)
[20:58:49.779244] Test:  [  0/345]  eta: 0:01:55  loss: 1.3400 (1.3400)  time: 0.3342  data: 0.1528  max mem: 14938
[20:58:51.614078] Test:  [ 10/345]  eta: 0:01:06  loss: 1.3419 (1.3417)  time: 0.1971  data: 0.0140  max mem: 14938
[20:58:53.451414] Test:  [ 20/345]  eta: 0:01:01  loss: 1.3419 (1.3414)  time: 0.1835  data: 0.0001  max mem: 14938
[20:58:55.293451] Test:  [ 30/345]  eta: 0:00:59  loss: 1.3428 (1.3417)  time: 0.1839  data: 0.0001  max mem: 14938
[20:58:57.137345] Test:  [ 40/345]  eta: 0:00:57  loss: 1.3432 (1.3422)  time: 0.1842  data: 0.0001  max mem: 14938
[20:58:58.987767] Test:  [ 50/345]  eta: 0:00:55  loss: 1.3432 (1.3424)  time: 0.1847  data: 0.0001  max mem: 14938
[20:59:00.838574] Test:  [ 60/345]  eta: 0:00:53  loss: 1.3425 (1.3426)  time: 0.1850  data: 0.0001  max mem: 14938
[20:59:02.692727] Test:  [ 70/345]  eta: 0:00:51  loss: 1.3422 (1.3425)  time: 0.1852  data: 0.0001  max mem: 14938
[20:59:04.550356] Test:  [ 80/345]  eta: 0:00:49  loss: 1.3431 (1.3426)  time: 0.1855  data: 0.0001  max mem: 14938
[20:59:06.411828] Test:  [ 90/345]  eta: 0:00:47  loss: 1.3438 (1.3427)  time: 0.1859  data: 0.0001  max mem: 14938
[20:59:08.275613] Test:  [100/345]  eta: 0:00:45  loss: 1.3435 (1.3427)  time: 0.1862  data: 0.0001  max mem: 14938
[20:59:10.142403] Test:  [110/345]  eta: 0:00:43  loss: 1.3428 (1.3427)  time: 0.1865  data: 0.0001  max mem: 14938
[20:59:12.013069] Test:  [120/345]  eta: 0:00:41  loss: 1.3422 (1.3427)  time: 0.1868  data: 0.0001  max mem: 14938
[20:59:13.888417] Test:  [130/345]  eta: 0:00:40  loss: 1.3427 (1.3428)  time: 0.1873  data: 0.0001  max mem: 14938
[20:59:15.767967] Test:  [140/345]  eta: 0:00:38  loss: 1.3430 (1.3427)  time: 0.1877  data: 0.0001  max mem: 14938
[20:59:17.650623] Test:  [150/345]  eta: 0:00:36  loss: 1.3427 (1.3427)  time: 0.1881  data: 0.0001  max mem: 14938
[20:59:19.534529] Test:  [160/345]  eta: 0:00:34  loss: 1.3429 (1.3428)  time: 0.1883  data: 0.0001  max mem: 14938
[20:59:21.421642] Test:  [170/345]  eta: 0:00:32  loss: 1.3425 (1.3427)  time: 0.1885  data: 0.0001  max mem: 14938
[20:59:23.315295] Test:  [180/345]  eta: 0:00:30  loss: 1.3411 (1.3427)  time: 0.1890  data: 0.0001  max mem: 14938
[20:59:25.210817] Test:  [190/345]  eta: 0:00:29  loss: 1.3422 (1.3427)  time: 0.1894  data: 0.0001  max mem: 14938
[20:59:27.110524] Test:  [200/345]  eta: 0:00:27  loss: 1.3427 (1.3427)  time: 0.1897  data: 0.0001  max mem: 14938
[20:59:29.014667] Test:  [210/345]  eta: 0:00:25  loss: 1.3426 (1.3426)  time: 0.1901  data: 0.0001  max mem: 14938
[20:59:30.921725] Test:  [220/345]  eta: 0:00:23  loss: 1.3418 (1.3426)  time: 0.1905  data: 0.0001  max mem: 14938
[20:59:32.831684] Test:  [230/345]  eta: 0:00:21  loss: 1.3439 (1.3427)  time: 0.1908  data: 0.0001  max mem: 14938
[20:59:34.745801] Test:  [240/345]  eta: 0:00:19  loss: 1.3440 (1.3427)  time: 0.1912  data: 0.0001  max mem: 14938
[20:59:36.663989] Test:  [250/345]  eta: 0:00:17  loss: 1.3431 (1.3427)  time: 0.1916  data: 0.0001  max mem: 14938
[20:59:38.584112] Test:  [260/345]  eta: 0:00:15  loss: 1.3429 (1.3427)  time: 0.1919  data: 0.0001  max mem: 14938
[20:59:40.508612] Test:  [270/345]  eta: 0:00:14  loss: 1.3436 (1.3428)  time: 0.1922  data: 0.0001  max mem: 14938
[20:59:42.436901] Test:  [280/345]  eta: 0:00:12  loss: 1.3447 (1.3429)  time: 0.1926  data: 0.0001  max mem: 14938
[20:59:44.368102] Test:  [290/345]  eta: 0:00:10  loss: 1.3435 (1.3428)  time: 0.1929  data: 0.0001  max mem: 14938
[20:59:46.303650] Test:  [300/345]  eta: 0:00:08  loss: 1.3426 (1.3428)  time: 0.1933  data: 0.0001  max mem: 14938
[20:59:48.242425] Test:  [310/345]  eta: 0:00:06  loss: 1.3420 (1.3428)  time: 0.1937  data: 0.0001  max mem: 14938
[20:59:50.183676] Test:  [320/345]  eta: 0:00:04  loss: 1.3427 (1.3428)  time: 0.1940  data: 0.0001  max mem: 14938
[20:59:52.126839] Test:  [330/345]  eta: 0:00:02  loss: 1.3439 (1.3428)  time: 0.1942  data: 0.0001  max mem: 14938
[20:59:54.074492] Test:  [340/345]  eta: 0:00:00  loss: 1.3431 (1.3428)  time: 0.1945  data: 0.0001  max mem: 14938
[20:59:54.855718] Test:  [344/345]  eta: 0:00:00  loss: 1.3434 (1.3429)  time: 0.1947  data: 0.0001  max mem: 14938
[20:59:54.915419] Test: Total time: 0:01:05 (0.1898 s / it)
[21:00:06.422120] Test:  [ 0/57]  eta: 0:00:18  loss: 1.3513 (1.3513)  time: 0.3250  data: 0.1454  max mem: 14938
[21:00:08.232843] Test:  [10/57]  eta: 0:00:09  loss: 1.3483 (1.3474)  time: 0.1941  data: 0.0133  max mem: 14938
[21:00:10.050585] Test:  [20/57]  eta: 0:00:06  loss: 1.3483 (1.3466)  time: 0.1814  data: 0.0001  max mem: 14938
[21:00:11.872117] Test:  [30/57]  eta: 0:00:05  loss: 1.3410 (1.3424)  time: 0.1819  data: 0.0001  max mem: 14938
[21:00:13.698510] Test:  [40/57]  eta: 0:00:03  loss: 1.3345 (1.3397)  time: 0.1823  data: 0.0001  max mem: 14938
[21:00:15.529845] Test:  [50/57]  eta: 0:00:01  loss: 1.3345 (1.3388)  time: 0.1828  data: 0.0001  max mem: 14938
[21:00:16.517533] Test:  [56/57]  eta: 0:00:00  loss: 1.3385 (1.3387)  time: 0.1775  data: 0.0001  max mem: 14938
[21:00:16.571199] Test: Total time: 0:00:10 (0.1838 s / it)
[21:00:18.607273] Dice score of the network on the train images: 0.000000, val images: 0.000000
[21:00:18.612288] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[21:00:19.501167] Epoch: [3]  [  0/345]  eta: 0:05:06  lr: 0.000019  loss: 1.3233 (1.3233)  time: 0.8879  data: 0.1470  max mem: 14938
[21:00:34.342383] Epoch: [3]  [ 20/345]  eta: 0:04:03  lr: 0.000019  loss: 1.3268 (1.3270)  time: 0.7420  data: 0.0001  max mem: 14938
[21:00:49.251872] Epoch: [3]  [ 40/345]  eta: 0:03:47  lr: 0.000019  loss: 1.3226 (1.3284)  time: 0.7454  data: 0.0001  max mem: 14938
[21:01:04.195359] Epoch: [3]  [ 60/345]  eta: 0:03:32  lr: 0.000020  loss: 1.3264 (1.3277)  time: 0.7471  data: 0.0001  max mem: 14938
[21:01:19.161505] Epoch: [3]  [ 80/345]  eta: 0:03:18  lr: 0.000020  loss: 1.3173 (1.3259)  time: 0.7483  data: 0.0001  max mem: 14938
[21:01:34.151524] Epoch: [3]  [100/345]  eta: 0:03:03  lr: 0.000021  loss: 1.3134 (1.3249)  time: 0.7495  data: 0.0001  max mem: 14938
[21:01:49.152389] Epoch: [3]  [120/345]  eta: 0:02:48  lr: 0.000021  loss: 1.3152 (1.3246)  time: 0.7500  data: 0.0001  max mem: 14938
[21:02:04.145144] Epoch: [3]  [140/345]  eta: 0:02:33  lr: 0.000021  loss: 1.3108 (1.3230)  time: 0.7496  data: 0.0001  max mem: 14938
[21:02:19.124628] Epoch: [3]  [160/345]  eta: 0:02:18  lr: 0.000022  loss: 1.3077 (1.3214)  time: 0.7489  data: 0.0001  max mem: 14938
[21:02:34.096335] Epoch: [3]  [180/345]  eta: 0:02:03  lr: 0.000022  loss: 1.3088 (1.3203)  time: 0.7485  data: 0.0001  max mem: 14938
[21:02:49.062639] Epoch: [3]  [200/345]  eta: 0:01:48  lr: 0.000022  loss: 1.3048 (1.3189)  time: 0.7483  data: 0.0001  max mem: 14938
[21:03:04.026673] Epoch: [3]  [220/345]  eta: 0:01:33  lr: 0.000023  loss: 1.3027 (1.3180)  time: 0.7482  data: 0.0001  max mem: 14938
[21:03:18.981226] Epoch: [3]  [240/345]  eta: 0:01:18  lr: 0.000023  loss: 1.2995 (1.3170)  time: 0.7477  data: 0.0001  max mem: 14938
[21:03:33.935199] Epoch: [3]  [260/345]  eta: 0:01:03  lr: 0.000023  loss: 1.2995 (1.3157)  time: 0.7477  data: 0.0001  max mem: 14938
[21:03:48.889847] Epoch: [3]  [280/345]  eta: 0:00:48  lr: 0.000024  loss: 1.2979 (1.3145)  time: 0.7477  data: 0.0001  max mem: 14938
[21:04:03.841277] Epoch: [3]  [300/345]  eta: 0:00:33  lr: 0.000024  loss: 1.2990 (1.3137)  time: 0.7475  data: 0.0001  max mem: 14938
[21:04:18.802866] Epoch: [3]  [320/345]  eta: 0:00:18  lr: 0.000025  loss: 1.2966 (1.3128)  time: 0.7480  data: 0.0001  max mem: 14938
[21:04:33.755445] Epoch: [3]  [340/345]  eta: 0:00:03  lr: 0.000025  loss: 1.2925 (1.3118)  time: 0.7476  data: 0.0001  max mem: 14938
[21:04:36.742383] Epoch: [3]  [344/345]  eta: 0:00:00  lr: 0.000025  loss: 1.2922 (1.3117)  time: 0.7474  data: 0.0001  max mem: 14938
[21:04:36.808815] Epoch: [3] Total time: 0:04:18 (0.7484 s / it)
[21:04:36.809241] Averaged stats: lr: 0.000025  loss: 1.2922 (1.3117)
[21:04:37.147297] Test:  [  0/345]  eta: 0:01:55  loss: 1.3001 (1.3001)  time: 0.3346  data: 0.1538  max mem: 14938
[21:04:38.984804] Test:  [ 10/345]  eta: 0:01:06  loss: 1.2927 (1.2933)  time: 0.1974  data: 0.0140  max mem: 14938
[21:04:40.822595] Test:  [ 20/345]  eta: 0:01:02  loss: 1.2916 (1.2923)  time: 0.1837  data: 0.0001  max mem: 14938
[21:04:42.664233] Test:  [ 30/345]  eta: 0:00:59  loss: 1.2909 (1.2924)  time: 0.1839  data: 0.0001  max mem: 14938
[21:04:44.507725] Test:  [ 40/345]  eta: 0:00:57  loss: 1.2922 (1.2922)  time: 0.1842  data: 0.0001  max mem: 14938
[21:04:46.356222] Test:  [ 50/345]  eta: 0:00:55  loss: 1.2924 (1.2922)  time: 0.1845  data: 0.0001  max mem: 14938
[21:04:48.209051] Test:  [ 60/345]  eta: 0:00:53  loss: 1.2911 (1.2920)  time: 0.1850  data: 0.0001  max mem: 14938
[21:04:50.064507] Test:  [ 70/345]  eta: 0:00:51  loss: 1.2923 (1.2921)  time: 0.1854  data: 0.0001  max mem: 14938
[21:04:51.923006] Test:  [ 80/345]  eta: 0:00:49  loss: 1.2923 (1.2920)  time: 0.1856  data: 0.0001  max mem: 14938
[21:04:53.787350] Test:  [ 90/345]  eta: 0:00:47  loss: 1.2897 (1.2917)  time: 0.1861  data: 0.0001  max mem: 14938
[21:04:55.653977] Test:  [100/345]  eta: 0:00:45  loss: 1.2895 (1.2916)  time: 0.1865  data: 0.0001  max mem: 14938
[21:04:57.522539] Test:  [110/345]  eta: 0:00:43  loss: 1.2896 (1.2916)  time: 0.1867  data: 0.0001  max mem: 14938
[21:04:59.394954] Test:  [120/345]  eta: 0:00:41  loss: 1.2920 (1.2916)  time: 0.1870  data: 0.0001  max mem: 14938
[21:05:01.271400] Test:  [130/345]  eta: 0:00:40  loss: 1.2904 (1.2914)  time: 0.1874  data: 0.0001  max mem: 14938
[21:05:03.151638] Test:  [140/345]  eta: 0:00:38  loss: 1.2904 (1.2914)  time: 0.1878  data: 0.0001  max mem: 14938
[21:05:05.036085] Test:  [150/345]  eta: 0:00:36  loss: 1.2922 (1.2914)  time: 0.1882  data: 0.0001  max mem: 14938
[21:05:06.922006] Test:  [160/345]  eta: 0:00:34  loss: 1.2918 (1.2915)  time: 0.1885  data: 0.0001  max mem: 14938
[21:05:08.811831] Test:  [170/345]  eta: 0:00:32  loss: 1.2909 (1.2914)  time: 0.1887  data: 0.0001  max mem: 14938
[21:05:10.707165] Test:  [180/345]  eta: 0:00:30  loss: 1.2902 (1.2913)  time: 0.1892  data: 0.0001  max mem: 14938
[21:05:12.607280] Test:  [190/345]  eta: 0:00:29  loss: 1.2911 (1.2913)  time: 0.1897  data: 0.0001  max mem: 14938
[21:05:14.508436] Test:  [200/345]  eta: 0:00:27  loss: 1.2925 (1.2915)  time: 0.1900  data: 0.0001  max mem: 14938
[21:05:16.414947] Test:  [210/345]  eta: 0:00:25  loss: 1.2925 (1.2914)  time: 0.1903  data: 0.0001  max mem: 14938
[21:05:18.324027] Test:  [220/345]  eta: 0:00:23  loss: 1.2900 (1.2914)  time: 0.1907  data: 0.0001  max mem: 14938
[21:05:20.236180] Test:  [230/345]  eta: 0:00:21  loss: 1.2900 (1.2913)  time: 0.1910  data: 0.0001  max mem: 14938
[21:05:22.150537] Test:  [240/345]  eta: 0:00:19  loss: 1.2873 (1.2912)  time: 0.1913  data: 0.0001  max mem: 14938
[21:05:24.069610] Test:  [250/345]  eta: 0:00:17  loss: 1.2919 (1.2913)  time: 0.1916  data: 0.0001  max mem: 14938
[21:05:25.991110] Test:  [260/345]  eta: 0:00:16  loss: 1.2922 (1.2913)  time: 0.1920  data: 0.0001  max mem: 14938
[21:05:27.916702] Test:  [270/345]  eta: 0:00:14  loss: 1.2915 (1.2913)  time: 0.1923  data: 0.0001  max mem: 14938
[21:05:29.847601] Test:  [280/345]  eta: 0:00:12  loss: 1.2904 (1.2913)  time: 0.1928  data: 0.0001  max mem: 14938
[21:05:31.782286] Test:  [290/345]  eta: 0:00:10  loss: 1.2920 (1.2913)  time: 0.1932  data: 0.0001  max mem: 14938
[21:05:33.718433] Test:  [300/345]  eta: 0:00:08  loss: 1.2933 (1.2913)  time: 0.1935  data: 0.0001  max mem: 14938
[21:05:35.658494] Test:  [310/345]  eta: 0:00:06  loss: 1.2933 (1.2913)  time: 0.1938  data: 0.0001  max mem: 14938
[21:05:37.600784] Test:  [320/345]  eta: 0:00:04  loss: 1.2901 (1.2913)  time: 0.1941  data: 0.0001  max mem: 14938
[21:05:39.547534] Test:  [330/345]  eta: 0:00:02  loss: 1.2898 (1.2913)  time: 0.1944  data: 0.0001  max mem: 14938
[21:05:41.497466] Test:  [340/345]  eta: 0:00:00  loss: 1.2910 (1.2913)  time: 0.1948  data: 0.0001  max mem: 14938
[21:05:42.279342] Test:  [344/345]  eta: 0:00:00  loss: 1.2898 (1.2913)  time: 0.1950  data: 0.0001  max mem: 14938
[21:05:42.340440] Test: Total time: 0:01:05 (0.1899 s / it)
[21:05:53.892995] Test:  [ 0/57]  eta: 0:00:18  loss: 1.3050 (1.3050)  time: 0.3255  data: 0.1467  max mem: 14938
[21:05:55.704791] Test:  [10/57]  eta: 0:00:09  loss: 1.2995 (1.2985)  time: 0.1942  data: 0.0134  max mem: 14938
[21:05:57.522832] Test:  [20/57]  eta: 0:00:06  loss: 1.2995 (1.2979)  time: 0.1814  data: 0.0001  max mem: 14938
[21:05:59.345122] Test:  [30/57]  eta: 0:00:05  loss: 1.2839 (1.2901)  time: 0.1820  data: 0.0001  max mem: 14938
[21:06:01.172268] Test:  [40/57]  eta: 0:00:03  loss: 1.2725 (1.2850)  time: 0.1824  data: 0.0001  max mem: 14938
[21:06:03.003783] Test:  [50/57]  eta: 0:00:01  loss: 1.2725 (1.2832)  time: 0.1829  data: 0.0001  max mem: 14938
[21:06:03.992286] Test:  [56/57]  eta: 0:00:00  loss: 1.2801 (1.2829)  time: 0.1775  data: 0.0001  max mem: 14938
[21:06:04.051848] Test: Total time: 0:00:10 (0.1839 s / it)
[21:06:06.100174] Dice score of the network on the train images: 0.000000, val images: 0.000000
[21:06:06.105319] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[21:06:06.995299] Epoch: [4]  [  0/345]  eta: 0:05:06  lr: 0.000025  loss: 1.2985 (1.2985)  time: 0.8890  data: 0.1482  max mem: 14938
[21:06:21.865795] Epoch: [4]  [ 20/345]  eta: 0:04:03  lr: 0.000025  loss: 1.2862 (1.2910)  time: 0.7435  data: 0.0001  max mem: 14938
[21:06:36.800175] Epoch: [4]  [ 40/345]  eta: 0:03:48  lr: 0.000026  loss: 1.2880 (1.2895)  time: 0.7467  data: 0.0001  max mem: 14938
[21:06:51.769128] Epoch: [4]  [ 60/345]  eta: 0:03:33  lr: 0.000026  loss: 1.2772 (1.2875)  time: 0.7484  data: 0.0001  max mem: 14938
[21:07:06.765636] Epoch: [4]  [ 80/345]  eta: 0:03:18  lr: 0.000026  loss: 1.2739 (1.2849)  time: 0.7498  data: 0.0001  max mem: 14938
[21:07:21.782860] Epoch: [4]  [100/345]  eta: 0:03:03  lr: 0.000027  loss: 1.2696 (1.2826)  time: 0.7508  data: 0.0001  max mem: 14938
[21:07:36.797687] Epoch: [4]  [120/345]  eta: 0:02:48  lr: 0.000027  loss: 1.2637 (1.2795)  time: 0.7507  data: 0.0001  max mem: 14938
[21:07:51.801832] Epoch: [4]  [140/345]  eta: 0:02:33  lr: 0.000028  loss: 1.2584 (1.2768)  time: 0.7502  data: 0.0001  max mem: 14938
[21:08:06.810079] Epoch: [4]  [160/345]  eta: 0:02:18  lr: 0.000028  loss: 1.2645 (1.2751)  time: 0.7504  data: 0.0001  max mem: 14938
[21:08:21.808283] Epoch: [4]  [180/345]  eta: 0:02:03  lr: 0.000028  loss: 1.2414 (1.2719)  time: 0.7499  data: 0.0001  max mem: 14938
[21:08:36.801711] Epoch: [4]  [200/345]  eta: 0:01:48  lr: 0.000029  loss: 1.2341 (1.2682)  time: 0.7496  data: 0.0001  max mem: 14938
[21:08:51.788367] Epoch: [4]  [220/345]  eta: 0:01:33  lr: 0.000029  loss: 1.2286 (1.2650)  time: 0.7493  data: 0.0001  max mem: 14938
[21:09:06.775989] Epoch: [4]  [240/345]  eta: 0:01:18  lr: 0.000029  loss: 1.2204 (1.2614)  time: 0.7493  data: 0.0001  max mem: 14938
[21:09:21.762066] Epoch: [4]  [260/345]  eta: 0:01:03  lr: 0.000030  loss: 1.2116 (1.2577)  time: 0.7493  data: 0.0001  max mem: 14938
[21:09:36.743133] Epoch: [4]  [280/345]  eta: 0:00:48  lr: 0.000030  loss: 1.2044 (1.2539)  time: 0.7490  data: 0.0001  max mem: 14938
[21:09:51.717890] Epoch: [4]  [300/345]  eta: 0:00:33  lr: 0.000030  loss: 1.1858 (1.2498)  time: 0.7487  data: 0.0001  max mem: 14938
[21:10:06.690730] Epoch: [4]  [320/345]  eta: 0:00:18  lr: 0.000031  loss: 1.1773 (1.2456)  time: 0.7486  data: 0.0001  max mem: 14938
[21:10:21.669165] Epoch: [4]  [340/345]  eta: 0:00:03  lr: 0.000031  loss: 1.1822 (1.2419)  time: 0.7489  data: 0.0001  max mem: 14938
[21:10:24.665999] Epoch: [4]  [344/345]  eta: 0:00:00  lr: 0.000031  loss: 1.1822 (1.2414)  time: 0.7489  data: 0.0001  max mem: 14938
[21:10:24.731769] Epoch: [4] Total time: 0:04:18 (0.7496 s / it)
[21:10:24.732084] Averaged stats: lr: 0.000031  loss: 1.1822 (1.2414)
[21:10:25.075195] Test:  [  0/345]  eta: 0:01:57  loss: 1.1806 (1.1806)  time: 0.3391  data: 0.1581  max mem: 14938
[21:10:26.909544] Test:  [ 10/345]  eta: 0:01:06  loss: 1.1650 (1.1606)  time: 0.1975  data: 0.0144  max mem: 14938
[21:10:28.747989] Test:  [ 20/345]  eta: 0:01:02  loss: 1.1548 (1.1591)  time: 0.1836  data: 0.0001  max mem: 14938
[21:10:30.590177] Test:  [ 30/345]  eta: 0:00:59  loss: 1.1520 (1.1531)  time: 0.1840  data: 0.0001  max mem: 14938
[21:10:32.433633] Test:  [ 40/345]  eta: 0:00:57  loss: 1.1520 (1.1553)  time: 0.1842  data: 0.0001  max mem: 14938
[21:10:34.282602] Test:  [ 50/345]  eta: 0:00:55  loss: 1.1536 (1.1534)  time: 0.1846  data: 0.0001  max mem: 14938
[21:10:36.134893] Test:  [ 60/345]  eta: 0:00:53  loss: 1.1499 (1.1536)  time: 0.1850  data: 0.0001  max mem: 14938
[21:10:37.989989] Test:  [ 70/345]  eta: 0:00:51  loss: 1.1496 (1.1529)  time: 0.1853  data: 0.0001  max mem: 14938
[21:10:39.849079] Test:  [ 80/345]  eta: 0:00:49  loss: 1.1450 (1.1521)  time: 0.1857  data: 0.0001  max mem: 14938
[21:10:41.711330] Test:  [ 90/345]  eta: 0:00:47  loss: 1.1494 (1.1524)  time: 0.1860  data: 0.0001  max mem: 14938
[21:10:43.577499] Test:  [100/345]  eta: 0:00:45  loss: 1.1513 (1.1517)  time: 0.1864  data: 0.0001  max mem: 14938
[21:10:45.445715] Test:  [110/345]  eta: 0:00:43  loss: 1.1513 (1.1523)  time: 0.1867  data: 0.0001  max mem: 14938
[21:10:47.319266] Test:  [120/345]  eta: 0:00:41  loss: 1.1475 (1.1517)  time: 0.1870  data: 0.0001  max mem: 14938
[21:10:49.193288] Test:  [130/345]  eta: 0:00:40  loss: 1.1429 (1.1504)  time: 0.1873  data: 0.0001  max mem: 14938
[21:10:51.072319] Test:  [140/345]  eta: 0:00:38  loss: 1.1488 (1.1508)  time: 0.1876  data: 0.0001  max mem: 14938
[21:10:52.956711] Test:  [150/345]  eta: 0:00:36  loss: 1.1548 (1.1512)  time: 0.1881  data: 0.0001  max mem: 14938
[21:10:54.842881] Test:  [160/345]  eta: 0:00:34  loss: 1.1489 (1.1499)  time: 0.1885  data: 0.0001  max mem: 14938
[21:10:56.733344] Test:  [170/345]  eta: 0:00:32  loss: 1.1451 (1.1504)  time: 0.1888  data: 0.0001  max mem: 14938
[21:10:58.627111] Test:  [180/345]  eta: 0:00:30  loss: 1.1456 (1.1500)  time: 0.1892  data: 0.0001  max mem: 14938
[21:11:00.524615] Test:  [190/345]  eta: 0:00:29  loss: 1.1442 (1.1497)  time: 0.1895  data: 0.0001  max mem: 14938
[21:11:02.425593] Test:  [200/345]  eta: 0:00:27  loss: 1.1533 (1.1501)  time: 0.1899  data: 0.0001  max mem: 14938
[21:11:04.330001] Test:  [210/345]  eta: 0:00:25  loss: 1.1617 (1.1508)  time: 0.1902  data: 0.0001  max mem: 14938
[21:11:06.237174] Test:  [220/345]  eta: 0:00:23  loss: 1.1640 (1.1513)  time: 0.1905  data: 0.0001  max mem: 14938
[21:11:08.148062] Test:  [230/345]  eta: 0:00:21  loss: 1.1522 (1.1513)  time: 0.1909  data: 0.0001  max mem: 14938
[21:11:10.063740] Test:  [240/345]  eta: 0:00:19  loss: 1.1591 (1.1518)  time: 0.1913  data: 0.0001  max mem: 14938
[21:11:11.981652] Test:  [250/345]  eta: 0:00:17  loss: 1.1617 (1.1519)  time: 0.1916  data: 0.0001  max mem: 14938
[21:11:13.904745] Test:  [260/345]  eta: 0:00:16  loss: 1.1579 (1.1520)  time: 0.1920  data: 0.0001  max mem: 14938
[21:11:15.830089] Test:  [270/345]  eta: 0:00:14  loss: 1.1543 (1.1521)  time: 0.1924  data: 0.0001  max mem: 14938
[21:11:17.759979] Test:  [280/345]  eta: 0:00:12  loss: 1.1561 (1.1527)  time: 0.1927  data: 0.0001  max mem: 14938
[21:11:19.693285] Test:  [290/345]  eta: 0:00:10  loss: 1.1690 (1.1533)  time: 0.1931  data: 0.0001  max mem: 14938
[21:11:21.629542] Test:  [300/345]  eta: 0:00:08  loss: 1.1533 (1.1532)  time: 0.1934  data: 0.0001  max mem: 14938
[21:11:23.567715] Test:  [310/345]  eta: 0:00:06  loss: 1.1510 (1.1534)  time: 0.1937  data: 0.0001  max mem: 14938
[21:11:25.510705] Test:  [320/345]  eta: 0:00:04  loss: 1.1510 (1.1532)  time: 0.1940  data: 0.0001  max mem: 14938
[21:11:27.455033] Test:  [330/345]  eta: 0:00:02  loss: 1.1547 (1.1533)  time: 0.1943  data: 0.0001  max mem: 14938
[21:11:29.405656] Test:  [340/345]  eta: 0:00:00  loss: 1.1589 (1.1533)  time: 0.1947  data: 0.0001  max mem: 14938
[21:11:30.185980] Test:  [344/345]  eta: 0:00:00  loss: 1.1547 (1.1531)  time: 0.1948  data: 0.0001  max mem: 14938
[21:11:30.245091] Test: Total time: 0:01:05 (0.1899 s / it)
[21:11:41.868007] Test:  [ 0/57]  eta: 0:00:18  loss: 1.2296 (1.2296)  time: 0.3240  data: 0.1450  max mem: 14938
[21:11:43.681309] Test:  [10/57]  eta: 0:00:09  loss: 1.2043 (1.1935)  time: 0.1942  data: 0.0132  max mem: 14938
[21:11:45.499187] Test:  [20/57]  eta: 0:00:06  loss: 1.2148 (1.1950)  time: 0.1815  data: 0.0001  max mem: 14938
[21:11:47.321075] Test:  [30/57]  eta: 0:00:05  loss: 1.1270 (1.1478)  time: 0.1819  data: 0.0001  max mem: 14938
[21:11:49.147794] Test:  [40/57]  eta: 0:00:03  loss: 1.0371 (1.1180)  time: 0.1824  data: 0.0001  max mem: 14938
[21:11:50.979099] Test:  [50/57]  eta: 0:00:01  loss: 1.0395 (1.1096)  time: 0.1829  data: 0.0001  max mem: 14938
[21:11:51.968142] Test:  [56/57]  eta: 0:00:00  loss: 1.0971 (1.1136)  time: 0.1775  data: 0.0001  max mem: 14938
[21:11:52.026190] Test: Total time: 0:00:10 (0.1839 s / it)
[21:11:54.042522] Dice score of the network on the train images: 0.523013, val images: 0.622425
[21:11:54.042777] saving best_prec_model_0 @ epoch 4
[21:11:55.093895] saving best_rec_model_0 @ epoch 4
[21:11:56.072430] saving best_dice_model_0 @ epoch 4
[21:11:57.180643] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[21:11:58.068443] Epoch: [5]  [  0/345]  eta: 0:05:05  lr: 0.000031  loss: 1.2068 (1.2068)  time: 0.8867  data: 0.1450  max mem: 14938
[21:12:12.915821] Epoch: [5]  [ 20/345]  eta: 0:04:03  lr: 0.000032  loss: 1.1559 (1.1634)  time: 0.7423  data: 0.0001  max mem: 14938
[21:12:27.832929] Epoch: [5]  [ 40/345]  eta: 0:03:47  lr: 0.000032  loss: 1.1516 (1.1596)  time: 0.7458  data: 0.0001  max mem: 14938
[21:12:42.799110] Epoch: [5]  [ 60/345]  eta: 0:03:33  lr: 0.000032  loss: 1.1606 (1.1604)  time: 0.7483  data: 0.0001  max mem: 14938
[21:12:57.801631] Epoch: [5]  [ 80/345]  eta: 0:03:18  lr: 0.000033  loss: 1.1417 (1.1572)  time: 0.7501  data: 0.0001  max mem: 14938
[21:13:12.825119] Epoch: [5]  [100/345]  eta: 0:03:03  lr: 0.000033  loss: 1.1394 (1.1539)  time: 0.7511  data: 0.0001  max mem: 14938
[21:13:27.842456] Epoch: [5]  [120/345]  eta: 0:02:48  lr: 0.000033  loss: 1.1146 (1.1488)  time: 0.7508  data: 0.0001  max mem: 14938
[21:13:42.844011] Epoch: [5]  [140/345]  eta: 0:02:33  lr: 0.000034  loss: 1.1219 (1.1451)  time: 0.7500  data: 0.0001  max mem: 14938
[21:13:57.842549] Epoch: [5]  [160/345]  eta: 0:02:18  lr: 0.000034  loss: 1.1241 (1.1422)  time: 0.7499  data: 0.0001  max mem: 14938
[21:14:12.837954] Epoch: [5]  [180/345]  eta: 0:02:03  lr: 0.000035  loss: 1.1247 (1.1398)  time: 0.7497  data: 0.0001  max mem: 14938
[21:14:27.830383] Epoch: [5]  [200/345]  eta: 0:01:48  lr: 0.000035  loss: 1.0986 (1.1365)  time: 0.7496  data: 0.0001  max mem: 14938
[21:14:42.823352] Epoch: [5]  [220/345]  eta: 0:01:33  lr: 0.000035  loss: 1.0901 (1.1323)  time: 0.7496  data: 0.0001  max mem: 14938
[21:14:57.944153] Epoch: [5]  [240/345]  eta: 0:01:18  lr: 0.000036  loss: 1.0832 (1.1286)  time: 0.7560  data: 0.0001  max mem: 14938
[21:15:12.918668] Epoch: [5]  [260/345]  eta: 0:01:03  lr: 0.000036  loss: 1.0780 (1.1252)  time: 0.7487  data: 0.0001  max mem: 14938
[21:15:27.902415] Epoch: [5]  [280/345]  eta: 0:00:48  lr: 0.000036  loss: 1.0884 (1.1231)  time: 0.7491  data: 0.0000  max mem: 14938
[21:15:42.876960] Epoch: [5]  [300/345]  eta: 0:00:33  lr: 0.000037  loss: 1.0672 (1.1192)  time: 0.7487  data: 0.0001  max mem: 14938
[21:15:57.867891] Epoch: [5]  [320/345]  eta: 0:00:18  lr: 0.000037  loss: 1.0660 (1.1165)  time: 0.7495  data: 0.0001  max mem: 14938
[21:16:12.846358] Epoch: [5]  [340/345]  eta: 0:00:03  lr: 0.000037  loss: 1.0659 (1.1136)  time: 0.7489  data: 0.0001  max mem: 14938
[21:16:15.841532] Epoch: [5]  [344/345]  eta: 0:00:00  lr: 0.000037  loss: 1.0666 (1.1129)  time: 0.7488  data: 0.0001  max mem: 14938
[21:16:15.908438] Epoch: [5] Total time: 0:04:18 (0.7499 s / it)
[21:16:15.908728] Averaged stats: lr: 0.000037  loss: 1.0666 (1.1129)
[21:16:16.246102] Test:  [  0/345]  eta: 0:01:55  loss: 0.9765 (0.9765)  time: 0.3334  data: 0.1517  max mem: 14938
[21:16:18.080764] Test:  [ 10/345]  eta: 0:01:06  loss: 1.0177 (1.0132)  time: 0.1970  data: 0.0139  max mem: 14938
[21:16:19.918469] Test:  [ 20/345]  eta: 0:01:01  loss: 1.0186 (1.0172)  time: 0.1836  data: 0.0001  max mem: 14938
[21:16:21.759569] Test:  [ 30/345]  eta: 0:00:59  loss: 1.0222 (1.0217)  time: 0.1839  data: 0.0001  max mem: 14938
[21:16:23.604290] Test:  [ 40/345]  eta: 0:00:57  loss: 1.0183 (1.0203)  time: 0.1842  data: 0.0001  max mem: 14938
[21:16:25.452438] Test:  [ 50/345]  eta: 0:00:55  loss: 1.0156 (1.0209)  time: 0.1846  data: 0.0001  max mem: 14938
[21:16:27.305235] Test:  [ 60/345]  eta: 0:00:53  loss: 1.0097 (1.0193)  time: 0.1850  data: 0.0001  max mem: 14938
[21:16:29.158810] Test:  [ 70/345]  eta: 0:00:51  loss: 1.0141 (1.0205)  time: 0.1853  data: 0.0001  max mem: 14938
[21:16:31.016359] Test:  [ 80/345]  eta: 0:00:49  loss: 1.0207 (1.0206)  time: 0.1855  data: 0.0001  max mem: 14938
[21:16:32.879094] Test:  [ 90/345]  eta: 0:00:47  loss: 1.0121 (1.0198)  time: 0.1860  data: 0.0001  max mem: 14938
[21:16:34.744099] Test:  [100/345]  eta: 0:00:45  loss: 1.0121 (1.0191)  time: 0.1863  data: 0.0001  max mem: 14938
[21:16:36.610409] Test:  [110/345]  eta: 0:00:43  loss: 1.0096 (1.0186)  time: 0.1865  data: 0.0001  max mem: 14938
[21:16:38.483252] Test:  [120/345]  eta: 0:00:41  loss: 1.0164 (1.0185)  time: 0.1869  data: 0.0001  max mem: 14938
[21:16:40.357392] Test:  [130/345]  eta: 0:00:40  loss: 1.0225 (1.0193)  time: 0.1873  data: 0.0001  max mem: 14938
[21:16:42.237331] Test:  [140/345]  eta: 0:00:38  loss: 1.0297 (1.0201)  time: 0.1877  data: 0.0001  max mem: 14938
[21:16:44.119691] Test:  [150/345]  eta: 0:00:36  loss: 1.0297 (1.0203)  time: 0.1881  data: 0.0001  max mem: 14938
[21:16:46.006004] Test:  [160/345]  eta: 0:00:34  loss: 1.0262 (1.0200)  time: 0.1884  data: 0.0001  max mem: 14938
[21:16:47.894509] Test:  [170/345]  eta: 0:00:32  loss: 1.0156 (1.0199)  time: 0.1887  data: 0.0001  max mem: 14938
[21:16:49.788151] Test:  [180/345]  eta: 0:00:30  loss: 1.0178 (1.0199)  time: 0.1891  data: 0.0001  max mem: 14938
[21:16:51.686050] Test:  [190/345]  eta: 0:00:29  loss: 1.0164 (1.0196)  time: 0.1895  data: 0.0001  max mem: 14938
[21:16:53.585749] Test:  [200/345]  eta: 0:00:27  loss: 1.0264 (1.0203)  time: 0.1898  data: 0.0001  max mem: 14938
[21:16:55.488589] Test:  [210/345]  eta: 0:00:25  loss: 1.0288 (1.0203)  time: 0.1901  data: 0.0001  max mem: 14938
[21:16:57.395547] Test:  [220/345]  eta: 0:00:23  loss: 1.0049 (1.0194)  time: 0.1904  data: 0.0001  max mem: 14938
[21:16:59.307273] Test:  [230/345]  eta: 0:00:21  loss: 1.0046 (1.0190)  time: 0.1909  data: 0.0001  max mem: 14938
[21:17:01.225089] Test:  [240/345]  eta: 0:00:19  loss: 1.0187 (1.0196)  time: 0.1914  data: 0.0001  max mem: 14938
[21:17:03.142437] Test:  [250/345]  eta: 0:00:17  loss: 1.0283 (1.0196)  time: 0.1917  data: 0.0001  max mem: 14938
[21:17:05.063156] Test:  [260/345]  eta: 0:00:16  loss: 1.0272 (1.0201)  time: 0.1919  data: 0.0001  max mem: 14938
[21:17:06.987117] Test:  [270/345]  eta: 0:00:14  loss: 1.0295 (1.0202)  time: 0.1922  data: 0.0001  max mem: 14938
[21:17:08.916663] Test:  [280/345]  eta: 0:00:12  loss: 1.0286 (1.0205)  time: 0.1926  data: 0.0001  max mem: 14938
[21:17:10.846867] Test:  [290/345]  eta: 0:00:10  loss: 1.0133 (1.0208)  time: 0.1929  data: 0.0001  max mem: 14938
[21:17:12.782244] Test:  [300/345]  eta: 0:00:08  loss: 1.0255 (1.0214)  time: 0.1932  data: 0.0001  max mem: 14938
[21:17:14.720922] Test:  [310/345]  eta: 0:00:06  loss: 1.0219 (1.0211)  time: 0.1937  data: 0.0001  max mem: 14938
[21:17:16.662624] Test:  [320/345]  eta: 0:00:04  loss: 1.0134 (1.0211)  time: 0.1940  data: 0.0001  max mem: 14938
[21:17:18.607901] Test:  [330/345]  eta: 0:00:02  loss: 1.0148 (1.0211)  time: 0.1943  data: 0.0001  max mem: 14938
[21:17:20.556597] Test:  [340/345]  eta: 0:00:00  loss: 1.0152 (1.0207)  time: 0.1946  data: 0.0001  max mem: 14938
[21:17:21.337930] Test:  [344/345]  eta: 0:00:00  loss: 1.0152 (1.0208)  time: 0.1948  data: 0.0001  max mem: 14938
[21:17:21.399007] Test: Total time: 0:01:05 (0.1898 s / it)
[21:17:32.885036] Test:  [ 0/57]  eta: 0:00:18  loss: 1.1069 (1.1069)  time: 0.3220  data: 0.1428  max mem: 14938
[21:17:34.700169] Test:  [10/57]  eta: 0:00:09  loss: 1.0801 (1.0726)  time: 0.1942  data: 0.0130  max mem: 14938
[21:17:36.517500] Test:  [20/57]  eta: 0:00:06  loss: 1.1038 (1.0790)  time: 0.1816  data: 0.0001  max mem: 14938
[21:17:38.340351] Test:  [30/57]  eta: 0:00:05  loss: 0.9619 (1.0227)  time: 0.1820  data: 0.0001  max mem: 14938
[21:17:40.165495] Test:  [40/57]  eta: 0:00:03  loss: 0.8895 (0.9887)  time: 0.1823  data: 0.0001  max mem: 14938
[21:17:41.999237] Test:  [50/57]  eta: 0:00:01  loss: 0.8895 (0.9790)  time: 0.1829  data: 0.0001  max mem: 14938
[21:17:42.987765] Test:  [56/57]  eta: 0:00:00  loss: 0.9679 (0.9842)  time: 0.1776  data: 0.0001  max mem: 14938
[21:17:43.044128] Test: Total time: 0:00:10 (0.1839 s / it)
[21:17:45.053148] Dice score of the network on the train images: 0.667542, val images: 0.729856
[21:17:45.053394] saving best_prec_model_0 @ epoch 5
[21:17:46.154411] saving best_dice_model_0 @ epoch 5
[21:17:47.205605] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[21:17:48.103158] Epoch: [6]  [  0/345]  eta: 0:05:09  lr: 0.000038  loss: 1.0378 (1.0378)  time: 0.8965  data: 0.1550  max mem: 14938
[21:18:02.957651] Epoch: [6]  [ 20/345]  eta: 0:04:03  lr: 0.000038  loss: 1.0476 (1.0478)  time: 0.7427  data: 0.0001  max mem: 14938
[21:18:17.893910] Epoch: [6]  [ 40/345]  eta: 0:03:48  lr: 0.000038  loss: 1.0441 (1.0500)  time: 0.7468  data: 0.0001  max mem: 14938
[21:18:32.867084] Epoch: [6]  [ 60/345]  eta: 0:03:33  lr: 0.000039  loss: 1.0404 (1.0452)  time: 0.7486  data: 0.0001  max mem: 14938
[21:18:48.000955] Epoch: [6]  [ 80/345]  eta: 0:03:18  lr: 0.000039  loss: 1.0372 (1.0423)  time: 0.7566  data: 0.0001  max mem: 14938
[21:19:03.026924] Epoch: [6]  [100/345]  eta: 0:03:03  lr: 0.000039  loss: 1.0320 (1.0420)  time: 0.7513  data: 0.0001  max mem: 14938
[21:19:18.034668] Epoch: [6]  [120/345]  eta: 0:02:48  lr: 0.000040  loss: 1.0226 (1.0392)  time: 0.7503  data: 0.0001  max mem: 14938
[21:19:33.035392] Epoch: [6]  [140/345]  eta: 0:02:33  lr: 0.000040  loss: 1.0408 (1.0393)  time: 0.7500  data: 0.0001  max mem: 14938
[21:19:48.020867] Epoch: [6]  [160/345]  eta: 0:02:18  lr: 0.000040  loss: 1.0077 (1.0355)  time: 0.7492  data: 0.0001  max mem: 14938
[21:20:02.998208] Epoch: [6]  [180/345]  eta: 0:02:03  lr: 0.000041  loss: 1.0069 (1.0328)  time: 0.7488  data: 0.0001  max mem: 14938
[21:20:17.973585] Epoch: [6]  [200/345]  eta: 0:01:48  lr: 0.000041  loss: 1.0063 (1.0307)  time: 0.7487  data: 0.0001  max mem: 14938
[21:20:32.942364] Epoch: [6]  [220/345]  eta: 0:01:33  lr: 0.000041  loss: 1.0121 (1.0292)  time: 0.7484  data: 0.0001  max mem: 14938
[21:20:47.915790] Epoch: [6]  [240/345]  eta: 0:01:18  lr: 0.000042  loss: 1.0006 (1.0264)  time: 0.7486  data: 0.0001  max mem: 14938
[21:21:02.884545] Epoch: [6]  [260/345]  eta: 0:01:03  lr: 0.000042  loss: 0.9850 (1.0244)  time: 0.7484  data: 0.0001  max mem: 14938
[21:21:17.843361] Epoch: [6]  [280/345]  eta: 0:00:48  lr: 0.000043  loss: 0.9724 (1.0210)  time: 0.7479  data: 0.0001  max mem: 14938

[21:21:32.804008] Epoch: [6]  [300/345]  eta: 0:00:33  lr: 0.000043  loss: 0.9761 (1.0187)  time: 0.7480  data: 0.0001  max mem: 14938
[21:21:47.758279] Epoch: [6]  [320/345]  eta: 0:00:18  lr: 0.000043  loss: 0.9724 (1.0166)  time: 0.7477  data: 0.0001  max mem: 14938
[21:22:02.714653] Epoch: [6]  [340/345]  eta: 0:00:03  lr: 0.000044  loss: 0.9729 (1.0146)  time: 0.7478  data: 0.0001  max mem: 14938
[21:22:05.707016] Epoch: [6]  [344/345]  eta: 0:00:00  lr: 0.000044  loss: 0.9871 (1.0146)  time: 0.7478  data: 0.0001  max mem: 14938
[21:22:05.774575] Epoch: [6] Total time: 0:04:18 (0.7495 s / it)
[21:22:05.775045] Averaged stats: lr: 0.000044  loss: 0.9871 (1.0146)
[21:22:06.117652] Test:  [  0/345]  eta: 0:01:56  loss: 0.9462 (0.9462)  time: 0.3389  data: 0.1571  max mem: 14938
[21:22:07.952479] Test:  [ 10/345]  eta: 0:01:06  loss: 0.9381 (0.9352)  time: 0.1975  data: 0.0143  max mem: 14938
[21:22:09.791005] Test:  [ 20/345]  eta: 0:01:02  loss: 0.9439 (0.9464)  time: 0.1836  data: 0.0001  max mem: 14938
[21:22:11.632914] Test:  [ 30/345]  eta: 0:00:59  loss: 0.9434 (0.9448)  time: 0.1840  data: 0.0001  max mem: 14938
[21:22:13.477286] Test:  [ 40/345]  eta: 0:00:57  loss: 0.9469 (0.9482)  time: 0.1843  data: 0.0001  max mem: 14938
[21:22:15.327008] Test:  [ 50/345]  eta: 0:00:55  loss: 0.9454 (0.9456)  time: 0.1847  data: 0.0001  max mem: 14938
[21:22:17.178100] Test:  [ 60/345]  eta: 0:00:53  loss: 0.9329 (0.9444)  time: 0.1850  data: 0.0001  max mem: 14938
[21:22:19.033097] Test:  [ 70/345]  eta: 0:00:51  loss: 0.9533 (0.9480)  time: 0.1853  data: 0.0001  max mem: 14938
[21:22:20.891004] Test:  [ 80/345]  eta: 0:00:49  loss: 0.9660 (0.9501)  time: 0.1856  data: 0.0001  max mem: 14938
[21:22:22.752126] Test:  [ 90/345]  eta: 0:00:47  loss: 0.9513 (0.9501)  time: 0.1859  data: 0.0001  max mem: 14938
[21:22:24.617606] Test:  [100/345]  eta: 0:00:45  loss: 0.9389 (0.9489)  time: 0.1863  data: 0.0001  max mem: 14938
[21:22:26.486144] Test:  [110/345]  eta: 0:00:43  loss: 0.9386 (0.9482)  time: 0.1867  data: 0.0001  max mem: 14938
[21:22:28.358142] Test:  [120/345]  eta: 0:00:41  loss: 0.9386 (0.9480)  time: 0.1870  data: 0.0001  max mem: 14938
[21:22:30.233019] Test:  [130/345]  eta: 0:00:40  loss: 0.9315 (0.9476)  time: 0.1873  data: 0.0001  max mem: 14938
[21:22:32.111458] Test:  [140/345]  eta: 0:00:38  loss: 0.9315 (0.9464)  time: 0.1876  data: 0.0001  max mem: 14938
[21:22:33.993951] Test:  [150/345]  eta: 0:00:36  loss: 0.9389 (0.9468)  time: 0.1880  data: 0.0001  max mem: 14938
[21:22:35.878284] Test:  [160/345]  eta: 0:00:34  loss: 0.9282 (0.9464)  time: 0.1883  data: 0.0001  max mem: 14938
[21:22:37.767275] Test:  [170/345]  eta: 0:00:32  loss: 0.9318 (0.9470)  time: 0.1886  data: 0.0001  max mem: 14938
[21:22:39.659515] Test:  [180/345]  eta: 0:00:30  loss: 0.9514 (0.9465)  time: 0.1890  data: 0.0001  max mem: 14938
[21:22:41.556128] Test:  [190/345]  eta: 0:00:29  loss: 0.9337 (0.9454)  time: 0.1894  data: 0.0001  max mem: 14938
[21:22:43.458464] Test:  [200/345]  eta: 0:00:27  loss: 0.9295 (0.9452)  time: 0.1899  data: 0.0001  max mem: 14938
[21:22:45.360923] Test:  [210/345]  eta: 0:00:25  loss: 0.9462 (0.9460)  time: 0.1902  data: 0.0001  max mem: 14938
[21:22:47.267536] Test:  [220/345]  eta: 0:00:23  loss: 0.9483 (0.9455)  time: 0.1904  data: 0.0001  max mem: 14938
[21:22:49.178622] Test:  [230/345]  eta: 0:00:21  loss: 0.9293 (0.9452)  time: 0.1908  data: 0.0001  max mem: 14938
[21:22:51.094591] Test:  [240/345]  eta: 0:00:19  loss: 0.9440 (0.9452)  time: 0.1913  data: 0.0001  max mem: 14938
[21:22:53.013177] Test:  [250/345]  eta: 0:00:17  loss: 0.9413 (0.9449)  time: 0.1917  data: 0.0001  max mem: 14938
[21:22:54.934117] Test:  [260/345]  eta: 0:00:16  loss: 0.9208 (0.9443)  time: 0.1919  data: 0.0001  max mem: 14938
[21:22:56.858627] Test:  [270/345]  eta: 0:00:14  loss: 0.9362 (0.9446)  time: 0.1922  data: 0.0001  max mem: 14938
[21:22:58.785937] Test:  [280/345]  eta: 0:00:12  loss: 0.9543 (0.9448)  time: 0.1925  data: 0.0001  max mem: 14938
[21:23:00.717488] Test:  [290/345]  eta: 0:00:10  loss: 0.9413 (0.9442)  time: 0.1929  data: 0.0001  max mem: 14938
[21:23:02.652104] Test:  [300/345]  eta: 0:00:08  loss: 0.9369 (0.9441)  time: 0.1933  data: 0.0001  max mem: 14938
[21:23:04.592287] Test:  [310/345]  eta: 0:00:06  loss: 0.9424 (0.9439)  time: 0.1937  data: 0.0001  max mem: 14938
[21:23:06.533760] Test:  [320/345]  eta: 0:00:04  loss: 0.9450 (0.9441)  time: 0.1940  data: 0.0001  max mem: 14938
[21:23:08.479853] Test:  [330/345]  eta: 0:00:02  loss: 0.9397 (0.9438)  time: 0.1943  data: 0.0001  max mem: 14938
[21:23:10.429804] Test:  [340/345]  eta: 0:00:00  loss: 0.9348 (0.9435)  time: 0.1948  data: 0.0001  max mem: 14938
[21:23:11.212117] Test:  [344/345]  eta: 0:00:00  loss: 0.9348 (0.9434)  time: 0.1949  data: 0.0001  max mem: 14938
[21:23:11.271547] Test: Total time: 0:01:05 (0.1898 s / it)
[21:23:22.699713] Test:  [ 0/57]  eta: 0:00:18  loss: 1.0368 (1.0368)  time: 0.3253  data: 0.1461  max mem: 14938
[21:23:24.515081] Test:  [10/57]  eta: 0:00:09  loss: 1.0106 (1.0009)  time: 0.1945  data: 0.0133  max mem: 14938
[21:23:26.334068] Test:  [20/57]  eta: 0:00:06  loss: 0.9887 (0.9956)  time: 0.1817  data: 0.0001  max mem: 14938
[21:23:28.156579] Test:  [30/57]  eta: 0:00:05  loss: 0.9031 (0.9477)  time: 0.1820  data: 0.0001  max mem: 14938
[21:23:29.986723] Test:  [40/57]  eta: 0:00:03  loss: 0.8273 (0.9188)  time: 0.1826  data: 0.0001  max mem: 14938
[21:23:31.821479] Test:  [50/57]  eta: 0:00:01  loss: 0.8273 (0.9096)  time: 0.1832  data: 0.0001  max mem: 14938
[21:23:32.809354] Test:  [56/57]  eta: 0:00:00  loss: 0.8847 (0.9140)  time: 0.1777  data: 0.0001  max mem: 14938
[21:23:32.869009] Test: Total time: 0:00:10 (0.1841 s / it)
[21:23:34.839029] Dice score of the network on the train images: 0.693393, val images: 0.760227
[21:23:34.839271] saving best_prec_model_0 @ epoch 6
[21:23:35.939663] saving best_rec_model_0 @ epoch 6
[21:23:37.074606] saving best_dice_model_0 @ epoch 6
[21:23:38.125398] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[21:23:39.014742] Epoch: [7]  [  0/345]  eta: 0:05:06  lr: 0.000044  loss: 0.9395 (0.9395)  time: 0.8882  data: 0.1512  max mem: 14938
[21:23:53.851128] Epoch: [7]  [ 20/345]  eta: 0:04:03  lr: 0.000044  loss: 0.9633 (0.9697)  time: 0.7418  data: 0.0001  max mem: 14938
[21:24:08.726447] Epoch: [7]  [ 40/345]  eta: 0:03:47  lr: 0.000044  loss: 0.9694 (0.9707)  time: 0.7437  data: 0.0001  max mem: 14938
[21:24:23.673578] Epoch: [7]  [ 60/345]  eta: 0:03:32  lr: 0.000045  loss: 0.9533 (0.9670)  time: 0.7473  data: 0.0001  max mem: 14938
[21:24:38.659800] Epoch: [7]  [ 80/345]  eta: 0:03:18  lr: 0.000045  loss: 0.9595 (0.9648)  time: 0.7493  data: 0.0001  max mem: 14938
[21:24:53.675628] Epoch: [7]  [100/345]  eta: 0:03:03  lr: 0.000046  loss: 0.9716 (0.9681)  time: 0.7507  data: 0.0001  max mem: 14938
[21:25:08.682718] Epoch: [7]  [120/345]  eta: 0:02:48  lr: 0.000046  loss: 0.9659 (0.9677)  time: 0.7503  data: 0.0001  max mem: 14938
[21:25:23.674856] Epoch: [7]  [140/345]  eta: 0:02:33  lr: 0.000046  loss: 0.9608 (0.9661)  time: 0.7496  data: 0.0001  max mem: 14938
[21:25:38.656325] Epoch: [7]  [160/345]  eta: 0:02:18  lr: 0.000047  loss: 0.9524 (0.9648)  time: 0.7490  data: 0.0001  max mem: 14938
[21:25:53.631267] Epoch: [7]  [180/345]  eta: 0:02:03  lr: 0.000047  loss: 0.9439 (0.9641)  time: 0.7487  data: 0.0001  max mem: 14938
[21:26:08.608839] Epoch: [7]  [200/345]  eta: 0:01:48  lr: 0.000047  loss: 0.9483 (0.9624)  time: 0.7488  data: 0.0001  max mem: 14938
[21:26:23.579903] Epoch: [7]  [220/345]  eta: 0:01:33  lr: 0.000048  loss: 0.9381 (0.9608)  time: 0.7485  data: 0.0001  max mem: 14938
[21:26:38.544167] Epoch: [7]  [240/345]  eta: 0:01:18  lr: 0.000048  loss: 0.9372 (0.9585)  time: 0.7482  data: 0.0001  max mem: 14938
[21:26:53.510011] Epoch: [7]  [260/345]  eta: 0:01:03  lr: 0.000048  loss: 0.9237 (0.9561)  time: 0.7482  data: 0.0001  max mem: 14938
[21:27:08.467869] Epoch: [7]  [280/345]  eta: 0:00:48  lr: 0.000049  loss: 0.9296 (0.9547)  time: 0.7478  data: 0.0001  max mem: 14938
[21:27:23.423498] Epoch: [7]  [300/345]  eta: 0:00:33  lr: 0.000049  loss: 0.9257 (0.9530)  time: 0.7477  data: 0.0001  max mem: 14938
[21:27:38.384148] Epoch: [7]  [320/345]  eta: 0:00:18  lr: 0.000050  loss: 0.9343 (0.9521)  time: 0.7480  data: 0.0001  max mem: 14938
[21:27:53.342253] Epoch: [7]  [340/345]  eta: 0:00:03  lr: 0.000050  loss: 0.9245 (0.9508)  time: 0.7479  data: 0.0001  max mem: 14938
[21:27:56.335224] Epoch: [7]  [344/345]  eta: 0:00:00  lr: 0.000050  loss: 0.9210 (0.9503)  time: 0.7479  data: 0.0001  max mem: 14938
[21:27:56.394664] Epoch: [7] Total time: 0:04:18 (0.7486 s / it)
[21:27:56.394969] Averaged stats: lr: 0.000050  loss: 0.9210 (0.9503)
[21:27:56.737829] Test:  [  0/345]  eta: 0:01:57  loss: 0.8678 (0.8678)  time: 0.3396  data: 0.1585  max mem: 14938
[21:27:58.575075] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8678 (0.8675)  time: 0.1978  data: 0.0145  max mem: 14938
[21:28:00.413376] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8733 (0.8766)  time: 0.1837  data: 0.0001  max mem: 14938
[21:28:02.256254] Test:  [ 30/345]  eta: 0:00:59  loss: 0.8746 (0.8749)  time: 0.1840  data: 0.0001  max mem: 14938
[21:28:04.101321] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8590 (0.8683)  time: 0.1843  data: 0.0001  max mem: 14938
[21:28:05.952106] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8667 (0.8700)  time: 0.1847  data: 0.0001  max mem: 14938
[21:28:07.804997] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8667 (0.8714)  time: 0.1851  data: 0.0001  max mem: 14938
[21:28:09.659151] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8632 (0.8703)  time: 0.1853  data: 0.0001  max mem: 14938
[21:28:11.517721] Test:  [ 80/345]  eta: 0:00:49  loss: 0.8632 (0.8706)  time: 0.1856  data: 0.0001  max mem: 14938
[21:28:13.379164] Test:  [ 90/345]  eta: 0:00:47  loss: 0.8625 (0.8712)  time: 0.1859  data: 0.0001  max mem: 14938
[21:28:15.245415] Test:  [100/345]  eta: 0:00:45  loss: 0.8755 (0.8718)  time: 0.1863  data: 0.0001  max mem: 14938
[21:28:17.114033] Test:  [110/345]  eta: 0:00:43  loss: 0.8787 (0.8727)  time: 0.1867  data: 0.0001  max mem: 14938
[21:28:18.986504] Test:  [120/345]  eta: 0:00:41  loss: 0.8824 (0.8731)  time: 0.1870  data: 0.0001  max mem: 14938
[21:28:20.863115] Test:  [130/345]  eta: 0:00:40  loss: 0.8795 (0.8733)  time: 0.1874  data: 0.0001  max mem: 14938
[21:28:22.742844] Test:  [140/345]  eta: 0:00:38  loss: 0.8663 (0.8717)  time: 0.1878  data: 0.0001  max mem: 14938
[21:28:24.624921] Test:  [150/345]  eta: 0:00:36  loss: 0.8715 (0.8731)  time: 0.1880  data: 0.0001  max mem: 14938
[21:28:26.512723] Test:  [160/345]  eta: 0:00:34  loss: 0.8899 (0.8734)  time: 0.1884  data: 0.0001  max mem: 14938
[21:28:28.402467] Test:  [170/345]  eta: 0:00:32  loss: 0.8799 (0.8735)  time: 0.1888  data: 0.0001  max mem: 14938
[21:28:30.296501] Test:  [180/345]  eta: 0:00:30  loss: 0.8779 (0.8739)  time: 0.1891  data: 0.0001  max mem: 14938
[21:28:32.194967] Test:  [190/345]  eta: 0:00:29  loss: 0.8725 (0.8733)  time: 0.1896  data: 0.0001  max mem: 14938
[21:28:34.096706] Test:  [200/345]  eta: 0:00:27  loss: 0.8570 (0.8730)  time: 0.1900  data: 0.0001  max mem: 14938
[21:28:36.001348] Test:  [210/345]  eta: 0:00:25  loss: 0.8715 (0.8730)  time: 0.1903  data: 0.0001  max mem: 14938
[21:28:37.908852] Test:  [220/345]  eta: 0:00:23  loss: 0.8715 (0.8731)  time: 0.1906  data: 0.0001  max mem: 14938
[21:28:39.823580] Test:  [230/345]  eta: 0:00:21  loss: 0.8753 (0.8734)  time: 0.1911  data: 0.0001  max mem: 14938
[21:28:41.739189] Test:  [240/345]  eta: 0:00:19  loss: 0.8764 (0.8738)  time: 0.1915  data: 0.0001  max mem: 14938
[21:28:43.658500] Test:  [250/345]  eta: 0:00:17  loss: 0.8727 (0.8731)  time: 0.1917  data: 0.0001  max mem: 14938
[21:28:45.581402] Test:  [260/345]  eta: 0:00:16  loss: 0.8579 (0.8729)  time: 0.1921  data: 0.0001  max mem: 14938
[21:28:47.506676] Test:  [270/345]  eta: 0:00:14  loss: 0.8637 (0.8731)  time: 0.1924  data: 0.0001  max mem: 14938
[21:28:49.437174] Test:  [280/345]  eta: 0:00:12  loss: 0.8650 (0.8732)  time: 0.1927  data: 0.0001  max mem: 14938
[21:28:51.368634] Test:  [290/345]  eta: 0:00:10  loss: 0.8730 (0.8734)  time: 0.1930  data: 0.0001  max mem: 14938
[21:28:53.303842] Test:  [300/345]  eta: 0:00:08  loss: 0.8753 (0.8734)  time: 0.1933  data: 0.0001  max mem: 14938
[21:28:55.241943] Test:  [310/345]  eta: 0:00:06  loss: 0.8760 (0.8738)  time: 0.1936  data: 0.0001  max mem: 14938
[21:28:57.184242] Test:  [320/345]  eta: 0:00:04  loss: 0.8784 (0.8741)  time: 0.1940  data: 0.0001  max mem: 14938
[21:28:59.128892] Test:  [330/345]  eta: 0:00:02  loss: 0.8688 (0.8738)  time: 0.1943  data: 0.0001  max mem: 14938
[21:29:01.077291] Test:  [340/345]  eta: 0:00:00  loss: 0.8682 (0.8739)  time: 0.1946  data: 0.0001  max mem: 14938
[21:29:01.858241] Test:  [344/345]  eta: 0:00:00  loss: 0.8774 (0.8741)  time: 0.1948  data: 0.0001  max mem: 14938
[21:29:01.917879] Test: Total time: 0:01:05 (0.1899 s / it)
[21:29:13.167877] Test:  [ 0/57]  eta: 0:00:18  loss: 0.9838 (0.9838)  time: 0.3210  data: 0.1417  max mem: 14938
[21:29:14.982060] Test:  [10/57]  eta: 0:00:09  loss: 0.9383 (0.9433)  time: 0.1940  data: 0.0129  max mem: 14938
[21:29:16.801225] Test:  [20/57]  eta: 0:00:06  loss: 0.9383 (0.9349)  time: 0.1816  data: 0.0001  max mem: 14938
[21:29:18.622913] Test:  [30/57]  eta: 0:00:05  loss: 0.8351 (0.8909)  time: 0.1820  data: 0.0001  max mem: 14938
[21:29:20.450257] Test:  [40/57]  eta: 0:00:03  loss: 0.7914 (0.8643)  time: 0.1824  data: 0.0001  max mem: 14938
[21:29:22.281280] Test:  [50/57]  eta: 0:00:01  loss: 0.7914 (0.8559)  time: 0.1829  data: 0.0001  max mem: 14938
[21:29:23.269949] Test:  [56/57]  eta: 0:00:00  loss: 0.8232 (0.8604)  time: 0.1775  data: 0.0001  max mem: 14938
[21:29:23.328377] Test: Total time: 0:00:10 (0.1839 s / it)
[21:29:25.278791] Dice score of the network on the train images: 0.732219, val images: 0.785879
[21:29:25.279030] saving best_prec_model_0 @ epoch 7
[21:29:26.471118] saving best_dice_model_0 @ epoch 7
[21:29:27.528398] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[21:29:28.418547] Epoch: [8]  [  0/345]  eta: 0:05:06  lr: 0.000050  loss: 0.9461 (0.9461)  time: 0.8890  data: 0.1460  max mem: 14938
[21:29:43.295771] Epoch: [8]  [ 20/345]  eta: 0:04:03  lr: 0.000050  loss: 0.9146 (0.9243)  time: 0.7438  data: 0.0001  max mem: 14938
[21:29:58.218035] Epoch: [8]  [ 40/345]  eta: 0:03:48  lr: 0.000051  loss: 0.9088 (0.9212)  time: 0.7461  data: 0.0001  max mem: 14938
[21:30:13.181414] Epoch: [8]  [ 60/345]  eta: 0:03:33  lr: 0.000051  loss: 0.9210 (0.9240)  time: 0.7481  data: 0.0001  max mem: 14938
[21:30:28.182290] Epoch: [8]  [ 80/345]  eta: 0:03:18  lr: 0.000051  loss: 0.9160 (0.9213)  time: 0.7500  data: 0.0001  max mem: 14938
[21:30:43.212296] Epoch: [8]  [100/345]  eta: 0:03:03  lr: 0.000052  loss: 0.9146 (0.9212)  time: 0.7515  data: 0.0001  max mem: 14938
[21:30:58.236336] Epoch: [8]  [120/345]  eta: 0:02:48  lr: 0.000052  loss: 0.9204 (0.9207)  time: 0.7512  data: 0.0001  max mem: 14938
[21:31:13.253773] Epoch: [8]  [140/345]  eta: 0:02:33  lr: 0.000053  loss: 0.9310 (0.9217)  time: 0.7508  data: 0.0001  max mem: 14938
[21:31:28.259845] Epoch: [8]  [160/345]  eta: 0:02:18  lr: 0.000053  loss: 0.9082 (0.9208)  time: 0.7503  data: 0.0001  max mem: 14938
[21:31:43.261457] Epoch: [8]  [180/345]  eta: 0:02:03  lr: 0.000053  loss: 0.9013 (0.9198)  time: 0.7500  data: 0.0001  max mem: 14938
[21:31:58.258591] Epoch: [8]  [200/345]  eta: 0:01:48  lr: 0.000054  loss: 0.8906 (0.9173)  time: 0.7498  data: 0.0001  max mem: 14938
[21:32:13.255997] Epoch: [8]  [220/345]  eta: 0:01:33  lr: 0.000054  loss: 0.8946 (0.9153)  time: 0.7498  data: 0.0001  max mem: 14938
[21:32:28.242189] Epoch: [8]  [240/345]  eta: 0:01:18  lr: 0.000054  loss: 0.8983 (0.9141)  time: 0.7493  data: 0.0001  max mem: 14938
[21:32:43.225505] Epoch: [8]  [260/345]  eta: 0:01:03  lr: 0.000055  loss: 0.9040 (0.9135)  time: 0.7491  data: 0.0001  max mem: 14938
[21:32:58.209875] Epoch: [8]  [280/345]  eta: 0:00:48  lr: 0.000055  loss: 0.8965 (0.9123)  time: 0.7492  data: 0.0001  max mem: 14938
[21:33:13.192175] Epoch: [8]  [300/345]  eta: 0:00:33  lr: 0.000055  loss: 0.8903 (0.9109)  time: 0.7491  data: 0.0001  max mem: 14938
[21:33:28.177674] Epoch: [8]  [320/345]  eta: 0:00:18  lr: 0.000056  loss: 0.8821 (0.9098)  time: 0.7492  data: 0.0001  max mem: 14938
[21:33:43.153841] Epoch: [8]  [340/345]  eta: 0:00:03  lr: 0.000056  loss: 0.8820 (0.9082)  time: 0.7488  data: 0.0001  max mem: 14938
[21:33:46.147938] Epoch: [8]  [344/345]  eta: 0:00:00  lr: 0.000056  loss: 0.8820 (0.9079)  time: 0.7488  data: 0.0001  max mem: 14938
[21:33:46.214384] Epoch: [8] Total time: 0:04:18 (0.7498 s / it)
[21:33:46.214738] Averaged stats: lr: 0.000056  loss: 0.8820 (0.9079)
[21:33:46.556718] Test:  [  0/345]  eta: 0:01:56  loss: 0.8698 (0.8698)  time: 0.3377  data: 0.1563  max mem: 14938
[21:33:48.393750] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8429 (0.8445)  time: 0.1976  data: 0.0143  max mem: 14938
[21:33:50.232990] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8326 (0.8352)  time: 0.1837  data: 0.0001  max mem: 14938
[21:33:52.074645] Test:  [ 30/345]  eta: 0:00:59  loss: 0.8398 (0.8393)  time: 0.1840  data: 0.0001  max mem: 14938
[21:33:53.918876] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8472 (0.8421)  time: 0.1842  data: 0.0001  max mem: 14938
[21:33:55.769610] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8431 (0.8412)  time: 0.1847  data: 0.0001  max mem: 14938
[21:33:57.622618] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8401 (0.8428)  time: 0.1851  data: 0.0001  max mem: 14938
[21:33:59.479555] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8440 (0.8440)  time: 0.1854  data: 0.0001  max mem: 14938
[21:34:01.339378] Test:  [ 80/345]  eta: 0:00:49  loss: 0.8516 (0.8442)  time: 0.1858  data: 0.0001  max mem: 14938
[21:34:03.204216] Test:  [ 90/345]  eta: 0:00:47  loss: 0.8492 (0.8456)  time: 0.1862  data: 0.0001  max mem: 14938
[21:34:05.071867] Test:  [100/345]  eta: 0:00:45  loss: 0.8467 (0.8453)  time: 0.1866  data: 0.0001  max mem: 14938
[21:34:06.941675] Test:  [110/345]  eta: 0:00:43  loss: 0.8442 (0.8459)  time: 0.1868  data: 0.0001  max mem: 14938
[21:34:08.815282] Test:  [120/345]  eta: 0:00:42  loss: 0.8548 (0.8460)  time: 0.1871  data: 0.0001  max mem: 14938
[21:34:10.693288] Test:  [130/345]  eta: 0:00:40  loss: 0.8314 (0.8453)  time: 0.1875  data: 0.0001  max mem: 14938
[21:34:12.573781] Test:  [140/345]  eta: 0:00:38  loss: 0.8395 (0.8458)  time: 0.1879  data: 0.0001  max mem: 14938
[21:34:14.457244] Test:  [150/345]  eta: 0:00:36  loss: 0.8395 (0.8455)  time: 0.1881  data: 0.0001  max mem: 14938
[21:34:16.343393] Test:  [160/345]  eta: 0:00:34  loss: 0.8374 (0.8457)  time: 0.1884  data: 0.0001  max mem: 14938
[21:34:18.233970] Test:  [170/345]  eta: 0:00:32  loss: 0.8595 (0.8467)  time: 0.1888  data: 0.0001  max mem: 14938
[21:34:20.128495] Test:  [180/345]  eta: 0:00:30  loss: 0.8525 (0.8461)  time: 0.1892  data: 0.0001  max mem: 14938
[21:34:22.027034] Test:  [190/345]  eta: 0:00:29  loss: 0.8469 (0.8466)  time: 0.1896  data: 0.0001  max mem: 14938
[21:34:23.930206] Test:  [200/345]  eta: 0:00:27  loss: 0.8442 (0.8460)  time: 0.1900  data: 0.0001  max mem: 14938
[21:34:25.833805] Test:  [210/345]  eta: 0:00:25  loss: 0.8436 (0.8462)  time: 0.1903  data: 0.0001  max mem: 14938
[21:34:27.741920] Test:  [220/345]  eta: 0:00:23  loss: 0.8436 (0.8461)  time: 0.1905  data: 0.0001  max mem: 14938
[21:34:29.655407] Test:  [230/345]  eta: 0:00:21  loss: 0.8441 (0.8468)  time: 0.1910  data: 0.0001  max mem: 14938
[21:34:31.571196] Test:  [240/345]  eta: 0:00:19  loss: 0.8511 (0.8468)  time: 0.1914  data: 0.0001  max mem: 14938
[21:34:33.490709] Test:  [250/345]  eta: 0:00:17  loss: 0.8409 (0.8462)  time: 0.1917  data: 0.0001  max mem: 14938
[21:34:35.412602] Test:  [260/345]  eta: 0:00:16  loss: 0.8510 (0.8469)  time: 0.1920  data: 0.0001  max mem: 14938
[21:34:37.339860] Test:  [270/345]  eta: 0:00:14  loss: 0.8553 (0.8468)  time: 0.1924  data: 0.0001  max mem: 14938
[21:34:39.270798] Test:  [280/345]  eta: 0:00:12  loss: 0.8546 (0.8475)  time: 0.1929  data: 0.0001  max mem: 14938
[21:34:41.204184] Test:  [290/345]  eta: 0:00:10  loss: 0.8546 (0.8479)  time: 0.1932  data: 0.0001  max mem: 14938
[21:34:43.141961] Test:  [300/345]  eta: 0:00:08  loss: 0.8518 (0.8481)  time: 0.1935  data: 0.0001  max mem: 14938
[21:34:45.082186] Test:  [310/345]  eta: 0:00:06  loss: 0.8522 (0.8483)  time: 0.1939  data: 0.0001  max mem: 14938
[21:34:47.023591] Test:  [320/345]  eta: 0:00:04  loss: 0.8304 (0.8476)  time: 0.1940  data: 0.0001  max mem: 14938
[21:34:48.969815] Test:  [330/345]  eta: 0:00:02  loss: 0.8385 (0.8476)  time: 0.1943  data: 0.0001  max mem: 14938
[21:34:50.918226] Test:  [340/345]  eta: 0:00:00  loss: 0.8488 (0.8482)  time: 0.1947  data: 0.0001  max mem: 14938
[21:34:51.699248] Test:  [344/345]  eta: 0:00:00  loss: 0.8610 (0.8484)  time: 0.1948  data: 0.0001  max mem: 14938
[21:34:51.758358] Test: Total time: 0:01:05 (0.1900 s / it)
[21:35:03.134958] Test:  [ 0/57]  eta: 0:00:18  loss: 0.9252 (0.9252)  time: 0.3215  data: 0.1429  max mem: 14938
[21:35:04.948454] Test:  [10/57]  eta: 0:00:09  loss: 0.8956 (0.9101)  time: 0.1940  data: 0.0130  max mem: 14938
[21:35:06.767372] Test:  [20/57]  eta: 0:00:06  loss: 0.8956 (0.9036)  time: 0.1816  data: 0.0001  max mem: 14938
[21:35:08.588649] Test:  [30/57]  eta: 0:00:05  loss: 0.8235 (0.8657)  time: 0.1820  data: 0.0001  max mem: 14938
[21:35:10.416572] Test:  [40/57]  eta: 0:00:03  loss: 0.7713 (0.8429)  time: 0.1824  data: 0.0001  max mem: 14938
[21:35:12.248454] Test:  [50/57]  eta: 0:00:01  loss: 0.7713 (0.8359)  time: 0.1829  data: 0.0001  max mem: 14938
[21:35:13.236477] Test:  [56/57]  eta: 0:00:00  loss: 0.8171 (0.8416)  time: 0.1775  data: 0.0001  max mem: 14938
[21:35:13.293858] Test: Total time: 0:00:10 (0.1839 s / it)
[21:35:15.291452] Dice score of the network on the train images: 0.726147, val images: 0.793489
[21:35:15.291688] saving best_rec_model_0 @ epoch 8
[21:35:16.396608] saving best_dice_model_0 @ epoch 8
[21:35:17.465308] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[21:35:18.358238] Epoch: [9]  [  0/345]  eta: 0:05:07  lr: 0.000056  loss: 0.8741 (0.8741)  time: 0.8918  data: 0.1526  max mem: 14938
[21:35:33.211284] Epoch: [9]  [ 20/345]  eta: 0:04:03  lr: 0.000057  loss: 0.8658 (0.8770)  time: 0.7426  data: 0.0001  max mem: 14938
[21:35:48.136808] Epoch: [9]  [ 40/345]  eta: 0:03:48  lr: 0.000057  loss: 0.9069 (0.8890)  time: 0.7462  data: 0.0001  max mem: 14938
[21:36:03.103066] Epoch: [9]  [ 60/345]  eta: 0:03:33  lr: 0.000057  loss: 0.8950 (0.8918)  time: 0.7483  data: 0.0001  max mem: 14938
[21:36:18.104679] Epoch: [9]  [ 80/345]  eta: 0:03:18  lr: 0.000058  loss: 0.8769 (0.8889)  time: 0.7500  data: 0.0001  max mem: 14938
[21:36:33.135729] Epoch: [9]  [100/345]  eta: 0:03:03  lr: 0.000058  loss: 0.8837 (0.8873)  time: 0.7515  data: 0.0001  max mem: 14938
[21:36:48.160847] Epoch: [9]  [120/345]  eta: 0:02:48  lr: 0.000058  loss: 0.8773 (0.8871)  time: 0.7512  data: 0.0001  max mem: 14938
[21:37:03.175074] Epoch: [9]  [140/345]  eta: 0:02:33  lr: 0.000059  loss: 0.8803 (0.8860)  time: 0.7507  data: 0.0001  max mem: 14938
[21:37:18.177865] Epoch: [9]  [160/345]  eta: 0:02:18  lr: 0.000059  loss: 0.8888 (0.8853)  time: 0.7501  data: 0.0001  max mem: 14938
[21:37:33.180833] Epoch: [9]  [180/345]  eta: 0:02:03  lr: 0.000060  loss: 0.8896 (0.8861)  time: 0.7501  data: 0.0001  max mem: 14938
[21:37:48.177020] Epoch: [9]  [200/345]  eta: 0:01:48  lr: 0.000060  loss: 0.8717 (0.8847)  time: 0.7498  data: 0.0001  max mem: 14938
[21:38:03.174171] Epoch: [9]  [220/345]  eta: 0:01:33  lr: 0.000060  loss: 0.8829 (0.8846)  time: 0.7498  data: 0.0001  max mem: 14938
[21:38:18.168999] Epoch: [9]  [240/345]  eta: 0:01:18  lr: 0.000061  loss: 0.8562 (0.8828)  time: 0.7497  data: 0.0001  max mem: 14938
[21:38:33.155075] Epoch: [9]  [260/345]  eta: 0:01:03  lr: 0.000061  loss: 0.8878 (0.8833)  time: 0.7493  data: 0.0001  max mem: 14938
[21:38:48.134181] Epoch: [9]  [280/345]  eta: 0:00:48  lr: 0.000061  loss: 0.8810 (0.8830)  time: 0.7489  data: 0.0001  max mem: 14938
[21:39:03.121123] Epoch: [9]  [300/345]  eta: 0:00:33  lr: 0.000062  loss: 0.8755 (0.8823)  time: 0.7493  data: 0.0001  max mem: 14938

[21:39:18.104490] Epoch: [9]  [320/345]  eta: 0:00:18  lr: 0.000062  loss: 0.8727 (0.8819)  time: 0.7491  data: 0.0001  max mem: 14938
[21:39:33.084773] Epoch: [9]  [340/345]  eta: 0:00:03  lr: 0.000062  loss: 0.8739 (0.8814)  time: 0.7490  data: 0.0001  max mem: 14938
[21:39:36.083190] Epoch: [9]  [344/345]  eta: 0:00:00  lr: 0.000062  loss: 0.8538 (0.8813)  time: 0.7491  data: 0.0001  max mem: 14938
[21:39:36.149255] Epoch: [9] Total time: 0:04:18 (0.7498 s / it)
[21:39:36.149586] Averaged stats: lr: 0.000062  loss: 0.8538 (0.8813)
[21:39:36.488175] Test:  [  0/345]  eta: 0:01:55  loss: 0.8461 (0.8461)  time: 0.3339  data: 0.1523  max mem: 14938
[21:39:38.325229] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8456 (0.8412)  time: 0.1973  data: 0.0139  max mem: 14938
[21:39:40.164874] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8371 (0.8376)  time: 0.1838  data: 0.0001  max mem: 14938
[21:39:42.008665] Test:  [ 30/345]  eta: 0:00:59  loss: 0.8220 (0.8339)  time: 0.1841  data: 0.0001  max mem: 14938
[21:39:43.854536] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8308 (0.8361)  time: 0.1844  data: 0.0001  max mem: 14938
[21:39:45.703434] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8475 (0.8388)  time: 0.1847  data: 0.0001  max mem: 14938
[21:39:47.557100] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8330 (0.8362)  time: 0.1851  data: 0.0001  max mem: 14938
[21:39:49.412117] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8180 (0.8358)  time: 0.1854  data: 0.0001  max mem: 14938
[21:39:51.271160] Test:  [ 80/345]  eta: 0:00:49  loss: 0.8180 (0.8340)  time: 0.1857  data: 0.0001  max mem: 14938
[21:39:53.133074] Test:  [ 90/345]  eta: 0:00:47  loss: 0.8245 (0.8344)  time: 0.1860  data: 0.0001  max mem: 14938
[21:39:54.998942] Test:  [100/345]  eta: 0:00:45  loss: 0.8316 (0.8344)  time: 0.1863  data: 0.0001  max mem: 14938
[21:39:56.866518] Test:  [110/345]  eta: 0:00:43  loss: 0.8226 (0.8339)  time: 0.1866  data: 0.0001  max mem: 14938
[21:39:58.738252] Test:  [120/345]  eta: 0:00:41  loss: 0.8287 (0.8345)  time: 0.1869  data: 0.0001  max mem: 14938
[21:40:00.614134] Test:  [130/345]  eta: 0:00:40  loss: 0.8287 (0.8344)  time: 0.1873  data: 0.0001  max mem: 14938
[21:40:02.494141] Test:  [140/345]  eta: 0:00:38  loss: 0.8251 (0.8334)  time: 0.1877  data: 0.0001  max mem: 14938
[21:40:04.378000] Test:  [150/345]  eta: 0:00:36  loss: 0.8230 (0.8334)  time: 0.1881  data: 0.0001  max mem: 14938
[21:40:06.263218] Test:  [160/345]  eta: 0:00:34  loss: 0.8251 (0.8331)  time: 0.1884  data: 0.0001  max mem: 14938
[21:40:08.154144] Test:  [170/345]  eta: 0:00:32  loss: 0.8342 (0.8338)  time: 0.1888  data: 0.0001  max mem: 14938
[21:40:10.048124] Test:  [180/345]  eta: 0:00:30  loss: 0.8455 (0.8346)  time: 0.1892  data: 0.0001  max mem: 14938
[21:40:11.946953] Test:  [190/345]  eta: 0:00:29  loss: 0.8335 (0.8345)  time: 0.1896  data: 0.0001  max mem: 14938
[21:40:13.849228] Test:  [200/345]  eta: 0:00:27  loss: 0.8309 (0.8342)  time: 0.1900  data: 0.0001  max mem: 14938
[21:40:15.753144] Test:  [210/345]  eta: 0:00:25  loss: 0.8383 (0.8343)  time: 0.1903  data: 0.0001  max mem: 14938
[21:40:17.662194] Test:  [220/345]  eta: 0:00:23  loss: 0.8382 (0.8345)  time: 0.1906  data: 0.0001  max mem: 14938
[21:40:19.573560] Test:  [230/345]  eta: 0:00:21  loss: 0.8467 (0.8352)  time: 0.1910  data: 0.0001  max mem: 14938
[21:40:21.488854] Test:  [240/345]  eta: 0:00:19  loss: 0.8467 (0.8354)  time: 0.1913  data: 0.0001  max mem: 14938
[21:40:23.408819] Test:  [250/345]  eta: 0:00:17  loss: 0.8320 (0.8354)  time: 0.1917  data: 0.0001  max mem: 14938
[21:40:25.330895] Test:  [260/345]  eta: 0:00:16  loss: 0.8314 (0.8353)  time: 0.1921  data: 0.0001  max mem: 14938
[21:40:27.256652] Test:  [270/345]  eta: 0:00:14  loss: 0.8265 (0.8349)  time: 0.1923  data: 0.0001  max mem: 14938
[21:40:29.187244] Test:  [280/345]  eta: 0:00:12  loss: 0.8314 (0.8350)  time: 0.1928  data: 0.0001  max mem: 14938
[21:40:31.119666] Test:  [290/345]  eta: 0:00:10  loss: 0.8289 (0.8348)  time: 0.1931  data: 0.0001  max mem: 14938
[21:40:33.056488] Test:  [300/345]  eta: 0:00:08  loss: 0.8247 (0.8346)  time: 0.1934  data: 0.0001  max mem: 14938
[21:40:34.995421] Test:  [310/345]  eta: 0:00:06  loss: 0.8277 (0.8349)  time: 0.1937  data: 0.0001  max mem: 14938
[21:40:36.939771] Test:  [320/345]  eta: 0:00:04  loss: 0.8416 (0.8351)  time: 0.1941  data: 0.0001  max mem: 14938
[21:40:38.884733] Test:  [330/345]  eta: 0:00:02  loss: 0.8298 (0.8348)  time: 0.1944  data: 0.0001  max mem: 14938
[21:40:40.833664] Test:  [340/345]  eta: 0:00:00  loss: 0.8298 (0.8350)  time: 0.1946  data: 0.0001  max mem: 14938
[21:40:41.614390] Test:  [344/345]  eta: 0:00:00  loss: 0.8298 (0.8349)  time: 0.1948  data: 0.0001  max mem: 14938
[21:40:41.675592] Test: Total time: 0:01:05 (0.1899 s / it)
[21:40:53.104523] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8807 (0.8807)  time: 0.3263  data: 0.1473  max mem: 14938
[21:40:54.917899] Test:  [10/57]  eta: 0:00:09  loss: 0.8807 (0.8992)  time: 0.1944  data: 0.0134  max mem: 14938
[21:40:56.736817] Test:  [20/57]  eta: 0:00:06  loss: 0.8861 (0.8966)  time: 0.1815  data: 0.0001  max mem: 14938
[21:40:58.560694] Test:  [30/57]  eta: 0:00:05  loss: 0.8086 (0.8593)  time: 0.1821  data: 0.0001  max mem: 14938
[21:41:00.387352] Test:  [40/57]  eta: 0:00:03  loss: 0.7706 (0.8372)  time: 0.1825  data: 0.0001  max mem: 14938
[21:41:02.220380] Test:  [50/57]  eta: 0:00:01  loss: 0.7706 (0.8305)  time: 0.1829  data: 0.0001  max mem: 14938
[21:41:03.208622] Test:  [56/57]  eta: 0:00:00  loss: 0.8128 (0.8361)  time: 0.1776  data: 0.0001  max mem: 14938
[21:41:03.266271] Test: Total time: 0:00:10 (0.1840 s / it)
[21:41:05.249787] Dice score of the network on the train images: 0.733549, val images: 0.798470
[21:41:05.250023] saving best_rec_model_0 @ epoch 9
[21:41:06.321096] saving best_dice_model_0 @ epoch 9
[21:41:07.494250] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[21:41:08.382512] Epoch: [10]  [  0/345]  eta: 0:05:06  lr: 0.000063  loss: 0.8916 (0.8916)  time: 0.8872  data: 0.1471  max mem: 14938
[21:41:23.251739] Epoch: [10]  [ 20/345]  eta: 0:04:03  lr: 0.000063  loss: 0.8705 (0.8720)  time: 0.7434  data: 0.0001  max mem: 14938
[21:41:38.185521] Epoch: [10]  [ 40/345]  eta: 0:03:48  lr: 0.000063  loss: 0.8562 (0.8654)  time: 0.7466  data: 0.0001  max mem: 14938
[21:41:53.161735] Epoch: [10]  [ 60/345]  eta: 0:03:33  lr: 0.000064  loss: 0.8610 (0.8650)  time: 0.7488  data: 0.0001  max mem: 14938
[21:42:08.175899] Epoch: [10]  [ 80/345]  eta: 0:03:18  lr: 0.000064  loss: 0.8624 (0.8657)  time: 0.7507  data: 0.0001  max mem: 14938
[21:42:23.209965] Epoch: [10]  [100/345]  eta: 0:03:03  lr: 0.000064  loss: 0.8661 (0.8656)  time: 0.7517  data: 0.0001  max mem: 14938
[21:42:38.223810] Epoch: [10]  [120/345]  eta: 0:02:48  lr: 0.000065  loss: 0.8595 (0.8642)  time: 0.7507  data: 0.0001  max mem: 14938
[21:42:53.222075] Epoch: [10]  [140/345]  eta: 0:02:33  lr: 0.000065  loss: 0.8434 (0.8619)  time: 0.7499  data: 0.0001  max mem: 14938
[21:43:08.213702] Epoch: [10]  [160/345]  eta: 0:02:18  lr: 0.000065  loss: 0.8560 (0.8618)  time: 0.7495  data: 0.0001  max mem: 14938
[21:43:23.193343] Epoch: [10]  [180/345]  eta: 0:02:03  lr: 0.000066  loss: 0.8487 (0.8618)  time: 0.7489  data: 0.0001  max mem: 14938
[21:43:38.182347] Epoch: [10]  [200/345]  eta: 0:01:48  lr: 0.000066  loss: 0.8437 (0.8611)  time: 0.7494  data: 0.0001  max mem: 14938
[21:43:53.178317] Epoch: [10]  [220/345]  eta: 0:01:33  lr: 0.000066  loss: 0.8425 (0.8596)  time: 0.7498  data: 0.0001  max mem: 14938
[21:44:08.166823] Epoch: [10]  [240/345]  eta: 0:01:18  lr: 0.000067  loss: 0.8488 (0.8593)  time: 0.7494  data: 0.0001  max mem: 14938
[21:44:23.152274] Epoch: [10]  [260/345]  eta: 0:01:03  lr: 0.000067  loss: 0.8594 (0.8593)  time: 0.7492  data: 0.0001  max mem: 14938
[21:44:38.141526] Epoch: [10]  [280/345]  eta: 0:00:48  lr: 0.000068  loss: 0.8457 (0.8586)  time: 0.7494  data: 0.0001  max mem: 14938
[21:44:53.120461] Epoch: [10]  [300/345]  eta: 0:00:33  lr: 0.000068  loss: 0.8324 (0.8575)  time: 0.7489  data: 0.0001  max mem: 14938
[21:45:08.101580] Epoch: [10]  [320/345]  eta: 0:00:18  lr: 0.000068  loss: 0.8355 (0.8567)  time: 0.7490  data: 0.0001  max mem: 14938
[21:45:23.078248] Epoch: [10]  [340/345]  eta: 0:00:03  lr: 0.000069  loss: 0.8348 (0.8554)  time: 0.7488  data: 0.0001  max mem: 14938
[21:45:26.075046] Epoch: [10]  [344/345]  eta: 0:00:00  lr: 0.000069  loss: 0.8310 (0.8551)  time: 0.7489  data: 0.0001  max mem: 14938
[21:45:26.133380] Epoch: [10] Total time: 0:04:18 (0.7497 s / it)
[21:45:26.133824] Averaged stats: lr: 0.000069  loss: 0.8310 (0.8551)
[21:45:26.472300] Test:  [  0/345]  eta: 0:01:55  loss: 0.7681 (0.7681)  time: 0.3343  data: 0.1528  max mem: 14938
[21:45:28.307831] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8113 (0.8043)  time: 0.1972  data: 0.0139  max mem: 14938
[21:45:30.146321] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8082 (0.8037)  time: 0.1836  data: 0.0001  max mem: 14938
[21:45:31.988458] Test:  [ 30/345]  eta: 0:00:59  loss: 0.8051 (0.8069)  time: 0.1840  data: 0.0001  max mem: 14938
[21:45:33.835088] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8177 (0.8098)  time: 0.1844  data: 0.0001  max mem: 14938
[21:45:35.685064] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8037 (0.8078)  time: 0.1848  data: 0.0001  max mem: 14938
[21:45:37.539596] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7987 (0.8093)  time: 0.1852  data: 0.0001  max mem: 14938
[21:45:39.395194] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8044 (0.8095)  time: 0.1855  data: 0.0001  max mem: 14938
[21:45:41.254552] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7996 (0.8080)  time: 0.1857  data: 0.0001  max mem: 14938
[21:45:43.117538] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7955 (0.8079)  time: 0.1861  data: 0.0001  max mem: 14938
[21:45:44.983726] Test:  [100/345]  eta: 0:00:45  loss: 0.8143 (0.8088)  time: 0.1864  data: 0.0001  max mem: 14938
[21:45:46.852290] Test:  [110/345]  eta: 0:00:43  loss: 0.8150 (0.8092)  time: 0.1867  data: 0.0001  max mem: 14938
[21:45:48.723454] Test:  [120/345]  eta: 0:00:41  loss: 0.8058 (0.8087)  time: 0.1869  data: 0.0001  max mem: 14938
[21:45:50.599603] Test:  [130/345]  eta: 0:00:40  loss: 0.8053 (0.8084)  time: 0.1873  data: 0.0001  max mem: 14938
[21:45:52.479936] Test:  [140/345]  eta: 0:00:38  loss: 0.7988 (0.8078)  time: 0.1878  data: 0.0001  max mem: 14938
[21:45:54.363623] Test:  [150/345]  eta: 0:00:36  loss: 0.7990 (0.8075)  time: 0.1882  data: 0.0001  max mem: 14938
[21:45:56.249731] Test:  [160/345]  eta: 0:00:34  loss: 0.8131 (0.8081)  time: 0.1884  data: 0.0001  max mem: 14938
[21:45:58.140646] Test:  [170/345]  eta: 0:00:32  loss: 0.8131 (0.8080)  time: 0.1888  data: 0.0001  max mem: 14938
[21:46:00.036567] Test:  [180/345]  eta: 0:00:30  loss: 0.8112 (0.8080)  time: 0.1893  data: 0.0001  max mem: 14938
[21:46:01.933688] Test:  [190/345]  eta: 0:00:29  loss: 0.8162 (0.8089)  time: 0.1896  data: 0.0001  max mem: 14938
[21:46:03.834508] Test:  [200/345]  eta: 0:00:27  loss: 0.8204 (0.8097)  time: 0.1898  data: 0.0001  max mem: 14938
[21:46:05.738316] Test:  [210/345]  eta: 0:00:25  loss: 0.8159 (0.8097)  time: 0.1902  data: 0.0001  max mem: 14938
[21:46:07.648360] Test:  [220/345]  eta: 0:00:23  loss: 0.8086 (0.8101)  time: 0.1906  data: 0.0001  max mem: 14938
[21:46:09.561494] Test:  [230/345]  eta: 0:00:21  loss: 0.8093 (0.8108)  time: 0.1911  data: 0.0001  max mem: 14938
[21:46:11.476587] Test:  [240/345]  eta: 0:00:19  loss: 0.7985 (0.8102)  time: 0.1914  data: 0.0001  max mem: 14938
[21:46:13.395548] Test:  [250/345]  eta: 0:00:17  loss: 0.8078 (0.8104)  time: 0.1917  data: 0.0001  max mem: 14938
[21:46:15.316745] Test:  [260/345]  eta: 0:00:16  loss: 0.8099 (0.8104)  time: 0.1920  data: 0.0001  max mem: 14938
[21:46:17.242110] Test:  [270/345]  eta: 0:00:14  loss: 0.8063 (0.8102)  time: 0.1923  data: 0.0001  max mem: 14938
[21:46:19.171442] Test:  [280/345]  eta: 0:00:12  loss: 0.8056 (0.8101)  time: 0.1927  data: 0.0001  max mem: 14938
[21:46:21.103378] Test:  [290/345]  eta: 0:00:10  loss: 0.8103 (0.8103)  time: 0.1930  data: 0.0001  max mem: 14938
[21:46:23.041229] Test:  [300/345]  eta: 0:00:08  loss: 0.7905 (0.8101)  time: 0.1934  data: 0.0001  max mem: 14938
[21:46:24.981522] Test:  [310/345]  eta: 0:00:06  loss: 0.7840 (0.8093)  time: 0.1939  data: 0.0001  max mem: 14938
[21:46:26.923573] Test:  [320/345]  eta: 0:00:04  loss: 0.7999 (0.8095)  time: 0.1941  data: 0.0001  max mem: 14938
[21:46:28.870463] Test:  [330/345]  eta: 0:00:02  loss: 0.8107 (0.8095)  time: 0.1944  data: 0.0001  max mem: 14938
[21:46:30.818291] Test:  [340/345]  eta: 0:00:00  loss: 0.8039 (0.8097)  time: 0.1947  data: 0.0001  max mem: 14938
[21:46:31.598949] Test:  [344/345]  eta: 0:00:00  loss: 0.8122 (0.8097)  time: 0.1948  data: 0.0001  max mem: 14938
[21:46:31.660814] Test: Total time: 0:01:05 (0.1899 s / it)
[21:46:42.990900] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8919 (0.8919)  time: 0.3248  data: 0.1450  max mem: 14938
[21:46:44.805272] Test:  [10/57]  eta: 0:00:09  loss: 0.8918 (0.8976)  time: 0.1944  data: 0.0132  max mem: 14938
[21:46:46.625227] Test:  [20/57]  eta: 0:00:06  loss: 0.8918 (0.8956)  time: 0.1816  data: 0.0001  max mem: 14938
[21:46:48.448454] Test:  [30/57]  eta: 0:00:05  loss: 0.7993 (0.8545)  time: 0.1821  data: 0.0001  max mem: 14938
[21:46:50.277599] Test:  [40/57]  eta: 0:00:03  loss: 0.7620 (0.8302)  time: 0.1826  data: 0.0001  max mem: 14938
[21:46:52.111657] Test:  [50/57]  eta: 0:00:01  loss: 0.7620 (0.8223)  time: 0.1831  data: 0.0001  max mem: 14938
[21:46:53.100120] Test:  [56/57]  eta: 0:00:00  loss: 0.7860 (0.8276)  time: 0.1777  data: 0.0001  max mem: 14938
[21:46:53.155304] Test: Total time: 0:00:10 (0.1840 s / it)
[21:46:55.123553] Dice score of the network on the train images: 0.766778, val images: 0.808977
[21:46:55.123781] saving best_prec_model_0 @ epoch 10
[21:46:56.293800] saving best_dice_model_0 @ epoch 10
[21:46:57.305144] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[21:46:58.194604] Epoch: [11]  [  0/345]  eta: 0:05:06  lr: 0.000069  loss: 0.8719 (0.8719)  time: 0.8887  data: 0.1485  max mem: 14938
[21:47:13.052389] Epoch: [11]  [ 20/345]  eta: 0:04:03  lr: 0.000069  loss: 0.8525 (0.8497)  time: 0.7428  data: 0.0001  max mem: 14938
[21:47:27.959159] Epoch: [11]  [ 40/345]  eta: 0:03:48  lr: 0.000069  loss: 0.8266 (0.8435)  time: 0.7453  data: 0.0001  max mem: 14938
[21:47:42.904227] Epoch: [11]  [ 60/345]  eta: 0:03:33  lr: 0.000070  loss: 0.8259 (0.8402)  time: 0.7472  data: 0.0001  max mem: 14938
[21:47:57.881298] Epoch: [11]  [ 80/345]  eta: 0:03:18  lr: 0.000070  loss: 0.8403 (0.8408)  time: 0.7488  data: 0.0001  max mem: 14938
[21:48:12.906740] Epoch: [11]  [100/345]  eta: 0:03:03  lr: 0.000071  loss: 0.8369 (0.8415)  time: 0.7512  data: 0.0001  max mem: 14938
[21:48:27.939962] Epoch: [11]  [120/345]  eta: 0:02:48  lr: 0.000071  loss: 0.8345 (0.8427)  time: 0.7516  data: 0.0001  max mem: 14938
[21:48:42.948453] Epoch: [11]  [140/345]  eta: 0:02:33  lr: 0.000071  loss: 0.8350 (0.8424)  time: 0.7504  data: 0.0001  max mem: 14938
[21:48:57.931897] Epoch: [11]  [160/345]  eta: 0:02:18  lr: 0.000072  loss: 0.8334 (0.8412)  time: 0.7491  data: 0.0001  max mem: 14938
[21:49:12.924984] Epoch: [11]  [180/345]  eta: 0:02:03  lr: 0.000072  loss: 0.8366 (0.8410)  time: 0.7496  data: 0.0001  max mem: 14938
[21:49:27.917983] Epoch: [11]  [200/345]  eta: 0:01:48  lr: 0.000072  loss: 0.8311 (0.8405)  time: 0.7496  data: 0.0001  max mem: 14938
[21:49:42.906398] Epoch: [11]  [220/345]  eta: 0:01:33  lr: 0.000073  loss: 0.8497 (0.8409)  time: 0.7494  data: 0.0001  max mem: 14938
[21:49:57.895861] Epoch: [11]  [240/345]  eta: 0:01:18  lr: 0.000073  loss: 0.8282 (0.8401)  time: 0.7494  data: 0.0001  max mem: 14938

[21:50:12.885790] Epoch: [11]  [260/345]  eta: 0:01:03  lr: 0.000073  loss: 0.8260 (0.8395)  time: 0.7495  data: 0.0001  max mem: 14938
[21:50:27.871475] Epoch: [11]  [280/345]  eta: 0:00:48  lr: 0.000074  loss: 0.8354 (0.8396)  time: 0.7492  data: 0.0001  max mem: 14938
[21:50:42.846532] Epoch: [11]  [300/345]  eta: 0:00:33  lr: 0.000074  loss: 0.8154 (0.8382)  time: 0.7487  data: 0.0001  max mem: 14938
[21:50:57.823446] Epoch: [11]  [320/345]  eta: 0:00:18  lr: 0.000075  loss: 0.8245 (0.8377)  time: 0.7488  data: 0.0001  max mem: 14938
[21:51:12.799522] Epoch: [11]  [340/345]  eta: 0:00:03  lr: 0.000075  loss: 0.8193 (0.8367)  time: 0.7488  data: 0.0001  max mem: 14938
[21:51:15.798567] Epoch: [11]  [344/345]  eta: 0:00:00  lr: 0.000075  loss: 0.8247 (0.8365)  time: 0.7489  data: 0.0001  max mem: 14938
[21:51:15.862391] Epoch: [11] Total time: 0:04:18 (0.7494 s / it)
[21:51:15.862763] Averaged stats: lr: 0.000075  loss: 0.8247 (0.8365)
[21:51:16.199160] Test:  [  0/345]  eta: 0:01:53  loss: 0.8066 (0.8066)  time: 0.3304  data: 0.1491  max mem: 14938
[21:51:18.036415] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7916 (0.7954)  time: 0.1970  data: 0.0136  max mem: 14938
[21:51:19.873239] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7826 (0.7885)  time: 0.1836  data: 0.0001  max mem: 14938
[21:51:21.715362] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7749 (0.7866)  time: 0.1839  data: 0.0001  max mem: 14938
[21:51:23.561466] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7945 (0.7929)  time: 0.1844  data: 0.0001  max mem: 14938
[21:51:25.411231] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8001 (0.7936)  time: 0.1847  data: 0.0001  max mem: 14938
[21:51:27.265243] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7956 (0.7926)  time: 0.1851  data: 0.0001  max mem: 14938
[21:51:29.121285] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7999 (0.7931)  time: 0.1855  data: 0.0001  max mem: 14938
[21:51:30.979759] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7959 (0.7933)  time: 0.1857  data: 0.0001  max mem: 14938
[21:51:32.842836] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7965 (0.7942)  time: 0.1860  data: 0.0001  max mem: 14938
[21:51:34.709650] Test:  [100/345]  eta: 0:00:45  loss: 0.7991 (0.7942)  time: 0.1864  data: 0.0001  max mem: 14938
[21:51:36.578233] Test:  [110/345]  eta: 0:00:43  loss: 0.7991 (0.7949)  time: 0.1867  data: 0.0001  max mem: 14938
[21:51:38.451053] Test:  [120/345]  eta: 0:00:41  loss: 0.7982 (0.7940)  time: 0.1870  data: 0.0001  max mem: 14938
[21:51:40.327300] Test:  [130/345]  eta: 0:00:40  loss: 0.7811 (0.7938)  time: 0.1874  data: 0.0001  max mem: 14938
[21:51:42.206823] Test:  [140/345]  eta: 0:00:38  loss: 0.7921 (0.7940)  time: 0.1877  data: 0.0001  max mem: 14938
[21:51:44.089628] Test:  [150/345]  eta: 0:00:36  loss: 0.7936 (0.7942)  time: 0.1881  data: 0.0001  max mem: 14938
[21:51:45.975530] Test:  [160/345]  eta: 0:00:34  loss: 0.7929 (0.7937)  time: 0.1884  data: 0.0001  max mem: 14938
[21:51:47.865421] Test:  [170/345]  eta: 0:00:32  loss: 0.7802 (0.7932)  time: 0.1887  data: 0.0001  max mem: 14938
[21:51:49.760474] Test:  [180/345]  eta: 0:00:30  loss: 0.7947 (0.7938)  time: 0.1892  data: 0.0001  max mem: 14938
[21:51:51.658450] Test:  [190/345]  eta: 0:00:29  loss: 0.7997 (0.7941)  time: 0.1896  data: 0.0001  max mem: 14938
[21:51:53.559279] Test:  [200/345]  eta: 0:00:27  loss: 0.7878 (0.7935)  time: 0.1899  data: 0.0001  max mem: 14938
[21:51:55.462911] Test:  [210/345]  eta: 0:00:25  loss: 0.7848 (0.7934)  time: 0.1902  data: 0.0001  max mem: 14938
[21:51:57.369915] Test:  [220/345]  eta: 0:00:23  loss: 0.7940 (0.7931)  time: 0.1905  data: 0.0001  max mem: 14938
[21:51:59.281589] Test:  [230/345]  eta: 0:00:21  loss: 0.7898 (0.7928)  time: 0.1909  data: 0.0001  max mem: 14938
[21:52:01.197109] Test:  [240/345]  eta: 0:00:19  loss: 0.7891 (0.7926)  time: 0.1913  data: 0.0001  max mem: 14938
[21:52:03.117425] Test:  [250/345]  eta: 0:00:17  loss: 0.7778 (0.7919)  time: 0.1917  data: 0.0001  max mem: 14938
[21:52:05.040757] Test:  [260/345]  eta: 0:00:16  loss: 0.7759 (0.7919)  time: 0.1921  data: 0.0001  max mem: 14938
[21:52:06.966335] Test:  [270/345]  eta: 0:00:14  loss: 0.7854 (0.7916)  time: 0.1924  data: 0.0001  max mem: 14938
[21:52:08.897075] Test:  [280/345]  eta: 0:00:12  loss: 0.7884 (0.7921)  time: 0.1928  data: 0.0001  max mem: 14938
[21:52:10.828674] Test:  [290/345]  eta: 0:00:10  loss: 0.8029 (0.7924)  time: 0.1931  data: 0.0001  max mem: 14938
[21:52:12.765726] Test:  [300/345]  eta: 0:00:08  loss: 0.7926 (0.7923)  time: 0.1934  data: 0.0001  max mem: 14938
[21:52:14.704364] Test:  [310/345]  eta: 0:00:06  loss: 0.7888 (0.7921)  time: 0.1937  data: 0.0001  max mem: 14938
[21:52:16.647045] Test:  [320/345]  eta: 0:00:04  loss: 0.7847 (0.7922)  time: 0.1940  data: 0.0001  max mem: 14938
[21:52:18.593228] Test:  [330/345]  eta: 0:00:02  loss: 0.7885 (0.7920)  time: 0.1944  data: 0.0001  max mem: 14938
[21:52:20.543347] Test:  [340/345]  eta: 0:00:00  loss: 0.7885 (0.7921)  time: 0.1948  data: 0.0001  max mem: 14938
[21:52:21.324730] Test:  [344/345]  eta: 0:00:00  loss: 0.7851 (0.7920)  time: 0.1949  data: 0.0001  max mem: 14938
[21:52:21.388591] Test: Total time: 0:01:05 (0.1899 s / it)
[21:52:32.816806] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8485 (0.8485)  time: 0.3211  data: 0.1419  max mem: 14938
[21:52:34.630369] Test:  [10/57]  eta: 0:00:09  loss: 0.8506 (0.8754)  time: 0.1940  data: 0.0130  max mem: 14938
[21:52:36.448424] Test:  [20/57]  eta: 0:00:06  loss: 0.8602 (0.8684)  time: 0.1815  data: 0.0001  max mem: 14938
[21:52:38.269839] Test:  [30/57]  eta: 0:00:05  loss: 0.7797 (0.8343)  time: 0.1819  data: 0.0001  max mem: 14938
[21:52:40.098624] Test:  [40/57]  eta: 0:00:03  loss: 0.7524 (0.8151)  time: 0.1825  data: 0.0001  max mem: 14938
[21:52:41.930225] Test:  [50/57]  eta: 0:00:01  loss: 0.7466 (0.8085)  time: 0.1830  data: 0.0001  max mem: 14938
[21:52:42.917902] Test:  [56/57]  eta: 0:00:00  loss: 0.7911 (0.8135)  time: 0.1775  data: 0.0001  max mem: 14938
[21:52:42.974578] Test: Total time: 0:00:10 (0.1838 s / it)
[21:52:44.862185] Dice score of the network on the train images: 0.754765, val images: 0.798209
[21:52:44.866383] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[21:52:45.756944] Epoch: [12]  [  0/345]  eta: 0:05:06  lr: 0.000075  loss: 0.8794 (0.8794)  time: 0.8897  data: 0.1454  max mem: 14938
[21:53:00.622863] Epoch: [12]  [ 20/345]  eta: 0:04:03  lr: 0.000075  loss: 0.8205 (0.8296)  time: 0.7432  data: 0.0001  max mem: 14938
[21:53:15.546113] Epoch: [12]  [ 40/345]  eta: 0:03:48  lr: 0.000076  loss: 0.8237 (0.8278)  time: 0.7461  data: 0.0001  max mem: 14938
[21:53:30.516913] Epoch: [12]  [ 60/345]  eta: 0:03:33  lr: 0.000076  loss: 0.8303 (0.8308)  time: 0.7485  data: 0.0001  max mem: 14938
[21:53:45.510794] Epoch: [12]  [ 80/345]  eta: 0:03:18  lr: 0.000076  loss: 0.8462 (0.8360)  time: 0.7497  data: 0.0001  max mem: 14938
[21:54:00.524269] Epoch: [12]  [100/345]  eta: 0:03:03  lr: 0.000077  loss: 0.8197 (0.8341)  time: 0.7506  data: 0.0001  max mem: 14938
[21:54:15.529149] Epoch: [12]  [120/345]  eta: 0:02:48  lr: 0.000077  loss: 0.8281 (0.8340)  time: 0.7502  data: 0.0001  max mem: 14938
[21:54:30.525968] Epoch: [12]  [140/345]  eta: 0:02:33  lr: 0.000078  loss: 0.8214 (0.8321)  time: 0.7498  data: 0.0001  max mem: 14938
[21:54:45.518423] Epoch: [12]  [160/345]  eta: 0:02:18  lr: 0.000078  loss: 0.8287 (0.8320)  time: 0.7496  data: 0.0001  max mem: 14938
[21:55:00.516833] Epoch: [12]  [180/345]  eta: 0:02:03  lr: 0.000078  loss: 0.8280 (0.8318)  time: 0.7499  data: 0.0001  max mem: 14938
[21:55:15.509640] Epoch: [12]  [200/345]  eta: 0:01:48  lr: 0.000079  loss: 0.8214 (0.8310)  time: 0.7496  data: 0.0001  max mem: 14938
[21:55:30.497419] Epoch: [12]  [220/345]  eta: 0:01:33  lr: 0.000079  loss: 0.8205 (0.8304)  time: 0.7493  data: 0.0001  max mem: 14938
[21:55:45.609758] Epoch: [12]  [240/345]  eta: 0:01:18  lr: 0.000079  loss: 0.8051 (0.8290)  time: 0.7556  data: 0.0001  max mem: 14938
[21:56:00.589448] Epoch: [12]  [260/345]  eta: 0:01:03  lr: 0.000080  loss: 0.8224 (0.8287)  time: 0.7489  data: 0.0001  max mem: 14938
[21:56:15.573184] Epoch: [12]  [280/345]  eta: 0:00:48  lr: 0.000080  loss: 0.8151 (0.8285)  time: 0.7491  data: 0.0001  max mem: 14938
[21:56:30.553138] Epoch: [12]  [300/345]  eta: 0:00:33  lr: 0.000080  loss: 0.8241 (0.8285)  time: 0.7490  data: 0.0001  max mem: 14938
[21:56:45.523181] Epoch: [12]  [320/345]  eta: 0:00:18  lr: 0.000081  loss: 0.8179 (0.8280)  time: 0.7485  data: 0.0001  max mem: 14938
[21:57:00.504707] Epoch: [12]  [340/345]  eta: 0:00:03  lr: 0.000081  loss: 0.8258 (0.8279)  time: 0.7490  data: 0.0001  max mem: 14938
[21:57:03.502444] Epoch: [12]  [344/345]  eta: 0:00:00  lr: 0.000081  loss: 0.8291 (0.8280)  time: 0.7490  data: 0.0001  max mem: 14938
[21:57:03.568442] Epoch: [12] Total time: 0:04:18 (0.7499 s / it)
[21:57:03.568734] Averaged stats: lr: 0.000081  loss: 0.8291 (0.8280)
[21:57:03.904779] Test:  [  0/345]  eta: 0:01:54  loss: 0.8321 (0.8321)  time: 0.3321  data: 0.1507  max mem: 14938
[21:57:05.739492] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7901 (0.7943)  time: 0.1969  data: 0.0138  max mem: 14938
[21:57:07.578053] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7847 (0.7887)  time: 0.1836  data: 0.0001  max mem: 14938
[21:57:09.420774] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7764 (0.7890)  time: 0.1840  data: 0.0001  max mem: 14938
[21:57:11.267101] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7778 (0.7886)  time: 0.1844  data: 0.0001  max mem: 14938
[21:57:13.117272] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7852 (0.7870)  time: 0.1848  data: 0.0001  max mem: 14938
[21:57:14.970400] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7912 (0.7889)  time: 0.1851  data: 0.0001  max mem: 14938
[21:57:16.825523] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7935 (0.7886)  time: 0.1854  data: 0.0001  max mem: 14938
[21:57:18.683007] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7870 (0.7890)  time: 0.1856  data: 0.0001  max mem: 14938
[21:57:20.546016] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7945 (0.7887)  time: 0.1860  data: 0.0001  max mem: 14938
[21:57:22.413219] Test:  [100/345]  eta: 0:00:45  loss: 0.7827 (0.7876)  time: 0.1865  data: 0.0001  max mem: 14938
[21:57:24.282664] Test:  [110/345]  eta: 0:00:43  loss: 0.7819 (0.7877)  time: 0.1868  data: 0.0001  max mem: 14938
[21:57:26.156200] Test:  [120/345]  eta: 0:00:41  loss: 0.7838 (0.7877)  time: 0.1871  data: 0.0001  max mem: 14938
[21:57:28.033424] Test:  [130/345]  eta: 0:00:40  loss: 0.7867 (0.7874)  time: 0.1875  data: 0.0001  max mem: 14938
[21:57:29.915234] Test:  [140/345]  eta: 0:00:38  loss: 0.7855 (0.7874)  time: 0.1879  data: 0.0001  max mem: 14938
[21:57:31.798350] Test:  [150/345]  eta: 0:00:36  loss: 0.7820 (0.7870)  time: 0.1882  data: 0.0001  max mem: 14938
[21:57:33.685641] Test:  [160/345]  eta: 0:00:34  loss: 0.7878 (0.7870)  time: 0.1885  data: 0.0001  max mem: 14938
[21:57:35.575416] Test:  [170/345]  eta: 0:00:32  loss: 0.7848 (0.7869)  time: 0.1888  data: 0.0001  max mem: 14938
[21:57:37.470630] Test:  [180/345]  eta: 0:00:30  loss: 0.7774 (0.7866)  time: 0.1892  data: 0.0001  max mem: 14938
[21:57:39.370261] Test:  [190/345]  eta: 0:00:29  loss: 0.7774 (0.7859)  time: 0.1897  data: 0.0001  max mem: 14938
[21:57:41.270487] Test:  [200/345]  eta: 0:00:27  loss: 0.7782 (0.7860)  time: 0.1899  data: 0.0001  max mem: 14938
[21:57:43.176582] Test:  [210/345]  eta: 0:00:25  loss: 0.7851 (0.7859)  time: 0.1903  data: 0.0001  max mem: 14938
[21:57:45.084265] Test:  [220/345]  eta: 0:00:23  loss: 0.7851 (0.7860)  time: 0.1906  data: 0.0001  max mem: 14938
[21:57:46.997629] Test:  [230/345]  eta: 0:00:21  loss: 0.7819 (0.7860)  time: 0.1910  data: 0.0001  max mem: 14938
[21:57:48.914759] Test:  [240/345]  eta: 0:00:19  loss: 0.7819 (0.7860)  time: 0.1915  data: 0.0001  max mem: 14938
[21:57:50.835082] Test:  [250/345]  eta: 0:00:17  loss: 0.7929 (0.7864)  time: 0.1918  data: 0.0001  max mem: 14938
[21:57:52.757173] Test:  [260/345]  eta: 0:00:16  loss: 0.7885 (0.7863)  time: 0.1921  data: 0.0001  max mem: 14938
[21:57:54.682871] Test:  [270/345]  eta: 0:00:14  loss: 0.7823 (0.7862)  time: 0.1923  data: 0.0001  max mem: 14938
[21:57:56.612567] Test:  [280/345]  eta: 0:00:12  loss: 0.7965 (0.7867)  time: 0.1927  data: 0.0001  max mem: 14938
[21:57:58.545169] Test:  [290/345]  eta: 0:00:10  loss: 0.8024 (0.7871)  time: 0.1931  data: 0.0001  max mem: 14938
[21:58:00.480942] Test:  [300/345]  eta: 0:00:08  loss: 0.7878 (0.7871)  time: 0.1934  data: 0.0001  max mem: 14938
[21:58:02.419257] Test:  [310/345]  eta: 0:00:06  loss: 0.7843 (0.7869)  time: 0.1937  data: 0.0001  max mem: 14938
[21:58:04.361187] Test:  [320/345]  eta: 0:00:04  loss: 0.7843 (0.7869)  time: 0.1940  data: 0.0001  max mem: 14938
[21:58:06.307793] Test:  [330/345]  eta: 0:00:02  loss: 0.7746 (0.7867)  time: 0.1944  data: 0.0001  max mem: 14938
[21:58:08.256036] Test:  [340/345]  eta: 0:00:00  loss: 0.7812 (0.7868)  time: 0.1947  data: 0.0001  max mem: 14938
[21:58:09.037444] Test:  [344/345]  eta: 0:00:00  loss: 0.7864 (0.7869)  time: 0.1948  data: 0.0001  max mem: 14938
[21:58:09.098288] Test: Total time: 0:01:05 (0.1899 s / it)
[21:58:20.499146] Test:  [ 0/57]  eta: 0:00:18  loss: 0.9036 (0.9036)  time: 0.3278  data: 0.1487  max mem: 14938
[21:58:22.314045] Test:  [10/57]  eta: 0:00:09  loss: 0.8835 (0.8805)  time: 0.1947  data: 0.0136  max mem: 14938
[21:58:24.132266] Test:  [20/57]  eta: 0:00:06  loss: 0.8604 (0.8659)  time: 0.1816  data: 0.0001  max mem: 14938
[21:58:25.954297] Test:  [30/57]  eta: 0:00:05  loss: 0.7694 (0.8305)  time: 0.1820  data: 0.0001  max mem: 14938
[21:58:27.781039] Test:  [40/57]  eta: 0:00:03  loss: 0.7623 (0.8113)  time: 0.1824  data: 0.0001  max mem: 14938
[21:58:29.613372] Test:  [50/57]  eta: 0:00:01  loss: 0.7470 (0.8051)  time: 0.1829  data: 0.0001  max mem: 14938
[21:58:30.601107] Test:  [56/57]  eta: 0:00:00  loss: 0.7806 (0.8113)  time: 0.1775  data: 0.0001  max mem: 14938
[21:58:30.661672] Test: Total time: 0:00:10 (0.1840 s / it)
[21:58:32.617892] Dice score of the network on the train images: 0.747558, val images: 0.802665
[21:58:32.622203] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[21:58:33.512094] Epoch: [13]  [  0/345]  eta: 0:05:06  lr: 0.000081  loss: 0.8166 (0.8166)  time: 0.8888  data: 0.1461  max mem: 14938
[21:58:48.391598] Epoch: [13]  [ 20/345]  eta: 0:04:04  lr: 0.000082  loss: 0.8127 (0.8087)  time: 0.7439  data: 0.0001  max mem: 14938
[21:59:03.326514] Epoch: [13]  [ 40/345]  eta: 0:03:48  lr: 0.000082  loss: 0.8163 (0.8140)  time: 0.7467  data: 0.0001  max mem: 14938
[21:59:18.303953] Epoch: [13]  [ 60/345]  eta: 0:03:33  lr: 0.000082  loss: 0.8194 (0.8172)  time: 0.7488  data: 0.0001  max mem: 14938
[21:59:33.305089] Epoch: [13]  [ 80/345]  eta: 0:03:18  lr: 0.000083  loss: 0.8046 (0.8162)  time: 0.7500  data: 0.0001  max mem: 14938
[21:59:48.315996] Epoch: [13]  [100/345]  eta: 0:03:03  lr: 0.000083  loss: 0.8134 (0.8173)  time: 0.7505  data: 0.0001  max mem: 14938
[22:00:03.351365] Epoch: [13]  [120/345]  eta: 0:02:48  lr: 0.000083  loss: 0.8010 (0.8155)  time: 0.7517  data: 0.0001  max mem: 14938
[22:00:18.369353] Epoch: [13]  [140/345]  eta: 0:02:33  lr: 0.000084  loss: 0.8088 (0.8148)  time: 0.7509  data: 0.0001  max mem: 14938
[22:00:33.380825] Epoch: [13]  [160/345]  eta: 0:02:18  lr: 0.000084  loss: 0.8139 (0.8143)  time: 0.7505  data: 0.0001  max mem: 14938
[22:00:48.508876] Epoch: [13]  [180/345]  eta: 0:02:03  lr: 0.000085  loss: 0.8395 (0.8162)  time: 0.7564  data: 0.0001  max mem: 14938
[22:01:03.502310] Epoch: [13]  [200/345]  eta: 0:01:48  lr: 0.000085  loss: 0.8294 (0.8180)  time: 0.7496  data: 0.0001  max mem: 14938
[22:01:18.491072] Epoch: [13]  [220/345]  eta: 0:01:33  lr: 0.000085  loss: 0.8104 (0.8174)  time: 0.7494  data: 0.0001  max mem: 14938
[22:01:33.479686] Epoch: [13]  [240/345]  eta: 0:01:18  lr: 0.000086  loss: 0.8155 (0.8179)  time: 0.7494  data: 0.0001  max mem: 14938
[22:01:48.471787] Epoch: [13]  [260/345]  eta: 0:01:03  lr: 0.000086  loss: 0.8146 (0.8178)  time: 0.7496  data: 0.0001  max mem: 14938
[22:02:03.467904] Epoch: [13]  [280/345]  eta: 0:00:48  lr: 0.000086  loss: 0.8013 (0.8172)  time: 0.7498  data: 0.0001  max mem: 14938
[22:02:18.461450] Epoch: [13]  [300/345]  eta: 0:00:33  lr: 0.000087  loss: 0.8120 (0.8167)  time: 0.7496  data: 0.0001  max mem: 14938
[22:02:33.445585] Epoch: [13]  [320/345]  eta: 0:00:18  lr: 0.000087  loss: 0.8048 (0.8159)  time: 0.7492  data: 0.0001  max mem: 14938
[22:02:48.435882] Epoch: [13]  [340/345]  eta: 0:00:03  lr: 0.000087  loss: 0.8130 (0.8164)  time: 0.7495  data: 0.0001  max mem: 14938
[22:02:51.432008] Epoch: [13]  [344/345]  eta: 0:00:00  lr: 0.000087  loss: 0.8211 (0.8164)  time: 0.7492  data: 0.0001  max mem: 14938
[22:02:51.498012] Epoch: [13] Total time: 0:04:18 (0.7504 s / it)
[22:02:51.498381] Averaged stats: lr: 0.000087  loss: 0.8211 (0.8164)
[22:02:51.836384] Test:  [  0/345]  eta: 0:01:55  loss: 0.7876 (0.7876)  time: 0.3338  data: 0.1519  max mem: 14938
[22:02:53.672149] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7923 (0.7942)  time: 0.1972  data: 0.0139  max mem: 14938
[22:02:55.511785] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7923 (0.7935)  time: 0.1837  data: 0.0001  max mem: 14938
[22:02:57.353276] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7998 (0.7975)  time: 0.1840  data: 0.0001  max mem: 14938
[22:02:59.197832] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7940 (0.7959)  time: 0.1843  data: 0.0001  max mem: 14938
[22:03:01.046574] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7940 (0.7983)  time: 0.1846  data: 0.0001  max mem: 14938
[22:03:02.899318] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8002 (0.7978)  time: 0.1850  data: 0.0001  max mem: 14938
[22:03:04.755518] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7929 (0.7979)  time: 0.1854  data: 0.0001  max mem: 14938
[22:03:06.613546] Test:  [ 80/345]  eta: 0:00:49  loss: 0.8073 (0.8000)  time: 0.1857  data: 0.0001  max mem: 14938
[22:03:08.477016] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7962 (0.7977)  time: 0.1860  data: 0.0001  max mem: 14938
[22:03:10.343016] Test:  [100/345]  eta: 0:00:45  loss: 0.7916 (0.7973)  time: 0.1864  data: 0.0001  max mem: 14938
[22:03:12.211121] Test:  [110/345]  eta: 0:00:43  loss: 0.7916 (0.7971)  time: 0.1866  data: 0.0001  max mem: 14938
[22:03:14.084576] Test:  [120/345]  eta: 0:00:41  loss: 0.7908 (0.7972)  time: 0.1870  data: 0.0001  max mem: 14938
[22:03:15.961303] Test:  [130/345]  eta: 0:00:40  loss: 0.8033 (0.7980)  time: 0.1875  data: 0.0001  max mem: 14938
[22:03:17.842070] Test:  [140/345]  eta: 0:00:38  loss: 0.8075 (0.7983)  time: 0.1878  data: 0.0001  max mem: 14938
[22:03:19.725196] Test:  [150/345]  eta: 0:00:36  loss: 0.8075 (0.7989)  time: 0.1881  data: 0.0001  max mem: 14938
[22:03:21.611685] Test:  [160/345]  eta: 0:00:34  loss: 0.7956 (0.7981)  time: 0.1884  data: 0.0001  max mem: 14938
[22:03:23.502456] Test:  [170/345]  eta: 0:00:32  loss: 0.7955 (0.7982)  time: 0.1888  data: 0.0001  max mem: 14938
[22:03:25.397158] Test:  [180/345]  eta: 0:00:30  loss: 0.7955 (0.7977)  time: 0.1892  data: 0.0001  max mem: 14938
[22:03:27.295709] Test:  [190/345]  eta: 0:00:29  loss: 0.7978 (0.7979)  time: 0.1896  data: 0.0001  max mem: 14938
[22:03:29.198263] Test:  [200/345]  eta: 0:00:27  loss: 0.8011 (0.7981)  time: 0.1900  data: 0.0001  max mem: 14938
[22:03:31.102461] Test:  [210/345]  eta: 0:00:25  loss: 0.8011 (0.7979)  time: 0.1903  data: 0.0001  max mem: 14938
[22:03:33.010617] Test:  [220/345]  eta: 0:00:23  loss: 0.7933 (0.7978)  time: 0.1906  data: 0.0001  max mem: 14938
[22:03:34.925272] Test:  [230/345]  eta: 0:00:21  loss: 0.7920 (0.7980)  time: 0.1911  data: 0.0001  max mem: 14938
[22:03:36.841003] Test:  [240/345]  eta: 0:00:19  loss: 0.7920 (0.7977)  time: 0.1915  data: 0.0001  max mem: 14938
[22:03:38.760085] Test:  [250/345]  eta: 0:00:17  loss: 0.7847 (0.7973)  time: 0.1917  data: 0.0001  max mem: 14938
[22:03:40.684089] Test:  [260/345]  eta: 0:00:16  loss: 0.7869 (0.7975)  time: 0.1921  data: 0.0001  max mem: 14938
[22:03:42.611136] Test:  [270/345]  eta: 0:00:14  loss: 0.7927 (0.7974)  time: 0.1925  data: 0.0001  max mem: 14938
[22:03:44.540177] Test:  [280/345]  eta: 0:00:12  loss: 0.7919 (0.7973)  time: 0.1928  data: 0.0001  max mem: 14938
[22:03:46.475539] Test:  [290/345]  eta: 0:00:10  loss: 0.7921 (0.7972)  time: 0.1932  data: 0.0001  max mem: 14938
[22:03:48.413852] Test:  [300/345]  eta: 0:00:08  loss: 0.7945 (0.7972)  time: 0.1936  data: 0.0001  max mem: 14938
[22:03:50.354042] Test:  [310/345]  eta: 0:00:06  loss: 0.7945 (0.7970)  time: 0.1939  data: 0.0001  max mem: 14938
[22:03:52.296345] Test:  [320/345]  eta: 0:00:04  loss: 0.7942 (0.7971)  time: 0.1941  data: 0.0001  max mem: 14938
[22:03:54.242348] Test:  [330/345]  eta: 0:00:02  loss: 0.7922 (0.7968)  time: 0.1944  data: 0.0001  max mem: 14938
[22:03:56.190788] Test:  [340/345]  eta: 0:00:00  loss: 0.7868 (0.7967)  time: 0.1947  data: 0.0001  max mem: 14938
[22:03:56.971564] Test:  [344/345]  eta: 0:00:00  loss: 0.7868 (0.7966)  time: 0.1948  data: 0.0001  max mem: 14938
[22:03:57.031638] Test: Total time: 0:01:05 (0.1899 s / it)
[22:04:08.368365] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8672 (0.8672)  time: 0.3244  data: 0.1461  max mem: 14938
[22:04:10.183895] Test:  [10/57]  eta: 0:00:09  loss: 0.8682 (0.8854)  time: 0.1945  data: 0.0133  max mem: 14938
[22:04:12.004134] Test:  [20/57]  eta: 0:00:06  loss: 0.8685 (0.8775)  time: 0.1817  data: 0.0001  max mem: 14938
[22:04:13.827450] Test:  [30/57]  eta: 0:00:05  loss: 0.7890 (0.8438)  time: 0.1821  data: 0.0001  max mem: 14938
[22:04:15.655452] Test:  [40/57]  eta: 0:00:03  loss: 0.7678 (0.8236)  time: 0.1825  data: 0.0001  max mem: 14938
[22:04:17.488039] Test:  [50/57]  eta: 0:00:01  loss: 0.7576 (0.8155)  time: 0.1830  data: 0.0001  max mem: 14938
[22:04:18.475849] Test:  [56/57]  eta: 0:00:00  loss: 0.7893 (0.8197)  time: 0.1776  data: 0.0001  max mem: 14938
[22:04:18.533743] Test: Total time: 0:00:10 (0.1840 s / it)
[22:04:20.495717] Dice score of the network on the train images: 0.740724, val images: 0.791764
[22:04:20.500301] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:04:21.392115] Epoch: [14]  [  0/345]  eta: 0:05:07  lr: 0.000087  loss: 0.8263 (0.8263)  time: 0.8908  data: 0.1483  max mem: 14938
[22:04:36.272057] Epoch: [14]  [ 20/345]  eta: 0:04:04  lr: 0.000088  loss: 0.8118 (0.8182)  time: 0.7439  data: 0.0001  max mem: 14938
[22:04:51.213129] Epoch: [14]  [ 40/345]  eta: 0:03:48  lr: 0.000088  loss: 0.8118 (0.8156)  time: 0.7470  data: 0.0001  max mem: 14938
[22:05:06.185106] Epoch: [14]  [ 60/345]  eta: 0:03:33  lr: 0.000089  loss: 0.8109 (0.8140)  time: 0.7486  data: 0.0001  max mem: 14938
[22:05:21.191431] Epoch: [14]  [ 80/345]  eta: 0:03:18  lr: 0.000089  loss: 0.8048 (0.8121)  time: 0.7503  data: 0.0001  max mem: 14938
[22:05:36.211187] Epoch: [14]  [100/345]  eta: 0:03:03  lr: 0.000089  loss: 0.8200 (0.8137)  time: 0.7509  data: 0.0001  max mem: 14938
[22:05:51.225490] Epoch: [14]  [120/345]  eta: 0:02:48  lr: 0.000090  loss: 0.8051 (0.8128)  time: 0.7507  data: 0.0001  max mem: 14938
[22:06:06.216280] Epoch: [14]  [140/345]  eta: 0:02:33  lr: 0.000090  loss: 0.8115 (0.8125)  time: 0.7495  data: 0.0001  max mem: 14938
[22:06:21.204293] Epoch: [14]  [160/345]  eta: 0:02:18  lr: 0.000090  loss: 0.7999 (0.8113)  time: 0.7494  data: 0.0001  max mem: 14938
[22:06:36.188647] Epoch: [14]  [180/345]  eta: 0:02:03  lr: 0.000091  loss: 0.8081 (0.8116)  time: 0.7492  data: 0.0001  max mem: 14938
[22:06:51.188935] Epoch: [14]  [200/345]  eta: 0:01:48  lr: 0.000091  loss: 0.8090 (0.8120)  time: 0.7500  data: 0.0001  max mem: 14938
[22:07:06.177491] Epoch: [14]  [220/345]  eta: 0:01:33  lr: 0.000091  loss: 0.8074 (0.8122)  time: 0.7494  data: 0.0001  max mem: 14938
[22:07:21.175930] Epoch: [14]  [240/345]  eta: 0:01:18  lr: 0.000092  loss: 0.8045 (0.8122)  time: 0.7499  data: 0.0001  max mem: 14938
[22:07:36.174742] Epoch: [14]  [260/345]  eta: 0:01:03  lr: 0.000092  loss: 0.7985 (0.8116)  time: 0.7499  data: 0.0001  max mem: 14938
[22:07:51.160640] Epoch: [14]  [280/345]  eta: 0:00:48  lr: 0.000093  loss: 0.8065 (0.8114)  time: 0.7492  data: 0.0001  max mem: 14938
[22:08:06.130253] Epoch: [14]  [300/345]  eta: 0:00:33  lr: 0.000093  loss: 0.8073 (0.8112)  time: 0.7484  data: 0.0001  max mem: 14938
[22:08:21.106914] Epoch: [14]  [320/345]  eta: 0:00:18  lr: 0.000093  loss: 0.7986 (0.8108)  time: 0.7488  data: 0.0001  max mem: 14938
[22:08:36.077265] Epoch: [14]  [340/345]  eta: 0:00:03  lr: 0.000094  loss: 0.7952 (0.8100)  time: 0.7485  data: 0.0001  max mem: 14938
[22:08:39.072444] Epoch: [14]  [344/345]  eta: 0:00:00  lr: 0.000094  loss: 0.7952 (0.8099)  time: 0.7485  data: 0.0001  max mem: 14938
[22:08:39.137781] Epoch: [14] Total time: 0:04:18 (0.7497 s / it)
[22:08:39.138259] Averaged stats: lr: 0.000094  loss: 0.7952 (0.8099)
[22:08:39.477231] Test:  [  0/345]  eta: 0:01:55  loss: 0.7734 (0.7734)  time: 0.3339  data: 0.1522  max mem: 14938
[22:08:41.313394] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7627 (0.7705)  time: 0.1972  data: 0.0139  max mem: 14938
[22:08:43.152426] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7675 (0.7757)  time: 0.1837  data: 0.0001  max mem: 14938
[22:08:44.994365] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7716 (0.7736)  time: 0.1840  data: 0.0001  max mem: 14938
[22:08:46.839213] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7674 (0.7724)  time: 0.1843  data: 0.0001  max mem: 14938
[22:08:48.690082] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7644 (0.7718)  time: 0.1847  data: 0.0001  max mem: 14938
[22:08:50.543600] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7687 (0.7731)  time: 0.1852  data: 0.0001  max mem: 14938
[22:08:52.399894] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7771 (0.7735)  time: 0.1854  data: 0.0001  max mem: 14938
[22:08:54.259567] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7771 (0.7753)  time: 0.1857  data: 0.0001  max mem: 14938
[22:08:56.123204] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7753 (0.7746)  time: 0.1861  data: 0.0001  max mem: 14938
[22:08:57.990202] Test:  [100/345]  eta: 0:00:45  loss: 0.7694 (0.7751)  time: 0.1865  data: 0.0001  max mem: 14938
[22:08:59.858162] Test:  [110/345]  eta: 0:00:43  loss: 0.7710 (0.7751)  time: 0.1867  data: 0.0001  max mem: 14938
[22:09:01.732567] Test:  [120/345]  eta: 0:00:41  loss: 0.7712 (0.7749)  time: 0.1871  data: 0.0001  max mem: 14938
[22:09:03.610330] Test:  [130/345]  eta: 0:00:40  loss: 0.7743 (0.7750)  time: 0.1876  data: 0.0001  max mem: 14938
[22:09:05.491565] Test:  [140/345]  eta: 0:00:38  loss: 0.7717 (0.7747)  time: 0.1879  data: 0.0001  max mem: 14938
[22:09:07.374951] Test:  [150/345]  eta: 0:00:36  loss: 0.7677 (0.7741)  time: 0.1882  data: 0.0001  max mem: 14938
[22:09:09.263351] Test:  [160/345]  eta: 0:00:34  loss: 0.7770 (0.7744)  time: 0.1885  data: 0.0001  max mem: 14938
[22:09:11.155199] Test:  [170/345]  eta: 0:00:32  loss: 0.7779 (0.7742)  time: 0.1890  data: 0.0001  max mem: 14938
[22:09:13.052524] Test:  [180/345]  eta: 0:00:30  loss: 0.7736 (0.7741)  time: 0.1894  data: 0.0001  max mem: 14938
[22:09:14.953711] Test:  [190/345]  eta: 0:00:29  loss: 0.7751 (0.7741)  time: 0.1899  data: 0.0001  max mem: 14938
[22:09:16.855787] Test:  [200/345]  eta: 0:00:27  loss: 0.7766 (0.7740)  time: 0.1901  data: 0.0001  max mem: 14938
[22:09:18.762112] Test:  [210/345]  eta: 0:00:25  loss: 0.7713 (0.7737)  time: 0.1904  data: 0.0001  max mem: 14938
[22:09:20.673619] Test:  [220/345]  eta: 0:00:23  loss: 0.7697 (0.7736)  time: 0.1908  data: 0.0001  max mem: 14938
[22:09:22.587349] Test:  [230/345]  eta: 0:00:21  loss: 0.7672 (0.7731)  time: 0.1912  data: 0.0001  max mem: 14938
[22:09:24.505590] Test:  [240/345]  eta: 0:00:19  loss: 0.7696 (0.7735)  time: 0.1915  data: 0.0001  max mem: 14938
[22:09:26.427378] Test:  [250/345]  eta: 0:00:17  loss: 0.7782 (0.7735)  time: 0.1920  data: 0.0001  max mem: 14938
[22:09:28.354320] Test:  [260/345]  eta: 0:00:16  loss: 0.7759 (0.7736)  time: 0.1924  data: 0.0001  max mem: 14938
[22:09:30.282008] Test:  [270/345]  eta: 0:00:14  loss: 0.7705 (0.7736)  time: 0.1927  data: 0.0001  max mem: 14938
[22:09:32.212975] Test:  [280/345]  eta: 0:00:12  loss: 0.7705 (0.7741)  time: 0.1929  data: 0.0001  max mem: 14938
[22:09:34.146861] Test:  [290/345]  eta: 0:00:10  loss: 0.7701 (0.7740)  time: 0.1932  data: 0.0001  max mem: 14938
[22:09:36.084701] Test:  [300/345]  eta: 0:00:08  loss: 0.7701 (0.7740)  time: 0.1935  data: 0.0001  max mem: 14938
[22:09:38.023371] Test:  [310/345]  eta: 0:00:06  loss: 0.7692 (0.7738)  time: 0.1938  data: 0.0001  max mem: 14938
[22:09:39.965674] Test:  [320/345]  eta: 0:00:04  loss: 0.7692 (0.7738)  time: 0.1940  data: 0.0001  max mem: 14938
[22:09:41.910963] Test:  [330/345]  eta: 0:00:02  loss: 0.7741 (0.7740)  time: 0.1943  data: 0.0001  max mem: 14938
[22:09:43.860865] Test:  [340/345]  eta: 0:00:00  loss: 0.7678 (0.7736)  time: 0.1947  data: 0.0001  max mem: 14938
[22:09:44.642242] Test:  [344/345]  eta: 0:00:00  loss: 0.7678 (0.7736)  time: 0.1949  data: 0.0001  max mem: 14938
[22:09:44.701911] Test: Total time: 0:01:05 (0.1900 s / it)
[22:09:56.008669] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8799 (0.8799)  time: 0.3204  data: 0.1406  max mem: 14938
[22:09:57.823447] Test:  [10/57]  eta: 0:00:09  loss: 0.8969 (0.8928)  time: 0.1940  data: 0.0128  max mem: 14938
[22:09:59.644168] Test:  [20/57]  eta: 0:00:06  loss: 0.8910 (0.8781)  time: 0.1817  data: 0.0001  max mem: 14938
[22:10:01.468884] Test:  [30/57]  eta: 0:00:05  loss: 0.7826 (0.8404)  time: 0.1822  data: 0.0001  max mem: 14938
[22:10:03.298012] Test:  [40/57]  eta: 0:00:03  loss: 0.7526 (0.8199)  time: 0.1826  data: 0.0001  max mem: 14938
[22:10:05.131569] Test:  [50/57]  eta: 0:00:01  loss: 0.7448 (0.8113)  time: 0.1831  data: 0.0001  max mem: 14938
[22:10:06.120104] Test:  [56/57]  eta: 0:00:00  loss: 0.7834 (0.8165)  time: 0.1776  data: 0.0001  max mem: 14938
[22:10:06.180150] Test: Total time: 0:00:10 (0.1841 s / it)
[22:10:08.091089] Dice score of the network on the train images: 0.767447, val images: 0.814209
[22:10:08.091329] saving best_prec_model_0 @ epoch 14
[22:10:09.296134] saving best_dice_model_0 @ epoch 14
[22:10:10.334514] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:10:11.224576] Epoch: [15]  [  0/345]  eta: 0:05:06  lr: 0.000094  loss: 0.8039 (0.8039)  time: 0.8888  data: 0.1476  max mem: 14938
[22:10:26.061950] Epoch: [15]  [ 20/345]  eta: 0:04:03  lr: 0.000094  loss: 0.7880 (0.7896)  time: 0.7418  data: 0.0001  max mem: 14938
[22:10:40.961763] Epoch: [15]  [ 40/345]  eta: 0:03:47  lr: 0.000094  loss: 0.7914 (0.7944)  time: 0.7449  data: 0.0001  max mem: 14938
[22:10:55.906965] Epoch: [15]  [ 60/345]  eta: 0:03:32  lr: 0.000095  loss: 0.8002 (0.7950)  time: 0.7472  data: 0.0001  max mem: 14938
[22:11:10.888109] Epoch: [15]  [ 80/345]  eta: 0:03:18  lr: 0.000095  loss: 0.8217 (0.8015)  time: 0.7490  data: 0.0001  max mem: 14938
[22:11:25.904475] Epoch: [15]  [100/345]  eta: 0:03:03  lr: 0.000096  loss: 0.8008 (0.8017)  time: 0.7508  data: 0.0001  max mem: 14938
[22:11:40.920042] Epoch: [15]  [120/345]  eta: 0:02:48  lr: 0.000096  loss: 0.8074 (0.8034)  time: 0.7507  data: 0.0001  max mem: 14938
[22:11:55.912266] Epoch: [15]  [140/345]  eta: 0:02:33  lr: 0.000096  loss: 0.8053 (0.8039)  time: 0.7496  data: 0.0001  max mem: 14938
[22:12:10.894737] Epoch: [15]  [160/345]  eta: 0:02:18  lr: 0.000097  loss: 0.7984 (0.8046)  time: 0.7491  data: 0.0001  max mem: 14938
[22:12:25.874879] Epoch: [15]  [180/345]  eta: 0:02:03  lr: 0.000097  loss: 0.8122 (0.8062)  time: 0.7490  data: 0.0001  max mem: 14938
[22:12:40.842945] Epoch: [15]  [200/345]  eta: 0:01:48  lr: 0.000097  loss: 0.8063 (0.8062)  time: 0.7484  data: 0.0001  max mem: 14938
[22:12:55.814735] Epoch: [15]  [220/345]  eta: 0:01:33  lr: 0.000098  loss: 0.7984 (0.8055)  time: 0.7485  data: 0.0001  max mem: 14938
[22:13:10.789233] Epoch: [15]  [240/345]  eta: 0:01:18  lr: 0.000098  loss: 0.7964 (0.8050)  time: 0.7487  data: 0.0001  max mem: 14938
[22:13:25.751017] Epoch: [15]  [260/345]  eta: 0:01:03  lr: 0.000098  loss: 0.7843 (0.8043)  time: 0.7480  data: 0.0001  max mem: 14938
[22:13:40.707996] Epoch: [15]  [280/345]  eta: 0:00:48  lr: 0.000099  loss: 0.8250 (0.8059)  time: 0.7478  data: 0.0001  max mem: 14938
[22:13:55.664475] Epoch: [15]  [300/345]  eta: 0:00:33  lr: 0.000099  loss: 0.8043 (0.8062)  time: 0.7478  data: 0.0001  max mem: 14938
[22:14:10.621127] Epoch: [15]  [320/345]  eta: 0:00:18  lr: 0.000100  loss: 0.8090 (0.8069)  time: 0.7478  data: 0.0001  max mem: 14938
[22:14:25.572930] Epoch: [15]  [340/345]  eta: 0:00:03  lr: 0.000100  loss: 0.8146 (0.8072)  time: 0.7475  data: 0.0001  max mem: 14938
[22:14:28.567152] Epoch: [15]  [344/345]  eta: 0:00:00  lr: 0.000100  loss: 0.7999 (0.8070)  time: 0.7475  data: 0.0001  max mem: 14938
[22:14:28.626397] Epoch: [15] Total time: 0:04:18 (0.7487 s / it)
[22:14:28.626822] Averaged stats: lr: 0.000100  loss: 0.7999 (0.8070)
[22:14:28.966018] Test:  [  0/345]  eta: 0:01:55  loss: 0.7683 (0.7683)  time: 0.3356  data: 0.1541  max mem: 14938
[22:14:30.804490] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7735 (0.7704)  time: 0.1976  data: 0.0141  max mem: 14938
[22:14:32.644645] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7665 (0.7650)  time: 0.1839  data: 0.0001  max mem: 14938
[22:14:34.487458] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7620 (0.7646)  time: 0.1841  data: 0.0001  max mem: 14938
[22:14:36.333385] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7651 (0.7644)  time: 0.1844  data: 0.0001  max mem: 14938
[22:14:38.182944] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7689 (0.7656)  time: 0.1847  data: 0.0001  max mem: 14938
[22:14:40.036843] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7658 (0.7652)  time: 0.1851  data: 0.0001  max mem: 14938
[22:14:41.893788] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7597 (0.7649)  time: 0.1855  data: 0.0001  max mem: 14938
[22:14:43.753402] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7616 (0.7646)  time: 0.1858  data: 0.0001  max mem: 14938
[22:14:45.616627] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7585 (0.7640)  time: 0.1861  data: 0.0001  max mem: 14938
[22:14:47.481720] Test:  [100/345]  eta: 0:00:45  loss: 0.7588 (0.7645)  time: 0.1864  data: 0.0001  max mem: 14938
[22:14:49.351221] Test:  [110/345]  eta: 0:00:43  loss: 0.7698 (0.7648)  time: 0.1867  data: 0.0001  max mem: 14938
[22:14:51.225176] Test:  [120/345]  eta: 0:00:42  loss: 0.7697 (0.7646)  time: 0.1871  data: 0.0001  max mem: 14938
[22:14:53.103363] Test:  [130/345]  eta: 0:00:40  loss: 0.7571 (0.7642)  time: 0.1876  data: 0.0001  max mem: 14938
[22:14:54.985290] Test:  [140/345]  eta: 0:00:38  loss: 0.7573 (0.7645)  time: 0.1880  data: 0.0001  max mem: 14938
[22:14:56.869917] Test:  [150/345]  eta: 0:00:36  loss: 0.7624 (0.7642)  time: 0.1883  data: 0.0001  max mem: 14938
[22:14:58.756103] Test:  [160/345]  eta: 0:00:34  loss: 0.7693 (0.7649)  time: 0.1885  data: 0.0001  max mem: 14938
[22:15:00.647436] Test:  [170/345]  eta: 0:00:32  loss: 0.7719 (0.7648)  time: 0.1888  data: 0.0001  max mem: 14938
[22:15:02.543240] Test:  [180/345]  eta: 0:00:30  loss: 0.7659 (0.7651)  time: 0.1893  data: 0.0001  max mem: 14938
[22:15:04.441112] Test:  [190/345]  eta: 0:00:29  loss: 0.7675 (0.7654)  time: 0.1896  data: 0.0001  max mem: 14938
[22:15:06.342308] Test:  [200/345]  eta: 0:00:27  loss: 0.7622 (0.7652)  time: 0.1899  data: 0.0001  max mem: 14938
[22:15:08.248116] Test:  [210/345]  eta: 0:00:25  loss: 0.7620 (0.7650)  time: 0.1903  data: 0.0001  max mem: 14938
[22:15:10.156115] Test:  [220/345]  eta: 0:00:23  loss: 0.7638 (0.7651)  time: 0.1906  data: 0.0001  max mem: 14938
[22:15:12.070926] Test:  [230/345]  eta: 0:00:21  loss: 0.7653 (0.7651)  time: 0.1911  data: 0.0001  max mem: 14938
[22:15:13.987093] Test:  [240/345]  eta: 0:00:19  loss: 0.7631 (0.7653)  time: 0.1915  data: 0.0001  max mem: 14938
[22:15:15.906958] Test:  [250/345]  eta: 0:00:17  loss: 0.7631 (0.7655)  time: 0.1918  data: 0.0001  max mem: 14938
[22:15:17.829397] Test:  [260/345]  eta: 0:00:16  loss: 0.7610 (0.7652)  time: 0.1921  data: 0.0001  max mem: 14938
[22:15:19.756979] Test:  [270/345]  eta: 0:00:14  loss: 0.7554 (0.7651)  time: 0.1925  data: 0.0001  max mem: 14938
[22:15:21.687429] Test:  [280/345]  eta: 0:00:12  loss: 0.7719 (0.7655)  time: 0.1929  data: 0.0001  max mem: 14938
[22:15:23.620185] Test:  [290/345]  eta: 0:00:10  loss: 0.7764 (0.7659)  time: 0.1931  data: 0.0001  max mem: 14938
[22:15:25.556738] Test:  [300/345]  eta: 0:00:08  loss: 0.7692 (0.7659)  time: 0.1934  data: 0.0001  max mem: 14938
[22:15:27.495724] Test:  [310/345]  eta: 0:00:06  loss: 0.7633 (0.7658)  time: 0.1937  data: 0.0001  max mem: 14938
[22:15:29.437989] Test:  [320/345]  eta: 0:00:04  loss: 0.7637 (0.7658)  time: 0.1940  data: 0.0001  max mem: 14938
[22:15:31.385205] Test:  [330/345]  eta: 0:00:02  loss: 0.7669 (0.7657)  time: 0.1944  data: 0.0001  max mem: 14938
[22:15:33.335156] Test:  [340/345]  eta: 0:00:00  loss: 0.7669 (0.7658)  time: 0.1948  data: 0.0001  max mem: 14938
[22:15:34.117419] Test:  [344/345]  eta: 0:00:00  loss: 0.7583 (0.7656)  time: 0.1950  data: 0.0001  max mem: 14938
[22:15:34.155990] Test: Total time: 0:01:05 (0.1899 s / it)
[22:15:45.507958] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8516 (0.8516)  time: 0.3244  data: 0.1449  max mem: 14938
[22:15:47.322235] Test:  [10/57]  eta: 0:00:09  loss: 0.8786 (0.8805)  time: 0.1943  data: 0.0132  max mem: 14938
[22:15:49.141283] Test:  [20/57]  eta: 0:00:06  loss: 0.8786 (0.8719)  time: 0.1816  data: 0.0001  max mem: 14938
[22:15:50.965663] Test:  [30/57]  eta: 0:00:05  loss: 0.7873 (0.8332)  time: 0.1821  data: 0.0001  max mem: 14938
[22:15:52.793741] Test:  [40/57]  eta: 0:00:03  loss: 0.7494 (0.8127)  time: 0.1826  data: 0.0001  max mem: 14938
[22:15:54.626743] Test:  [50/57]  eta: 0:00:01  loss: 0.7369 (0.8049)  time: 0.1830  data: 0.0001  max mem: 14938
[22:15:55.614311] Test:  [56/57]  eta: 0:00:00  loss: 0.7709 (0.8107)  time: 0.1776  data: 0.0001  max mem: 14938
[22:15:55.674633] Test: Total time: 0:00:10 (0.1841 s / it)
[22:15:57.635744] Dice score of the network on the train images: 0.773450, val images: 0.817338
[22:15:57.635974] saving best_prec_model_0 @ epoch 15
[22:15:58.846777] saving best_dice_model_0 @ epoch 15
[22:15:59.904037] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:16:00.791348] Epoch: [16]  [  0/345]  eta: 0:05:05  lr: 0.000100  loss: 0.7585 (0.7585)  time: 0.8863  data: 0.1468  max mem: 14938
[22:16:15.637435] Epoch: [16]  [ 20/345]  eta: 0:04:03  lr: 0.000100  loss: 0.7977 (0.7954)  time: 0.7422  data: 0.0001  max mem: 14938
[22:16:30.545005] Epoch: [16]  [ 40/345]  eta: 0:03:47  lr: 0.000101  loss: 0.8036 (0.8007)  time: 0.7453  data: 0.0001  max mem: 14938
[22:16:45.485380] Epoch: [16]  [ 60/345]  eta: 0:03:32  lr: 0.000101  loss: 0.7947 (0.8003)  time: 0.7470  data: 0.0001  max mem: 14938
[22:17:00.471824] Epoch: [16]  [ 80/345]  eta: 0:03:18  lr: 0.000101  loss: 0.7897 (0.7976)  time: 0.7493  data: 0.0001  max mem: 14938
[22:17:15.487773] Epoch: [16]  [100/345]  eta: 0:03:03  lr: 0.000102  loss: 0.7841 (0.7957)  time: 0.7508  data: 0.0001  max mem: 14938
[22:17:30.501886] Epoch: [16]  [120/345]  eta: 0:02:48  lr: 0.000102  loss: 0.7843 (0.7946)  time: 0.7507  data: 0.0001  max mem: 14938
[22:17:45.497125] Epoch: [16]  [140/345]  eta: 0:02:33  lr: 0.000103  loss: 0.7825 (0.7934)  time: 0.7497  data: 0.0001  max mem: 14938
[22:18:00.481306] Epoch: [16]  [160/345]  eta: 0:02:18  lr: 0.000103  loss: 0.7859 (0.7932)  time: 0.7492  data: 0.0001  max mem: 14938
[22:18:15.585053] Epoch: [16]  [180/345]  eta: 0:02:03  lr: 0.000103  loss: 0.7820 (0.7923)  time: 0.7551  data: 0.0001  max mem: 14938
[22:18:30.563497] Epoch: [16]  [200/345]  eta: 0:01:48  lr: 0.000104  loss: 0.7997 (0.7925)  time: 0.7489  data: 0.0001  max mem: 14938
[22:18:45.534811] Epoch: [16]  [220/345]  eta: 0:01:33  lr: 0.000104  loss: 0.7894 (0.7925)  time: 0.7485  data: 0.0001  max mem: 14938
[22:19:00.501037] Epoch: [16]  [240/345]  eta: 0:01:18  lr: 0.000104  loss: 0.7913 (0.7926)  time: 0.7483  data: 0.0001  max mem: 14938
[22:19:15.462440] Epoch: [16]  [260/345]  eta: 0:01:03  lr: 0.000105  loss: 0.7901 (0.7927)  time: 0.7480  data: 0.0001  max mem: 14938
[22:19:30.423249] Epoch: [16]  [280/345]  eta: 0:00:48  lr: 0.000105  loss: 0.7934 (0.7925)  time: 0.7480  data: 0.0001  max mem: 14938
[22:19:45.375801] Epoch: [16]  [300/345]  eta: 0:00:33  lr: 0.000105  loss: 0.7956 (0.7927)  time: 0.7476  data: 0.0001  max mem: 14938
[22:20:00.323548] Epoch: [16]  [320/345]  eta: 0:00:18  lr: 0.000106  loss: 0.7970 (0.7930)  time: 0.7473  data: 0.0001  max mem: 14938
[22:20:15.269323] Epoch: [16]  [340/345]  eta: 0:00:03  lr: 0.000106  loss: 0.8012 (0.7937)  time: 0.7472  data: 0.0001  max mem: 14938
[22:20:18.258803] Epoch: [16]  [344/345]  eta: 0:00:00  lr: 0.000106  loss: 0.7923 (0.7938)  time: 0.7473  data: 0.0001  max mem: 14938
[22:20:18.326384] Epoch: [16] Total time: 0:04:18 (0.7490 s / it)
[22:20:18.326849] Averaged stats: lr: 0.000106  loss: 0.7923 (0.7938)
[22:20:18.663063] Test:  [  0/345]  eta: 0:01:54  loss: 0.8111 (0.8111)  time: 0.3326  data: 0.1510  max mem: 14938
[22:20:20.501364] Test:  [ 10/345]  eta: 0:01:06  loss: 0.8111 (0.8092)  time: 0.1973  data: 0.0138  max mem: 14938
[22:20:22.342058] Test:  [ 20/345]  eta: 0:01:02  loss: 0.8090 (0.8081)  time: 0.1839  data: 0.0001  max mem: 14938
[22:20:24.183161] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7996 (0.8066)  time: 0.1840  data: 0.0001  max mem: 14938
[22:20:26.028510] Test:  [ 40/345]  eta: 0:00:57  loss: 0.8050 (0.8066)  time: 0.1843  data: 0.0001  max mem: 14938
[22:20:27.878276] Test:  [ 50/345]  eta: 0:00:55  loss: 0.8054 (0.8067)  time: 0.1847  data: 0.0001  max mem: 14938
[22:20:29.730573] Test:  [ 60/345]  eta: 0:00:53  loss: 0.8030 (0.8057)  time: 0.1851  data: 0.0001  max mem: 14938
[22:20:31.586482] Test:  [ 70/345]  eta: 0:00:51  loss: 0.8079 (0.8072)  time: 0.1854  data: 0.0001  max mem: 14938
[22:20:33.443972] Test:  [ 80/345]  eta: 0:00:49  loss: 0.8125 (0.8077)  time: 0.1856  data: 0.0001  max mem: 14938
[22:20:35.305519] Test:  [ 90/345]  eta: 0:00:47  loss: 0.8125 (0.8074)  time: 0.1859  data: 0.0001  max mem: 14938
[22:20:37.170842] Test:  [100/345]  eta: 0:00:45  loss: 0.8038 (0.8067)  time: 0.1863  data: 0.0001  max mem: 14938
[22:20:39.039169] Test:  [110/345]  eta: 0:00:43  loss: 0.8046 (0.8066)  time: 0.1866  data: 0.0001  max mem: 14938
[22:20:40.913745] Test:  [120/345]  eta: 0:00:41  loss: 0.7942 (0.8062)  time: 0.1871  data: 0.0001  max mem: 14938
[22:20:42.790644] Test:  [130/345]  eta: 0:00:40  loss: 0.7912 (0.8049)  time: 0.1875  data: 0.0001  max mem: 14938
[22:20:44.672197] Test:  [140/345]  eta: 0:00:38  loss: 0.7931 (0.8054)  time: 0.1879  data: 0.0001  max mem: 14938
[22:20:46.555439] Test:  [150/345]  eta: 0:00:36  loss: 0.8127 (0.8069)  time: 0.1882  data: 0.0001  max mem: 14938
[22:20:48.442688] Test:  [160/345]  eta: 0:00:34  loss: 0.8126 (0.8074)  time: 0.1885  data: 0.0001  max mem: 14938
[22:20:50.333799] Test:  [170/345]  eta: 0:00:32  loss: 0.7944 (0.8068)  time: 0.1889  data: 0.0001  max mem: 14938
[22:20:52.227796] Test:  [180/345]  eta: 0:00:30  loss: 0.7944 (0.8067)  time: 0.1892  data: 0.0001  max mem: 14938
[22:20:54.128126] Test:  [190/345]  eta: 0:00:29  loss: 0.7920 (0.8058)  time: 0.1897  data: 0.0001  max mem: 14938
[22:20:56.029318] Test:  [200/345]  eta: 0:00:27  loss: 0.7938 (0.8060)  time: 0.1900  data: 0.0001  max mem: 14938
[22:20:57.934935] Test:  [210/345]  eta: 0:00:25  loss: 0.7947 (0.8053)  time: 0.1903  data: 0.0001  max mem: 14938
[22:20:59.845855] Test:  [220/345]  eta: 0:00:23  loss: 0.8072 (0.8055)  time: 0.1908  data: 0.0001  max mem: 14938
[22:21:01.756860] Test:  [230/345]  eta: 0:00:21  loss: 0.7988 (0.8053)  time: 0.1910  data: 0.0001  max mem: 14938
[22:21:03.671707] Test:  [240/345]  eta: 0:00:19  loss: 0.7936 (0.8055)  time: 0.1912  data: 0.0001  max mem: 14938
[22:21:05.590547] Test:  [250/345]  eta: 0:00:17  loss: 0.8104 (0.8055)  time: 0.1916  data: 0.0001  max mem: 14938
[22:21:07.514579] Test:  [260/345]  eta: 0:00:16  loss: 0.7956 (0.8050)  time: 0.1921  data: 0.0001  max mem: 14938
[22:21:09.443124] Test:  [270/345]  eta: 0:00:14  loss: 0.7900 (0.8046)  time: 0.1926  data: 0.0001  max mem: 14938
[22:21:11.373591] Test:  [280/345]  eta: 0:00:12  loss: 0.7941 (0.8042)  time: 0.1929  data: 0.0001  max mem: 14938
[22:21:13.304985] Test:  [290/345]  eta: 0:00:10  loss: 0.8036 (0.8043)  time: 0.1930  data: 0.0001  max mem: 14938
[22:21:15.240560] Test:  [300/345]  eta: 0:00:08  loss: 0.8094 (0.8044)  time: 0.1933  data: 0.0001  max mem: 14938
[22:21:17.180690] Test:  [310/345]  eta: 0:00:06  loss: 0.8039 (0.8045)  time: 0.1937  data: 0.0001  max mem: 14938
[22:21:19.123732] Test:  [320/345]  eta: 0:00:04  loss: 0.8091 (0.8049)  time: 0.1941  data: 0.0001  max mem: 14938
[22:21:21.069506] Test:  [330/345]  eta: 0:00:02  loss: 0.8115 (0.8049)  time: 0.1944  data: 0.0001  max mem: 14938
[22:21:23.018616] Test:  [340/345]  eta: 0:00:00  loss: 0.7964 (0.8047)  time: 0.1947  data: 0.0001  max mem: 14938
[22:21:23.801083] Test:  [344/345]  eta: 0:00:00  loss: 0.8030 (0.8047)  time: 0.1949  data: 0.0001  max mem: 14938
[22:21:23.861710] Test: Total time: 0:01:05 (0.1899 s / it)
[22:21:35.256995] Test:  [ 0/57]  eta: 0:00:18  loss: 0.9369 (0.9369)  time: 0.3222  data: 0.1429  max mem: 14938
[22:21:37.070258] Test:  [10/57]  eta: 0:00:09  loss: 0.9375 (0.9386)  time: 0.1941  data: 0.0130  max mem: 14938
[22:21:38.890924] Test:  [20/57]  eta: 0:00:06  loss: 0.9396 (0.9274)  time: 0.1816  data: 0.0001  max mem: 14938
[22:21:40.714058] Test:  [30/57]  eta: 0:00:05  loss: 0.8417 (0.8944)  time: 0.1821  data: 0.0001  max mem: 14938
[22:21:42.541693] Test:  [40/57]  eta: 0:00:03  loss: 0.8194 (0.8773)  time: 0.1825  data: 0.0001  max mem: 14938
[22:21:44.377236] Test:  [50/57]  eta: 0:00:01  loss: 0.8412 (0.8768)  time: 0.1831  data: 0.0001  max mem: 14938
[22:21:45.365766] Test:  [56/57]  eta: 0:00:00  loss: 0.8654 (0.8810)  time: 0.1777  data: 0.0001  max mem: 14938
[22:21:45.422251] Test: Total time: 0:00:10 (0.1840 s / it)
[22:21:47.400741] Dice score of the network on the train images: 0.775010, val images: 0.715715
[22:21:47.400974] saving best_prec_model_0 @ epoch 16
[22:21:48.506600] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:21:49.395302] Epoch: [17]  [  0/345]  eta: 0:05:06  lr: 0.000106  loss: 0.8054 (0.8054)  time: 0.8876  data: 0.1451  max mem: 14938
[22:22:04.266502] Epoch: [17]  [ 20/345]  eta: 0:04:03  lr: 0.000107  loss: 0.8196 (0.8215)  time: 0.7435  data: 0.0001  max mem: 14938
[22:22:19.183318] Epoch: [17]  [ 40/345]  eta: 0:03:48  lr: 0.000107  loss: 0.7966 (0.8109)  time: 0.7458  data: 0.0001  max mem: 14938
[22:22:34.151851] Epoch: [17]  [ 60/345]  eta: 0:03:33  lr: 0.000107  loss: 0.8039 (0.8122)  time: 0.7484  data: 0.0001  max mem: 14938
[22:22:49.144631] Epoch: [17]  [ 80/345]  eta: 0:03:18  lr: 0.000108  loss: 0.7961 (0.8095)  time: 0.7496  data: 0.0001  max mem: 14938
[22:23:04.294442] Epoch: [17]  [100/345]  eta: 0:03:03  lr: 0.000108  loss: 0.7873 (0.8064)  time: 0.7574  data: 0.0001  max mem: 14938
[22:23:19.315928] Epoch: [17]  [120/345]  eta: 0:02:48  lr: 0.000108  loss: 0.8073 (0.8060)  time: 0.7510  data: 0.0001  max mem: 14938
[22:23:34.326004] Epoch: [17]  [140/345]  eta: 0:02:33  lr: 0.000109  loss: 0.7874 (0.8038)  time: 0.7505  data: 0.0001  max mem: 14938
[22:23:49.332686] Epoch: [17]  [160/345]  eta: 0:02:18  lr: 0.000109  loss: 0.8010 (0.8030)  time: 0.7503  data: 0.0001  max mem: 14938
[22:24:04.353206] Epoch: [17]  [180/345]  eta: 0:02:03  lr: 0.000110  loss: 0.7959 (0.8023)  time: 0.7510  data: 0.0001  max mem: 14938
[22:24:19.357684] Epoch: [17]  [200/345]  eta: 0:01:48  lr: 0.000110  loss: 0.7902 (0.8007)  time: 0.7502  data: 0.0001  max mem: 14938
[22:24:34.348078] Epoch: [17]  [220/345]  eta: 0:01:33  lr: 0.000110  loss: 0.7908 (0.7999)  time: 0.7495  data: 0.0001  max mem: 14938
[22:24:49.334715] Epoch: [17]  [240/345]  eta: 0:01:18  lr: 0.000111  loss: 0.7838 (0.7993)  time: 0.7493  data: 0.0001  max mem: 14938
[22:25:04.323800] Epoch: [17]  [260/345]  eta: 0:01:03  lr: 0.000111  loss: 0.8002 (0.7995)  time: 0.7494  data: 0.0001  max mem: 14938
[22:25:19.308379] Epoch: [17]  [280/345]  eta: 0:00:48  lr: 0.000111  loss: 0.7876 (0.7987)  time: 0.7492  data: 0.0001  max mem: 14938
[22:25:34.289714] Epoch: [17]  [300/345]  eta: 0:00:33  lr: 0.000112  loss: 0.7897 (0.7984)  time: 0.7490  data: 0.0001  max mem: 14938
[22:25:49.265998] Epoch: [17]  [320/345]  eta: 0:00:18  lr: 0.000112  loss: 0.7860 (0.7978)  time: 0.7488  data: 0.0001  max mem: 14938
[22:26:04.241707] Epoch: [17]  [340/345]  eta: 0:00:03  lr: 0.000112  loss: 0.7820 (0.7971)  time: 0.7487  data: 0.0001  max mem: 14938
[22:26:07.238793] Epoch: [17]  [344/345]  eta: 0:00:00  lr: 0.000112  loss: 0.7786 (0.7969)  time: 0.7488  data: 0.0001  max mem: 14938
[22:26:07.300650] Epoch: [17] Total time: 0:04:18 (0.7501 s / it)
[22:26:07.301119] Averaged stats: lr: 0.000112  loss: 0.7786 (0.7969)
[22:26:07.633761] Test:  [  0/345]  eta: 0:01:53  loss: 0.7737 (0.7737)  time: 0.3283  data: 0.1466  max mem: 14938
[22:26:09.470216] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7610 (0.7568)  time: 0.1967  data: 0.0134  max mem: 14938
[22:26:11.310110] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7453 (0.7480)  time: 0.1838  data: 0.0001  max mem: 14938
[22:26:13.153090] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7453 (0.7536)  time: 0.1841  data: 0.0001  max mem: 14938
[22:26:14.998627] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7539 (0.7526)  time: 0.1844  data: 0.0001  max mem: 14938
[22:26:16.849541] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7630 (0.7557)  time: 0.1848  data: 0.0001  max mem: 14938
[22:26:18.703330] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7546 (0.7544)  time: 0.1852  data: 0.0001  max mem: 14938
[22:26:20.559582] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7529 (0.7546)  time: 0.1855  data: 0.0001  max mem: 14938
[22:26:22.418342] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7551 (0.7540)  time: 0.1857  data: 0.0001  max mem: 14938
[22:26:24.283095] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7518 (0.7540)  time: 0.1861  data: 0.0001  max mem: 14938
[22:26:26.151203] Test:  [100/345]  eta: 0:00:45  loss: 0.7487 (0.7537)  time: 0.1866  data: 0.0001  max mem: 14938
[22:26:28.021803] Test:  [110/345]  eta: 0:00:43  loss: 0.7484 (0.7530)  time: 0.1869  data: 0.0001  max mem: 14938
[22:26:29.895614] Test:  [120/345]  eta: 0:00:41  loss: 0.7510 (0.7534)  time: 0.1872  data: 0.0001  max mem: 14938
[22:26:31.773197] Test:  [130/345]  eta: 0:00:40  loss: 0.7538 (0.7535)  time: 0.1875  data: 0.0001  max mem: 14938
[22:26:33.652573] Test:  [140/345]  eta: 0:00:38  loss: 0.7538 (0.7533)  time: 0.1878  data: 0.0001  max mem: 14938
[22:26:35.537140] Test:  [150/345]  eta: 0:00:36  loss: 0.7504 (0.7530)  time: 0.1881  data: 0.0001  max mem: 14938
[22:26:37.424231] Test:  [160/345]  eta: 0:00:34  loss: 0.7499 (0.7531)  time: 0.1885  data: 0.0001  max mem: 14938
[22:26:39.314332] Test:  [170/345]  eta: 0:00:32  loss: 0.7499 (0.7529)  time: 0.1888  data: 0.0001  max mem: 14938
[22:26:41.208703] Test:  [180/345]  eta: 0:00:30  loss: 0.7532 (0.7533)  time: 0.1892  data: 0.0001  max mem: 14938
[22:26:43.106584] Test:  [190/345]  eta: 0:00:29  loss: 0.7566 (0.7537)  time: 0.1896  data: 0.0001  max mem: 14938
[22:26:45.010510] Test:  [200/345]  eta: 0:00:27  loss: 0.7629 (0.7538)  time: 0.1900  data: 0.0001  max mem: 14938
[22:26:46.915879] Test:  [210/345]  eta: 0:00:25  loss: 0.7558 (0.7543)  time: 0.1904  data: 0.0001  max mem: 14938
[22:26:48.826302] Test:  [220/345]  eta: 0:00:23  loss: 0.7533 (0.7540)  time: 0.1907  data: 0.0001  max mem: 14938
[22:26:50.737920] Test:  [230/345]  eta: 0:00:21  loss: 0.7485 (0.7539)  time: 0.1911  data: 0.0001  max mem: 14938
[22:26:52.653671] Test:  [240/345]  eta: 0:00:19  loss: 0.7485 (0.7537)  time: 0.1913  data: 0.0001  max mem: 14938
[22:26:54.573509] Test:  [250/345]  eta: 0:00:17  loss: 0.7500 (0.7538)  time: 0.1917  data: 0.0001  max mem: 14938
[22:26:56.497580] Test:  [260/345]  eta: 0:00:16  loss: 0.7545 (0.7538)  time: 0.1921  data: 0.0001  max mem: 14938
[22:26:58.425327] Test:  [270/345]  eta: 0:00:14  loss: 0.7571 (0.7541)  time: 0.1925  data: 0.0001  max mem: 14938
[22:27:00.355683] Test:  [280/345]  eta: 0:00:12  loss: 0.7519 (0.7542)  time: 0.1929  data: 0.0001  max mem: 14938
[22:27:02.290325] Test:  [290/345]  eta: 0:00:10  loss: 0.7435 (0.7538)  time: 0.1932  data: 0.0001  max mem: 14938
[22:27:04.227160] Test:  [300/345]  eta: 0:00:08  loss: 0.7471 (0.7540)  time: 0.1935  data: 0.0001  max mem: 14938
[22:27:06.169545] Test:  [310/345]  eta: 0:00:06  loss: 0.7628 (0.7542)  time: 0.1939  data: 0.0001  max mem: 14938
[22:27:08.112761] Test:  [320/345]  eta: 0:00:04  loss: 0.7482 (0.7539)  time: 0.1942  data: 0.0001  max mem: 14938
[22:27:10.059119] Test:  [330/345]  eta: 0:00:02  loss: 0.7454 (0.7535)  time: 0.1944  data: 0.0001  max mem: 14938
[22:27:12.007222] Test:  [340/345]  eta: 0:00:00  loss: 0.7460 (0.7533)  time: 0.1947  data: 0.0001  max mem: 14938
[22:27:12.789906] Test:  [344/345]  eta: 0:00:00  loss: 0.7460 (0.7532)  time: 0.1949  data: 0.0001  max mem: 14938
[22:27:12.849373] Test: Total time: 0:01:05 (0.1900 s / it)
[22:27:24.282773] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8552 (0.8552)  time: 0.3249  data: 0.1459  max mem: 14938
[22:27:26.096878] Test:  [10/57]  eta: 0:00:09  loss: 0.8779 (0.8735)  time: 0.1944  data: 0.0133  max mem: 14938
[22:27:27.916778] Test:  [20/57]  eta: 0:00:06  loss: 0.8808 (0.8652)  time: 0.1816  data: 0.0001  max mem: 14938
[22:27:29.740481] Test:  [30/57]  eta: 0:00:05  loss: 0.7703 (0.8277)  time: 0.1821  data: 0.0001  max mem: 14938
[22:27:31.566112] Test:  [40/57]  eta: 0:00:03  loss: 0.7508 (0.8082)  time: 0.1824  data: 0.0001  max mem: 14938
[22:27:33.399950] Test:  [50/57]  eta: 0:00:01  loss: 0.7437 (0.8007)  time: 0.1829  data: 0.0001  max mem: 14938
[22:27:34.388762] Test:  [56/57]  eta: 0:00:00  loss: 0.7697 (0.8063)  time: 0.1777  data: 0.0001  max mem: 14938
[22:27:34.449824] Test: Total time: 0:00:10 (0.1841 s / it)
[22:27:36.416697] Dice score of the network on the train images: 0.782493, val images: 0.811110
[22:27:36.421344] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:27:37.315334] Epoch: [18]  [  0/345]  eta: 0:05:08  lr: 0.000113  loss: 0.7779 (0.7779)  time: 0.8928  data: 0.1513  max mem: 14938
[22:27:52.187868] Epoch: [18]  [ 20/345]  eta: 0:04:03  lr: 0.000113  loss: 0.7682 (0.7743)  time: 0.7436  data: 0.0001  max mem: 14938
[22:28:07.121735] Epoch: [18]  [ 40/345]  eta: 0:03:48  lr: 0.000113  loss: 0.7790 (0.7792)  time: 0.7466  data: 0.0001  max mem: 14938
[22:28:22.096505] Epoch: [18]  [ 60/345]  eta: 0:03:33  lr: 0.000114  loss: 0.7807 (0.7788)  time: 0.7487  data: 0.0001  max mem: 14938
[22:28:37.092269] Epoch: [18]  [ 80/345]  eta: 0:03:18  lr: 0.000114  loss: 0.7771 (0.7789)  time: 0.7498  data: 0.0001  max mem: 14938
[22:28:52.113575] Epoch: [18]  [100/345]  eta: 0:03:03  lr: 0.000114  loss: 0.7789 (0.7804)  time: 0.7510  data: 0.0001  max mem: 14938
[22:29:07.261988] Epoch: [18]  [120/345]  eta: 0:02:48  lr: 0.000115  loss: 0.7930 (0.7819)  time: 0.7574  data: 0.0001  max mem: 14938
[22:29:22.270448] Epoch: [18]  [140/345]  eta: 0:02:33  lr: 0.000115  loss: 0.7830 (0.7818)  time: 0.7504  data: 0.0001  max mem: 14938
[22:29:37.280477] Epoch: [18]  [160/345]  eta: 0:02:18  lr: 0.000115  loss: 0.7810 (0.7822)  time: 0.7505  data: 0.0001  max mem: 14938
[22:29:52.264915] Epoch: [18]  [180/345]  eta: 0:02:03  lr: 0.000116  loss: 0.7838 (0.7824)  time: 0.7492  data: 0.0001  max mem: 14938
[22:30:07.240980] Epoch: [18]  [200/345]  eta: 0:01:48  lr: 0.000116  loss: 0.8095 (0.7848)  time: 0.7488  data: 0.0001  max mem: 14938
[22:30:22.222662] Epoch: [18]  [220/345]  eta: 0:01:33  lr: 0.000116  loss: 0.8066 (0.7868)  time: 0.7490  data: 0.0001  max mem: 14938
[22:30:37.215187] Epoch: [18]  [240/345]  eta: 0:01:18  lr: 0.000117  loss: 0.8174 (0.7891)  time: 0.7496  data: 0.0001  max mem: 14938

[22:30:52.190802] Epoch: [18]  [260/345]  eta: 0:01:03  lr: 0.000117  loss: 0.7844 (0.7891)  time: 0.7487  data: 0.0001  max mem: 14938
[22:31:07.177766] Epoch: [18]  [280/345]  eta: 0:00:48  lr: 0.000118  loss: 0.7725 (0.7888)  time: 0.7493  data: 0.0001  max mem: 14938
[22:31:22.164354] Epoch: [18]  [300/345]  eta: 0:00:33  lr: 0.000118  loss: 0.7836 (0.7884)  time: 0.7493  data: 0.0001  max mem: 14938
[22:31:37.147371] Epoch: [18]  [320/345]  eta: 0:00:18  lr: 0.000118  loss: 0.7818 (0.7885)  time: 0.7491  data: 0.0001  max mem: 14938
[22:31:52.120648] Epoch: [18]  [340/345]  eta: 0:00:03  lr: 0.000119  loss: 0.7844 (0.7883)  time: 0.7486  data: 0.0001  max mem: 14938
[22:31:55.114730] Epoch: [18]  [344/345]  eta: 0:00:00  lr: 0.000119  loss: 0.7848 (0.7882)  time: 0.7485  data: 0.0001  max mem: 14938
[22:31:55.178633] Epoch: [18] Total time: 0:04:18 (0.7500 s / it)
[22:31:55.178945] Averaged stats: lr: 0.000119  loss: 0.7848 (0.7882)
[22:31:55.520213] Test:  [  0/345]  eta: 0:01:56  loss: 0.7470 (0.7470)  time: 0.3371  data: 0.1558  max mem: 14938
[22:31:57.357442] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7430 (0.7412)  time: 0.1976  data: 0.0142  max mem: 14938
[22:31:59.197594] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7511 (0.7514)  time: 0.1838  data: 0.0001  max mem: 14938
[22:32:01.040600] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7581 (0.7522)  time: 0.1841  data: 0.0001  max mem: 14938
[22:32:02.886789] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7507 (0.7502)  time: 0.1844  data: 0.0001  max mem: 14938
[22:32:04.738233] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7507 (0.7514)  time: 0.1848  data: 0.0001  max mem: 14938
[22:32:06.592538] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7463 (0.7510)  time: 0.1852  data: 0.0001  max mem: 14938
[22:32:08.447948] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7400 (0.7490)  time: 0.1854  data: 0.0001  max mem: 14938
[22:32:10.308739] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7453 (0.7498)  time: 0.1858  data: 0.0001  max mem: 14938
[22:32:12.172457] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7499 (0.7494)  time: 0.1862  data: 0.0001  max mem: 14938
[22:32:14.039143] Test:  [100/345]  eta: 0:00:45  loss: 0.7425 (0.7488)  time: 0.1865  data: 0.0001  max mem: 14938
[22:32:15.909771] Test:  [110/345]  eta: 0:00:43  loss: 0.7441 (0.7493)  time: 0.1868  data: 0.0001  max mem: 14938
[22:32:17.784422] Test:  [120/345]  eta: 0:00:42  loss: 0.7420 (0.7493)  time: 0.1872  data: 0.0001  max mem: 14938
[22:32:19.662988] Test:  [130/345]  eta: 0:00:40  loss: 0.7448 (0.7494)  time: 0.1876  data: 0.0001  max mem: 14938
[22:32:21.545806] Test:  [140/345]  eta: 0:00:38  loss: 0.7448 (0.7490)  time: 0.1880  data: 0.0001  max mem: 14938
[22:32:23.429328] Test:  [150/345]  eta: 0:00:36  loss: 0.7396 (0.7486)  time: 0.1883  data: 0.0001  max mem: 14938
[22:32:25.317083] Test:  [160/345]  eta: 0:00:34  loss: 0.7472 (0.7489)  time: 0.1885  data: 0.0001  max mem: 14938
[22:32:27.207970] Test:  [170/345]  eta: 0:00:32  loss: 0.7576 (0.7493)  time: 0.1889  data: 0.0001  max mem: 14938
[22:32:29.103575] Test:  [180/345]  eta: 0:00:30  loss: 0.7555 (0.7496)  time: 0.1893  data: 0.0001  max mem: 14938
[22:32:31.002542] Test:  [190/345]  eta: 0:00:29  loss: 0.7489 (0.7497)  time: 0.1897  data: 0.0001  max mem: 14938
[22:32:32.904302] Test:  [200/345]  eta: 0:00:27  loss: 0.7516 (0.7500)  time: 0.1900  data: 0.0001  max mem: 14938
[22:32:34.809905] Test:  [210/345]  eta: 0:00:25  loss: 0.7457 (0.7497)  time: 0.1903  data: 0.0001  max mem: 14938
[22:32:36.719560] Test:  [220/345]  eta: 0:00:23  loss: 0.7434 (0.7494)  time: 0.1907  data: 0.0001  max mem: 14938
[22:32:38.631911] Test:  [230/345]  eta: 0:00:21  loss: 0.7439 (0.7494)  time: 0.1910  data: 0.0001  max mem: 14938
[22:32:40.548910] Test:  [240/345]  eta: 0:00:19  loss: 0.7476 (0.7496)  time: 0.1914  data: 0.0001  max mem: 14938
[22:32:42.470258] Test:  [250/345]  eta: 0:00:17  loss: 0.7476 (0.7493)  time: 0.1919  data: 0.0001  max mem: 14938
[22:32:44.395505] Test:  [260/345]  eta: 0:00:16  loss: 0.7533 (0.7498)  time: 0.1923  data: 0.0001  max mem: 14938
[22:32:46.321949] Test:  [270/345]  eta: 0:00:14  loss: 0.7533 (0.7498)  time: 0.1925  data: 0.0001  max mem: 14938
[22:32:48.254206] Test:  [280/345]  eta: 0:00:12  loss: 0.7427 (0.7499)  time: 0.1929  data: 0.0001  max mem: 14938
[22:32:50.187764] Test:  [290/345]  eta: 0:00:10  loss: 0.7479 (0.7502)  time: 0.1932  data: 0.0001  max mem: 14938
[22:32:52.126664] Test:  [300/345]  eta: 0:00:08  loss: 0.7472 (0.7499)  time: 0.1936  data: 0.0001  max mem: 14938
[22:32:54.068023] Test:  [310/345]  eta: 0:00:06  loss: 0.7452 (0.7498)  time: 0.1940  data: 0.0001  max mem: 14938
[22:32:56.013281] Test:  [320/345]  eta: 0:00:04  loss: 0.7484 (0.7497)  time: 0.1943  data: 0.0001  max mem: 14938
[22:32:57.960510] Test:  [330/345]  eta: 0:00:02  loss: 0.7484 (0.7496)  time: 0.1946  data: 0.0001  max mem: 14938
[22:32:59.910736] Test:  [340/345]  eta: 0:00:00  loss: 0.7397 (0.7494)  time: 0.1948  data: 0.0001  max mem: 14938
[22:33:00.692105] Test:  [344/345]  eta: 0:00:00  loss: 0.7415 (0.7495)  time: 0.1949  data: 0.0001  max mem: 14938
[22:33:00.752127] Test: Total time: 0:01:05 (0.1901 s / it)
[22:33:12.063161] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8396 (0.8396)  time: 0.3207  data: 0.1419  max mem: 14938
[22:33:13.877921] Test:  [10/57]  eta: 0:00:09  loss: 0.8854 (0.8795)  time: 0.1941  data: 0.0130  max mem: 14938
[22:33:15.698653] Test:  [20/57]  eta: 0:00:06  loss: 0.8854 (0.8664)  time: 0.1817  data: 0.0001  max mem: 14938
[22:33:17.521925] Test:  [30/57]  eta: 0:00:05  loss: 0.7801 (0.8286)  time: 0.1821  data: 0.0001  max mem: 14938
[22:33:19.351494] Test:  [40/57]  eta: 0:00:03  loss: 0.7476 (0.8085)  time: 0.1826  data: 0.0001  max mem: 14938
[22:33:21.184188] Test:  [50/57]  eta: 0:00:01  loss: 0.7370 (0.8010)  time: 0.1831  data: 0.0001  max mem: 14938
[22:33:22.172979] Test:  [56/57]  eta: 0:00:00  loss: 0.7766 (0.8073)  time: 0.1777  data: 0.0001  max mem: 14938
[22:33:22.228756] Test: Total time: 0:00:10 (0.1840 s / it)
[22:33:24.194783] Dice score of the network on the train images: 0.776502, val images: 0.815787
[22:33:24.199215] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:33:25.088904] Epoch: [19]  [  0/345]  eta: 0:05:06  lr: 0.000119  loss: 0.7797 (0.7797)  time: 0.8887  data: 0.1457  max mem: 14938
[22:33:39.974869] Epoch: [19]  [ 20/345]  eta: 0:04:04  lr: 0.000119  loss: 0.7717 (0.7748)  time: 0.7442  data: 0.0001  max mem: 14938
[22:33:54.898221] Epoch: [19]  [ 40/345]  eta: 0:03:48  lr: 0.000119  loss: 0.7833 (0.7788)  time: 0.7461  data: 0.0001  max mem: 14938
[22:34:09.863952] Epoch: [19]  [ 60/345]  eta: 0:03:33  lr: 0.000120  loss: 0.7887 (0.7820)  time: 0.7482  data: 0.0001  max mem: 14938
[22:34:24.864630] Epoch: [19]  [ 80/345]  eta: 0:03:18  lr: 0.000120  loss: 0.7804 (0.7814)  time: 0.7500  data: 0.0001  max mem: 14938
[22:34:39.902457] Epoch: [19]  [100/345]  eta: 0:03:03  lr: 0.000121  loss: 0.7893 (0.7828)  time: 0.7518  data: 0.0001  max mem: 14938
[22:34:54.939716] Epoch: [19]  [120/345]  eta: 0:02:48  lr: 0.000121  loss: 0.7739 (0.7825)  time: 0.7518  data: 0.0001  max mem: 14938
[22:35:09.961294] Epoch: [19]  [140/345]  eta: 0:02:33  lr: 0.000121  loss: 0.7903 (0.7840)  time: 0.7510  data: 0.0001  max mem: 14938
[22:35:24.970107] Epoch: [19]  [160/345]  eta: 0:02:18  lr: 0.000122  loss: 0.7796 (0.7841)  time: 0.7504  data: 0.0001  max mem: 14938
[22:35:39.973625] Epoch: [19]  [180/345]  eta: 0:02:03  lr: 0.000122  loss: 0.7872 (0.7851)  time: 0.7501  data: 0.0001  max mem: 14938
[22:35:54.971959] Epoch: [19]  [200/345]  eta: 0:01:48  lr: 0.000122  loss: 0.7784 (0.7847)  time: 0.7499  data: 0.0001  max mem: 14938
[22:36:09.964196] Epoch: [19]  [220/345]  eta: 0:01:33  lr: 0.000123  loss: 0.7747 (0.7837)  time: 0.7496  data: 0.0001  max mem: 14938
[22:36:24.957363] Epoch: [19]  [240/345]  eta: 0:01:18  lr: 0.000123  loss: 0.7807 (0.7835)  time: 0.7496  data: 0.0001  max mem: 14938
[22:36:39.944981] Epoch: [19]  [260/345]  eta: 0:01:03  lr: 0.000123  loss: 0.7658 (0.7825)  time: 0.7493  data: 0.0001  max mem: 14938
[22:36:54.929015] Epoch: [19]  [280/345]  eta: 0:00:48  lr: 0.000124  loss: 0.7691 (0.7817)  time: 0.7492  data: 0.0000  max mem: 14938
[22:37:09.916199] Epoch: [19]  [300/345]  eta: 0:00:33  lr: 0.000124  loss: 0.7769 (0.7816)  time: 0.7493  data: 0.0001  max mem: 14938
[22:37:24.897066] Epoch: [19]  [320/345]  eta: 0:00:18  lr: 0.000125  loss: 0.7765 (0.7815)  time: 0.7490  data: 0.0001  max mem: 14938
[22:37:39.871488] Epoch: [19]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.8140 (0.7833)  time: 0.7487  data: 0.0001  max mem: 14938
[22:37:42.868891] Epoch: [19]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.8015 (0.7835)  time: 0.7488  data: 0.0001  max mem: 14938
[22:37:42.935076] Epoch: [19] Total time: 0:04:18 (0.7500 s / it)
[22:37:42.935426] Averaged stats: lr: 0.000125  loss: 0.8015 (0.7835)
[22:37:43.273739] Test:  [  0/345]  eta: 0:01:55  loss: 0.7779 (0.7779)  time: 0.3343  data: 0.1522  max mem: 14938
[22:37:45.109761] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7713 (0.7703)  time: 0.1972  data: 0.0139  max mem: 14938
[22:37:46.951739] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7713 (0.7723)  time: 0.1838  data: 0.0001  max mem: 14938
[22:37:48.795058] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7672 (0.7693)  time: 0.1842  data: 0.0001  max mem: 14938
[22:37:50.640852] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7689 (0.7730)  time: 0.1844  data: 0.0001  max mem: 14938
[22:37:52.491611] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7729 (0.7727)  time: 0.1848  data: 0.0001  max mem: 14938
[22:37:54.345416] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7662 (0.7729)  time: 0.1852  data: 0.0001  max mem: 14938
[22:37:56.201924] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7667 (0.7718)  time: 0.1855  data: 0.0001  max mem: 14938
[22:37:58.060779] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7674 (0.7724)  time: 0.1857  data: 0.0001  max mem: 14938
[22:37:59.924176] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7772 (0.7733)  time: 0.1861  data: 0.0001  max mem: 14938
[22:38:01.791573] Test:  [100/345]  eta: 0:00:45  loss: 0.7684 (0.7730)  time: 0.1865  data: 0.0001  max mem: 14938
[22:38:03.663477] Test:  [110/345]  eta: 0:00:43  loss: 0.7677 (0.7725)  time: 0.1869  data: 0.0001  max mem: 14938
[22:38:05.538705] Test:  [120/345]  eta: 0:00:42  loss: 0.7620 (0.7727)  time: 0.1873  data: 0.0001  max mem: 14938
[22:38:07.416494] Test:  [130/345]  eta: 0:00:40  loss: 0.7698 (0.7726)  time: 0.1876  data: 0.0001  max mem: 14938
[22:38:09.296974] Test:  [140/345]  eta: 0:00:38  loss: 0.7762 (0.7732)  time: 0.1879  data: 0.0001  max mem: 14938
[22:38:11.180794] Test:  [150/345]  eta: 0:00:36  loss: 0.7820 (0.7733)  time: 0.1882  data: 0.0001  max mem: 14938
[22:38:13.069559] Test:  [160/345]  eta: 0:00:34  loss: 0.7834 (0.7741)  time: 0.1886  data: 0.0001  max mem: 14938
[22:38:14.961293] Test:  [170/345]  eta: 0:00:32  loss: 0.7759 (0.7742)  time: 0.1890  data: 0.0001  max mem: 14938
[22:38:16.857790] Test:  [180/345]  eta: 0:00:30  loss: 0.7729 (0.7737)  time: 0.1894  data: 0.0001  max mem: 14938
[22:38:18.760498] Test:  [190/345]  eta: 0:00:29  loss: 0.7686 (0.7738)  time: 0.1899  data: 0.0001  max mem: 14938
[22:38:20.661593] Test:  [200/345]  eta: 0:00:27  loss: 0.7746 (0.7740)  time: 0.1901  data: 0.0001  max mem: 14938
[22:38:22.568550] Test:  [210/345]  eta: 0:00:25  loss: 0.7746 (0.7738)  time: 0.1904  data: 0.0001  max mem: 14938
[22:38:24.476887] Test:  [220/345]  eta: 0:00:23  loss: 0.7736 (0.7739)  time: 0.1907  data: 0.0001  max mem: 14938
[22:38:26.390338] Test:  [230/345]  eta: 0:00:21  loss: 0.7702 (0.7740)  time: 0.1910  data: 0.0001  max mem: 14938
[22:38:28.306624] Test:  [240/345]  eta: 0:00:19  loss: 0.7705 (0.7740)  time: 0.1914  data: 0.0001  max mem: 14938
[22:38:30.227610] Test:  [250/345]  eta: 0:00:17  loss: 0.7705 (0.7741)  time: 0.1918  data: 0.0001  max mem: 14938
[22:38:32.152319] Test:  [260/345]  eta: 0:00:16  loss: 0.7688 (0.7738)  time: 0.1922  data: 0.0001  max mem: 14938
[22:38:34.079221] Test:  [270/345]  eta: 0:00:14  loss: 0.7677 (0.7739)  time: 0.1925  data: 0.0001  max mem: 14938
[22:38:36.009328] Test:  [280/345]  eta: 0:00:12  loss: 0.7861 (0.7744)  time: 0.1928  data: 0.0001  max mem: 14938
[22:38:37.944197] Test:  [290/345]  eta: 0:00:10  loss: 0.7798 (0.7745)  time: 0.1932  data: 0.0001  max mem: 14938
[22:38:39.880848] Test:  [300/345]  eta: 0:00:08  loss: 0.7799 (0.7749)  time: 0.1935  data: 0.0001  max mem: 14938
[22:38:41.821695] Test:  [310/345]  eta: 0:00:06  loss: 0.7840 (0.7751)  time: 0.1938  data: 0.0001  max mem: 14938
[22:38:43.764825] Test:  [320/345]  eta: 0:00:04  loss: 0.7775 (0.7750)  time: 0.1942  data: 0.0001  max mem: 14938
[22:38:45.712049] Test:  [330/345]  eta: 0:00:02  loss: 0.7639 (0.7746)  time: 0.1945  data: 0.0001  max mem: 14938
[22:38:47.663325] Test:  [340/345]  eta: 0:00:00  loss: 0.7725 (0.7748)  time: 0.1949  data: 0.0001  max mem: 14938
[22:38:48.444985] Test:  [344/345]  eta: 0:00:00  loss: 0.7754 (0.7748)  time: 0.1950  data: 0.0001  max mem: 14938
[22:38:48.504003] Test: Total time: 0:01:05 (0.1900 s / it)
[22:38:59.908811] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8149 (0.8149)  time: 0.3217  data: 0.1425  max mem: 14938
[22:39:01.725376] Test:  [10/57]  eta: 0:00:09  loss: 0.8659 (0.8732)  time: 0.1943  data: 0.0130  max mem: 14938
[22:39:03.545971] Test:  [20/57]  eta: 0:00:06  loss: 0.8757 (0.8683)  time: 0.1818  data: 0.0001  max mem: 14938
[22:39:05.371230] Test:  [30/57]  eta: 0:00:05  loss: 0.7693 (0.8320)  time: 0.1822  data: 0.0001  max mem: 14938
[22:39:07.202091] Test:  [40/57]  eta: 0:00:03  loss: 0.7497 (0.8111)  time: 0.1828  data: 0.0001  max mem: 14938
[22:39:09.037759] Test:  [50/57]  eta: 0:00:01  loss: 0.7463 (0.8039)  time: 0.1833  data: 0.0001  max mem: 14938
[22:39:10.026100] Test:  [56/57]  eta: 0:00:00  loss: 0.7909 (0.8105)  time: 0.1778  data: 0.0001  max mem: 14938
[22:39:10.086632] Test: Total time: 0:00:10 (0.1842 s / it)
[22:39:12.066228] Dice score of the network on the train images: 0.733658, val images: 0.802719
[22:39:12.066463] saving best_rec_model_0 @ epoch 19
[22:39:13.279373] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:39:14.162698] Epoch: [20]  [  0/345]  eta: 0:05:04  lr: 0.000125  loss: 0.7911 (0.7911)  time: 0.8822  data: 0.1439  max mem: 14938
[22:39:29.019451] Epoch: [20]  [ 20/345]  eta: 0:04:03  lr: 0.000125  loss: 0.7772 (0.7805)  time: 0.7428  data: 0.0001  max mem: 14938
[22:39:43.940425] Epoch: [20]  [ 40/345]  eta: 0:03:48  lr: 0.000125  loss: 0.7967 (0.7868)  time: 0.7460  data: 0.0001  max mem: 14938
[22:39:58.910969] Epoch: [20]  [ 60/345]  eta: 0:03:33  lr: 0.000125  loss: 0.7886 (0.7883)  time: 0.7485  data: 0.0001  max mem: 14938
[22:40:13.916942] Epoch: [20]  [ 80/345]  eta: 0:03:18  lr: 0.000125  loss: 0.7774 (0.7874)  time: 0.7502  data: 0.0001  max mem: 14938
[22:40:28.958965] Epoch: [20]  [100/345]  eta: 0:03:03  lr: 0.000125  loss: 0.7811 (0.7864)  time: 0.7521  data: 0.0001  max mem: 14938
[22:40:43.986698] Epoch: [20]  [120/345]  eta: 0:02:48  lr: 0.000125  loss: 0.7787 (0.7851)  time: 0.7513  data: 0.0001  max mem: 14938
[22:40:59.010609] Epoch: [20]  [140/345]  eta: 0:02:33  lr: 0.000125  loss: 0.7826 (0.7852)  time: 0.7511  data: 0.0001  max mem: 14938
[22:41:14.025518] Epoch: [20]  [160/345]  eta: 0:02:18  lr: 0.000125  loss: 0.7831 (0.7851)  time: 0.7507  data: 0.0001  max mem: 14938
[22:41:29.029895] Epoch: [20]  [180/345]  eta: 0:02:03  lr: 0.000125  loss: 0.7689 (0.7840)  time: 0.7502  data: 0.0001  max mem: 14938
[22:41:44.028878] Epoch: [20]  [200/345]  eta: 0:01:48  lr: 0.000125  loss: 0.7775 (0.7832)  time: 0.7499  data: 0.0001  max mem: 14938
[22:41:59.022600] Epoch: [20]  [220/345]  eta: 0:01:33  lr: 0.000125  loss: 0.7760 (0.7829)  time: 0.7496  data: 0.0001  max mem: 14938
[22:42:14.012690] Epoch: [20]  [240/345]  eta: 0:01:18  lr: 0.000125  loss: 0.7766 (0.7823)  time: 0.7495  data: 0.0001  max mem: 14938
[22:42:29.125597] Epoch: [20]  [260/345]  eta: 0:01:03  lr: 0.000125  loss: 0.7714 (0.7816)  time: 0.7556  data: 0.0001  max mem: 14938
[22:42:44.114263] Epoch: [20]  [280/345]  eta: 0:00:48  lr: 0.000125  loss: 0.7684 (0.7807)  time: 0.7494  data: 0.0001  max mem: 14938
[22:42:59.099419] Epoch: [20]  [300/345]  eta: 0:00:33  lr: 0.000125  loss: 0.7695 (0.7799)  time: 0.7492  data: 0.0001  max mem: 14938
[22:43:14.086175] Epoch: [20]  [320/345]  eta: 0:00:18  lr: 0.000125  loss: 0.7727 (0.7792)  time: 0.7493  data: 0.0001  max mem: 14938
[22:43:29.070131] Epoch: [20]  [340/345]  eta: 0:00:03  lr: 0.000125  loss: 0.7818 (0.7795)  time: 0.7492  data: 0.0001  max mem: 14938
[22:43:32.066957] Epoch: [20]  [344/345]  eta: 0:00:00  lr: 0.000125  loss: 0.7823 (0.7795)  time: 0.7492  data: 0.0001  max mem: 14938
[22:43:32.124476] Epoch: [20] Total time: 0:04:18 (0.7503 s / it)
[22:43:32.124935] Averaged stats: lr: 0.000125  loss: 0.7823 (0.7795)
[22:43:32.462946] Test:  [  0/345]  eta: 0:01:55  loss: 0.7772 (0.7772)  time: 0.3339  data: 0.1545  max mem: 14938
[22:43:34.299151] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7621 (0.7570)  time: 0.1972  data: 0.0141  max mem: 14938
[22:43:36.139143] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7493 (0.7508)  time: 0.1838  data: 0.0001  max mem: 14938
[22:43:37.982431] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7496 (0.7526)  time: 0.1841  data: 0.0001  max mem: 14938
[22:43:39.829082] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7522 (0.7523)  time: 0.1844  data: 0.0001  max mem: 14938
[22:43:41.678742] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7536 (0.7533)  time: 0.1848  data: 0.0001  max mem: 14938
[22:43:43.532447] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7536 (0.7547)  time: 0.1851  data: 0.0001  max mem: 14938
[22:43:45.389480] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7527 (0.7549)  time: 0.1855  data: 0.0001  max mem: 14938
[22:43:47.250131] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7506 (0.7535)  time: 0.1858  data: 0.0001  max mem: 14938
[22:43:49.115665] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7467 (0.7546)  time: 0.1863  data: 0.0001  max mem: 14938
[22:43:50.982770] Test:  [100/345]  eta: 0:00:45  loss: 0.7534 (0.7550)  time: 0.1866  data: 0.0001  max mem: 14938
[22:43:52.853576] Test:  [110/345]  eta: 0:00:43  loss: 0.7544 (0.7549)  time: 0.1868  data: 0.0001  max mem: 14938
[22:43:54.728753] Test:  [120/345]  eta: 0:00:42  loss: 0.7565 (0.7550)  time: 0.1872  data: 0.0001  max mem: 14938
[22:43:56.605524] Test:  [130/345]  eta: 0:00:40  loss: 0.7565 (0.7554)  time: 0.1875  data: 0.0001  max mem: 14938
[22:43:58.485743] Test:  [140/345]  eta: 0:00:38  loss: 0.7540 (0.7557)  time: 0.1878  data: 0.0001  max mem: 14938
[22:44:00.370427] Test:  [150/345]  eta: 0:00:36  loss: 0.7576 (0.7563)  time: 0.1882  data: 0.0001  max mem: 14938
[22:44:02.257895] Test:  [160/345]  eta: 0:00:34  loss: 0.7600 (0.7563)  time: 0.1886  data: 0.0001  max mem: 14938
[22:44:04.149780] Test:  [170/345]  eta: 0:00:32  loss: 0.7589 (0.7563)  time: 0.1889  data: 0.0001  max mem: 14938
[22:44:06.045070] Test:  [180/345]  eta: 0:00:30  loss: 0.7498 (0.7560)  time: 0.1893  data: 0.0001  max mem: 14938
[22:44:07.945349] Test:  [190/345]  eta: 0:00:29  loss: 0.7421 (0.7555)  time: 0.1897  data: 0.0001  max mem: 14938
[22:44:09.846986] Test:  [200/345]  eta: 0:00:27  loss: 0.7474 (0.7552)  time: 0.1900  data: 0.0001  max mem: 14938
[22:44:11.753880] Test:  [210/345]  eta: 0:00:25  loss: 0.7539 (0.7561)  time: 0.1904  data: 0.0001  max mem: 14938
[22:44:13.662873] Test:  [220/345]  eta: 0:00:23  loss: 0.7593 (0.7560)  time: 0.1907  data: 0.0001  max mem: 14938
[22:44:15.575786] Test:  [230/345]  eta: 0:00:21  loss: 0.7557 (0.7563)  time: 0.1910  data: 0.0001  max mem: 14938
[22:44:17.495162] Test:  [240/345]  eta: 0:00:19  loss: 0.7563 (0.7565)  time: 0.1916  data: 0.0001  max mem: 14938
[22:44:19.415151] Test:  [250/345]  eta: 0:00:17  loss: 0.7574 (0.7568)  time: 0.1919  data: 0.0001  max mem: 14938
[22:44:21.338676] Test:  [260/345]  eta: 0:00:16  loss: 0.7565 (0.7566)  time: 0.1921  data: 0.0001  max mem: 14938
[22:44:23.266699] Test:  [270/345]  eta: 0:00:14  loss: 0.7565 (0.7568)  time: 0.1925  data: 0.0001  max mem: 14938
[22:44:25.198362] Test:  [280/345]  eta: 0:00:12  loss: 0.7568 (0.7571)  time: 0.1929  data: 0.0001  max mem: 14938
[22:44:27.134158] Test:  [290/345]  eta: 0:00:10  loss: 0.7591 (0.7573)  time: 0.1933  data: 0.0001  max mem: 14938
[22:44:29.073053] Test:  [300/345]  eta: 0:00:08  loss: 0.7566 (0.7571)  time: 0.1937  data: 0.0001  max mem: 14938
[22:44:31.013616] Test:  [310/345]  eta: 0:00:06  loss: 0.7487 (0.7568)  time: 0.1939  data: 0.0001  max mem: 14938
[22:44:32.957743] Test:  [320/345]  eta: 0:00:04  loss: 0.7540 (0.7572)  time: 0.1942  data: 0.0001  max mem: 14938
[22:44:34.906739] Test:  [330/345]  eta: 0:00:02  loss: 0.7605 (0.7570)  time: 0.1946  data: 0.0001  max mem: 14938
[22:44:36.857387] Test:  [340/345]  eta: 0:00:00  loss: 0.7594 (0.7574)  time: 0.1949  data: 0.0001  max mem: 14938
[22:44:37.639777] Test:  [344/345]  eta: 0:00:00  loss: 0.7605 (0.7575)  time: 0.1951  data: 0.0001  max mem: 14938
[22:44:37.699903] Test: Total time: 0:01:05 (0.1901 s / it)
[22:44:48.993717] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8336 (0.8336)  time: 0.3244  data: 0.1452  max mem: 14938
[22:44:50.811464] Test:  [10/57]  eta: 0:00:09  loss: 0.8561 (0.8731)  time: 0.1947  data: 0.0133  max mem: 14938
[22:44:52.632865] Test:  [20/57]  eta: 0:00:06  loss: 0.8657 (0.8686)  time: 0.1819  data: 0.0001  max mem: 14938
[22:44:54.457503] Test:  [30/57]  eta: 0:00:05  loss: 0.7645 (0.8314)  time: 0.1823  data: 0.0001  max mem: 14938
[22:44:56.286685] Test:  [40/57]  eta: 0:00:03  loss: 0.7513 (0.8105)  time: 0.1826  data: 0.0001  max mem: 14938
[22:44:58.122326] Test:  [50/57]  eta: 0:00:01  loss: 0.7420 (0.8025)  time: 0.1832  data: 0.0001  max mem: 14938
[22:44:59.112396] Test:  [56/57]  eta: 0:00:00  loss: 0.7798 (0.8080)  time: 0.1779  data: 0.0001  max mem: 14938
[22:44:59.167075] Test: Total time: 0:00:10 (0.1842 s / it)
[22:45:01.113522] Dice score of the network on the train images: 0.752470, val images: 0.811338
[22:45:01.113759] saving best_rec_model_0 @ epoch 20
[22:45:02.173686] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:45:03.067288] Epoch: [21]  [  0/345]  eta: 0:05:07  lr: 0.000125  loss: 0.7917 (0.7917)  time: 0.8925  data: 0.1480  max mem: 14938
[22:45:17.941088] Epoch: [21]  [ 20/345]  eta: 0:04:03  lr: 0.000125  loss: 0.7640 (0.7691)  time: 0.7436  data: 0.0001  max mem: 14938
[22:45:32.872240] Epoch: [21]  [ 40/345]  eta: 0:03:48  lr: 0.000125  loss: 0.7744 (0.7728)  time: 0.7465  data: 0.0001  max mem: 14938
[22:45:47.823167] Epoch: [21]  [ 60/345]  eta: 0:03:33  lr: 0.000125  loss: 0.7690 (0.7723)  time: 0.7475  data: 0.0001  max mem: 14938
[22:46:02.798911] Epoch: [21]  [ 80/345]  eta: 0:03:18  lr: 0.000124  loss: 0.7701 (0.7724)  time: 0.7487  data: 0.0001  max mem: 14938
[22:46:17.808190] Epoch: [21]  [100/345]  eta: 0:03:03  lr: 0.000124  loss: 0.7604 (0.7705)  time: 0.7504  data: 0.0001  max mem: 14938
[22:46:32.817947] Epoch: [21]  [120/345]  eta: 0:02:48  lr: 0.000124  loss: 0.7582 (0.7688)  time: 0.7504  data: 0.0001  max mem: 14938
[22:46:47.951282] Epoch: [21]  [140/345]  eta: 0:02:33  lr: 0.000124  loss: 0.7709 (0.7690)  time: 0.7566  data: 0.0001  max mem: 14938
[22:47:02.946694] Epoch: [21]  [160/345]  eta: 0:02:18  lr: 0.000124  loss: 0.7887 (0.7715)  time: 0.7497  data: 0.0001  max mem: 14938
[22:47:17.944248] Epoch: [21]  [180/345]  eta: 0:02:03  lr: 0.000124  loss: 0.7882 (0.7733)  time: 0.7498  data: 0.0001  max mem: 14938
[22:47:32.955668] Epoch: [21]  [200/345]  eta: 0:01:48  lr: 0.000124  loss: 0.7695 (0.7733)  time: 0.7505  data: 0.0001  max mem: 14938
[22:47:47.957826] Epoch: [21]  [220/345]  eta: 0:01:33  lr: 0.000124  loss: 0.7637 (0.7725)  time: 0.7501  data: 0.0001  max mem: 14938
[22:48:02.955005] Epoch: [21]  [240/345]  eta: 0:01:18  lr: 0.000124  loss: 0.7639 (0.7723)  time: 0.7498  data: 0.0001  max mem: 14938
[22:48:17.947528] Epoch: [21]  [260/345]  eta: 0:01:03  lr: 0.000124  loss: 0.7668 (0.7720)  time: 0.7496  data: 0.0001  max mem: 14938
[22:48:32.924266] Epoch: [21]  [280/345]  eta: 0:00:48  lr: 0.000124  loss: 0.7632 (0.7715)  time: 0.7488  data: 0.0001  max mem: 14938
[22:48:47.889041] Epoch: [21]  [300/345]  eta: 0:00:33  lr: 0.000124  loss: 0.7605 (0.7710)  time: 0.7482  data: 0.0001  max mem: 14938
[22:49:02.825732] Epoch: [21]  [320/345]  eta: 0:00:18  lr: 0.000124  loss: 0.7658 (0.7710)  time: 0.7468  data: 0.0001  max mem: 14938
[22:49:17.783544] Epoch: [21]  [340/345]  eta: 0:00:03  lr: 0.000124  loss: 0.7726 (0.7712)  time: 0.7478  data: 0.0001  max mem: 14938
[22:49:20.774599] Epoch: [21]  [344/345]  eta: 0:00:00  lr: 0.000124  loss: 0.7792 (0.7715)  time: 0.7478  data: 0.0001  max mem: 14938
[22:49:20.838541] Epoch: [21] Total time: 0:04:18 (0.7498 s / it)
[22:49:20.838837] Averaged stats: lr: 0.000124  loss: 0.7792 (0.7715)
[22:49:21.177715] Test:  [  0/345]  eta: 0:01:55  loss: 0.7490 (0.7490)  time: 0.3348  data: 0.1530  max mem: 14938
[22:49:23.013816] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7679 (0.7673)  time: 0.1973  data: 0.0140  max mem: 14938
[22:49:24.853293] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7704 (0.7700)  time: 0.1837  data: 0.0001  max mem: 14938
[22:49:26.696246] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7718 (0.7722)  time: 0.1841  data: 0.0001  max mem: 14938
[22:49:28.541662] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7734 (0.7737)  time: 0.1844  data: 0.0001  max mem: 14938
[22:49:30.391625] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7734 (0.7744)  time: 0.1847  data: 0.0001  max mem: 14938
[22:49:32.245471] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7704 (0.7733)  time: 0.1851  data: 0.0001  max mem: 14938
[22:49:34.102836] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7751 (0.7740)  time: 0.1855  data: 0.0001  max mem: 14938
[22:49:35.961829] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7766 (0.7738)  time: 0.1858  data: 0.0001  max mem: 14938
[22:49:37.823872] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7758 (0.7737)  time: 0.1860  data: 0.0001  max mem: 14938
[22:49:39.690569] Test:  [100/345]  eta: 0:00:45  loss: 0.7682 (0.7731)  time: 0.1864  data: 0.0001  max mem: 14938
[22:49:41.560541] Test:  [110/345]  eta: 0:00:43  loss: 0.7710 (0.7743)  time: 0.1868  data: 0.0001  max mem: 14938
[22:49:43.434515] Test:  [120/345]  eta: 0:00:41  loss: 0.7853 (0.7747)  time: 0.1871  data: 0.0001  max mem: 14938
[22:49:45.311795] Test:  [130/345]  eta: 0:00:40  loss: 0.7716 (0.7746)  time: 0.1875  data: 0.0001  max mem: 14938
[22:49:47.192936] Test:  [140/345]  eta: 0:00:38  loss: 0.7731 (0.7749)  time: 0.1879  data: 0.0001  max mem: 14938
[22:49:49.077380] Test:  [150/345]  eta: 0:00:36  loss: 0.7785 (0.7749)  time: 0.1882  data: 0.0001  max mem: 14938
[22:49:50.965285] Test:  [160/345]  eta: 0:00:34  loss: 0.7735 (0.7748)  time: 0.1886  data: 0.0001  max mem: 14938
[22:49:52.856396] Test:  [170/345]  eta: 0:00:32  loss: 0.7684 (0.7745)  time: 0.1889  data: 0.0001  max mem: 14938
[22:49:54.750711] Test:  [180/345]  eta: 0:00:30  loss: 0.7668 (0.7741)  time: 0.1892  data: 0.0001  max mem: 14938
[22:49:56.650241] Test:  [190/345]  eta: 0:00:29  loss: 0.7663 (0.7739)  time: 0.1896  data: 0.0001  max mem: 14938
[22:49:58.552486] Test:  [200/345]  eta: 0:00:27  loss: 0.7660 (0.7736)  time: 0.1900  data: 0.0001  max mem: 14938
[22:50:00.457236] Test:  [210/345]  eta: 0:00:25  loss: 0.7660 (0.7735)  time: 0.1903  data: 0.0001  max mem: 14938
[22:50:02.367314] Test:  [220/345]  eta: 0:00:23  loss: 0.7688 (0.7733)  time: 0.1907  data: 0.0001  max mem: 14938
[22:50:04.280039] Test:  [230/345]  eta: 0:00:21  loss: 0.7688 (0.7735)  time: 0.1911  data: 0.0001  max mem: 14938
[22:50:06.195786] Test:  [240/345]  eta: 0:00:19  loss: 0.7747 (0.7736)  time: 0.1914  data: 0.0001  max mem: 14938
[22:50:08.116251] Test:  [250/345]  eta: 0:00:17  loss: 0.7756 (0.7737)  time: 0.1917  data: 0.0001  max mem: 14938
[22:50:10.038190] Test:  [260/345]  eta: 0:00:16  loss: 0.7753 (0.7735)  time: 0.1921  data: 0.0001  max mem: 14938
[22:50:11.964035] Test:  [270/345]  eta: 0:00:14  loss: 0.7698 (0.7740)  time: 0.1923  data: 0.0001  max mem: 14938
[22:50:13.893215] Test:  [280/345]  eta: 0:00:12  loss: 0.7776 (0.7741)  time: 0.1927  data: 0.0001  max mem: 14938
[22:50:15.826068] Test:  [290/345]  eta: 0:00:10  loss: 0.7683 (0.7740)  time: 0.1931  data: 0.0001  max mem: 14938
[22:50:17.761724] Test:  [300/345]  eta: 0:00:08  loss: 0.7645 (0.7738)  time: 0.1934  data: 0.0001  max mem: 14938
[22:50:19.701974] Test:  [310/345]  eta: 0:00:06  loss: 0.7724 (0.7742)  time: 0.1937  data: 0.0001  max mem: 14938
[22:50:21.645630] Test:  [320/345]  eta: 0:00:04  loss: 0.7699 (0.7739)  time: 0.1941  data: 0.0001  max mem: 14938
[22:50:23.592821] Test:  [330/345]  eta: 0:00:02  loss: 0.7638 (0.7737)  time: 0.1945  data: 0.0001  max mem: 14938
[22:50:25.542049] Test:  [340/345]  eta: 0:00:00  loss: 0.7663 (0.7734)  time: 0.1948  data: 0.0001  max mem: 14938
[22:50:26.324957] Test:  [344/345]  eta: 0:00:00  loss: 0.7689 (0.7735)  time: 0.1950  data: 0.0001  max mem: 14938
[22:50:26.385459] Test: Total time: 0:01:05 (0.1900 s / it)
[22:50:37.728017] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8871 (0.8871)  time: 0.3248  data: 0.1457  max mem: 14938
[22:50:39.542354] Test:  [10/57]  eta: 0:00:09  loss: 0.8871 (0.8826)  time: 0.1944  data: 0.0133  max mem: 14938
[22:50:41.363982] Test:  [20/57]  eta: 0:00:06  loss: 0.8666 (0.8698)  time: 0.1817  data: 0.0001  max mem: 14938
[22:50:43.189830] Test:  [30/57]  eta: 0:00:05  loss: 0.7662 (0.8326)  time: 0.1823  data: 0.0001  max mem: 14938
[22:50:45.019531] Test:  [40/57]  eta: 0:00:03  loss: 0.7471 (0.8124)  time: 0.1827  data: 0.0001  max mem: 14938
[22:50:46.853714] Test:  [50/57]  eta: 0:00:01  loss: 0.7408 (0.8035)  time: 0.1831  data: 0.0001  max mem: 14938
[22:50:47.842971] Test:  [56/57]  eta: 0:00:00  loss: 0.7703 (0.8072)  time: 0.1777  data: 0.0001  max mem: 14938
[22:50:47.902569] Test: Total time: 0:00:10 (0.1842 s / it)
[22:50:49.880176] Dice score of the network on the train images: 0.714476, val images: 0.798129
[22:50:49.880419] saving best_rec_model_0 @ epoch 21
[22:50:50.958338] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:50:51.847922] Epoch: [22]  [  0/345]  eta: 0:05:06  lr: 0.000124  loss: 0.7731 (0.7731)  time: 0.8884  data: 0.1474  max mem: 14938
[22:51:06.707894] Epoch: [22]  [ 20/345]  eta: 0:04:03  lr: 0.000124  loss: 0.7929 (0.7887)  time: 0.7430  data: 0.0001  max mem: 14938
[22:51:21.610695] Epoch: [22]  [ 40/345]  eta: 0:03:48  lr: 0.000123  loss: 0.8116 (0.8037)  time: 0.7451  data: 0.0001  max mem: 14938
[22:51:36.574995] Epoch: [22]  [ 60/345]  eta: 0:03:33  lr: 0.000123  loss: 0.7988 (0.8019)  time: 0.7482  data: 0.0001  max mem: 14938
[22:51:51.555449] Epoch: [22]  [ 80/345]  eta: 0:03:18  lr: 0.000123  loss: 0.7878 (0.7997)  time: 0.7490  data: 0.0001  max mem: 14938
[22:52:06.569496] Epoch: [22]  [100/345]  eta: 0:03:03  lr: 0.000123  loss: 0.7794 (0.7975)  time: 0.7507  data: 0.0001  max mem: 14938
[22:52:21.603761] Epoch: [22]  [120/345]  eta: 0:02:48  lr: 0.000123  loss: 0.7619 (0.7925)  time: 0.7517  data: 0.0001  max mem: 14938
[22:52:36.616898] Epoch: [22]  [140/345]  eta: 0:02:33  lr: 0.000123  loss: 0.7748 (0.7900)  time: 0.7506  data: 0.0001  max mem: 14938
[22:52:51.627862] Epoch: [22]  [160/345]  eta: 0:02:18  lr: 0.000123  loss: 0.7710 (0.7875)  time: 0.7505  data: 0.0001  max mem: 14938
[22:53:06.621941] Epoch: [22]  [180/345]  eta: 0:02:03  lr: 0.000123  loss: 0.7662 (0.7856)  time: 0.7497  data: 0.0001  max mem: 14938
[22:53:21.624954] Epoch: [22]  [200/345]  eta: 0:01:48  lr: 0.000123  loss: 0.7636 (0.7844)  time: 0.7501  data: 0.0001  max mem: 14938
[22:53:36.622971] Epoch: [22]  [220/345]  eta: 0:01:33  lr: 0.000123  loss: 0.7676 (0.7826)  time: 0.7499  data: 0.0001  max mem: 14938
[22:53:51.612503] Epoch: [22]  [240/345]  eta: 0:01:18  lr: 0.000123  loss: 0.7712 (0.7814)  time: 0.7494  data: 0.0001  max mem: 14938
[22:54:06.595085] Epoch: [22]  [260/345]  eta: 0:01:03  lr: 0.000122  loss: 0.7626 (0.7801)  time: 0.7491  data: 0.0001  max mem: 14938
[22:54:21.574726] Epoch: [22]  [280/345]  eta: 0:00:48  lr: 0.000122  loss: 0.7552 (0.7786)  time: 0.7489  data: 0.0001  max mem: 14938
[22:54:36.642166] Epoch: [22]  [300/345]  eta: 0:00:33  lr: 0.000122  loss: 0.7657 (0.7777)  time: 0.7533  data: 0.0001  max mem: 14938
[22:54:51.631064] Epoch: [22]  [320/345]  eta: 0:00:18  lr: 0.000122  loss: 0.7619 (0.7769)  time: 0.7494  data: 0.0001  max mem: 14938
[22:55:06.614112] Epoch: [22]  [340/345]  eta: 0:00:03  lr: 0.000122  loss: 0.7552 (0.7759)  time: 0.7491  data: 0.0001  max mem: 14938
[22:55:09.608764] Epoch: [22]  [344/345]  eta: 0:00:00  lr: 0.000122  loss: 0.7538 (0.7756)  time: 0.7487  data: 0.0001  max mem: 14938
[22:55:09.673723] Epoch: [22] Total time: 0:04:18 (0.7499 s / it)
[22:55:09.674188] Averaged stats: lr: 0.000122  loss: 0.7538 (0.7756)
[22:55:10.013101] Test:  [  0/345]  eta: 0:01:55  loss: 0.7266 (0.7266)  time: 0.3347  data: 0.1529  max mem: 14938
[22:55:11.849950] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7270 (0.7326)  time: 0.1973  data: 0.0140  max mem: 14938
[22:55:13.687614] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7261 (0.7279)  time: 0.1837  data: 0.0001  max mem: 14938
[22:55:15.529807] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7292 (0.7298)  time: 0.1839  data: 0.0001  max mem: 14938
[22:55:17.374984] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7350 (0.7309)  time: 0.1843  data: 0.0001  max mem: 14938
[22:55:19.223601] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7258 (0.7301)  time: 0.1846  data: 0.0001  max mem: 14938
[22:55:21.077841] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7249 (0.7296)  time: 0.1851  data: 0.0001  max mem: 14938
[22:55:22.934450] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7279 (0.7297)  time: 0.1855  data: 0.0001  max mem: 14938
[22:55:24.794094] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7285 (0.7294)  time: 0.1857  data: 0.0001  max mem: 14938
[22:55:26.656474] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7194 (0.7288)  time: 0.1860  data: 0.0001  max mem: 14938
[22:55:28.523067] Test:  [100/345]  eta: 0:00:45  loss: 0.7265 (0.7291)  time: 0.1864  data: 0.0001  max mem: 14938
[22:55:30.391575] Test:  [110/345]  eta: 0:00:43  loss: 0.7300 (0.7295)  time: 0.1867  data: 0.0001  max mem: 14938
[22:55:32.265782] Test:  [120/345]  eta: 0:00:41  loss: 0.7300 (0.7296)  time: 0.1871  data: 0.0001  max mem: 14938
[22:55:34.142777] Test:  [130/345]  eta: 0:00:40  loss: 0.7345 (0.7302)  time: 0.1875  data: 0.0001  max mem: 14938
[22:55:36.024614] Test:  [140/345]  eta: 0:00:38  loss: 0.7399 (0.7307)  time: 0.1879  data: 0.0001  max mem: 14938
[22:55:37.908660] Test:  [150/345]  eta: 0:00:36  loss: 0.7254 (0.7303)  time: 0.1882  data: 0.0001  max mem: 14938
[22:55:39.796250] Test:  [160/345]  eta: 0:00:34  loss: 0.7274 (0.7307)  time: 0.1885  data: 0.0001  max mem: 14938
[22:55:41.687437] Test:  [170/345]  eta: 0:00:32  loss: 0.7274 (0.7304)  time: 0.1889  data: 0.0001  max mem: 14938
[22:55:43.583055] Test:  [180/345]  eta: 0:00:30  loss: 0.7290 (0.7306)  time: 0.1893  data: 0.0001  max mem: 14938
[22:55:45.481245] Test:  [190/345]  eta: 0:00:29  loss: 0.7327 (0.7307)  time: 0.1896  data: 0.0001  max mem: 14938
[22:55:47.384162] Test:  [200/345]  eta: 0:00:27  loss: 0.7316 (0.7307)  time: 0.1900  data: 0.0001  max mem: 14938
[22:55:49.289017] Test:  [210/345]  eta: 0:00:25  loss: 0.7307 (0.7307)  time: 0.1903  data: 0.0001  max mem: 14938
[22:55:51.198569] Test:  [220/345]  eta: 0:00:23  loss: 0.7307 (0.7310)  time: 0.1907  data: 0.0001  max mem: 14938
[22:55:53.109814] Test:  [230/345]  eta: 0:00:21  loss: 0.7312 (0.7308)  time: 0.1910  data: 0.0001  max mem: 14938
[22:55:55.025758] Test:  [240/345]  eta: 0:00:19  loss: 0.7285 (0.7308)  time: 0.1913  data: 0.0001  max mem: 14938
[22:55:56.946217] Test:  [250/345]  eta: 0:00:17  loss: 0.7343 (0.7312)  time: 0.1918  data: 0.0001  max mem: 14938
[22:55:58.869767] Test:  [260/345]  eta: 0:00:16  loss: 0.7323 (0.7311)  time: 0.1921  data: 0.0001  max mem: 14938
[22:56:00.797541] Test:  [270/345]  eta: 0:00:14  loss: 0.7327 (0.7314)  time: 0.1925  data: 0.0001  max mem: 14938
[22:56:02.730368] Test:  [280/345]  eta: 0:00:12  loss: 0.7349 (0.7315)  time: 0.1930  data: 0.0001  max mem: 14938
[22:56:04.664229] Test:  [290/345]  eta: 0:00:10  loss: 0.7321 (0.7316)  time: 0.1933  data: 0.0001  max mem: 14938
[22:56:06.602618] Test:  [300/345]  eta: 0:00:08  loss: 0.7319 (0.7315)  time: 0.1936  data: 0.0001  max mem: 14938
[22:56:08.543607] Test:  [310/345]  eta: 0:00:06  loss: 0.7302 (0.7317)  time: 0.1939  data: 0.0001  max mem: 14938
[22:56:10.490314] Test:  [320/345]  eta: 0:00:04  loss: 0.7299 (0.7318)  time: 0.1943  data: 0.0001  max mem: 14938
[22:56:12.436963] Test:  [330/345]  eta: 0:00:02  loss: 0.7371 (0.7322)  time: 0.1946  data: 0.0001  max mem: 14938
[22:56:14.386824] Test:  [340/345]  eta: 0:00:00  loss: 0.7412 (0.7324)  time: 0.1948  data: 0.0001  max mem: 14938
[22:56:15.168619] Test:  [344/345]  eta: 0:00:00  loss: 0.7421 (0.7325)  time: 0.1950  data: 0.0001  max mem: 14938
[22:56:15.228840] Test: Total time: 0:01:05 (0.1900 s / it)
[22:56:26.609041] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8532 (0.8532)  time: 0.3230  data: 0.1440  max mem: 14938
[22:56:28.424238] Test:  [10/57]  eta: 0:00:09  loss: 0.8640 (0.8725)  time: 0.1943  data: 0.0131  max mem: 14938
[22:56:30.243796] Test:  [20/57]  eta: 0:00:06  loss: 0.8640 (0.8584)  time: 0.1817  data: 0.0001  max mem: 14938
[22:56:32.067889] Test:  [30/57]  eta: 0:00:05  loss: 0.7653 (0.8234)  time: 0.1821  data: 0.0001  max mem: 14938
[22:56:33.896362] Test:  [40/57]  eta: 0:00:03  loss: 0.7416 (0.8049)  time: 0.1826  data: 0.0001  max mem: 14938
[22:56:35.729854] Test:  [50/57]  eta: 0:00:01  loss: 0.7447 (0.7981)  time: 0.1830  data: 0.0001  max mem: 14938
[22:56:36.719166] Test:  [56/57]  eta: 0:00:00  loss: 0.7649 (0.8028)  time: 0.1777  data: 0.0001  max mem: 14938
[22:56:36.775697] Test: Total time: 0:00:10 (0.1840 s / it)
[22:56:38.696595] Dice score of the network on the train images: 0.791649, val images: 0.814023
[22:56:38.700795] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[22:56:39.585739] Epoch: [23]  [  0/345]  eta: 0:05:05  lr: 0.000122  loss: 0.7446 (0.7446)  time: 0.8842  data: 0.1443  max mem: 14938
[22:56:54.471497] Epoch: [23]  [ 20/345]  eta: 0:04:04  lr: 0.000122  loss: 0.7505 (0.7521)  time: 0.7442  data: 0.0001  max mem: 14938
[22:57:09.393467] Epoch: [23]  [ 40/345]  eta: 0:03:48  lr: 0.000122  loss: 0.7673 (0.7610)  time: 0.7460  data: 0.0001  max mem: 14938
[22:57:24.358894] Epoch: [23]  [ 60/345]  eta: 0:03:33  lr: 0.000122  loss: 0.7689 (0.7632)  time: 0.7482  data: 0.0001  max mem: 14938
[22:57:39.356802] Epoch: [23]  [ 80/345]  eta: 0:03:18  lr: 0.000121  loss: 0.7730 (0.7669)  time: 0.7498  data: 0.0001  max mem: 14938
[22:57:54.389499] Epoch: [23]  [100/345]  eta: 0:03:03  lr: 0.000121  loss: 0.7637 (0.7664)  time: 0.7516  data: 0.0001  max mem: 14938
[22:58:09.414016] Epoch: [23]  [120/345]  eta: 0:02:48  lr: 0.000121  loss: 0.7658 (0.7676)  time: 0.7512  data: 0.0001  max mem: 14938
[22:58:24.416758] Epoch: [23]  [140/345]  eta: 0:02:33  lr: 0.000121  loss: 0.7824 (0.7693)  time: 0.7501  data: 0.0001  max mem: 14938
[22:58:39.419754] Epoch: [23]  [160/345]  eta: 0:02:18  lr: 0.000121  loss: 0.7715 (0.7702)  time: 0.7501  data: 0.0001  max mem: 14938
[22:58:54.396984] Epoch: [23]  [180/345]  eta: 0:02:03  lr: 0.000121  loss: 0.7592 (0.7692)  time: 0.7488  data: 0.0001  max mem: 14938
[22:59:09.370158] Epoch: [23]  [200/345]  eta: 0:01:48  lr: 0.000121  loss: 0.7710 (0.7692)  time: 0.7486  data: 0.0001  max mem: 14938
[22:59:24.327589] Epoch: [23]  [220/345]  eta: 0:01:33  lr: 0.000121  loss: 0.7621 (0.7690)  time: 0.7478  data: 0.0001  max mem: 14938
[22:59:39.284358] Epoch: [23]  [240/345]  eta: 0:01:18  lr: 0.000120  loss: 0.7681 (0.7687)  time: 0.7478  data: 0.0001  max mem: 14938
[22:59:54.246896] Epoch: [23]  [260/345]  eta: 0:01:03  lr: 0.000120  loss: 0.7622 (0.7683)  time: 0.7481  data: 0.0001  max mem: 14938
[23:00:09.193795] Epoch: [23]  [280/345]  eta: 0:00:48  lr: 0.000120  loss: 0.7593 (0.7679)  time: 0.7473  data: 0.0001  max mem: 14938
[23:00:24.139912] Epoch: [23]  [300/345]  eta: 0:00:33  lr: 0.000120  loss: 0.7507 (0.7670)  time: 0.7473  data: 0.0001  max mem: 14938
[23:00:39.089987] Epoch: [23]  [320/345]  eta: 0:00:18  lr: 0.000120  loss: 0.7632 (0.7668)  time: 0.7475  data: 0.0001  max mem: 14938
[23:00:54.038259] Epoch: [23]  [340/345]  eta: 0:00:03  lr: 0.000120  loss: 0.7570 (0.7663)  time: 0.7474  data: 0.0001  max mem: 14938
[23:00:57.028117] Epoch: [23]  [344/345]  eta: 0:00:00  lr: 0.000120  loss: 0.7559 (0.7660)  time: 0.7473  data: 0.0001  max mem: 14938
[23:00:57.093980] Epoch: [23] Total time: 0:04:18 (0.7490 s / it)
[23:00:57.094471] Averaged stats: lr: 0.000120  loss: 0.7559 (0.7660)
[23:00:57.428793] Test:  [  0/345]  eta: 0:01:54  loss: 0.7459 (0.7459)  time: 0.3307  data: 0.1493  max mem: 14938
[23:00:59.265751] Test:  [ 10/345]  eta: 0:01:05  loss: 0.7399 (0.7359)  time: 0.1970  data: 0.0136  max mem: 14938
[23:01:01.105747] Test:  [ 20/345]  eta: 0:01:01  loss: 0.7362 (0.7340)  time: 0.1838  data: 0.0001  max mem: 14938
[23:01:02.948885] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7238 (0.7298)  time: 0.1841  data: 0.0001  max mem: 14938
[23:01:04.794903] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7238 (0.7307)  time: 0.1844  data: 0.0001  max mem: 14938
[23:01:06.645767] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7365 (0.7317)  time: 0.1848  data: 0.0001  max mem: 14938
[23:01:08.501872] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7347 (0.7317)  time: 0.1853  data: 0.0001  max mem: 14938
[23:01:10.359262] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7281 (0.7304)  time: 0.1856  data: 0.0001  max mem: 14938
[23:01:12.217882] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7264 (0.7304)  time: 0.1858  data: 0.0001  max mem: 14938
[23:01:14.083455] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7325 (0.7312)  time: 0.1862  data: 0.0001  max mem: 14938
[23:01:15.952350] Test:  [100/345]  eta: 0:00:45  loss: 0.7369 (0.7319)  time: 0.1867  data: 0.0001  max mem: 14938
[23:01:17.820469] Test:  [110/345]  eta: 0:00:43  loss: 0.7322 (0.7320)  time: 0.1868  data: 0.0001  max mem: 14938
[23:01:19.695103] Test:  [120/345]  eta: 0:00:42  loss: 0.7295 (0.7317)  time: 0.1871  data: 0.0001  max mem: 14938
[23:01:21.571802] Test:  [130/345]  eta: 0:00:40  loss: 0.7287 (0.7316)  time: 0.1875  data: 0.0001  max mem: 14938
[23:01:23.452643] Test:  [140/345]  eta: 0:00:38  loss: 0.7287 (0.7312)  time: 0.1878  data: 0.0001  max mem: 14938
[23:01:25.337260] Test:  [150/345]  eta: 0:00:36  loss: 0.7237 (0.7307)  time: 0.1882  data: 0.0001  max mem: 14938
[23:01:27.224619] Test:  [160/345]  eta: 0:00:34  loss: 0.7282 (0.7308)  time: 0.1886  data: 0.0001  max mem: 14938
[23:01:29.115364] Test:  [170/345]  eta: 0:00:32  loss: 0.7327 (0.7310)  time: 0.1889  data: 0.0001  max mem: 14938
[23:01:31.013086] Test:  [180/345]  eta: 0:00:30  loss: 0.7307 (0.7310)  time: 0.1894  data: 0.0001  max mem: 14938
[23:01:32.912179] Test:  [190/345]  eta: 0:00:29  loss: 0.7269 (0.7310)  time: 0.1898  data: 0.0001  max mem: 14938
[23:01:34.813724] Test:  [200/345]  eta: 0:00:27  loss: 0.7309 (0.7311)  time: 0.1900  data: 0.0001  max mem: 14938
[23:01:36.718119] Test:  [210/345]  eta: 0:00:25  loss: 0.7323 (0.7311)  time: 0.1902  data: 0.0001  max mem: 14938
[23:01:38.626359] Test:  [220/345]  eta: 0:00:23  loss: 0.7328 (0.7311)  time: 0.1906  data: 0.0001  max mem: 14938
[23:01:40.537799] Test:  [230/345]  eta: 0:00:21  loss: 0.7311 (0.7312)  time: 0.1909  data: 0.0001  max mem: 14938
[23:01:42.455031] Test:  [240/345]  eta: 0:00:19  loss: 0.7278 (0.7309)  time: 0.1914  data: 0.0001  max mem: 14938
[23:01:44.374808] Test:  [250/345]  eta: 0:00:17  loss: 0.7291 (0.7309)  time: 0.1918  data: 0.0001  max mem: 14938
[23:01:46.298923] Test:  [260/345]  eta: 0:00:16  loss: 0.7321 (0.7313)  time: 0.1921  data: 0.0001  max mem: 14938
[23:01:48.225203] Test:  [270/345]  eta: 0:00:14  loss: 0.7359 (0.7313)  time: 0.1925  data: 0.0001  max mem: 14938
[23:01:50.156024] Test:  [280/345]  eta: 0:00:12  loss: 0.7228 (0.7309)  time: 0.1928  data: 0.0001  max mem: 14938
[23:01:52.089725] Test:  [290/345]  eta: 0:00:10  loss: 0.7195 (0.7307)  time: 0.1932  data: 0.0001  max mem: 14938
[23:01:54.027188] Test:  [300/345]  eta: 0:00:08  loss: 0.7208 (0.7308)  time: 0.1935  data: 0.0001  max mem: 14938
[23:01:55.967739] Test:  [310/345]  eta: 0:00:06  loss: 0.7208 (0.7307)  time: 0.1939  data: 0.0001  max mem: 14938
[23:01:57.910455] Test:  [320/345]  eta: 0:00:04  loss: 0.7204 (0.7304)  time: 0.1941  data: 0.0001  max mem: 14938
[23:01:59.858112] Test:  [330/345]  eta: 0:00:02  loss: 0.7235 (0.7302)  time: 0.1945  data: 0.0001  max mem: 14938
[23:02:01.808440] Test:  [340/345]  eta: 0:00:00  loss: 0.7276 (0.7303)  time: 0.1948  data: 0.0001  max mem: 14938
[23:02:02.590375] Test:  [344/345]  eta: 0:00:00  loss: 0.7300 (0.7304)  time: 0.1949  data: 0.0001  max mem: 14938
[23:02:02.630459] Test: Total time: 0:01:05 (0.1900 s / it)
[23:02:13.902491] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8387 (0.8387)  time: 0.3255  data: 0.1461  max mem: 14938
[23:02:15.716387] Test:  [10/57]  eta: 0:00:09  loss: 0.8758 (0.8762)  time: 0.1944  data: 0.0133  max mem: 14938
[23:02:17.537189] Test:  [20/57]  eta: 0:00:06  loss: 0.8765 (0.8643)  time: 0.1817  data: 0.0001  max mem: 14938
[23:02:19.363677] Test:  [30/57]  eta: 0:00:05  loss: 0.7602 (0.8266)  time: 0.1823  data: 0.0001  max mem: 14938
[23:02:21.193141] Test:  [40/57]  eta: 0:00:03  loss: 0.7524 (0.8068)  time: 0.1827  data: 0.0001  max mem: 14938
[23:02:23.030764] Test:  [50/57]  eta: 0:00:01  loss: 0.7541 (0.8024)  time: 0.1833  data: 0.0001  max mem: 14938
[23:02:24.019669] Test:  [56/57]  eta: 0:00:00  loss: 0.7711 (0.8075)  time: 0.1778  data: 0.0001  max mem: 14938
[23:02:24.076000] Test: Total time: 0:00:10 (0.1842 s / it)
[23:02:26.038319] Dice score of the network on the train images: 0.788943, val images: 0.806045
[23:02:26.043418] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:02:26.936965] Epoch: [24]  [  0/345]  eta: 0:05:07  lr: 0.000120  loss: 0.7401 (0.7401)  time: 0.8925  data: 0.1512  max mem: 14938
[23:02:41.789652] Epoch: [24]  [ 20/345]  eta: 0:04:03  lr: 0.000119  loss: 0.7642 (0.7622)  time: 0.7426  data: 0.0001  max mem: 14938
[23:02:56.686885] Epoch: [24]  [ 40/345]  eta: 0:03:47  lr: 0.000119  loss: 0.7564 (0.7606)  time: 0.7448  data: 0.0001  max mem: 14938
[23:03:11.629064] Epoch: [24]  [ 60/345]  eta: 0:03:32  lr: 0.000119  loss: 0.7692 (0.7638)  time: 0.7471  data: 0.0001  max mem: 14938
[23:03:26.620813] Epoch: [24]  [ 80/345]  eta: 0:03:18  lr: 0.000119  loss: 0.7772 (0.7658)  time: 0.7495  data: 0.0001  max mem: 14938
[23:03:41.638358] Epoch: [24]  [100/345]  eta: 0:03:03  lr: 0.000119  loss: 0.7685 (0.7670)  time: 0.7508  data: 0.0001  max mem: 14938
[23:03:56.670597] Epoch: [24]  [120/345]  eta: 0:02:48  lr: 0.000119  loss: 0.7593 (0.7659)  time: 0.7516  data: 0.0001  max mem: 14938
[23:04:11.689813] Epoch: [24]  [140/345]  eta: 0:02:33  lr: 0.000118  loss: 0.7617 (0.7660)  time: 0.7509  data: 0.0001  max mem: 14938
[23:04:26.674612] Epoch: [24]  [160/345]  eta: 0:02:18  lr: 0.000118  loss: 0.7557 (0.7650)  time: 0.7492  data: 0.0001  max mem: 14938
[23:04:41.664345] Epoch: [24]  [180/345]  eta: 0:02:03  lr: 0.000118  loss: 0.7629 (0.7650)  time: 0.7494  data: 0.0001  max mem: 14938
[23:04:56.646706] Epoch: [24]  [200/345]  eta: 0:01:48  lr: 0.000118  loss: 0.7576 (0.7644)  time: 0.7491  data: 0.0001  max mem: 14938
[23:05:11.610537] Epoch: [24]  [220/345]  eta: 0:01:33  lr: 0.000118  loss: 0.7743 (0.7654)  time: 0.7481  data: 0.0001  max mem: 14938
[23:05:26.573129] Epoch: [24]  [240/345]  eta: 0:01:18  lr: 0.000118  loss: 0.7781 (0.7664)  time: 0.7481  data: 0.0001  max mem: 14938
[23:05:41.532511] Epoch: [24]  [260/345]  eta: 0:01:03  lr: 0.000117  loss: 0.7705 (0.7668)  time: 0.7479  data: 0.0001  max mem: 14938
[23:05:56.488728] Epoch: [24]  [280/345]  eta: 0:00:48  lr: 0.000117  loss: 0.7597 (0.7664)  time: 0.7478  data: 0.0001  max mem: 14938
[23:06:11.444479] Epoch: [24]  [300/345]  eta: 0:00:33  lr: 0.000117  loss: 0.7595 (0.7660)  time: 0.7477  data: 0.0001  max mem: 14938
[23:06:26.394557] Epoch: [24]  [320/345]  eta: 0:00:18  lr: 0.000117  loss: 0.7628 (0.7661)  time: 0.7475  data: 0.0001  max mem: 14938
[23:06:41.340418] Epoch: [24]  [340/345]  eta: 0:00:03  lr: 0.000117  loss: 0.7652 (0.7665)  time: 0.7472  data: 0.0001  max mem: 14938
[23:06:44.332161] Epoch: [24]  [344/345]  eta: 0:00:00  lr: 0.000117  loss: 0.7615 (0.7664)  time: 0.7475  data: 0.0001  max mem: 14938
[23:06:44.394364] Epoch: [24] Total time: 0:04:18 (0.7488 s / it)
[23:06:44.394804] Averaged stats: lr: 0.000117  loss: 0.7615 (0.7664)
[23:06:44.737761] Test:  [  0/345]  eta: 0:01:57  loss: 0.7398 (0.7398)  time: 0.3394  data: 0.1582  max mem: 14938
[23:06:46.574138] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7478 (0.7430)  time: 0.1977  data: 0.0144  max mem: 14938
[23:06:48.412655] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7478 (0.7439)  time: 0.1837  data: 0.0001  max mem: 14938
[23:06:50.256991] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7356 (0.7414)  time: 0.1841  data: 0.0001  max mem: 14938
[23:06:52.103655] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7406 (0.7430)  time: 0.1845  data: 0.0001  max mem: 14938
[23:06:53.954821] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7391 (0.7414)  time: 0.1848  data: 0.0001  max mem: 14938
[23:06:55.808492] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7341 (0.7397)  time: 0.1852  data: 0.0001  max mem: 14938
[23:06:57.663206] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7280 (0.7396)  time: 0.1854  data: 0.0001  max mem: 14938
[23:06:59.522663] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7342 (0.7389)  time: 0.1857  data: 0.0001  max mem: 14938
[23:07:01.387398] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7374 (0.7399)  time: 0.1862  data: 0.0001  max mem: 14938
[23:07:03.253563] Test:  [100/345]  eta: 0:00:45  loss: 0.7439 (0.7396)  time: 0.1865  data: 0.0001  max mem: 14938
[23:07:05.123004] Test:  [110/345]  eta: 0:00:43  loss: 0.7439 (0.7402)  time: 0.1867  data: 0.0001  max mem: 14938
[23:07:06.996426] Test:  [120/345]  eta: 0:00:42  loss: 0.7403 (0.7406)  time: 0.1871  data: 0.0001  max mem: 14938
[23:07:08.875297] Test:  [130/345]  eta: 0:00:40  loss: 0.7369 (0.7402)  time: 0.1876  data: 0.0001  max mem: 14938
[23:07:10.757226] Test:  [140/345]  eta: 0:00:38  loss: 0.7280 (0.7396)  time: 0.1880  data: 0.0001  max mem: 14938
[23:07:12.640744] Test:  [150/345]  eta: 0:00:36  loss: 0.7356 (0.7395)  time: 0.1882  data: 0.0001  max mem: 14938
[23:07:14.529169] Test:  [160/345]  eta: 0:00:34  loss: 0.7423 (0.7399)  time: 0.1885  data: 0.0001  max mem: 14938
[23:07:16.418891] Test:  [170/345]  eta: 0:00:32  loss: 0.7368 (0.7396)  time: 0.1889  data: 0.0001  max mem: 14938
[23:07:18.314161] Test:  [180/345]  eta: 0:00:30  loss: 0.7302 (0.7397)  time: 0.1892  data: 0.0001  max mem: 14938
[23:07:20.213125] Test:  [190/345]  eta: 0:00:29  loss: 0.7358 (0.7397)  time: 0.1897  data: 0.0001  max mem: 14938
[23:07:22.114692] Test:  [200/345]  eta: 0:00:27  loss: 0.7338 (0.7395)  time: 0.1900  data: 0.0001  max mem: 14938
[23:07:24.018735] Test:  [210/345]  eta: 0:00:25  loss: 0.7399 (0.7396)  time: 0.1902  data: 0.0001  max mem: 14938
[23:07:25.927705] Test:  [220/345]  eta: 0:00:23  loss: 0.7373 (0.7395)  time: 0.1906  data: 0.0001  max mem: 14938
[23:07:27.840436] Test:  [230/345]  eta: 0:00:21  loss: 0.7351 (0.7391)  time: 0.1910  data: 0.0001  max mem: 14938
[23:07:29.756511] Test:  [240/345]  eta: 0:00:19  loss: 0.7351 (0.7391)  time: 0.1914  data: 0.0001  max mem: 14938
[23:07:31.676628] Test:  [250/345]  eta: 0:00:17  loss: 0.7280 (0.7389)  time: 0.1918  data: 0.0001  max mem: 14938
[23:07:33.598488] Test:  [260/345]  eta: 0:00:16  loss: 0.7281 (0.7389)  time: 0.1920  data: 0.0001  max mem: 14938
[23:07:35.525806] Test:  [270/345]  eta: 0:00:14  loss: 0.7457 (0.7393)  time: 0.1924  data: 0.0001  max mem: 14938
[23:07:37.457820] Test:  [280/345]  eta: 0:00:12  loss: 0.7457 (0.7394)  time: 0.1929  data: 0.0001  max mem: 14938
[23:07:39.392436] Test:  [290/345]  eta: 0:00:10  loss: 0.7385 (0.7394)  time: 0.1933  data: 0.0001  max mem: 14938
[23:07:41.329095] Test:  [300/345]  eta: 0:00:08  loss: 0.7335 (0.7391)  time: 0.1935  data: 0.0001  max mem: 14938
[23:07:43.270558] Test:  [310/345]  eta: 0:00:06  loss: 0.7259 (0.7390)  time: 0.1939  data: 0.0001  max mem: 14938
[23:07:45.214105] Test:  [320/345]  eta: 0:00:04  loss: 0.7347 (0.7388)  time: 0.1942  data: 0.0001  max mem: 14938
[23:07:47.160045] Test:  [330/345]  eta: 0:00:02  loss: 0.7391 (0.7393)  time: 0.1944  data: 0.0001  max mem: 14938
[23:07:49.109497] Test:  [340/345]  eta: 0:00:00  loss: 0.7477 (0.7395)  time: 0.1947  data: 0.0001  max mem: 14938
[23:07:49.891342] Test:  [344/345]  eta: 0:00:00  loss: 0.7440 (0.7395)  time: 0.1949  data: 0.0001  max mem: 14938
[23:07:49.949717] Test: Total time: 0:01:05 (0.1900 s / it)
[23:08:01.335868] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8363 (0.8363)  time: 0.3276  data: 0.1482  max mem: 14938
[23:08:03.152812] Test:  [10/57]  eta: 0:00:09  loss: 0.8763 (0.8759)  time: 0.1949  data: 0.0135  max mem: 14938
[23:08:04.973105] Test:  [20/57]  eta: 0:00:06  loss: 0.8763 (0.8639)  time: 0.1818  data: 0.0001  max mem: 14938
[23:08:06.798228] Test:  [30/57]  eta: 0:00:05  loss: 0.7533 (0.8264)  time: 0.1822  data: 0.0001  max mem: 14938
[23:08:08.625703] Test:  [40/57]  eta: 0:00:03  loss: 0.7479 (0.8072)  time: 0.1826  data: 0.0001  max mem: 14938
[23:08:10.461429] Test:  [50/57]  eta: 0:00:01  loss: 0.7507 (0.8023)  time: 0.1831  data: 0.0001  max mem: 14938
[23:08:11.451280] Test:  [56/57]  eta: 0:00:00  loss: 0.7689 (0.8085)  time: 0.1778  data: 0.0001  max mem: 14938
[23:08:11.509343] Test: Total time: 0:00:10 (0.1842 s / it)
[23:08:13.470245] Dice score of the network on the train images: 0.785629, val images: 0.809340
[23:08:13.474428] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:08:14.364525] Epoch: [25]  [  0/345]  eta: 0:05:06  lr: 0.000117  loss: 0.7452 (0.7452)  time: 0.8891  data: 0.1480  max mem: 14938
[23:08:29.242454] Epoch: [25]  [ 20/345]  eta: 0:04:03  lr: 0.000116  loss: 0.7670 (0.7649)  time: 0.7438  data: 0.0001  max mem: 14938
[23:08:44.149287] Epoch: [25]  [ 40/345]  eta: 0:03:48  lr: 0.000116  loss: 0.7544 (0.7592)  time: 0.7453  data: 0.0001  max mem: 14938
[23:08:59.107922] Epoch: [25]  [ 60/345]  eta: 0:03:33  lr: 0.000116  loss: 0.7591 (0.7594)  time: 0.7479  data: 0.0001  max mem: 14938
[23:09:14.091022] Epoch: [25]  [ 80/345]  eta: 0:03:18  lr: 0.000116  loss: 0.7564 (0.7590)  time: 0.7491  data: 0.0001  max mem: 14938
[23:09:29.092600] Epoch: [25]  [100/345]  eta: 0:03:03  lr: 0.000116  loss: 0.7487 (0.7581)  time: 0.7500  data: 0.0001  max mem: 14938
[23:09:44.092839] Epoch: [25]  [120/345]  eta: 0:02:48  lr: 0.000115  loss: 0.7520 (0.7574)  time: 0.7500  data: 0.0001  max mem: 14938
[23:09:59.093267] Epoch: [25]  [140/345]  eta: 0:02:33  lr: 0.000115  loss: 0.7473 (0.7560)  time: 0.7500  data: 0.0001  max mem: 14938
[23:10:14.079868] Epoch: [25]  [160/345]  eta: 0:02:18  lr: 0.000115  loss: 0.7434 (0.7550)  time: 0.7493  data: 0.0001  max mem: 14938
[23:10:29.053251] Epoch: [25]  [180/345]  eta: 0:02:03  lr: 0.000115  loss: 0.7690 (0.7560)  time: 0.7486  data: 0.0001  max mem: 14938
[23:10:44.035682] Epoch: [25]  [200/345]  eta: 0:01:48  lr: 0.000115  loss: 0.7428 (0.7547)  time: 0.7491  data: 0.0001  max mem: 14938
[23:10:59.002217] Epoch: [25]  [220/345]  eta: 0:01:33  lr: 0.000114  loss: 0.7447 (0.7539)  time: 0.7483  data: 0.0001  max mem: 14938
[23:11:13.959788] Epoch: [25]  [240/345]  eta: 0:01:18  lr: 0.000114  loss: 0.7434 (0.7534)  time: 0.7478  data: 0.0001  max mem: 14938
[23:11:28.919638] Epoch: [25]  [260/345]  eta: 0:01:03  lr: 0.000114  loss: 0.7499 (0.7534)  time: 0.7479  data: 0.0001  max mem: 14938
[23:11:43.967030] Epoch: [25]  [280/345]  eta: 0:00:48  lr: 0.000114  loss: 0.7523 (0.7533)  time: 0.7523  data: 0.0001  max mem: 14938
[23:11:58.948831] Epoch: [25]  [300/345]  eta: 0:00:33  lr: 0.000114  loss: 0.7431 (0.7527)  time: 0.7490  data: 0.0001  max mem: 14938
[23:12:13.932023] Epoch: [25]  [320/345]  eta: 0:00:18  lr: 0.000113  loss: 0.7522 (0.7531)  time: 0.7491  data: 0.0001  max mem: 14938
[23:12:28.909154] Epoch: [25]  [340/345]  eta: 0:00:03  lr: 0.000113  loss: 0.7488 (0.7531)  time: 0.7488  data: 0.0001  max mem: 14938
[23:12:31.910247] Epoch: [25]  [344/345]  eta: 0:00:00  lr: 0.000113  loss: 0.7529 (0.7532)  time: 0.7491  data: 0.0001  max mem: 14938
[23:12:31.974860] Epoch: [25] Total time: 0:04:18 (0.7493 s / it)
[23:12:31.975129] Averaged stats: lr: 0.000113  loss: 0.7529 (0.7532)
[23:12:32.313361] Test:  [  0/345]  eta: 0:01:54  loss: 0.7013 (0.7013)  time: 0.3321  data: 0.1497  max mem: 14938
[23:12:34.150528] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7261 (0.7264)  time: 0.1971  data: 0.0137  max mem: 14938
[23:12:35.992492] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7261 (0.7253)  time: 0.1839  data: 0.0001  max mem: 14938
[23:12:37.836579] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7226 (0.7237)  time: 0.1842  data: 0.0001  max mem: 14938
[23:12:39.683830] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7246 (0.7243)  time: 0.1845  data: 0.0001  max mem: 14938
[23:12:41.535072] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7230 (0.7237)  time: 0.1849  data: 0.0001  max mem: 14938
[23:12:43.390468] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7198 (0.7246)  time: 0.1853  data: 0.0001  max mem: 14938
[23:12:45.248073] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7277 (0.7251)  time: 0.1856  data: 0.0001  max mem: 14938
[23:12:47.107881] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7234 (0.7246)  time: 0.1858  data: 0.0001  max mem: 14938
[23:12:48.972120] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7193 (0.7248)  time: 0.1862  data: 0.0001  max mem: 14938
[23:12:50.839319] Test:  [100/345]  eta: 0:00:45  loss: 0.7192 (0.7243)  time: 0.1865  data: 0.0001  max mem: 14938
[23:12:52.711233] Test:  [110/345]  eta: 0:00:43  loss: 0.7267 (0.7253)  time: 0.1869  data: 0.0001  max mem: 14938
[23:12:54.587033] Test:  [120/345]  eta: 0:00:42  loss: 0.7317 (0.7255)  time: 0.1873  data: 0.0001  max mem: 14938
[23:12:56.467073] Test:  [130/345]  eta: 0:00:40  loss: 0.7216 (0.7251)  time: 0.1877  data: 0.0001  max mem: 14938
[23:12:58.349322] Test:  [140/345]  eta: 0:00:38  loss: 0.7216 (0.7255)  time: 0.1881  data: 0.0001  max mem: 14938
[23:13:00.235622] Test:  [150/345]  eta: 0:00:36  loss: 0.7293 (0.7258)  time: 0.1884  data: 0.0001  max mem: 14938
[23:13:02.125286] Test:  [160/345]  eta: 0:00:34  loss: 0.7293 (0.7259)  time: 0.1887  data: 0.0001  max mem: 14938
[23:13:04.016222] Test:  [170/345]  eta: 0:00:32  loss: 0.7206 (0.7257)  time: 0.1890  data: 0.0001  max mem: 14938
[23:13:05.912567] Test:  [180/345]  eta: 0:00:30  loss: 0.7213 (0.7257)  time: 0.1893  data: 0.0001  max mem: 14938
[23:13:07.811923] Test:  [190/345]  eta: 0:00:29  loss: 0.7305 (0.7259)  time: 0.1897  data: 0.0001  max mem: 14938
[23:13:09.716135] Test:  [200/345]  eta: 0:00:27  loss: 0.7265 (0.7259)  time: 0.1901  data: 0.0001  max mem: 14938
[23:13:11.622880] Test:  [210/345]  eta: 0:00:25  loss: 0.7223 (0.7257)  time: 0.1905  data: 0.0001  max mem: 14938
[23:13:13.533025] Test:  [220/345]  eta: 0:00:23  loss: 0.7221 (0.7258)  time: 0.1908  data: 0.0001  max mem: 14938
[23:13:15.448062] Test:  [230/345]  eta: 0:00:21  loss: 0.7304 (0.7262)  time: 0.1912  data: 0.0001  max mem: 14938
[23:13:17.365712] Test:  [240/345]  eta: 0:00:19  loss: 0.7286 (0.7265)  time: 0.1916  data: 0.0001  max mem: 14938
[23:13:19.285639] Test:  [250/345]  eta: 0:00:17  loss: 0.7268 (0.7265)  time: 0.1918  data: 0.0001  max mem: 14938
[23:13:21.209620] Test:  [260/345]  eta: 0:00:16  loss: 0.7244 (0.7265)  time: 0.1921  data: 0.0001  max mem: 14938
[23:13:23.137910] Test:  [270/345]  eta: 0:00:14  loss: 0.7174 (0.7261)  time: 0.1926  data: 0.0001  max mem: 14938
[23:13:25.067843] Test:  [280/345]  eta: 0:00:12  loss: 0.7175 (0.7261)  time: 0.1929  data: 0.0001  max mem: 14938
[23:13:27.002270] Test:  [290/345]  eta: 0:00:10  loss: 0.7236 (0.7260)  time: 0.1932  data: 0.0001  max mem: 14938
[23:13:28.941011] Test:  [300/345]  eta: 0:00:08  loss: 0.7227 (0.7259)  time: 0.1936  data: 0.0001  max mem: 14938
[23:13:30.882386] Test:  [310/345]  eta: 0:00:06  loss: 0.7235 (0.7258)  time: 0.1940  data: 0.0001  max mem: 14938
[23:13:32.825498] Test:  [320/345]  eta: 0:00:04  loss: 0.7235 (0.7259)  time: 0.1942  data: 0.0001  max mem: 14938
[23:13:34.773460] Test:  [330/345]  eta: 0:00:02  loss: 0.7247 (0.7258)  time: 0.1945  data: 0.0001  max mem: 14938
[23:13:36.724166] Test:  [340/345]  eta: 0:00:00  loss: 0.7328 (0.7261)  time: 0.1949  data: 0.0001  max mem: 14938
[23:13:37.505609] Test:  [344/345]  eta: 0:00:00  loss: 0.7333 (0.7261)  time: 0.1950  data: 0.0001  max mem: 14938
[23:13:37.566545] Test: Total time: 0:01:05 (0.1901 s / it)
[23:13:48.837630] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8723 (0.8723)  time: 0.3238  data: 0.1448  max mem: 14938
[23:13:50.652447] Test:  [10/57]  eta: 0:00:09  loss: 0.8809 (0.8785)  time: 0.1944  data: 0.0132  max mem: 14938
[23:13:52.473315] Test:  [20/57]  eta: 0:00:06  loss: 0.8809 (0.8636)  time: 0.1817  data: 0.0001  max mem: 14938
[23:13:54.297533] Test:  [30/57]  eta: 0:00:05  loss: 0.7491 (0.8233)  time: 0.1822  data: 0.0001  max mem: 14938
[23:13:56.126748] Test:  [40/57]  eta: 0:00:03  loss: 0.7401 (0.8022)  time: 0.1826  data: 0.0001  max mem: 14938
[23:13:57.962084] Test:  [50/57]  eta: 0:00:01  loss: 0.7363 (0.7959)  time: 0.1832  data: 0.0001  max mem: 14938
[23:13:58.951736] Test:  [56/57]  eta: 0:00:00  loss: 0.7634 (0.8013)  time: 0.1778  data: 0.0001  max mem: 14938
[23:13:59.004712] Test: Total time: 0:00:10 (0.1841 s / it)
[23:14:00.985767] Dice score of the network on the train images: 0.791308, val images: 0.820387
[23:14:00.986005] saving best_dice_model_0 @ epoch 25
[23:14:02.081498] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:14:02.964994] Epoch: [26]  [  0/345]  eta: 0:05:04  lr: 0.000113  loss: 0.7309 (0.7309)  time: 0.8823  data: 0.1430  max mem: 14938
[23:14:17.821090] Epoch: [26]  [ 20/345]  eta: 0:04:03  lr: 0.000113  loss: 0.7578 (0.7572)  time: 0.7428  data: 0.0001  max mem: 14938
[23:14:32.726008] Epoch: [26]  [ 40/345]  eta: 0:03:47  lr: 0.000113  loss: 0.7471 (0.7548)  time: 0.7452  data: 0.0001  max mem: 14938
[23:14:47.672283] Epoch: [26]  [ 60/345]  eta: 0:03:32  lr: 0.000112  loss: 0.7620 (0.7571)  time: 0.7473  data: 0.0001  max mem: 14938
[23:15:02.643588] Epoch: [26]  [ 80/345]  eta: 0:03:18  lr: 0.000112  loss: 0.7465 (0.7559)  time: 0.7485  data: 0.0001  max mem: 14938
[23:15:17.642962] Epoch: [26]  [100/345]  eta: 0:03:03  lr: 0.000112  loss: 0.7524 (0.7551)  time: 0.7499  data: 0.0001  max mem: 14938
[23:15:32.650581] Epoch: [26]  [120/345]  eta: 0:02:48  lr: 0.000112  loss: 0.7602 (0.7568)  time: 0.7503  data: 0.0001  max mem: 14938
[23:15:47.647449] Epoch: [26]  [140/345]  eta: 0:02:33  lr: 0.000111  loss: 0.7579 (0.7569)  time: 0.7498  data: 0.0001  max mem: 14938
[23:16:02.636790] Epoch: [26]  [160/345]  eta: 0:02:18  lr: 0.000111  loss: 0.7556 (0.7572)  time: 0.7494  data: 0.0001  max mem: 14938
[23:16:17.608862] Epoch: [26]  [180/345]  eta: 0:02:03  lr: 0.000111  loss: 0.7503 (0.7566)  time: 0.7486  data: 0.0001  max mem: 14938
[23:16:32.577441] Epoch: [26]  [200/345]  eta: 0:01:48  lr: 0.000111  loss: 0.7486 (0.7561)  time: 0.7484  data: 0.0001  max mem: 14938
[23:16:47.538433] Epoch: [26]  [220/345]  eta: 0:01:33  lr: 0.000110  loss: 0.7516 (0.7556)  time: 0.7480  data: 0.0001  max mem: 14938
[23:17:02.502535] Epoch: [26]  [240/345]  eta: 0:01:18  lr: 0.000110  loss: 0.7528 (0.7555)  time: 0.7482  data: 0.0001  max mem: 14938
[23:17:17.461314] Epoch: [26]  [260/345]  eta: 0:01:03  lr: 0.000110  loss: 0.7449 (0.7549)  time: 0.7479  data: 0.0001  max mem: 14938
[23:17:32.413992] Epoch: [26]  [280/345]  eta: 0:00:48  lr: 0.000110  loss: 0.7456 (0.7544)  time: 0.7476  data: 0.0001  max mem: 14938
[23:17:47.367007] Epoch: [26]  [300/345]  eta: 0:00:33  lr: 0.000110  loss: 0.7481 (0.7545)  time: 0.7476  data: 0.0001  max mem: 14938
[23:18:02.317483] Epoch: [26]  [320/345]  eta: 0:00:18  lr: 0.000109  loss: 0.7480 (0.7540)  time: 0.7475  data: 0.0001  max mem: 14938
[23:18:17.350054] Epoch: [26]  [340/345]  eta: 0:00:03  lr: 0.000109  loss: 0.7482 (0.7534)  time: 0.7516  data: 0.0001  max mem: 14938
[23:18:20.340020] Epoch: [26]  [344/345]  eta: 0:00:00  lr: 0.000109  loss: 0.7412 (0.7532)  time: 0.7516  data: 0.0001  max mem: 14938
[23:18:20.407454] Epoch: [26] Total time: 0:04:18 (0.7488 s / it)
[23:18:20.407669] Averaged stats: lr: 0.000109  loss: 0.7412 (0.7532)
[23:18:20.747151] Test:  [  0/345]  eta: 0:01:56  loss: 0.7156 (0.7156)  time: 0.3363  data: 0.1541  max mem: 14938
[23:18:22.583684] Test:  [ 10/345]  eta: 0:01:06  loss: 0.7232 (0.7256)  time: 0.1975  data: 0.0141  max mem: 14938
[23:18:24.423258] Test:  [ 20/345]  eta: 0:01:02  loss: 0.7232 (0.7257)  time: 0.1838  data: 0.0001  max mem: 14938
[23:18:26.266833] Test:  [ 30/345]  eta: 0:00:59  loss: 0.7230 (0.7236)  time: 0.1841  data: 0.0001  max mem: 14938
[23:18:28.113047] Test:  [ 40/345]  eta: 0:00:57  loss: 0.7175 (0.7213)  time: 0.1844  data: 0.0001  max mem: 14938
[23:18:29.963511] Test:  [ 50/345]  eta: 0:00:55  loss: 0.7193 (0.7229)  time: 0.1848  data: 0.0001  max mem: 14938
[23:18:31.817308] Test:  [ 60/345]  eta: 0:00:53  loss: 0.7239 (0.7223)  time: 0.1852  data: 0.0001  max mem: 14938
[23:18:33.673316] Test:  [ 70/345]  eta: 0:00:51  loss: 0.7224 (0.7226)  time: 0.1854  data: 0.0001  max mem: 14938
[23:18:35.534439] Test:  [ 80/345]  eta: 0:00:49  loss: 0.7261 (0.7228)  time: 0.1858  data: 0.0001  max mem: 14938
[23:18:37.398234] Test:  [ 90/345]  eta: 0:00:47  loss: 0.7193 (0.7223)  time: 0.1862  data: 0.0001  max mem: 14938
[23:18:39.265808] Test:  [100/345]  eta: 0:00:45  loss: 0.7224 (0.7231)  time: 0.1865  data: 0.0001  max mem: 14938
[23:18:41.134733] Test:  [110/345]  eta: 0:00:43  loss: 0.7242 (0.7227)  time: 0.1868  data: 0.0001  max mem: 14938
[23:18:43.009139] Test:  [120/345]  eta: 0:00:42  loss: 0.7210 (0.7225)  time: 0.1871  data: 0.0001  max mem: 14938
[23:18:44.886036] Test:  [130/345]  eta: 0:00:40  loss: 0.7210 (0.7227)  time: 0.1875  data: 0.0001  max mem: 14938
[23:18:46.768175] Test:  [140/345]  eta: 0:00:38  loss: 0.7169 (0.7225)  time: 0.1879  data: 0.0001  max mem: 14938
[23:18:48.653787] Test:  [150/345]  eta: 0:00:36  loss: 0.7187 (0.7228)  time: 0.1883  data: 0.0001  max mem: 14938
[23:18:50.541683] Test:  [160/345]  eta: 0:00:34  loss: 0.7156 (0.7220)  time: 0.1886  data: 0.0001  max mem: 14938
[23:18:52.433623] Test:  [170/345]  eta: 0:00:32  loss: 0.7151 (0.7217)  time: 0.1889  data: 0.0001  max mem: 14938
[23:18:54.331288] Test:  [180/345]  eta: 0:00:30  loss: 0.7178 (0.7220)  time: 0.1894  data: 0.0001  max mem: 14938
[23:18:56.229766] Test:  [190/345]  eta: 0:00:29  loss: 0.7161 (0.7218)  time: 0.1898  data: 0.0001  max mem: 14938
[23:18:58.131773] Test:  [200/345]  eta: 0:00:27  loss: 0.7171 (0.7216)  time: 0.1900  data: 0.0001  max mem: 14938
[23:19:00.036935] Test:  [210/345]  eta: 0:00:25  loss: 0.7177 (0.7218)  time: 0.1903  data: 0.0001  max mem: 14938
[23:19:01.946824] Test:  [220/345]  eta: 0:00:23  loss: 0.7207 (0.7219)  time: 0.1907  data: 0.0001  max mem: 14938
[23:19:03.858693] Test:  [230/345]  eta: 0:00:21  loss: 0.7189 (0.7217)  time: 0.1910  data: 0.0001  max mem: 14938
[23:19:05.775000] Test:  [240/345]  eta: 0:00:19  loss: 0.7172 (0.7216)  time: 0.1914  data: 0.0001  max mem: 14938
[23:19:07.695998] Test:  [250/345]  eta: 0:00:17  loss: 0.7173 (0.7217)  time: 0.1918  data: 0.0001  max mem: 14938
[23:19:09.618826] Test:  [260/345]  eta: 0:00:16  loss: 0.7177 (0.7217)  time: 0.1921  data: 0.0001  max mem: 14938
[23:19:11.547408] Test:  [270/345]  eta: 0:00:14  loss: 0.7208 (0.7218)  time: 0.1925  data: 0.0001  max mem: 14938
[23:19:13.478217] Test:  [280/345]  eta: 0:00:12  loss: 0.7226 (0.7218)  time: 0.1929  data: 0.0001  max mem: 14938
[23:19:15.411764] Test:  [290/345]  eta: 0:00:10  loss: 0.7205 (0.7218)  time: 0.1932  data: 0.0001  max mem: 14938
[23:19:17.350311] Test:  [300/345]  eta: 0:00:08  loss: 0.7211 (0.7220)  time: 0.1936  data: 0.0001  max mem: 14938
[23:19:19.290801] Test:  [310/345]  eta: 0:00:06  loss: 0.7227 (0.7221)  time: 0.1939  data: 0.0001  max mem: 14938
[23:19:21.234882] Test:  [320/345]  eta: 0:00:04  loss: 0.7204 (0.7221)  time: 0.1942  data: 0.0001  max mem: 14938
[23:19:23.180453] Test:  [330/345]  eta: 0:00:02  loss: 0.7182 (0.7219)  time: 0.1944  data: 0.0001  max mem: 14938
[23:19:25.131027] Test:  [340/345]  eta: 0:00:00  loss: 0.7157 (0.7219)  time: 0.1948  data: 0.0001  max mem: 14938
[23:19:25.912608] Test:  [344/345]  eta: 0:00:00  loss: 0.7168 (0.7218)  time: 0.1949  data: 0.0001  max mem: 14938
[23:19:25.971952] Test: Total time: 0:01:05 (0.1900 s / it)
[23:19:37.251812] Test:  [ 0/57]  eta: 0:00:18  loss: 0.8566 (0.8566)  time: 0.3234  data: 0.1439  max mem: 14938
[23:19:39.067504] Test:  [10/57]  eta: 0:00:09  loss: 0.8863 (0.8953)  time: 0.1944  data: 0.0131  max mem: 14938
[23:19:40.887399] Test:  [20/57]  eta: 0:00:06  loss: 0.8938 (0.8821)  time: 0.1817  data: 0.0001  max mem: 14938
[23:19:42.711534] Test:  [30/57]  eta: 0:00:05  loss: 0.7868 (0.8439)  time: 0.1822  data: 0.0001  max mem: 14938
[23:19:44.540235] Test:  [40/57]  eta: 0:00:03  loss: 0.7589 (0.8239)  time: 0.1826  data: 0.0001  max mem: 14938
[23:19:46.375096] Test:  [50/57]  eta: 0:00:01  loss: 0.7587 (0.8178)  time: 0.1831  data: 0.0001  max mem: 14938
[23:19:47.365323] Test:  [56/57]  eta: 0:00:00  loss: 0.7675 (0.8227)  time: 0.1778  data: 0.0001  max mem: 14938
[23:19:47.424931] Test: Total time: 0:00:10 (0.1842 s / it)
[23:19:49.317464] Dice score of the network on the train images: 0.820453, val images: 0.809155
[23:19:49.317702] saving best_prec_model_0 @ epoch 26
[23:19:50.351864] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[23:19:51.243960] Epoch: [27]  [  0/345]  eta: 0:05:07  lr: 0.000109  loss: 0.7659 (0.7659)  time: 0.8910  data: 0.1484  max mem: 14938
[23:20:06.119418] Epoch: [27]  [ 20/345]  eta: 0:04:03  lr: 0.000109  loss: 0.7432 (0.7473)  time: 0.7437  data: 0.0001  max mem: 14938
[23:20:21.022014] Epoch: [27]  [ 40/345]  eta: 0:03:48  lr: 0.000108  loss: 0.7445 (0.7467)  time: 0.7451  data: 0.0001  max mem: 14938
[23:20:35.979311] Epoch: [27]  [ 60/345]  eta: 0:03:33  lr: 0.000108  loss: 0.7518 (0.7488)  time: 0.7478  data: 0.0001  max mem: 14938
[23:20:50.970515] Epoch: [27]  [ 80/345]  eta: 0:03:18  lr: 0.000108  loss: 0.7511 (0.7500)  time: 0.7495  data: 0.0001  max mem: 14938
[23:21:05.996627] Epoch: [27]  [100/345]  eta: 0:03:03  lr: 0.000108  loss: 0.7444 (0.7496)  time: 0.7513  data: 0.0001  max mem: 14938
[23:21:21.022875] Epoch: [27]  [120/345]  eta: 0:02:48  lr: 0.000107  loss: 0.7400 (0.7489)  time: 0.7513  data: 0.0001  max mem: 14938
[23:21:36.031805] Epoch: [27]  [140/345]  eta: 0:02:33  lr: 0.000107  loss: 0.7502 (0.7491)  time: 0.7504  data: 0.0001  max mem: 14938
[23:21:51.033284] Epoch: [27]  [160/345]  eta: 0:02:18  lr: 0.000107  loss: 0.7577 (0.7502)  time: 0.7500  data: 0.0001  max mem: 14938
[23:21:53.465636] NaN detected in inputs
[23:21:53.469953] Loss is nan, stopping training
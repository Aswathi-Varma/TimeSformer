Not using distributed mode
[15:31:51.586160] job dir: /root/seg_framework/MS-Mamba/run_scripts
[15:31:51.586303] Namespace(accum_iter=1,
model='SegFormer3D',
in_channels=1,
lr=None,
blr=0.001,
min_lr=0,
dist_on_itp=False,
warmup_epochs=20,
device='cuda:0',
seed=42,
layer_decay=0.75,
clip_grad=None,
num_workers=8,
pin_mem=True,
resume='',
world_size=1,
embed_dim=1,
local_rank=-1,
dist_url='env://',
nb_classes=2,
data_dir='/root/MSLesSeg24/data',
datalist=None,
modalities='T1,T2,FLAIR,T1ce',
preprocess=False,
distributed=False)
[15:31:51.586414] device  cuda:0
[15:31:52.235557] number of params: 16650433
[15:31:52.235874] model: SpatioTempMS(
  (encoder): ST_Block(
    (downsample_layers): MixVisionTransformer(
      (embeds): ModuleList(
        (0): PatchEmbedding(
          (patch_embeddings): Conv3d(3, 64, kernel_size=(7, 7, 7), stride=(4, 4, 4), padding=(3, 3, 3))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (1): PatchEmbedding(
          (patch_embeddings): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): PatchEmbedding(
          (patch_embeddings): Conv3d(128, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (3): PatchEmbedding(
          (patch_embeddings): Conv3d(320, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (blocks): ModuleList(
        (0): ModuleList(
          (0-1): 2 x TransformerBlock(
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (attention): SelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key_value): Linear(in_features=64, out_features=128, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=64, out_features=64, bias=True)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (sr): Conv3d(64, 64, kernel_size=(4, 4, 4), stride=(4, 4, 4))
              (sr_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): _MLP(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256)
                (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (act_fn): GELU(approximate='none')
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleList(
          (0-1): 2 x TransformerBlock(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): SelfAttention(
              (query): Linear(in_features=128, out_features=128, bias=True)
              (key_value): Linear(in_features=128, out_features=256, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (sr): Conv3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
              (sr_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): _MLP(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512)
                (bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (act_fn): GELU(approximate='none')
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleList(
          (0-1): 2 x TransformerBlock(
            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (attention): SelfAttention(
              (query): Linear(in_features=320, out_features=320, bias=True)
              (key_value): Linear(in_features=320, out_features=640, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=320, out_features=320, bias=True)
              (proj_dropout): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
            (mlp): _MLP(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(1280, 1280, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1280)
                (bn): BatchNorm3d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (act_fn): GELU(approximate='none')
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleList(
          (0-1): 2 x TransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): SelfAttention(
              (query): Linear(in_features=512, out_features=512, bias=True)
              (key_value): Linear(in_features=512, out_features=1024, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_dropout): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): _MLP(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv3d(2048, 2048, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2048)
                (bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (act_fn): GELU(approximate='none')
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (norms): ModuleList(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): SegFormerDecoderHead(
    (linear_c): ModuleList(
      (0): MLP_(
        (proj): Linear(in_features=512, out_features=256, bias=True)
        (bn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (1): MLP_(
        (proj): Linear(in_features=320, out_features=256, bias=True)
        (bn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (2): MLP_(
        (proj): Linear(in_features=128, out_features=256, bias=True)
        (bn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (3): MLP_(
        (proj): Linear(in_features=64, out_features=256, bias=True)
        (bn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (linear_fuse): Sequential(
      (0): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pred): Conv3d(256, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (upsample_volume): Upsample(scale_factor=4.0, mode='trilinear')
  )
)
[15:31:52.236752] base lr: 1.00e-03
[15:31:52.236826] actual lr: 7.81e-06
[15:31:52.236881] accumulate grad iterations: 1
[15:31:52.236932] effective batch size: 2
[15:31:52.237467] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 7.8125e-06
    maximize: False
    weight_decay: 0.01
)
[15:31:52.239500] Start training for 1 epochs
[15:31:52.240270] log_dir: /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/train_ft
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /root/seg_framework/MS-Mamba/output_dir/mslesseg/val_ft
[15:32:15.249354] [15:32:15.249711] [15:32:15.249783] [15:32:15.249849] [15:32:15.249910] [15:32:15.249967] [15:32:15.250023] [15:32:15.250092] [15:32:15.250150] [15:32:15.250205] [15:32:15.250260] [15:32:15.250314] [15:32:15.250368] [15:32:15.250422] [15:32:15.250477] [15:32:15.250532] [15:32:15.250586] [15:32:15.250640] [15:32:15.250693] [15:32:15.250750] [15:32:15.250811]
Traceback (most recent call last):
  File "/root/anaconda3/envs/vivim/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/anaconda3/envs/vivim/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/seg_framework/MS-Mamba/run_scripts/train_mslesseg.py", line 436, in <module>
    main(args)
  File "/root/seg_framework/MS-Mamba/run_scripts/train_mslesseg.py", line 339, in main
    train_stats = train_one_epoch(
  File "/root/seg_framework/MS-Mamba/run_scripts/train_mslesseg.py", line 123, in train_one_epoch
    outputs = model(samples)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/seg_framework/MS-Mamba/model/SpatioTempMS.py", line 294, in forward
    outs = self.encoder(x_in)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/seg_framework/MS-Mamba/model/SpatioTempMS.py", line 241, in forward
    x = self.forward_features(x)
  File "/root/seg_framework/MS-Mamba/model/SpatioTempMS.py", line 220, in forward_features
    hs, height, width, depth = self.downsample_layers.embeds[i](hs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/seg_framework/MS-Mamba/model/SegFormer3D.py", line 150, in forward
    patches = self.patch_embeddings(x)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 610, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/root/anaconda3/envs/vivim/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 605, in _conv_forward
    return F.conv3d(
RuntimeError: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [2, 64, 2, 44, 56, 44]